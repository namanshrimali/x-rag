,question,answer,group_id
0,In what platform do the key nn modules run?,FP32,11167
1,What module implements versions of the key nn modules Conv2d() and Linear() which run in FP32 but with rounding?,torch.nn.quantized,7751
2,What module implements versions of the key nn modules Conv2d() and Linear()?,torch.nn.quantized,7751
3,What converts the weights to int8 and replaces the operations with their quantized counterparts?,convert(),7751
4,What module implements the quantized implementations of fused operations like conv + relu?,torch.nn.intrinsic.quantized,7751
5,What is needed for quantization aware training?,fused operations,7754
6,Conv2d and Linear() use rounding to simulate the effect of what?,INT8 quantization,7754
7,In what platform do the modules Conv2d() and Linear() run?,FP32,7754
8,What are fused operations needed for?,quantization aware training,7754
9,In what platform do Conv2d() and Linear() run?,FP32,7753
10,What are some of the fused operations implemented by this module?,conv + relu,7753
11,What does torch.nn.quantized implement?,nn layers,7753
12,What does torch.nn.quantized.dynamic implement?,"Dynamically quantized Linear, LSTM, LSTMCell, GRUCell, and RNNCell",7753
13,What are the quantized implementations of fused operations?,conv + relu,7753
14,What quantization does the FP32 version of Conv2d and Linear() simulate?,INT8,7753
15,What does torch.nn.quantized implement the quantized versions of?,nn layers,11169
16,What is the name of the module that implements the quantized versions of the nn layers?,torch.nn.quantized,11169
17,What are some fused operations implemented by torch.nn.intrinsic.quantized?,conv + relu,11169
18,What quantization does the rounding of the nn modules simulate?,INT8,11169
19,What module implements quantized versions of the nn layers?,torch.nn.quantized.dynamic,11169
20,What is one of the quantized versions of the nn layers?,torch.nn.Conv2d,11206
21,What quantization does the rounding of the key nn modules simulate?,INT8,11206
22,What is another name for GRUCell?,RNNCell,11206
23,What are some of the fused operations implemented by torch.nn.intrinsic.quantized?,conv + relu,11206
24,What is one of the quantized versions of nn layers?,RNNCell,11164
25,What does torch.is_grad_enabled return if grad mode is currently enabled?,True,11081
26,What is used to fake quantize a tensor?,"scale, zero_point, quant_min and quant_max",5379
27,What is the input value in torch.fake_quantize_per_tensor_affine?,torch.float32,11054
28,What is quant_min in torch.fake_quantize_per_tensor_affine?,the lower bound of quantized domain,11054
29,What does torch.fake_quantize_per_tensor_affine return?,fake_quantized tensor,11054
30,What is zero_point in torch.fake_quantize_per_tensor_affine?,quantization zero_point,11054
31,What does Bernoulli(p) do?,Tensor.bernoulli_ fills each location of self with an independent sample from Bernoulli(p),6269
32,What type of dtype can self have?,integral dtype,6270
33,If input to Tensor.bernoulli_ is a tensor then what is the value of ith element,It is set to a value sampled from Bernoulli(p_tensor[i]),6270
34,If input to Tensor.bernoulli_ is a tensor then what is dtype of p?,floating point dtype,6270
35,What should p be?,scalar or tensor,6270
36,What should either be a scalar or tensor containing probabilities to be used for drawing the binary random number?,p,10364
37,Which matrices must be 3-D tensors each containing the same number of matrices as input to torch.baddbmm?,batch1 and batch2,11010
38,What is added to the final result of torch.baddbmm?,input,11010
39,What is the shape of batch1 temsor in torch.baddbmm?, (bnm)(b times n times m),3813
40,What are same in torch.baddbmm as the scaling factors used in torch.addbmm()?,alpha and beta,3815
41,"What will not be propagated, if beta is zero in torch.baddbmm?",nan and inf in batch1 and batch2 tensor,3815
42,When will input will be ignored and nan and inf in it will not be propagated in torch.baddbmm?,"If beta is 0,",3817
43,"For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be what in torch.baddbmm?",real numbers,3814
44,Which operator supports TensorFloat32?,torch.baddbmm,3816
45,"What will be ignored if beta is 0 , in torch.baddbmm?",input,3816
46,Which operator supports TensorFloat32?,torch.baddbmm,3816
47,What is the first batch to be multiplied in torch.baddbmm?,batch1,2544
48,Does torch.all test if all elements in input evaluate to True?,Yes,10984
49,torch.all matches the behavior of what function in returning output of dtype bool?,NumPy,4516
50,For what dtype is the dtype of output uint8 itself in torch.all?,uint8,10982
51,torch.all matches the behaviour of what function in returning output of dtype bool for all supported dtypes except uint8?,NumPy,10982
52,"In torch.all for each row of input in the dimension dim, what returns if all elements in the row evaluate to True?",True,10985
53,"In torch.all what is the output size, if keepdim is True?",same size as input,2316
54,"In torch.all if keepdim is True, what is an exception of output tensor size?",the output tensor is of the same size as input except in the dimension dim where it is of size 1.,3810
55,"In torch.all if keepdim is false, the output tensor has how many dimensions less than input?",1,3810
56,"In torch.all if keepdim is false, how is the output tensor size reduced?",by using torch.squeeze,3810
57,"In torch.all if keepdim is True, what is of the same size as input?",the output tensor,3812
58,"In torch.all if keepdim is True, the output tensor has how much less dimension than input?",1,3812
59,In torch.all what does dim (int) represent?,the dimension to reduce,3811
60,"In torch.all for each row of input in a given dimension dim, what returns if all elements in the row evaluate to True?",True,3811
61,What is the value of a one-dimensional tensor of size steps whose values are evenly spaced from start to end?,Warning Not providing a value for steps is deprecated,2011
62,What type of tensor of size steps does PyTorch create?,one-dimensional,2011
63,For what reason will not providing a value for steps create a tensor with 100 elements?,backwards compatibility,2011
64,"For backwards compatibility, not providing a value for steps will create a tensor with how many elements?",100 elements,2013
65,"In a future PyTorch release, failing to provide a value for steps will throw what?",runtime error,2013
66,Is not providing a value for steps reflected in the documented function signature?,not,8171
67,"For backwards compatibility, not providing a value for steps will create what with 100 elements?",tensor,8171
68,Why is not providing a value for steps deprecated?,not reflected in the documented function signature,4346
69,What type of dtype is used when both start and end are real?,Default,4346
70,"For backwards compatibility, not providing a value for steps will what?",create a tensor with 100 elements,4346
71,What is the starting value for the set of points end?,start,4346
72,Is this behavior reflected in the documented function signature?,not,4345
73,"For backwards compatibility, not providing a value for steps will create what?",a tensor with 100 elements,2012
74,What is the starting value for the set of points end (float)?,start (float),2012
75,What does if None use when both start and end are real?,global default dtype,2012
76,What is the size of the constructed tensor out?,the output tensor,10750
77,What default uses the global default dtype when both start and end are real?,if None,10750
78,What uses the global default dtype when both start and end are real?,if None,10749
79,What is start (float)?,the starting value for the set of points end,10749
80,What does torch.get_default_dtype() use when both start and end are real?,global default dtype,10749
81,"dtype (torch.dpython:type, optional) – what to perform the computation in?",the data type,9206
82,What is the ending value for the set of points steps?,end,9263
83,What does torch.autograd implement?,automatic differentiation of arbitrary scalar valued functions,10992
84,What keyword is used to declare Tensor s for which gradients should be computed?,requires_grad=True,10992
85,What does autograd compute?,the sum of gradients of given tensors with respect to graph leaves,10992
86,What type of tensor types does autograd only support?,floating point Tensor types,10992
87,What does autograd compute and return?,the sum of gradients of outputs with respect to the inputs,10992
88,What does torch.autograd only support for floating point Tensor types?,autograd,10993
89,What function computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs,function,10993
90,What can you use to capture arguments that are not Tensors or Tensors that don't have requires_grad set?,lambda,10993
91,Computes the sum of gradients of given tensors with respect to what?,graph leaves,1807
92,Computes and returns what?,the sum of gradients of outputs with respect to the inputs,1807
93,"As of now, we only support autograd for what?",floating point Tensor types,10991
94,Computes and returns the sum of gradients of outputs with respect to what?,inputs,1805
95,Warning This API is in what state?,beta,8192
96,What beta is this API in?,beta,8192
97,What is planned before we consider this stable?,major improvements to performances,8192
98,"What does this section contain for the autograd that builds on the basic API above and allows you to compute jacobians, hessians",higher level API,8192
99,What computes the Jacobian of a given function?,functional.jacobian Function,8192
100,What computes the dot product between a vector v and the Jacobian of a given function at the point given by the inputs,functional.vjp Function,8192
101,What computes the dot product between the Hessian of a given scalar function and a vector v at the point given,functional.hvp Function,8192
102,What is very unlikely to change in this API?,function signatures,8192
103,"What does f(input, constant, flag=flag) contain?",boolean flag,8192
104,What do user-provided functions take as input and return only Tensors?,only Tensors,8192
105,What is unlikely to change in the beta version of the autograd API?,function signatures,7453
106,What does this section contain for the autograd?,higher level API,7453
107,What is the higher level API for?,autograd,7794
108,What does the higher level API for the autograd allow you to compute?,"jacobians, hessians",1806
109,What is the function that returns the sum of gradients of outputs with respect to the inputs?,Computes,1652
110,What is computed and returned with respect to the inputs?,the sum of gradients of outputs,1652
111,What is the current state of the autograd API?,beta,7454
112,What level of API does this section contain?,higher level,7793
113,What does the higher level API for autograd allow you to compute?,"jacobians, hessians",7793
114,What does a functional.jacobian Function compute?,Jacobian,9423
115,What does a functional.hessian Function compute of a given scalar function?,Hessian,9423
116,What does a functional.vjp Function compute?,the dot product between a vector v and the Jacobian of the given function at the point given by the inputs,9421
117,What does functional.vjp compute between a vector v and the Jacobian of the given function at the point given by the inputs,the dot product,9421
118,What computes the dot product between the Jacobian of a given function at the point given by the inputs and a vector v,functional.jvp Function,2629
119,What does a function compute of a given scalar function?,Hessian,2629
120,What computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs?,functional.vjp Function,9416
121,What function computes the dot product between the Jacobian of a given function at the point given by the inputs and a vector,functional.jvp,2632
122,Function that computes what of a given function. functional.hessian Function that computes the Hessian of a given scal,Jacobian,2632
123,Who disabled gradient calculation?,Context-manager,2632
124,What does a function compute of a given function?,Jacobian,2633
125,Function that computes the what of a given function. functional.hessian Function that computes the Hessian of a given s,Jacobian,2630
126,Function that computes the Hessian of a given what?,scalar function,2630
127,Where does functional.vjp compute the dot product between a vector v and the Jacobian of a given function?,the point given by the inputs,2630
128,What function computes the Hessian of a given scalar function?,functional.hessian,7457
129,What function computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v,functional.jvp,7457
130,What function computes the dot product between the Hessian of a given scalar function and a vector v at the point,functional.hvp,7457
131,What is more information on the differences between no-grad and inference mode?,Locally disabling gradient computation,7457
132,What does this API work with?,user-provided functions,7457
133,What is a functional.hessian Function that computes the Hessian of?,scalar function,9415
134,Functional.vjp Function that computes the dot product between a vector v and what?,the Jacobian,9415
135,Function that computes what of a given scalar function?,Hessian,2628
136,Functional.vjp Function that computes the dot product between a vector v and what of the given function at the point given by the,the Jacobian,2627
137,What function computes the dot product between a vector v and the Hessian of a given scalar function at the point,functional.vhp,2638
138,Function that computes the dot product between a vector v and what of the given function at the point given by the inputs?,the Jacobian,2638
139,What is the name of the function that computes the dot product between a vector v and the Hessian of a given s,functional.hvp,9430
140,Where does a function compute the dot product between a vector v and the Jacobian of a given function?,the point given by the inputs,2637
141,What does a functional.hvp Function compute the dot product between?,the Hessian of a given scalar function and a vector v at the point given by the inputs,9425
142,What computes the dot product between a vector v and the Hessian of a given scalar function at the point given,functional.vhp Function,9428
143,What is the dot product between a given scalar function and a vector v?,Hessian,9428
144,What replaces.grad with a new tensor.grad + new grad?,backward(),9428
145,How many.grads are resetting before each accumulation phase?,None,9428
146,What is the dot product between a given function at the point given by the inputs and a vector v?,Jacobian,2634
147,What is another name for the difference between no-grad and inference mode?,Locally disabling gradient computation,9426
148,What is the function that computes the dot product between a vector v and the Hessian of a given scalar?,function,2636
149,What is the function that computes the dot product between a vector v and the Hessian of a given scalar function,computes the dot product between a vector v and the Hessian of a given scalar function,2636
150,"If param.grad is initially None, what is accumulated as follows?",If param.grad is initially None,2636
151,Where is the dot product between a vector v and the Hessian of a given scalar function computed?,the point given by the inputs,2635
152,What is the difference between no-grad and inference mode?,Locally disabling gradient computation,5762
153,What is the name of the entity that disables gradient calculation?,Context-manager,5762
154,What does a context-manager set gradient calculation to?,on or off,5762
155,What does context-manager do?,disabled gradient calculation,9418
156,What does functional.hvp function do?,computes the dot product between the Hessian of a given scalar function and a vector v,9417
157,What is the name of the program that disables gradient calculation?,Context-manager,9417
158,What is a function that computes the dot product between the Hessian of a given scalar function and a vector,computes the dot product between the Hessian of a given scalar function and a vector v,2639
159,"If param already has a non-sparse attribute, what attribute is created?",.grad attribute,2639
160,What is the name of the function that disables gradient calculation?,Context-manager,2639
161,When is param.grad initially None?,If param’s memory is non-overlapping and dense,2639
162,What mode does a context-manager enable or disable?,inference mode,7010
163,"What is the name of the context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_",Locally disabling gradient computation,7010
164,What does a context-manager enable or disable when a non-sparse param receives a non-sparse,inference mode,1869
165,What is the initial value of param.grad?,None,9427
166,What is the name of the entity that disabled gradient calculation?,Context-manager,5763
167,"If param.grad is initially None, what happens to param.grad?",If param.grad is initially None,5763
168,What accumulates into.grad in-place if create_graph=False?,backward(),5763
169,"If param's memory is non-overlapping and dense, what is.grad created with?",rowmajor-contiguous strides,5763
170,What does a Context-manager that disables gradient calculation do?,enables gradient calculation,1868
171,If param.grad is initially what?,None,1868
172,When does a non-sparse param receive a non-sparse gradient?,torch.autograd.backward() or torch.Tensor.backward(),8343
173,What does context-manager enable?,gradient calculation,1875
174,"If param's memory is non-overlapping and dense,.grad is created with what?",strides matching param,3471
175,"If param's memory is dense and non-overlapping, what strides are used to create.grad?",rowmajor-contiguous,3471
176,What makes autograd very efficient?,Autograd’s aggressive buffer freeing and reuse,3471
177,Context-manager that sets gradient calculation to what?,on or off,1873
178,What does context-manager enable or disable when a non-sparse param receives a non-sparse gradient?,inference mode,1874
179,What is initial None if a non-sparse param receives a non-sparse gradient?,param,1884
180,"If param.grad is initially None,.grad is created with what strides?",rowmajor-contiguous,3469
181,"If param’s memory is non-overlapping and dense, what is.grad created with?",rowmajor-contiguous strides,1878
182,"If param.grad is initially None,.grad is created with strides matching param (thus matching param’s layout)?",param’s memory is non-overlapping and dense,8342
183,What type of attribute does param already have?,non-sparse,8342
184,"If param.grad is initially None, what strides are created with?",rowmajor-contiguous,8342
185,What happens when a non-sparse param receives a non-sparse gradient?,If param.grad is initially None,8342
186,"If param.grad is initially None, what is.grad created with?",rowmajor-contiguous strides,8344
187,"If param.grad is initially None,.grad is created with strides matching param?",param’s memory is non-overlapping and dense,8344
188,What does backward() replace.grad with?,tensor,8344
189,What is the default behavior of letting.grads be before the first backward()?,None,8344
190,What will not affect.grad layouts?,Calls to model.zero_grad() or optimizer.zero_grad(),8344
191,Resetting all.grads to None before each accumulation phase is a valid alternative to what?,model.zero_grad() or optimizer.zero_grad(),8344
192,What is.grad created with?,rowmajor-contiguous strides,3468
193,"If param.grad is initially None, what causes.grad to be created with strides matching param?",If param’s memory is non-overlapping and dense,3468
194,"If create_graph=False, backward() accumulates into what?",in-place,2640
195,What is a valid alternative to model.zero_grad() or optimizer.zero_grad()?,None,2640
196,What is the name of the function that computes the dot product between the Hessian of a given scalar function and a,Locally disabling gradient computation,2640
197,"If the memory of what is dense and non-overlapping,.grad is created with strides matching param?",param,3470
198,"If param already has a non-sparse.grad attribute, backward() accumulates into.grad in-place,",rowmajor-contiguous,4694
199,What already has a non-sparse.grad attribute?,param,3467
200,Where does backward() accumulate into.grad?,in-place,3405
201,What does backward() replace with a new tensor.grad + new grad?,.grad,3406
202,What is the default behavior for letting.grads be None before the first backward()?,their layout is created according to 1 or 2,3406
203,Which calls will not affect.grad layouts?,model.zero_grad() or optimizer.zero_grad(),3406
204,What are two alternatives to model.zero_grad()?,model.zero_grad() or optimizer.zero_grad(),10779
205,What type of tensor does param.grad have?,zeroed,10779
206,What is a valid alternative to that may improve performance for some networks?,model.zero_grad() or optimizer.zero_grad(),10779
207,Set param.grad = a zeroed tensor with desired strides before the first backward() and never reset it to what,None,10779
208,How many guarantees your layout is preserved as long as create_graph=False?,3,3569
209,What indicates your layout is likely preserved even if create_graph=True?,4,3569
210,What guarantees your layout is preserved as long as create_graph=False?,3,3569
211,What is a zeroed tensor with desired strides before the first backward()?,param.grad,3569
212,"If you need manual control over.grad's strides, assign param.grad = a zeroed tensor with desired",None,3569
213,What is a hard matter in autograd?,Supporting in-place operations in autograd,6103
214,What is a hard matter to support in autograd?,in-place operations,6103
215,What makes Autograd very efficient?,aggressive buffer freeing and reuse,6103
216,Under what conditions might you never need to use in-place operations?,heavy memory pressure,6103
217,"Unless you are operating under heavy memory pressure, you might never need to use in-place operations?",under heavy memory pressure,6102
218,How much do in-place operations lower?,memory usage,6102
219,"If you’re operating under what type of pressure, you might never need to use in-place operations?",heavy memory pressure,6102
220,"What happens if a tensor was saved for backward in one of the functions, but it was modified in-place afterward?",an error will be raised once backward pass is started,1065
221,"If you're using in-place functions and not seeing errors, what can you be sure is correct?",computed gradients,1065
222,"If a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, what will be raised",an error,1065
223,What are correct if you're using in-place functions and not seeing any errors?,computed gradients,1065
224,Why is the Variable API deprecated?,Variables are no longer necessary to use autograd with tensors,8182
225,Autograd automatically supports Tensors with what setting set to True?,requires_grad,8182
226,What is the same thing as tensor.data?,var.data,8182
227,What is no longer necessary to use autograd with tensors?,Variables,8182
228,"What still work as expected, but return Tensors instead of Variables?","Variable(tensor) and Variable(tensor, requires_grad)",8182
229,What methods now work on tensors with the same method names?,"var.backward(), var.detach(), var.register_hook()",8182
230,What are some of the factory methods used to create tensors with requires_grad=True?,"torch.randn(), torch.zeros(), torch.ones()",8182
231,"What do Variable(tensor, requires_grad) return instead of Variables?",Tensors,6954
232,What API has been deprecated?,Variable API,6954
233,Autograd automatically supports Tensors with requires_grad set to what?,True,8181
234,"Which methods still work as expected, but return Tensors instead of Variables?","Variable(tensor) and Variable(tensor, requires_grad)",8128
235,What can one now create tensors with?,requires_grad=True,8128
236,"What do Variable(tensor) and Variable(tensor, requires_grad) do instead of Variables?",return Tensors,8128
237,"What does torch.randn((2, 3, 4), requires_grad=True) call?",autograd_tensor,8128
238,What is the default value of torch.Tensor.grad?,None,10962
239,What is true if gradients need to be computed for this Tensor?,False,10962
240,What are all Tensors that have requires_grad which is False?,leaf Tensors,10962
241,What does torch.Tensor.backward compute the gradient of current tensor?,w.r.t,10962
242,What is the Tensor that is detached from the graph that created it?,a leaf,10962
243,Where is the new Tensor?,detached from the current graph,7466
244,How is a new Tensor created?,detached from the current graph,10973
245,What is False in a Tensor?,requires_grad,10973
246,Where is the new Tensor detached from?,current graph,10973
247,What does torch.Tensor.detach_ Detach the Tensor from the graph that created it into?,leaf,10973
248,What type of hook does torch.Tensor.register_hook(hook) register?,backward,10973
249,What does torch.Tensor do?,register_hook,10977
250,Is True if gradients need to be computed for this Tensor?,False,3869
251,What does torch.Tensor.register_hook register?,backward hook,3869
252,What does the autograd class define formulas for?,differentiating ops,5108
253,What is the Note on how to use this class?,extending the autograd engine,5109
254,Every operation performed on what creates a new function object?,Tensor s,5109
255,What does torch.autograd define formulas for?,differentiating ops,5109
256,What are the edges of the DAG of functions denoting?,data dependencies,5109
257,How is the graph processed when backward is called?,"by calling backward() methods of each Function object, and passing returned gradients on to next Function s",5109
258,What is a recommended way of extending?,torch.autograd,5109
259,Where is the Note on extending the autograd engine?,https://pytorch.org/docs/stable/notes/extending.html,5782
260,What is the form of the history of a Tensor s function object?,DAG of functions,2266
261,What method is used to process a graph when backward is called?,backward() methods,2266
262,What happens to every operation performed on Tensor s?,creates a new function object,2266
263,What do the edges of the DAG of functions denote?,data dependencies,2266
264,What method is called when a graph is processed in the topological ordering?,backward(),2266
265,What is the only way users interact with functions?,creating subclasses and defining new operations,4342
266,What is a recommended way of extending functions?,torch.autograd,4342
267,To what are the following methods available when creating a new Function?,ctx,8351
268,How are tensors modified?,in-place operation,8351
269,What do function._ContextMethodMixin.mark_non_differentiable mark outputs as?,non-differentiable,8351
270,What function saves given tensors for a future call to backward()?,backward,8351
271,What does function._ContextMethodMixin.set_ do?,materialize_grads,8351
272,The following methods are available when creating a new Function?,ctx,8351
273,In what operation are given tensors modified?,in-place,8351
274,What do function._ContextMethodMixin.mark_non_differentiable Marks outputs as?,non-differentiable,8351
275,What does save_for_backward save?,given tensors for a future call to backward(),8351
276,What sets whether to materialize output grad tensors?,set_materialize_grads,8351
277,How are gradients computed?,small finite differences against analytical gradients,1564
278,How are gradients computed against analytical gradients w.r.t?,small finite differences,1564
279,What type of gradients computed via small finite differences against analytical gradients w.r.t?,Check gradients,1564
280,What does Autograd include that lets you inspect the cost of different operators inside your model?,profiler,1374
281,What's the name of the profiler that lets you inspect the cost of different operators inside your model?,profile,1374
282,What does nvprof use to register both CPU and GPU activity?,emit_nvtx,1374
283,What mode is implemented at the moment?,CPU-only,1374
284,What lets you inspect the cost of different operators inside your model?,profiler,1375
285,What is used to register both CPU and GPU activity?,emit_nvtx,1375
286,profiler is what?,thread local,1375
287,CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.,use_cuda,1375
288,What does the context manager manage?,autograd profiler state,1867
289,What is recommended to validate the timing?,separate runs with and without shape recording,1867
290,What language does autograd profiler expose events to?,Python,1865
291,under the hood it just records events of functions being executed in what language?,C++,1865
292,Wrap any code into it and it will only report runtime of what?,PyTorch functions,1376
293,What is thread local and is automatically propagated into the async tasks enabled?,profiler,1376
294,wrap any code into it and it will only what?,report runtime of PyTorch functions,1376
295,What types of operators does with_flops only work for?,matrix multiplication and 2D convolution operators,1376
296,autograd includes what that lets you inspect the cost of different operators inside your model - both on the CPU and GPU?,profiler,1376
297,profile,CPU-only,1376
298,nvprof based,emit_nvtx,1376
299,what manages autograd profiler state and holds a summary of results?,Context manager,1376
300,it just records events of functions being executed in what language?,C++,1376
301,approximately how much overhead does each tensor operation add?,4us,1376
302,Context manager that manages what?,autograd profiler state,1866
303,setting this to what makes this context manager a no-op?,False,1866
304,Setting enabled to what makes this context manager a no-op?,False,9258
305,What disables timing of CUDA events?,use_cuda,9258
306,How much overhead does use_cuda add to each tensor operation?,4us,9259
307,What enables timing of CUDA events as well using the cudaEvent API?,use_cuda,11311
308,What is the name of the API that allows timing of CUDA events?,use_cuda,11311
309,How much overhead is added to each tensor operation?,4us,11312
310,What enables timing of CUDA events as well as using the cudaEvent API?,use_cuda,11312
311,What data might shape recording skew?,profiling data,11312
312,Who will estimate the FLOPS value if with_flops is set?,the profiler,10561
313,Why might the total self cpu time be artificially increased for higher level functions?,the shape collection,10561
314,What is used to group input dimensions?,prof.key_averages,10561
315,Shape recording might skew what?,profiling data,10561
316,What might be artificially increased because of shape collection?,the total self cpu time,10561
317,What does with_flops allow one to estimate?,hardware performance,10562
318,What does FLOPS stand for?,floating pointer operations per second,10562
319,What operators does with_flops only work for?,matrix multiplication and 2D convolution operators,10562
320,What is the name of the experimental feature that allows profiling with Kineto profiler?,use_kineto,10562
321,What is the name of the program that makes every autograd operation emit an NVTX range?,nvprof,10562
322,What exports an EventList as a Chrome tracing tools file?,profiler.profile.export_chrome_trace,10562
323,What information will be collected if shapes recording is set?,input dimensions,10562
324,What allows one to see which dimensions have been used under the hood and further group by them using?,prof.key_averages,10562
325,What is the name of the event profiler that can be used to lower the overhead for GPU-only profiling?,use_cpu,10562
326,What does with_stack do?,record source information,11417
327,What does use_cpu do?,profile CPU events,11417
328,What does profiler.profile.export_chrome_trace export an EventList as?,Chrome tracing tools file,11417
329,What does profiler.profile.key_averages average?,all function events over their keys,11417
330,What does every autograd operation emit?,NVTX range,11417
331,What does nvvp stand for?,NVIDIA Visual Profiler,11417
332,What is the name of the plugin that allows profiling with Kineto profiler?,use_kineto,10445
333,"Use_kineto (bool, optional) enables profiling with Kineto profiler. use_cpu (bool, optional",experimental,11416
334,What enables profiling with Kineto profiler?,use_kineto,11416
335,What is the name of the profile CPU events?,use_cpu,11319
336,What Averages all events?,total_average,11319
337,"Use_kineto (bool, optional) - enable profiling with Kineto profiler. use_cpu (bool,",experimental,11415
338,What profile CPU events?,use_cpu,11415
339,What does profiler.profile.self_cpu_time_total do?,profiler.profile.self_cpu_time_total,11412
340,What is the name of the option to record source information for the ops?,with_stack,11412
341,What profiler.profile.self_cpu_time_total does?,profiler.profile.self_cpu_time_total,11412
342,What profiler.profile.key_averages Averages all function events over their keys?,profiler.profile.self_cpu_time_total,11318
343,What is the total time spent on CPU obtained as?,sum of all self times across all the events,10447
344,What is profiler.profile.total_?,average,10456
345,Under what program is the context manager useful?,nvprof,10456
346,What is used to profile CPU events?,use_cpu,11309
347,What returns total time spent on CPU obtained as a sum of all self times across all the events?,profiler.profile.self_cpu_time_total,10453
348,What Averages all function events over their keys?,profiler.profile.key_averages,10453
349,What averages all events?,profiler.profile.total_average,2359
350,What is nvvp?,NVIDIA Visual Profiler,2359
351,Setting enabled=False makes this context manager a what?,no-op,2359
352,What may increase the overhead of nvtx range creation?,shape recording,2359
353,What does profiler.profile.total_average Average?,all events,10454
354,Profiler.profile.self_cpu_time_total Returns total time spent on what?,CPU,10451
355,What is an EventList exported as?,Chrome tracing tools file,2358
356,Under what program is the NVTX range useful?,nvprof,10455
357,What is the name of all events in profiler.profile?,total_average,10455
358,Exports an EventList as a what browser tracing tools file?,Chrome,2356
359,Where are all function events averaged?,over their keys,1388
360,Returns total time spent on what?,CPU,5690
361,What is returned as a sum of all self times across all events?,total time spent on CPU,5690
362,What does profiler.profile.total_average do?,profiler.profile.total_average Averages all events,10459
363,What range does every autograd operation emit?,NVTX,10458
364,When is the NVTX range useful?,when running the program under nvprof,10458
365,What is a Context manager that makes every autograd operation emit an NVTX range?,Averages all events,1386
366,Context manager that makes every autograd operation emit what?,NVTX range,1386
367,When is the context manager useful?,when running the program under nvprof,1386
368,What is the name of the range that every autograd operation emits?,NVTX,1862
369,What does the context manager make every autograd operation emit?,NVTX range,1862
370,What is the name of the context manager that makes every autograd operation emit an NVTX range?,nvprof,1862
371,What makes every autograd operation emit an NVTX range?,Context manager,1863
372,What does the context manager do?,Averages all events,1387
373,What program is useful when running the program under?,nvprof,3952
374,What is a context manager used for?,CUDA profiling,3952
375,What is another name for torch.autograd.profiler.load_nvprof()?,Python REPL,3952
376,In what language can torch.autograd.profiler.load_nvprof() be used?,Python,8055
377,Is there a way to force nvprof to flush the data it collected to disk?,there’s no way to force nvprof to flush the data it collected to disk,8055
378,What does the context manager do to annotate nvprof traces and wait for the process to exit before inspecting them?,CUDA profiling,8056
379,What is the default value of enabled?,True,8056
380,What default setting makes enabled=False a no-op?,True,9261
381,What is an example of a difficult task when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?,Forward-backward correlation,2287
382,In what program is emit_nvtx used?,Nvidia Visual Profiler,2287
383,What does emit_nvtx append to the ranges it generates?,sequence number information,2594
384,What is difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?,Forward-backward correlation,2594
385,What type of correlation can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?,Forward-backward,2594
386,Where is emit_nvtx used?,Nvidia Visual Profiler,2594
387,What can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?,correlating each backward-pass op with the corresponding forward-pass op,8386
388,What is the term for a double backward?,Double-backward,2202
389,What is the term for a double-backward?,Double-backward,2202
390,What enable anomaly detection for the autograd engine?,Context-manager,1870
391,What type of test will slow down your program execution?,Example,1870
392,What does running the forward pass with detection enabled allow the backward pass to print?,the traceback,5722
393,Any backward computation that generate “nan” value will what?,raise an error,1871
394,Warning This mode should be enabled only for what purpose?,debugging,1871
395,Set_detect_anomaly can be used as what?,context-manager or as a function,1871
396,What does the context-manager do?,sets the anomaly detection for the autograd engine on or off,1871
397,What will enable or disable the autograd anomaly detection based on its argument mode?,set_detect_anomaly,1871
398,"For details of the anomaly detection behaviour, see what above?",detect_anomaly,1871
399,What is the name of the flag that indicates whether to enable or disable the autograd anomaly detection?,mode,1871
400,What does two things: Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing,Context-manager,1871
401,What does running the forward pass with detection enabled do?,will allow the backward pass to print the traceback of the forward operation that created the failing backward function,1871
402,What will raise an error?,Any backward computation that generate “nan” value,1148
403,This mode should be enabled only for what purpose?,debugging,1148
404,What is an example that sets the anomaly detection for the autograd engine on or off?,Context-manager,7744
405,Set_detect_anomaly will enable or disable the autograd anomaly detection based on what?,argument mode,7744
406,What should this mode be enabled only for?,debugging,7744
407,What is the name of the setting that sets the autograd anomaly detection?,detect_anomaly,7744
408,What does mode (bool) do to enable or disable anomaly detection?,Flag,7744
409,This mode should only be enabled for what purpose?,debugging,7744
410,"For details of the anomaly detection behaviour, see what?",detect_anomaly,8199
411,Warning This mode should only be enabled for what purpose?,debugging,8199
412,What algorithm is used to efficiently compute the matrix product of the NNN 2-D tensors?,matrix chain order algorithm,5598
413,"If NNN is greater than or equal to 2, what is the trivial matrix-matrix product returned?",if equal to 2,5598
414,"If NNN is greater than or equal to 2, then this is a no-op - the original matrix is returned as is.",1,5598
415,What is the warning that returns the matrix product of the NNN 2-D tensors?,Warning,5598
416,What algorithm computes the matrix product of the NNN 2-D tensors?,matrix chain order algorithm,5598
417,What is required to return the matrix product of the NNN 2-D tensors?,NNN needs to be greater than or equal to 2,5598
418,What is the name of the warning message that is sent when NNN is greater than or equal to 2?,Warning,5598
419,What is deprecated and will be removed in a future PyTorch release?,torch.chain_matmul(),11021
420,What is a sequence of 2 or more 2-D tensors whose product is to be determined?,matrices,11021
421,What is the output tensor ignored if out =?,None,11021
422,What does torch.linalg.multi_dot() accept instead of multiple arguments?,a list of two or more tensors,11021
423,Ignored if out = what?,None,11021
424,When will torch.chain_matmul() be removed?,PyTorch,11021
425,What does torch.linalg.multi_dot() accept?,a list of two or more tensors rather than multiple arguments,11021
426,"Out (Tensor, optional) – what is ignored if out = None?",the output tensor,11021
427,What does torch.linalg.multi_dot() replace?,multiple arguments,8212
428,What is the tangent of the elements of input?,hyperbolic,5384
429,Element-wise arctangent of inputi/otheri with consideration of what?,quadrant,2250
430,Returns a new tensor with the signed angles in what?,radians,2250
431,What is the first parameter of inputitextinput_iinputi?,y,2250
432,The arctangent of inputi/otheritextinput_iinputi /otheri is considered with,the quadrant,2250
433,What is inputitextinput_iinputi's first parameter?,y,2250
434,"What returns a new tensor with the signed angles in radians between vector (otheri,inputi)(text",Returns a new tensor,2251
435,With consideration of what is the arctangent of inputi/otheri/textother_iinputi /other,the quadrant,2251
436,"Inputitextinput_iinputi, the first parameter, is what -coordinate?",y,2251
437,What is the first input tensor other?,input (Tensor),2251
438,What is an example of a broadcastable input and output tensor?,Example,7286
439,What is the goal of this quickstart guide?,Familiarize yourself with PyTorch concepts and modules,2405
440,What do you learn in this quickstart guide?,build deep neural networks,2405
441,How do you use torch.nn to create and train a neural network?,Getting-Started,2405
442,How do you build models with PyTorch?,build deep neural networks,2405
443,What type of PyTorch code examples are included in this quickstart guide?,"Bite-size, ready-to-deploy PyTorch code examples",2405
444,What is the name of the tutorial that introduces the fundamental concepts of PyTorch?,Getting-Started,2405
445,What is a step-by-step guide to building a complete ML workflow with PyTorch?,"Bite-size, ready-to-deploy PyTorch code examples",1444
446,How is building a complete ML workflow with PyTorch?,step-by-step,1444
447,How does this tutorial introduce the fundamental concepts of PyTorch?,self-contained examples,1444
448,What is the main feature of TensorBoard?,Interpretability,1444
449,What is the purpose of PyTorch?,A step-by-step guide to building a complete ML workflow,1444
450,What is the second tutorial in a series of three tutorials?,leanr how to generate names from languages,1444
451,"Bite-size, ready-to-deploy what?",PyTorch code examples,1444
452,"What is used to visualize data and model training. Interpretability,Getting-Started,TensorBoard Finetune a pre",TensorBoard,1444
453,What is used to classify names?,character-level RNN,1444
454,What type of learning does PyTorch train?,Reinforcement-Learning,1444
455,What is used to create and train a neural network?,torch.nn,8091
456,What library can be used to load and preprocess data from a simple dataset?,PyTorch's torchaudio library,8091
457,"What is a Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN",Interpretability,8091
458,Familiarize yourself with what concepts and modules?,PyTorch,2406
459,"Learn how to load data, train and save your models in this quickstart guide.",build deep neural networks,2406
460,What are PyTorch code examples?,"Bite-size, ready-to-deploy PyTorch code examples",2406
461,What is a quickstart guide to building a complete ML workflow with PyTorch?,A step-by-step guide to building a complete ML workflow,2406
462,What is a generative adversarial network used to generate new celebrities?,Image/Video,2406
463,What tool can be used to visualize data and model training?,TensorBoard,4083
464,What type of video is used to train a convolutional neural network for image classification using transfer learning?,Image/Video,4083
465,What type of video train a convolutional neural network for image classification using transfer learning?,Image/Video,4083
466,What pre-trained model can TensorBoard fine tune?,Mask R-CNN,4085
467,What did you learn after using character-level RNN to classify names?,leanr how to generate names from languages,4085
468,How do you deploy a PyTorch model?,PyTorch model using Flask,4085
469,What is the term for a model that detects the image?,Production,4085
470,What are some of the features of TensorBoard?,"Interpretability,Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN model",4085
471,What type of data does TensorBoard fine tune?,Image/Video,851
472,What type of data does TensorBoard finetune?,Image/Video,851
473,What is ready-to-deploy?,PyTorch code examples,1443
474,What type of video learn how to augment your network using a visual attention mechanism?,Image/Video,1443
475,What is used to visualize data and model training?,TensorBoard,7821
476,What does this tutorial introduce the fundamental concepts of?,PyTorch,7821
477,What aspect of TensorBoard Finetune a pre-trained Mask R-CNN model?,Interpretability,2709
478,Getting-Started Learn to use what to visualize data and model training?,TensorBoard,2709
479,What is a sequence-to-sequence model that uses the nn.Transformer module?,Text,2709
480,What is a step-by-step guide to building a complete ML workflow with?,PyTorch,852
481,Image/Video Learn to load and preprocess data from a simple dataset with PyTorch's what library?,torchaudio library,852
482,What do you learn to use to visualize data and model training?,TensorBoard,7820
483,What is a convolutional neural network for image classification using transfer learning?,Image/Video,7820
484,This tutorial introduces the fundamental concepts of what?,PyTorch,7820
485,What is a convolutional neural network for image classification?,Image/Video,7820
486,TensorBoard Finetune a pre-trained what?,Mask R-CNN model,8090
487,What is one of the things that TensorBoard Finetune a pre-trained Mask R-CNN model?,Interpretability,8090
488,Image/Video Train a convolutional neural network for image classification using what?,transfer learning,3668
489,Image/Video Train a convolutional neural network for what?,image classification,3661
490,Image/Video Apply what to computer vision tasks?,"cutting-edge, attention-based transformer models",3661
491,"Image/Video Apply cutting edge, what to computer vision tasks?",attention-based transformer models,3661
492,Text Build and train a basic character-level RNN to classify word from scratch without the use of what?,torchtext,3661
493,Image/Video Learn to load and preprocess data from a simple dataset with PyTorch's what?,torchaudio library,3661
494,What does DQN stand for?,Deep Q Learning,3661
495,What library does PyTorch use to build the dataset and classify text?,torchtext library,3661
496,"After using character-level RNN to classify names, leanr how to generate names from what?",languages,3661
497,"What is the third in a series of three tutorials on doing ""NLP From Scratch""?",third and final tutorial,3661
498,What is the intermediate representation of a PyTorch model that can then be run in a high-performance environment such as C++?,Production,3661
499,What is a PyTorch model deployed using?,Flask,3661
500,Where can you train a convolutional neural network?,Image/Video,2711
501,What type of video is used to train a convolutional neural network?,Image/Video,8089
502,What to create and train a neural network?,torch.nn,8089
503,What pre-trained model is fine tuned by TensorBoard?,Mask R-CNN model,8089
504,"Getting-Started Learn to use what to visualize data and model training. Interpretability,Getting-Started,TensorBoard",TensorBoard,2710
505,How many tutorials are there?,three,2710
506,What do you learn how to generate names from?,languages,2710
507,"Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN model. What is",Interpretability,2710
508,"Interpretability,Getting-Started,TensorBoard Finetune a pre-trained what?",Mask R-CNN model,2713
509,"Image/Video Apply cutting-edge, what to computer vision tasks?",attention-based transformer models,7987
510,Image/Video Learn to load and preprocess data from a simple dataset with what?,PyTorch's torchaudio library,7987
511,What does PyTorch use to deploy a model?,Flask,7987
512,What is TorchScript an intermediate representation of a PyTorch model?,Production,7987
513,What does TensorBoard Finetune a pre-trained Mask R-CNN model?,Interpretability,4084
514,What library can you use to load and preprocess data from a simple dataset?,PyTorch's torchaudio library,4084
515,What type of dataset is a dataset?,audio,4084
516,What type of data does the nn.Transformer module train?,Text,4084
517,What type of dataset does PyTorch's torchaudio library help you format?,audio,4084
518,What is the name of the pre-trained model?,Mask R-CNN,3856
519,What does GAN stand for?,generative adversarial network,3670
520,"In a series of three tutorials, what is the third and final tutorial on doing ""NLP From Scratch""?",Second,3670
521,What library does PyTorch use to classify text?,torchtext library,3670
522,What is the third and final tutorial on doing?,NLP From Scratch,3670
523,What is a generative adversarial network?,Image/Video,3855
524,What is the main feature of a pre-trained Mask R-CNN model?,Interpretability,3855
525,What type of video train a generative adversarial network?,Image/Video,3855
526,Image/Video Learn how to augment your network using what?,visual attention mechanism,3667
527,What can you do with a pre-trained Mask R-CNN model?,Finetune a pre-trained Mask R-CNN model,2446
528,What is GAN?,generative adversarial network,1262
529,What is applied to computer vision tasks?,attention-based transformer models,1262
530,What is an example of a GAN?,Image/Video,3664
531,What is another name for a generative adversarial network?,GAN,7990
532,What is used to train a convolutional neural network for image classification?,transfer learning,7990
533,What do you use to augment your network?,visual attention mechanism,7990
534,"What is the final tutorial on doing ""NLP From Scratch""?",leanr how to generate names from languages,7990
535,What type of video learns how to augment your network using a visual attention mechanism?,Image/Video,7985
536,What is the name of the video that teaches how to augment your network using a visual attention mechanism?,Image/Video,7985
537,"Image/Video Apply cutting-edge, attention-based transformer models to what?",computer vision tasks,7985
538,Image/Video Train what for image classification using transfer learning?,convolutional neural network,3666
539,What type of network can be used to train/test an audio classifier network on a dataset?,Audio,3666
540,What is a generative adversarial network called?,GAN,7989
541,What is the first in a series of three tutorials?,Text,7989
542,What can you use to load and preprocess data from a simple dataset?,PyTorch's torchaudio library,7989
543,"In a series of three tutorials, what is the second in a series of three tutorials?",Second,7989
544,What do you want to generate names from?,languages,7989
545,Audio Learn how to correctly format a dataset?,audio,7989
546,What type of library does PyTorch's torchaudio library use?,Audio,2712
547,What library does PyTorch's torchaudio library contain?,Audio,2712
548,What programming language does this tutorial introduce?,PyTorch,2712
549,What type of data does PyTorch's torchaudio library provide?,Audio,1261
550,What can be applied to computer vision tasks?,"cutting-edge, attention-based transformer models",1263
551,Audio Learn how to correctly format an what type of dataset?,audio,1370
552,What type of network can be used to train a generative adversarial network?,Audio,7988
553,What module is used to train a sequence-to-sequence model?,nn.Transformer module,1372
554,How many tutorials are in this series?,three,1372
555,Text Train a language translation model from scratch using what?,Transformer,1372
556,What does PyTorch train to play Mario?,Double Q-learning agent,1372
557,"What is the third and final tutorial on doing ""NLP From Scratch""?",third and final,1372
558,What type of data is used to train a generative adversarial network?,Text,7991
559,What do you train/test on a dataset?,audio classifier network,7991
560,What does PyTorch's torchaudio library train/test on a dataset?,audio classifier network,3662
561,What type of data is used to train a sequence-to-sequence model?,Text,3669
562,What type of data does PyTorch's torchaudio library teach?,Text,4076
563,What type of data does PyTorch's nn.Transformer module train?,Text,4076
564,What do you learn how to augment your network using?,visual attention mechanism,4077
565,What type of dataset can PyTorch correctly format?,audio,4082
566,What library does PyTorch use to load and preprocess data from a simple dataset?,torchaudio library,4082
567,What do you need to learn to load and preprocess data from a simple dataset?,PyTorch's torchaudio library,3663
568,What is the term for a basic character-level RNN to classify word from scratch without the use of torchtext?,Text,3663
569,Text Build and train a basic character-level to classify word from scratch without the use of torchtext?,RNN,4081
570,What type of dataset can you format?,audio,4081
571,What is the term for a basic character-level RNN without the use of torchtext?,Text,4081
572,What type of dataset does PyTorch's torchaudio library format?,audio,4081
573,What does PyTorch's torchaudio library not use?,torchtext,4081
574,What is the second in a series of three tutorials?,leanr how to generate names from languages,4080
575,What is the name of the first in a series of three tutorials?,Text,7986
576,What do you train/test on the dataset?,audio classifier network,1369
577,Build and train a basic character-level RNN to classify word from scratch without the use of what?,torchtext,4078
578,What do students learn how to generate names from?,languages,4078
579,Learn how to correctly train/test an audio classifier network on the dataset?,format an audio dataset,4078
580,Learn how to correctly format a dataset and then train/test an audio classifier network on the dataset?,audio,4078
581,What type of RNN is used to classify words?,Text,1371
582,What are some examples around pytorch?,"Vision, Text, Reinforcement Learning",837
583,What is a quick overview of essential PyTorch elements?,Quick overview,837
584,Where can you access PyTorch Tutorials?,GitHub,837
585,Where can you copy tutorial data to?,Google Drive,837
586,"What is a set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.?",A set of examples around pytorch,837
587,What is the name of a set of examples around pytorch?,Quick overview,837
588,What is the name of the file that you can copy tutorial data into?,Google Drive,837
589,What is another name for python -m torch.utils.bottleneck?,python -m torch.utils.bottleneck,11370
590,What does the CUDA-mode autograd profiler do?,Note,11370
591,What is python -m torch.utils.bottleneck -h?,python -m torch.utils.bottleneck -h,11370
592,What should you do to ensure that your script exits in a finite amount of time?,ensure that it exits in a finite amount of time,8141
593,"If your script is what, looking at the results of the CPU-mode autograd profiler will help?",CPU-bound,8141
594,"If your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output",CUDA-mode autograd profiler,8141
595,What should you first check if your script is?,CPU-bound,8141
596,What does torch.autograd.profiler.emit_nvtx() use?,nvprof,8141
597,What is very high and often gives a heavily skewed timeline?,NVTX overhead,8141
598,What should not matter if your bottlenecks result in code much slower than the CUDA startup time?,if your bottlenecks result in code much slower than the CUDA startup time,8141
599,What should not matter if your bottlenecks result in code much slower than?,CUDA startup time,8141
600,What is an example of a more complicated use of profilers?,multi-GPU,8141
601,What is the nature of CUDA kernels?,asynchronous,8141
602,Ops that do synchronize appear to be what under regular CPU-mode profilers?,Ops that do synchronize appear to be extremely expensive,8141
603,What may be helpful in cases where timings are incorrect?,CUDA-mode autograd profiler,8141
604,What will help you decide if your script is CPU-bound?,CPU-mode autograd profiler,8141
605,What is the warning that your script will be profiled and exits in a finite amount of time?,Warning,5717
606,What is the name of the command to run for more usage instructions?,python -m torch.utils.bottleneck,11369
607,What is it called when a script is profiled?,Warning,11369
608,What will be profiled?,your script,1431
609,What is the name of the warning that your script will be profiled?,Warning,8140
610,What is the name of the warning that your script will exit in a finite amount of time?,Warning,1430
611,What should you do when your script is profiled?,ensure that it exits in a finite amount of time,1430
612,What is another name for CPU-only-mode?,CUDA-mode,4433
613,What is the default mode for autograd profiler output?,CPU-only,4433
614,"If your script is CPU-bound, what autograd profiler will help?",CPU-mode,4434
615,What is the name of the warning that the NVTX overhead is very high and often gives a heavily skewed timeline?,Warning,4434
616,What does it make sense to look for in the output of the CUDA-mode autograd profiler?,responsible CUDA operators,7889
617,What is the CUDA-mode autograd profiler?,CPU-only,7889
618,Which autograd profiler will help if your script is CPU-bound?,CPU-mode,7889
619,What type of autograd profiler will help if your script is CPU-bound?,CPU-mode,7890
620,What does the NVTX overhead give a heavily skewed timeline?,Warning,7890
621,What are the two extremes of autograd profiler output?,CPU-only-mode or CUDA-mode,7890
622,What might happen to your script depending on the part of the model you're evaluating?,your script might not be in one of those two extremes,4581
623,What does nvprof do if the profiler outputs don't help?,torch.autograd.profiler.emit_nvtx(),4581
624,What does cProfile include in its time reporting?,CUDA startup time,4581
625,What is the reality of profiling CUDA code?,more complicated,4581
626,What is the reality of NVTX?,more complicated,4580
627,What does the NVTX overhead often give?,heavily skewed timeline,4580
628,What is the NVTX overhead?,Warning,4580
629,What is a multi-GPU case?,more complicated uses of the profilers,8165
630,What is the CUDA buffer allocation cost?,CUDA startup time,8165
631,What is the name of the document?,Note,4352
632,What is the meaning of what?,Note,4352
633,What does the stashing logic save and restore for the current device?,the RNG state,7307
634,The checkpointed part recomputes intermediate activations in what?,backward pass,7307
635,Does the stashing logic have a way to anticipate if the user will move Tensors to a new device within the run_f,the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself,7306
636,What is never guaranteed if you move Tensors to a new device within run_fn?,deterministic output compared to non-checkpointed passes,7308
637,What is it called when a function invocation during backward does something different than the one during forward?,Warning,7308
638,The checkpointed part can be applied on what?,any part of a model,7308
639,What saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the run_f,The stashing logic,1574
640,What is never guaranteed if you move Tensors to a new device?,deterministic output,1574
641,How is checkpointing implemented?,rerunning a forward-pass segment,1574
642,What can be advanced by rerunning a forward-pass segment for each checkpointed segment during backward?,persistent states like the RNG state,1574
643,What do checkpointed passes using RNG have as compared to non-checkpointed passes?,deterministic output,1574
644,What can the logic to stash and restore RNG states incur depending on the runtime of checkpointed operations?,moderate performance hit,1574
645,Does the stashing logic have any way to anticipate if the user will move Tensors to a new device within the run_fn,no way to anticipate if the user will move Tensors to a new device within the run_fn itself,4361
646,How does Checkpointing work?,trading compute for memory,1569
647,What does Checkpointing trade?,compute for memory,1569
648,In what way does the checkpointed part recompute the intermediate activations?,backward pass,1567
649,Checkpointing can be applied on what?,any part of a model,1578
650,How does checkpointing work?,trading compute for memory,1578
651,What causes the checkpointed version to not be equivalent?,global variable,1578
652,What does Checkpointing currently only support?,torch.autograd.backward(),1578
653,What causes an error when a tensor is defined to have no gradient in the model?,makes all the outputs require gradients,1578
654,What does the checkpointed part do instead of saving intermediate activations?,recomputes them in backward pass,1576
655,How does checking work?,trading compute for memory,1576
656,Checkpointing can be applied to what part of a model?,any part of a model,1576
657,"In the forward pass, function will run in what manner?",torch.no_grad(),6010
658,What saves the inputs tuple and the function parameter?,forward pass,6010
659,In what pass are the saved inputs and function retrieved?,the backwards pass,6010
660,What are examples of nested structures?,"custom objects, lists, dicts",6010
661,What should function know how to handle the inputs passed as?,tuple,6010
662,In what model should function know how to handle the inputs passed as the tuple?,LSTM,6010
663,What is torch.no_grad()?,not storing the intermediate activations,6009
664,What does the forward pass save?,the inputs tuple and the function parameter,1577
665,"In the backwards pass, what is computed on function again?",the forward pass,1577
666,What does checking work by?,trading compute for memory,1577
667,Which part of the computation graph does not save intermediate activations?,the checkpointed part,1577
668,What can the output of a function contain?,Tensor values,7216
669,What does function describe?,what to run in the forward pass of the model or part of the model,7216
670,What does detach() or torch.no_grad() do to a checkpointed segment?,tensors detached from the computational graph,7216
671,What are Tensors nested in?,custom structures,7215
672,Is torch.autograd.grad() supported?,not supported,7215
673,What is the warning that torch.autograd.grad() is not supported?,Warning,7215
674,What does Warning Checkpointing currently only support?,torch.autograd.backward(),8144
675,How do functions run?,sequentially,8144
676,What currently only supports torch.autograd.backward()?,Warning Checkpointing,5817
677,What is the status of torch.autograd.grad()?,not supported,5817
678,What executes a list of modules/functions in order?,Sequential models,5817
679,What is input to functions preserve_rng_state?,A Tensor,5817
680,What is the output of running functions sequentially on?,*inputs Example,5817
681,What is another name for how checkpointing works?,checkpoint(),5817
682,What are the functions to run sequentially?,A torch.nn.Sequential or the list of modules or functions,5817
683,How do we divide a model in segments?,checkpoint,5817
684,What won't be equivalent if function invocation during backward does something different than the one during forward?,the checkpointed version,3413
685,What is a tuple containing inputs to the function Output of running function on *args?,args,3413
686,What is a helper function for checkpointing sequential models?,*args,3413
687,What is a way to circumvent the backward error?,detach the tensors outside of the checkpoint function,3413
688,What is a Tensor that is input to functions?,Number of chunks to create in the model input,3413
689,What is the name of the warning if function invocation during backward does something different than the one during forward?,Warning,8142
690,What will not be equivalent if function invocation during backward does something different than during forward?,checkpointed version,8160
691,What is a warning if function invocation during backward does anything different?,Warning,8160
692,"If function invocation during backward does something different than the one during forward, the checkpointed version won’t be equivalent, and unfortunately it",global variable,3412
693,What won’t be equivalent if function invocation during backward does something different than the one during forward?,checkpointed version,3412
694,What does the checkpointed version of the checkpointed version do?,Warning,3412
695,What methods are used to detach tensors from the computational graph?,detach() or torch.no_grad(),3404
696,"In LSTM, if user passes what, the function should use the first input as activation and the second input as hidden?",activation,3404
697,What is a tuple containing inputs to the function Output of running function on *args A helper function for checkpointing,args,3404
698,What is a way to circumvent a tensor error when a tensor is defined to have no gradient in the model,detach the tensors outside of the checkpoint function,3404
699,What makes all the outputs require gradients?,checkpoint,8158
700,What is detached from the computational graph by detach() or torch.no_grad()?,tensors,8158
701,What is a tuple containing inputs to the function?,args,8158
702,How can we divide a sequential model into segments?,checkpoint,8158
703,What models execute a list of modules/functions in order (sequentially)?,Sequential models,8158
704,All segments except the last will run in what manner?,torch.no_grad(),8158
705,What will be saved for re-running the segment in the backward pass?,The inputs of each checkpointed segment,8158
706,What is the name of the function that shows how checkpointing works?,checkpoint(),8158
707,What does checkpointing currently only support?,torch.autograd.backward(),8158
708,What is a function to run sequentially?,A torch.nn.Sequential or the list of modules or functions,8158
709,What is the purpose of a torch.nn.Sequential?,to run sequentially,8158
710,What are segments?,Number of chunks to create in the model input,8158
711,What is not supported by Checkpointing?,torch.autograd.grad(),8158
712,What makes all outputs require gradients?,checkpoint,8158
713,How can a tensor be circumvented if a tensor is defined to have no gradient in the model?,detach the tensors outside of the checkpoint function,8156
714,What is a way to circumvent the error when a tensor is defined to have no gradient in the model?,detach the tensors outside of the checkpoint function,8157
715,What is a way to circumvent the backward pass error?,detach the tensors outside of the checkpoint function,3403
716,How are tensors detached from the computational graph?,detach() or torch.no_grad(),3403
717,What is a way to circumvent the issue of a tensor being defined to have no gradient in the model?,detach the tensors outside of the checkpoint function,3403
718,What should function use the second input as in LSTM?,hidden preserve_rng_state,9409
719,args – tuple containing inputs to the function Output of running function on what?,*args,9409
720,In what manner will all segments except the last run?,torch.no_grad(),9409
721,What is the name of the warning that is displayed when a model is checked?,Warning,9409
722,"In LSTM, if user passes what, function should use the first input as activation and the second input as hidden.",activation,9409
723,What is the tuple containing inputs to the function Output of running function on *args?,args,9409
724,What is the name of the warning that is given when a model is checked?,Warning,9409
725,"In LSTM, if user passes what, hidden, function should use the first input as activation and the second input as hidden?",activation,9408
726,What omits stashing and restoring the RNG state during each checkpoint?,preserve_rng_state,10442
727,What tuple contains inputs to the function Output of running function on *args A helper function for checkpointing sequential models?,args,10441
728,A helper function for what kind of sequential models?,checkpointing,8890
729,Output of running function on what?,*args,4698
730,What is output of running function on *args?,A helper function,4698
731,The inputs of each checkpointed segment will be saved for re-running the segment in what?,backward pass,802
732,What is a helper function for?,checkpointing sequential models,802
733,What is used for checkpointing sequential models?,A helper function,802
734,What is output of running functions sequentially on?,*inputs,802
735,How do we divide a sequential model into segments?,checkpoint,8891
736,What do we do to each segment of a sequential model?,checkpoint,801
737,What function shows how checkpointing works?,checkpoint(),801
738,What is the name of the function that checks sequential models?,Warning,801
739,What does checkpointing do?,Warning,801
740,What do we do to each segment of a Sequential model?,checkpoint,5816
741,What models execute a list of modules/functions in order?,Sequential models,5816
742,How does checkpoint() work?,checkpointing,5816
743,What is the name of the warning that is displayed when a sequence model is checked?,Warning,5816
744,What is input to functions to create chunks in the model input?,Tensor,5776
745,What is the name of checkpointing?,checkpoint(),5776
746,What are segments used to create in the model input?,Number of chunks,5776
747,What is the name of the list of modules or functions to run sequentially?,A torch.nn.Sequential,9432
748,What dimension is removed?,tensor dimension,5147
749,What is the number of all slices along a given dimension?,tuple,5147
750,What is the tensor to unbind?,dim,5147
751,"Returns what of all slices along a given dimension, already without it?",tuple,5147
752,What is the tensor to unbind dim (int) – dimension to remove?,input,5147
753,"If upper is True or not provided, uuu is upper triangular such that the returned tensor is input (Tens",If upper is False,3541
754,What is the input 2-D tensor uuu?,Cholesky factor upper,3541
755,What is the returned tensor?,input (Tensor),1779
756,The inverse of a symmetric positive-definite matrix AAA is computed using what?,MAGMA routines,1779
757,If uuu is lower triangular such that the returned tensor is?,If upper is False,1779
758,What are Alias for?,torch.linalg.matrix_power(),1030
759,What is the Alias for?,torch.linalg.matrix_power,1030
760,Pivoting is done if pivot is set to what?,True,1685
761,Returns a tuple containing what?,LU factorization and pivots of A,1685
762,Is LU factorization with pivot = False available for CPU?,not available for CPU,1685
763,"If get_infos is False, what are the elements in the tuple?","Tensor, IntTensor",1685
764,What is the default value for a tuple?,False out,1685
765,What are the pivots returned by the function?,1-indexed,7223
766,What is LU factorization with pivot = False available for?,CUDA,7223
767,What are the returned pivots if pivot is False?,a tensor filled with zeros of the appropriate size,7223
768,What does this function not do?,check if the factorization was successful or not if get_infos is True,7223
769,"How can L, U, and P be derived?",torch.lu_unpack(),7223
770,What is the default value for get_infos?,True,7223
771,What does LU factorization have?,backward support,7223
772,What are the elements in the tuple if get_infos is True?,"Tensor, IntTensor, and IntTensor",7223
773,What are the elements in the tuple if get_infos is False?,"Tensor, IntTensor",7223
774,What returns an info IntTensor?,True get_infos,7223
775,What are the returned pivots filled with zeros of the appropriate size?,tensor,1684
776,What does torch.lu_unpack() do?,Warning,1684
777,What does the function A. Returns a tuple containing the LU factorization and pivots of A?,Computes the LU factorization of a matrix or batches of matrices,1684
778,What happens when batches of square matrices have size less than 32 on a CUDA device?,LU factorization is repeated for singular matrices,1684
779,What is the result of torch.lu_unpack()?,Warning,1684
780,What is not available for CPU?,LU factorization with pivot = False,4060
781,What does LU factorization with pivot = False not do?,check if the factorization was successful or not if get_infos is True,4060
782,What is the default setting for get_infos?,False,4060
783,What is the default setting for LU factorization with pivot = False?,False,4060
784,What does the function do to a matrix or batches of matrices A?,Computes the LU factorization,1683
785,"If pivot is False, the returned pivots is a what?",tensor,7222
786,What is the default value for pivoting?,True,7222
787,What is the default setting for pivoting?,True,7222
788,"If pivot is False, what are the returned pivots?",a tensor filled with zeros of the appropriate size,7222
789,"A (Tensor) is what to factor of size (,m,n)(*, m, n)(",tensor,7222
790,What is LU factorization with pivot = False available for CUDA?,Note,7221
791,What does this function not check if the factorization was successful or not if get_infos is True?,Note,4059
792,What library has a bug that causes the LU factorization to be repeated for singular matrices?,MAGMA library,7528
793,What happens in batches of square matrices with size less than 32 on a CUDA device?,the LU factorization is repeated for singular matrices,7528
794,What is the message that the function does not check if the factorization was successful or not?,Warning,7528
795,What is the name of the bug in the MAGMA library?,Warning,7528
796,What does get_infos not check if the factorization was successful or not if get_infos is True?,the status of the factorization is present in the third element of the return tuple,7529
797,What is the default value of get_infos?,None,7529
798,Why is the LU factorization repeated in batches of square matrices with size less than 32 on a CUDA device?,the LU factorization is repeated for singular matrices,7529
799,What is the result of the bug in the MAGMA library?,Warning,4416
800,Why is the LU factorization repeated for batches of square matrices with size less than 32 on a CUDA device?,MAGMA library,3770
801,"What does False out (tuple, optional) return?",optional output tuple,3770
802,What factorization is repeated in batches of square matrices with size less than 32 on a CUDA device?,LU,3770
803,What does the LU factorization have?,backward support,6927
804,"The LU factorization does have backward support, but only for square inputs of what?",full rank,6927
805,"What is False out (tuple, optional)?",optional output tuple,6927
806,What controls whether pivoting is done?,pivot,10421
807,What returns an info IntTensor if set to True?,True get_infos,10421
808,"What if set to True, returns an info IntTensor?",True get_infos,10421
809,"If get_infos is True, then the elements in the tuple are Tensor, IntTensor.",If get_infos is False,10421
810,What does pivots store?,all the intermediate transpositions of rows,10421
811,What is repeated in batches of square matrices with size less than 32 on a CUDA device?,LU factorization,3769
812,"A (Tensor) – the tensor to factor of size (,m,n)(*, m,",pivoting,3769
813,What is the tensor to factor of size?,A,758
814,What does pivot do?,controls whether pivoting is done,758
815,"If get_infos is True, then the elements in the tuple are Tensor, IntTensor?",If get_infos is False,758
816,"What is a tuple of tensors containing Tensor, IntTensor, and IntT",optional output tuple,757
817,What is the default value of a tuple of tensors containing a factor of size?,None,757
818,What is the default value for a tuple of tensors containing tensors?,None,757
819,"If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and In",False,10344
820,What does a tuple of tensors contain?,factorization,10344
821,What type of tuple is out?,optional output,10344
822,"If get_infos is what, then the elements in the tuple are Tensor, IntTensor?",False,10344
823,What stores all the intermediate transpositions of rows?,pivots,10344
824,The final permutation perm could be reconstructed by applying what?,"swap(perm[i], perm[pivots[i] - 1]",10344
825,"What utation perm could be reconstructed by applying swap(perm[i], perm[pivots[i]",perm,10344
826,"If get_infos is True, this is a tensor of size ()(*)() where non-zer",if get_infos is True,10344
827,"Out (tuple, optional) – what?",optional output tuple,10344
828,What is False out?,optional output tuple,9460
829,What are the elements in a tuple if get_infos is True?,"Tensor, IntTensor, and IntTensor",9460
830,"If get_infos is set to what, returns an info IntTensor?",True,9460
831,"If get_infos is True, what is infos?",if get_infos is True,9460
832,What is the default value for a tuple of tensors containing factorization?,None,4389
833,LU factorization with pivot = False is available for what?,CUDA,4389
834,What happens when batches of square matrices with size less than 32 are on a CUDA device?,LU factorization is repeated for singular matrices,4389
835,"What does pivot (bool, optional) do?",controls whether pivoting is done,4389
836,What are the elements in a tuple if get_infos is False?,"Tensor, IntTensor",9459
837,"If get_infos is what, the elements in the tuple are Tensor, IntTensor, and Int",True,10343
838,"If get_infos is what, then the elements in the tuple are Tensor, IntTensor, and In",False,10343
839,"Out (tuple, optional) – what type of output tuple?",optional output tuple,10343
840,What is a tuple of tensors?,tuple of tensors,873
841,What is the factorization of size?,factorization,9328
842,"What is the factorization of size (,m,n)(*, m, n)(,m,n)?",factorization,9328
843,What are the pivots of?,size,10422
844,What are the pivots of size?,pivots,10422
845,"Infos (IntTensor, IntTensor, IntTensor) is what?",optional,9616
846,What are both inputitextinput_iinputi and otheritextother_iotheri?,weakly positive,1801
847,What does torch.igammac() support?,broadcasting to a common shape and float inputs,1801
848,"If both inputitextinput_iinputi and otheritextother_iotheri are negative, what",outi,11372
849,What is the gamma function in the equation above?,gamma function,11372
850,What is the name of the function that supports broadcasting to a common shape and float inputs?,Note,11372
851,If both inputitextinput_iinputi and otheritextother_iotheri are what?,weakly positive,11372
852,If both inputitextinput_iinputi and otheritextother_iotheri are weakly positive,zero,11372
853,What does the gamma function support?,broadcasting to a common shape and float inputs,11372
854,What are related functions to the gamma function?,torch.igammac() and torch.lgamma(),11372
855,What is supported to a common shape and float inputs?,broadcasting,5787
856,What are two functions that support broadcasting to a common shape and float inputs?,torch.igammac() and torch.lgamma(),5787
857,What pass with respect to input is not yet supported?,backward,5787
858,Where can you open an issue with regards to the backward pass with respect to input?,PyTorch’s Github,5787
859,What is the second non-negative input tensor out?,output tensor,5787
860,What does torch.load() load an object saved with torch.save() from?,a file,4131
861,What happens when a storage is moved to the device it was saved from?,fails,4131
862,What do storages underlie?,tensors,11086
863,Where are storages first deserialized?,the CPU,11086
864,Why does torch.load() fail?,run time system doesn’t have certain devices,11086
865,What argument allows storages to be dynamically remapped to an alternative set of devices?,map_location,11086
866,What Python facility does torch.load() use?,unpickling,11086
867,What happens to storages when they are deserialized on the CPU and moved to the device they were saved from?,If this fails,11086
868,What indicates the location where all tensors should be loaded?,map_location is a torch.device object or a string containing a device tag,3451
869,"If map_location is a torch.device object or a string containing a device tag, it will be used to remap",a dict,3451
870,Map_location indicates the location where all what should be loaded?,tensors,3451
871,What is the name of the warning message that is passed over to pickle_module.load() and pickle_module.Unpickler,Warning,3451
872,"If map_location is a dict, it will be used to do what?",remap location tags appearing in the file,3450
873,How can user extensions register their own location tags and tagging and deserialization methods?,torch.serialization.register_package(),3450
874,"If map_location is a what, it indicates the location where all tensors should be loaded?",torch.device object or a string containing a device tag,3448
875,What identifies the device it was saved from?,location tag,3448
876,What type of tensors are 'cuda:device_id'?,CUDA,3448
877,What should map_location return?,None or a storage,3448
878,"If map_location returns None or a storage, it will be used as the final deserialized object, already moved to the right device?",a storage,3448
879,What will fall back to the default behavior if map_location wasn't specified?,torch.load(),3448
880,"If map_location is a string containing a device tag, it indicates the location where all tensors should be loaded?",torch.device object or a string containing a device tag,3448
881,What can user extensions use to register their own location tags and deserialization methods?,torch.serialization.register_package(),4695
882,How can user extensions register their own location tags and deserialization methods?,torch.serialization.register_package(),4695
883,What must a file-like object implement?,"read(), readline(), tell(), and seek()",4695
884,"If map_location is a what, it will be used to remap location tags appearing in the file?",dict,4695
885,What will be used to remap location tags appearing in a file?,map_location,4696
886,What is f?,a file-like object,4696
887,Who can register their own location tags and deserialization methods using torch.serialization.register_package()?,User extensions,4696
888,What will map_location be used to remap location tags appearing in the file?,if map_location is a dict,4696
889,"What type of object has to implement read(), readline(), tell(), and seek()?",a file-like object,9325
890,"What is a file-like object that has to implement read(), readline(), tell(), and seek()?",f,3449
891,How many arguments does map_location have?,two,3449
892,What is the storage argument?,initial deserialization of the storage,3449
893,What are the builtin location tags for CPU tensors?,'cpu',3449
894,What does user extensions use to register their own location tags and tagging and deserialization methods?,torch.serialization.register_package(),8095
895,What type of object is f?,a file-like object,8095
896,What can contain a file name map_location?,a string or os.PathLike object,8094
897,What can user extensions use to register their own location tags and tagging and deserialization methods?,torch.serialization.register_package(),8094
898,What is the pickle_module module used for?,unpickling metadata and objects,9871
899,What is the name of the warning message sent to pickle_module.load() and pickle_module.Unpickler()?,Warning,9871
900,What does map_location specify?,remap storage locations,9871
901,In what language are pickle_load_args used?,Python 3,9871
902,What is the name of the warning that is passed over to pickle_module.load() and pickle_module.Unpickler(),Warning,10409
903,What module is used for unpickling metadata and objects?,pickle_module,10409
904,Pickle_load_args – optional keyword arguments passed over to pickle_module.load() and pickle_module.Un,Python 3,10409
905,What is passed over to pickle_module.load() and pickle_module.Unpickler()?,optional keyword arguments,10409
906,What are optional keyword arguments passed over to?,pickle_module.load() and pickle_module.Unpickler(),10407
907,"Warning torch.load() uses pickle module implicitly, which is known to be what?",insecure,10407
908,What should you do when loading data that you trust?,Note,10407
909,Do you load data that could have come from an untrusted source or that could have been tampered with?,"Never load data that could have come from an untrusted source, or that could have been tampered with",11088
910,What does torch.load() do?,Only load data you trust,11088
911,What does torch.load() load when a file contains GPU?,tensors,11088
912,What should you do to avoid GPU RAM surge when loading a model checkpoint?,Note,11088
913,What should you never load data that could have come from an untrusted source or that could have been tampered with?,"Never load data that could have come from an untrusted source, or that could have been tampered with",10408
914,What uses pickle module implicitly?,torch.load(),11087
915,What is the pickle module known to be?,insecure,11087
916,"What does torch.load(.., map_location='cpu') mean?",map_location='cpu',8390
917,"By default, we decode byte strings as what?",utf-8,4451
918,What is an example of a byte array that can be decoded later with byte_array.decode(...)?,Example,4451
919,What is called when a file contains GPU tensors?,torch.load(),4451
920,What do you want to avoid when loading a model checkpoint?,GPU RAM surge,4451
921,What is a common error case when decoding byte strings as utf-8?,UnicodeDecodeError,4451
922,What decodes strings using latin1 encoding?,encoding='latin1',4451
923,What is a common error case?,UnicodeDecodeError,4359
924,What decodes byte strings using latin1 encoding?,encoding='latin1',4359
925,The median is not unique for what?,input tensors with an even number of elements,4407
926,Do not expect the same result when run on what?,CPU and GPU,4407
927,Int – the dimension to reduce. keepdim (bool) – whether the output tensor has dim retained or not.,dim,4407
928,"Which tensor will be populated with the median values and the second tensor, which must have dtype long, with their",out,4407
929,What is an example of a tensor that is populated with the median values and the second tensor?,Example,4407
930,The median is not unique for input tensors with an even number of elements.,lower,7173
931,What type of subgradients does torch.quantile() produce?,deterministic,7173
932,What is the value of the output tensors if keepdim is True?,Note,7173
933,What function computes the mean of both medians?,torch.quantile(),5630
934,"If dim is squeezed, outputs tensors have what?",1 fewer dimension than input,7553
935,What is the median for input tensors with an even number of elements in the dimension dim?,not unique,5313
936,Which of the two medians is returned if the input tensors have an even number of elements in the dimension dim?,lower,5313
937,"If dim is squeezed, output tensors have how many dimension less than input?",1,5313
938,What is returned if the input tensors have an even number of elements in the dimension dim?,lower of the two medians,1489
939,"If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of what",1,1489
940,What is returned if the input tensor has an even number of elements in the dimension dim?,lower of the two medians,9622
941,Do not expect the gradients to be what?,deterministic,3443
942,What is returned if the output tensors have an even number of elements in the dimension dim?,lower of the two medians,3443
943,Where does PyTorch transfer AlexNet from PyTorch to?,ONNX,2310
944,Tracing vs what type of annotations Write PyTorch model in Torch way Avoid using numpy Avoid using.data,Scripting,7983
945,What do you write PyTorch model in Torch way Avoid using numpy Avoid using.data field?,Type Annotations,8033
946,Using dictionaries to handle what as model inputs Avoid using numpy Avoid using.data field Using dictionaries to handle what,Named Arguments,1391
947,What is the name of the Operator Export Type ONNX?,Operator Export Type ONNX,953
948,What Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_F,Custom operators,2060
949,What is the name of the program that exports a pretrained AlexNet into ONNX?,torchvision,8083
950,What is an example of an external data format?,ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH,4572
951,What is the name of the external data format Training Functions?,ONNX_FALLTHROUGH,4574
952,"In the future, there will be what as well?",backends for other frameworks,2783
953,What will there be in the future for other frameworks as well?,backends,2783
954,What does this script export into ONNX?,a pretrained AlexNet,2783
955,What will you need to install to run the exported script?,caffe2,7917
956,What is a trace likely to be valid only for?,a specific input size,7917
957,What is the backend for the exported model?,ONNX Runtime,8568
958,A trace is likely to be valid only for what?,a specific input size,8568
959,What is the backend for the exported script?,Caffe2,8568
960,What is another tutorial for?,exporting the SuperResolution model to ONNX,2787
961,"What means that ONNX operates by executing your model once, and exporting the operators which were actually run during this run?",trace-based,4645
962,What does a dynamic model do?,changes behavior depending on input data,4645
963,"If you want to export your model with dynamic control flows, you will need to use what?",script-based exporter,4645
964,What happens when a model is dynamic?,changes behavior depending on input data,4645
965,What will there be in the future?,backends for other frameworks,4645
966,What do we recommend examining the model trace and making sure?,traced operators look reasonable,4645
967,What will unroll the loops and if conditions?,trace-based exporter,4645
968,What can you do to suit the particular requirements of a part of a model?,compose tracing and scripting,4645
969,What is the name of the exporter that unrolls the for loop?,trace-based exporter,4645
970,What is another tutorial for using the ONNX backend?,exporting the SuperResolution model to ONNX,4645
971,What two things do we allow mixing?,tracing and scripting,4645
972,What look reasonable in a model trace?,traced operators,8563
973,"In the future, there will be what?",backends for other frameworks,8563
974,What do we allow mixing?,tracing and scripting,8563
975,What can you run the exported model with?,ONNX Runtime,8563
976,What is another tutorial for ONNX Runtime?,exporting the SuperResolution model to ONNX,8563
977,Which exporter will unroll loops and if conditions?,trace-based exporter,7258
978,What is a binary protobuf file that contains both the network structure and parameters of the model you exported?,alexnet.onnx,7258
979,What looks reasonable in a model trace?,traced operators,3777
980,What can the ONNX exporter be?,both trace-based and script-based exporter,3777
981,What does a trace-based exporter do if your model contains control flows like for loops and if conditions?,unroll the loops and if conditions,6931
982,"What means that it operates by executing your model once, and exporting the operators which were actually run during this run?",trace-based,11246
983,What does a trace-based exporter do if your model contains control flows?,unroll the loops and if conditions,11246
984,What are the torch tensors and torch operators?,torch.concat,4969
985,What layer need to be defined in init function so that inferencing can handle it properly?,Dropout layer,4969
986,What can the use of the.data field produce?,incorrect trace graph,4969
987,What is the name of the torch operators?,torch.concat,4968
988,What is the latest version of opset?,opset,6193
989,What is below the list of supported patterns for RHS indexing?,unsupported patterns,6193
990,What is below the list of supported patterns for LHS indexing?,unsupported patterns,6193
991,"Users need to verify their dict inputs carefully, and keep in mind that what is not available?",dynamic lookups,4669
992,Which backends often have implementations of operators with some numeric differences?,PyTorch and ONNX backends,4669
993,FeatureDropout (not supported) Index MaxPool1d MaxPool2d MaxPool3d,training mode,2408
994,What is pixel_shuffle pow?,pixel_shuffle pow,3841
995,What is not supported in pixel_shuffle pow prelu?,single weight shared among input channels,4216
996,What type of narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones,multinomial,8804
997,What is the single weight shared among input channels not supported?,pixel_shuffle pow prelu,8790
998,What is sigmoid sign sin size slice softmax?,sigmoid sign sin size slice softmax,8836
999,What is a sin size slice softmax softplus?,sigmoid sign,8859
1000,What is argmax argmin?,arange,8859
1001,What is sigmoid sign sin size slice softmax softplus sort?,sigmoid sign sin size slice softmax softplus sort,8874
1002,What is a sin size slice softmax softplus sort split sqrt squeeze?,sigmoid sign,8905
1003,What avg_pool3d is as_strided baddbmm bitshift cat ceil?,avg_pool3d,8909
1004,What is the name of the sigmoid sign?,t tan,8902
1005,What is tan tanh?,tan tanh,9053
1006,What is the term for tanh?,tan tanh,9029
1007,"To add export support for operators, developers need to touch the source code of what?",PyTorch,950
1008,What should be easy to add support for exporting if the wanted operator is standardized in ONNX?,adding a symbolic function for the operator,950
1009,"If the operator is standardized in ONNX, what is the operator defined in VariableType.h?",ATen,950
1010,How do developers install PyTorch from source?,follow the instructions,951
1011,Adding export support for operators is an what?,advance usage,951
1012,Developers need to touch the source code of what?,PyTorch,951
1013,What should be easy to add support for exporting an operator?,adding a symbolic function for the operator,951
1014,"In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent what in",ONNX operator,951
1015,What does the symbolic function have to be added in the corresponding PyTorch Function class?,Create a symbolic,951
1016,"If the wanted operator is what in ONNX, it should be easy to add support for exporting such operator?",standardized,7859
1017,What is the name of the operator defined in VariableType.h?,ATen,7859
1018,What type of operator is the operator?,ATen operator,3509
1019,The first parameter is always the exported what graph?,ONNX,3509
1020,What must Parameter names EXACTLY match the names in VariableType.h?,Parameter names must EXACTLY match the names in VariableType.h,3509
1021,"Inputs are always first, then non-tensor arguments.",tensors,3509
1022,Where is the ATen operator/function defined?,VariableType.h,3509
1023,What is the name of the symbolic function in the corresponding Function class?,Create a symbolic function named symbolic,3494
1024,"In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ON",if the operator is already standardized in ONNX,3793
1025,"In the symbolic function, if the operator is already standardized in ONNX, we just need to do what?",create a node to represent the ONNX operator in the graph,3793
1026,"If the operator is a non-ATen operator, the symbolic function has to be added in what?",PyTorch Function class,3502
1027,What type of function is created in the corresponding PyTorch Function class?,symbolic,3502
1028,What do we try to do with missing symbolic function for elu operator?,export the model,3502
1029,"In the symbolic function, if the operator is already standardized in ONNX, what do we need to do to represent the ONNX operator",create a node,1943
1030,What do we try to do with the missing symbolic function for elu operator?,export the model,1943
1031,What do we try to do with the missing symbolic function?,export the model,4729
1032,Why does the export fail?,PyTorch does not support exporting elu operator,3503
1033,What operator does PyTorch not support exporting?,virtual Tensor elu,3503
1034,Who is able to export elu operator?,PyTorch,3503
1035,What are some examples of PyTorch's elu operator?,"symbolic_opset9.py, symbolic_opset10.py",3503
1036,What operator is standardized in ONNX?,Elu,7063
1037,What is the const override in VariableType.h?,virtual Tensor elu,6167
1038,What is another example of PyTorch exporting elu operator?,symbolic_opset10.py,6167
1039,What is PyTorch able to export?,elu operator,4534
1040,PyTorch is able to export elu operator in what languages?,"symbolic_opset9.py, symbolic_opset10.py",4534
1041,What can you specify when exporting a custom opset?,custom domain and version,4534
1042,What tutorial will allow you to create and register your own custom ops implementation in PyTorch?,Extending TorchScript with Custom C++ Operators,3504
1043,Where can you export your own custom ops implementation?,ONNX,2480
1044,What can you export a custom operator as?,one or a combination of existing ONNX ops,2480
1045,What can you export the model as in ONNX?,a custom op,2480
1046,"If not explicitly specified, the custom opset version is set to what by default?",1,2480
1047,What will you need to extend the backend of your choice with matching custom ops implementation?,custom ONNX ops,2480
1048,What is an example of a custom ops implementation?,Caffe2,2480
1049,Where can you export a custom ops implementation in PyTorch?,ONNX,2480
1050,What do you use to specify the custom domain and version?,custom_opsets dictionary,2480
1051,What operator does PyTorch not support exporting elu?,virtual Tensor elu,6935
1052,What are examples of PyTorch's exporting elu operator?,"symbolic_opset9.py, symbolic_opset10.py",6935
1053,Who will try to handle implicit scalar datatype casting?,the exporter,5034
1054,What is ONNX trying to do with scripted models?,improve the datatype propagation in the exporter,5034
1055,What version of ONNX supports data[index] = new_data?,ONNX opset version >= 11,5034
1056,What will you need to do if the exporter fails to figure out the right datatype for scalars?,manually provide the datatype information,4320
1057,What version of ONNX supports tensor in-place indexed assignment?,ONNX opset,4320
1058,What are we trying to do to prevent manual changes in the future?,improve the datatype propagation in the exporter,4320
1059,What type of models are not recorded in ONNX?,scripted models,6096
1060,What argument in export API enables export of models in ONNX external data format?,use_external_data_format,11317
1061,What type of models can be exported to ONNX with the use_external_data_format argument?,large models,11317
1062,Models larger than what size cannot be exported in one file because of the protobuf size limit?,2GB,11317
1063,Export a model into what format?,ONNX format,2353
1064,How often does the ONNX format exporter run a model?,runs your model once,2353
1065,What does model (torch.nn.Module) represent?,the model to be exported,2353
1066,The last value of a tuple consisting of named parameters and the corresponding inputs are structured as what?,key-value pairs,2353
1067,What does a TUPLE OF ARGUEMENTS contain?,A DICTIONARY OF NAMED PARAMETERS,8886
1068,What will become inputs of the exported model?,any Tensor arguments,2352
1069,"If args is a Tensor, this is equivalent to having called it with what?",1-ary tuple,7141
1070,"If dictionary input is the last input of the args tuple, what would happen when a dictionary of named parameters is used?",a conflict,7141
1071,What is placed to provide an empty dictionary as the last input in the tuple args in such cases?,constraint,7141
1072,What type of arguments will be hard-coded into the exported model?,non-Tensor,7141
1073,The inputs to the model are structured as what?,key-value pairs,7141
1074,What is the return value of Model() k?,x m,7141
1075,What is the name of the file-like object that has to implement fileno that returns a file descriptor?,export_params,7141
1076,What are the inputs to the model structured as?,a tuple consisting of non-keyword arguments,7137
1077,What provides an example of a tuple in which dictionary input is the last input of the args tuple?,model,7137
1078,What does model() k mean?,"torch.randn(2, 3) x",7137
1079,What does the model below provide an example of?,return x,7140
1080,What is the last value of a tuple consisting of non-keyword arguments?,DICTIONARY OF NAMED PARAMETERS,7140
1081,What model provides an example of a conflict when a dictionary of named parameters is used?,model below,1533
1082,Cases in which dictionary input is the last input of the args tuple would cause what?,a conflict,1533
1083,Names to assign to the input nodes of the model?,input_names,1533
1084,What provides an example of a tuple with a dictionary of named parameters?,model,776
1085,What does model() k represent?,"torch.randn(2, 3) x",776
1086,"m = Model() k = torch.randn(2, 3) x = torch.tensor(1",x,777
1087,What is provided as the last input in the tuple args?,an empty dictionary,777
1088,What would the new call to export look like?,this,777
1089,"What type of object does torch.onnx.export(model, (k, x, ), ‘test.onnx",a file-like object,777
1090,What model provides an example of a tuple with a dictionary of named parameters?,model below,777
1091,"What is a bool, default True value for a tuple of arguments with a dictionary of named parameters?",export_params,777
1092,The new call would look like what?,this,7848
1093,What will we print out if the trace is exported?,debug description,7848
1094,How would this work?,as intended,7848
1095,What does the export function assume that the x input is intended to represent?,the optional dictionary consisting of named arguments,7848
1096,The constraint is placed to provide what as the last input in the tuple args in such cases?,empty dictionary,7848
1097,"What does model, (k, x, ), ‘test.onnx’ mean?",torch.onnx.export,7848
1098,What does export_raw_ir do?,use operator_export,7848
1099,"What = Model() k = torch.randn(2, 3) x = torch.tensor(1)",m,9860
1100,What is the last input in the tuple args?,provide an empty dictionary,7847
1101,What would happen if the export function assumed that the x input was intended to represent the optional dictionary consisting of named arguments?,would work as intended,7847
1102,"What is the name of the export function that if specified, all parameters will be exported?",export_params,7847
1103,What is the name of the export function?,export_params,3789
1104,What does model.state_dict().values() specify?,model.state_dict().values(),3789
1105,What does f represent?,a file-like object,11180
1106,What is the name of the export function if all parameters are specified?,export_params,11180
1107,What would the export function assume that the x input is intended to represent?,the optional dictionary consisting of named arguments,11180
1108,"Training (enum, default) – TrainingMode.EVAL: export the model in inference mode.",TrainingMode.EVAL,11183
1109,What exports the model in inference mode if model.training is False and to a training friendly mode if model.training is,TrainingMode.PRESERVE,11183
1110,What mode is used to export a model in a training friendly mode?,TrainingMode.TRAINING,11183
1111,"What if specified, all parameters will be exported. Set this to False if you want to export an untrained model?",export_params,9323
1112,What is the name of the training friendly mode that the model is exported in?,TrainingMode.TRAINING,9298
1113,All parameters will be exported if specified?,export_params,9298
1114,"If model.training is set to what, the model will be exported in inference mode?",False,9298
1115,"If model.training is true, what is the default?",False,11338
1116,What mode is used to export the model in a training friendly mode?,TrainingMode.TRAINING,11338
1117,Export raw_ir directly instead of converting it to what?,ONNX ops,11338
1118,What exports the internal IR directly instead of converting it to ONNX ops?,export_raw_ir,9302
1119,What is exported as:,Example graph,9302
1120,What type of ir is exported?,OperatorExportTypes.RAW,9302
1121,What is the name of the mode that can be exported and implemented by the user for their runtime backend?,OperatorExportTypes.ONNX_FALLTHROUGH,9302
1122,"If model.training is what, what is the default value for training mode?",False,11253
1123,What is the default TrainingMode.PRESERVE?,TrainingMode.TRAINING,11253
1124,What does export_raw_ir convert the internal IR directly instead of converting it to?,ONNX ops,11253
1125,What does export the internal IR directly instead of converting it to ONNX ops?,operator_export_type,11253
1126,"Names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) – names",input_names,11494
1127,What is the default False value for the internal IR exported by the functions in symbolic_opsetversion>.py?,export_raw_ir,9678
1128,"input_names (list of strings, what list) – names to assign to the input nodes of the graph?",default empty list,9678
1129,"Operator_export_type (enum, default) – OperatorExportTypes.ONNX: All ops are exported as regular",OperatorExportTypes.ONNX,10311
1130,What is another name for OperatorExportTypes.ONNX_FALLBACK?,OperatorExportTypes.ONNX_FALLTHROUGH,10311
1131,What does OperatorExportTypes.ONNX call the raw ir?,OperatorExportTypes.RAW,10311
1132,What is the default False value of the model exported in aten mode?,aten,8914
1133,Who can implement a custom ONNX op?,the user,3765
1134,What can be exported and implemented by the user for their runtime backend?,OperatorExportTypes.ONNX_FALLTHROUGH,3765
1135,What is the name of the ir that can be exported in ONNX?,OperatorExportTypes.RAW,3765
1136,What does OperatorExportTypes.ONNX use to export raw ir?,OperatorExportTypes.RAW,10310
1137,What is not supported in the above example graph?,prim::ListConstruct,10310
1138,What is the namespace for ONNX ops?,OperatorExportTypes.ONNX,4675
1139,What is the name of the ir that is exported?,OperatorExportTypes.RAW,4675
1140,What is another name for OperatorExportTypes.ONNX_ATEN_FALLBACK?,OperatorExportTypes.ONNX_FALLTHROUGH,4675
1141,What type of op can be exported as a custom ONNX op?,OperatorExportTypes.RAW,9700
1142,What is the default for ONNX's latest opset?,one stable opset version,9702
1143,"What will replace some of the ops that have all constant inputs, with pre-computed constant nodes?",Constant-folding optimization,3766
1144,What does OperatorExportTypes.RAW do?,Export raw ir,3766
1145,What is the default version of the onnx submodule?,opset_version,10314
1146,What is applied to the model during export?,do_constant_folding,9196
1147,What does strip_doc_string do?,"if True, strips the field “doc_string” from the exported model",9196
1148,What is the name of the model's example outputs being exported?,example_outputs,10315
1149,What field is stripped from the exported model?,dynamic_axes,10315
1150,"If there is more than one item, it should be passed in tuple format, e.g. what = (x, y",example_outputs,10315
1151,"If there is more than one item, what should be passed as the example output?",only one item,10315
1152,"When does strip_doc_string (bool, default True) strip the field ""doc_string"" from the exported model?",if True,10315
1153,What does this allow for backends/runtimes that execute these graphs?,better optimizations,3370
1154,"If true, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph.",If True,3370
1155,Who executes graphs that are not initialized as inputs?,backends/runtimes,3370
1156,Who executes dynamic axes?,backends/runtimes,7388
1157,What are not added as inputs to the graph if False?,initializers,7388
1158,"If unspecified, the behavior is chosen automatically as follows.",default None,4169
1159,What is operator_export_type?,OperatorExportTypes,4169
1160,"If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to what?",setting this argument to True,4169
1161,What must be part of graph inputs for ONNX opset version  9?,initializers,4169
1162,Who executes graphs if initializers are not added as inputs?,backends/runtimes,4169
1163,"If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to what?",True,4169
1164,"If opset_version argument is set to a 8 or lower, what argument will be ignored?",if opset_version argument is set to a 8 or lower,4169
1165,What is a dictionary to indicate custom opset domain and version at export?,custom_opsets,4169
1166,"If model contains a custom opset, it is optional to specify the domain and what in the dictionary?",opset version,4169
1167,What will be run as part of the export to ensure the exported model is a valid ONNX model?,enable_onnx_checker,4169
1168,What is the onnx model checker run as part of the export?,ensure the exported model is a valid ONNX model,4169
1169,What does this allow for backends/runtimes that execute graphs?,better optimizations,4169
1170,"If ONNX opset version  9, what argument will be ignored?",opset_version argument is set to a 8 or lower,4168
1171,"If unspecified, what is the default?",default None,4168
1172,"If true, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs?",If False,4168
1173,Who executes the graphs?,backends/runtimes,4564
1174,"If unspecified, what is the behavior chosen automatically as follows?",default None,3369
1175,The torch package contains data structures for what?,multi-dimensional tensors,7333
1176,If the data type of input is a what?,complex data type,7333
1177,What efault floating point dtype is set to d?,d,7333
1178,What is the default torch.Tensor type to floating point tensor type t?,t,7333
1179,Returns True if the data type of input is a floating point data type?,if the data type of input is a floating point data type,5229
1180,Returns True if the input is a single element tensor?,if the input is a single element tensor,5229
1181,Sets what efault floating point dtype to d?,d,5229
1182,What does a tensor contain?,data,5241
1183,What does torch.Tensor type return?,the total number of elements in the input tensor,2695
1184,What does the data in the sparse tensor in COO(rdinate) format convert into?,torch.Tensor,2695
1185,"What is created with specified size, stride and storage_offset?",a view of an existing torch.Tensor input,2695
1186,What is the current floating point torch.dtype?,current default,2695
1187,What type of tensor of size steps is created?,one-dimensional,4395
1188,Constructs a tensor with what?,data,4395
1189,Create a view of an existing torch.Tensor input with what?,"specified size, stride and storage_offset",4395
1190,Returns a tensor filled with what value 1 with the same size as input?,scalar,4395
1191,What is created whose values are evenly spaced from basestarttext?,a one-dimensional tensor of size steps,4395
1192,What fills a tensor of size size filled with?,fill_value,2010
1193,Returns a tensor with the same size as input filled with what?,fill_value,2010
1194,Returns a 2-D tensor with what values?,ones on the diagonal and zeros elsewhere,2010
1195,What happens when horizontally stacking the tensors in tensors?,Creates a new tensor,1818
1196,"Splits input, a tensor with one or more dimensions, into what horizontally according to indices_or_sections?",multiple tensors,1818
1197,What happens to the given sequence of seq tensors in the given dimension?,Concatenates,1818
1198,Gathers values along what axis specified by dim. Splits input into multiple tensors horizontally according to indices_or,axis,1998
1199,What is used to split a tensor into multiple tensors?,indices_or_sections,1998
1200,What is an out-of-place version of torch.Tensor.scatter_()?,Alias of torch.vstack(),1998
1201,What happens to a tensor with one or more dimensions?,Splits input,1998
1202,What kind of version of the input tensor does torch.movedim() return?,narrowed,1998
1203,What type of tensor does torch.movedim() return?,1-D,1998
1204,What is the version of torch.Tensor.scatter_()?,Out-of-place,1998
1205,How are tensors stacked horizontally?,column wise,6058
1206,What axis is specified by dim?,axis,6058
1207,Splits input into what according to indices_or_sections?,multiple tensors horizontally,6023
1208,What is used to split a tensor into multiple tensors depthwise?,indices_or_sections,6023
1209,What version of torch.Tensor.scatter_() Splits the tensor into chunks?,Out-of-place,5469
1210,Returns a tensor with all the dimensions of input of what size removed?,size 1,5469
1211,Returns a new tensor with the elements of input at the given what?,indices,5469
1212,Selects values from input at what indices from indices along the given dim?,1-dimensional,5469
1213,Out-of-place version of torch.Tensor.scatter_add_() Splits the tensor into what?,chunks,5469
1214,What does torch.transpose() do?,Selects values from input at the 1-dimensional indices from indices along the given dim,5469
1215,What is used for torch.transpose()?,Alias,5469
1216,What is another name for torch.transpose()?,Alias for torch.transpose(),5469
1217,"What is returned with the same data and number of elements as input, but with the specified shape?",a tensor,5469
1218,Gathers values along what axis specified by dim?,axis,2670
1219,What does it do to split a tensor in sequence horizontally?,Stack tensors,2670
1220,What specifies the axis of a tensor?,dim,2670
1221,What is used to split input into multiple tensors horizontally?,indices_or_sections,2670
1222,What is the name of the tensor that indexes the input tensor along dimension dim?,LongTensor,2670
1223,What dimensions does torch.transpose() transpose?,0 and 1,1032
1224,What does the tensor return with the same data and number of elements as input but with?,the specified shape,1032
1225,A tensor with all the dimensions of input of what size is removed?,size 1,1032
1226,From what distribution are binary random numbers drawn?,Bernoulli,5855
1227,Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean what?,0 and variance 1,5855
1228,What returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the,a tensor filled with random numbers,5572
1229,What does ByteTensor do?,Sets the random number generator state,5572
1230,Where are binary random numbers drawn from?,a Bernoulli distribution,2207
1231,Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (what?,exclusive,2207
1232,What is sampled from the multinomial probability distribution located in the corresponding row of tensor input?,num_samples indices,5463
1233,Random integers generated uniformly between low (inclusive) and high (exclusive) Returns a tensor with the same shape as input,exclusive,5463
1234,Returns a tensor of the same size as input with each element sampled from what distribution with rate parameter given by the corresponding element,Poisson,5462
1235,Returns a tensor of random numbers drawn from separate normal distributions whose what are given?,mean and standard deviation,5462
1236,What is returned from a uniform distribution on the interval?,a tensor filled with random numbers,5453
1237,What is returned when a tensor input is filled with random numbers from a normal distribution with mean 0 and variance 1?,a tensor with the same size,5414
1238,Tensor.geometric_() - elements drawn from what distribution?,the geometric distribution,5414
1239,Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive)?,exclusive,5414
1240,What is filled with random integers generated uniformly between low (inclusive) and high (exclusive)?,tensor,5414
1241,Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive),exclusive,5414
1242,What returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance,a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.,5414
1243,What is returned when input is filled with random numbers from a normal distribution with mean 0 and variance 1?,a tensor with the same size,5475
1244,What distribution does tensor.log_normal_() sample from?,log-normal,5475
1245,Returns a tensor with the same shape as input filled with random integers generated uniformly between low (inclusive) and high (,exclusive,5475
1246,Alias for torch.acos(). Returns a new tensor with what cosine of the elements of input?,inverse hyperbolic,1006
1247,Which function returns a new tensor with the inverse hyperbolic cosine of the elements of input?,Alias for torch.acos(),1006
1248,What is the tensor with the arctangent of the elements of input?,inverse hyperbolic tangent,1006
1249,What adds the scalar other to each element of the input and returns a new resulting tensor?,Alias for torch.acosh(),1692
1250,Alias for what function returns a new tensor with the arctangent of the elements of input?,torch.atan(),1692
1251,What does Alias for torch.abs() compute of each element in input?,absolute value,1692
1252,What is the value of each element in input?,absolute value,1692
1253,Performs what element-wise division of tensor1 by tensor2?,multiplication,969
1254,What does it do to return a new resulting tensor?,Adds the scalar other to each element of the input input,969
1255,Which function returns a new tensor with the inverse hyperbolic sine of the elements of input?,Alias for torch.asin(),5370
1256,What does Alias for computes the element-wise conjugate of the given input tensor?,torch.clamp(),5370
1257,What does Alias for return a new tensor with the inverse hyperbolic tangent of the elements of input?,torch.atanh(),1012
1258,What is added to each element of the input input and returns a new resulting tensor?,scalar,1012
1259,The element-wise arctangent of inputi/otheritextinputi / textother_i,quadrant,1012
1260,Computes the bitwise XOR of input and other. Computes the bitwise OR of input and other. Computes the what,bitwise XOR,1012
1261,What does it do to all elements in input into the range?,Clamps all elements in input into the range,1012
1262,What code computes the element-wise conjugate of the given input tensor?,torch.clamp(),1012
1263,Performs the element-wise division of what?,"tensor1 by tensor2, multiply the result by the scalar value and add it to input",1012
1264,What does Computes the bitwise AND of input and other?,Computes the bitwise AND,4814
1265,Performs the element-wise division of what by tensor2?,tensor1,4814
1266,What does the element-wise division of tensor1 by tensor2 multiply the result by?,scalar value,4814
1267,Performs the element-wise multiplication of what by tensor2?,tensor1,4814
1268,The element-wise arctangent of inputi/otheritextinput_i / textother,the quadrant,4814
1269,What does Computes the bitwise OR of input and other?,Computes the bitwise OR of input and other,5395
1270,Computes the bitwise what of input and other?,XOR,5395
1271,What is the element-wise arctangent of inputi/otheritextinput_i / text,Element-wise arctangent,5395
1272,What is the name of the function that computes the element-wise conjugate of the given input tensor?,torch.clamp(),5395
1273,What is the arctangent of the elements of input?,inverse hyperbolic tangent,5395
1274,What is the element-wise tangent of inputi/otheritextinput_i / text,arctangent,5395
1275,Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of input and other.,Computes the bitwise AND,4826
1276,Performs the element-wise multiplication of what?,tensor1 by tensor2,4826
1277,What is the result of the element-wise multiplication of tensor1 by tensor2 multiplied by?,scalar value,4826
1278,What Alias for torch.atan(). Returns a new tensor with the inverse hyperbolic tangent of,torch.atanh(),1733
1279,What is the name of the function that clamps all elements in input into the range?,Alias for torch.clamp(),1733
1280,What does torch.clamp() do?,Clamps all elements in input into the range,1021
1281,Computes the what of input and other?,bitwise XOR,1021
1282,Computes the bitwise AND of input and other. Computes the bitwise XOR of input and other.,Computes the bitwise AND of input and other,970
1283,Computes the bitwise OR of input and other. Computes the bitwise XOR of input and other.,Computes the bitwise OR of input and other,970
1284,With consideration of what is the element-wise arctangent of inputi/otheritextinputi / text,quadrant,970
1285,Returns a new tensor with the cosine of the elements of input. Returns a new tensor with the,Returns a new tensor,970
1286,What does Returns the indices of the minimum value(s) of the flattened tensor or along a dimension?,the indices of the minimum value(s) of the flattened tensor or along a dimension,5547
1287,Returns the maximum value of all elements in the input tensor.,minimum value,5547
1288,Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. Returns the mean value,p-norm,5612
1289,Returns the maximum value of all elements in the input tensor. Returns what value of all elements in the input tensor?,minimum value,5558
1290,Returns the maximum value of all elements in the input tensor. Returns the minimum value of each slice of the input tensor,True,5641
1291,Returns the maximum value of all elements in the input tensor. Returns the minimum value of all elements in what?,input tensor,5641
1292,What does Returns the sum of all elements in the input tensor return?,the product of all elements in the input tensor,5641
1293,What is used to test if all elements in input evaluate to True?,input tensor,10983
1294,"This is a variant of what function that ""ignores"" NaN values?",torch.quantile(),5617
1295,What will be used if unbiased is True?,Bessel’s correction,5617
1296,What is the value of the values in input?,median,5617
1297,Returns what value of the values in input?,the median,5617
1298,"If unbiased is True, what will be used to calculate the standard deviation?",Bessel’s correction,5587
1299,What does each row of the input tensor in the given dimension dim return?,the log of summed exponentials,5587
1300,This is a variant of what function that ignores NaN values?,torch.quantile(),5626
1301,Returns the sum of all elements in the input tensor. Eliminates all but the first element from every consecutive group of equivalent elements.,unique elements,5626
1302,"Returns what of the values in input, ignoring NaN values?",the median,5626
1303,"Returns the median of the values in input, ignoring what?",NaN values,5626
1304,What is returned when the sum of all elements in the input tensor is returned?,the product of all elements in the input tensor,5626
1305,"If unbiased is what, Bessel's correction will be used?",True,5626
1306,What happens to every consecutive group of equivalent elements in the input tensor?,Eliminates all but the first element,5628
1307,What does torch.quantile() do?,Eliminates all but the first element from every consecutive group of equivalent elements,5628
1308,Returns what of the values in input?,median,5628
1309,What can be done from provided tensors?,Create a block diagonal matrix,5270
1310,Returns the indices of what to which each value in the input belongs?,buckets,5270
1311,What happens to the given tensors according to Broadcasting semantics?,Broadcasts,1939
1312,Do what product of the given sequence of tensors?,cartesian,1939
1313,What do provided tensors do?,Create a block diagonal matrix,1939
1314,What is the name of the tensor that returns a copy of input?,Compute combinations of length rrr,1939
1315,What shape does broadcasts input to?,shape,1929
1316,What do you do with provided tensors?,Create a block diagonal matrix,1929
1317,What is the value counted in an array of non-negative ints?,frequency,1929
1318,What does broadcast_tensors() return of the buckets to which each value in the input belongs?,the indices,1461
1319,What does it do when it returns a copy of input?,Compute combinations of length rrr of the given tensor,1461
1320,"What is returned when a namedtuple (values, indices) is the cumulative minimum of elements of input in the dimension dim?",the cumulative product of elements of input in the dimension dim,1461
1321,What is returned if input is a vector?,the cumulative sum of elements of input in the dimension dim,1461
1322,What is broadcast_tensors() used for?,shapes,1461
1323,What does broadcast_tensors() return?,a copy of input,1461
1324,What input to the shape shape is similar to broadcast_tensors() but for shapes?,Broadcasts,1461
1325,What does broadcast_tensors() do?,Compute combinations of length rrr of the given tensor,1461
1326,What is the given sequence of tensors?,Do cartesian product,1463
1327,"What is returned when a namedtuple (values, indices) is called?",the cumulative sum of elements of input in the dimension dim,1463
1328,What does broadcast the given tensors according to?,Broadcasting semantics,1463
1329,What does broadcasts input to?,shape shape,1463
1330,What is the name of the given tensor?,Compute combinations of length rrr,1463
1331,What returns values where values is the cumulative minimum of elements of input in the dimension dim?,a namedtuple,1463
1332,What is returned when values is the cumulative minimum of elements of input in the dimension dim?,the cumulative product of elements of input in the dimension dim,1635
1333,What is the function that returns the cross product of vectors in dimension dim of input and other?,Compute combinations of length rrr of the given tensor,1635
1334,Returns what product of input and other?,the cross product of vectors in dimension dim,1635
1335,"If input is a vector, then returns a 2-D square tensor.",1-D tensor,5906
1336,What does return a copy of input do?,Compute combinations of length rrr of the given tensor,5906
1337,What is similar to broadcast_tensors() but for?,shapes,5906
1338,"If input is a vector, then returns a 2-D square tensor Creates a tensor whose diagonals of",1-D tensor,5515
1339,What is returned when a namedtuple returns the cumulative product of elements of input in the dimension dim?,the cumulative sum of elements of input in the dimension dim,5515
1340,What is the difference along the given dimension?,n-th forward,5515
1341,Returns the cross product of vectors in what dimension of input and other?,dimension dim,5515
1342,What returns a partial view of input with its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the,2-D square tensor,5516
1343,Sums the product of the elements of the input operands along dimensions specified using a notation based on what?,Einstein,5516
1344,Flattens input by reshaping it into what?,one-dimensional tensor,5516
1345,"In the left/right direction, return a new tensor. In the up/down direction, return a new tensor",Flip tensor,5516
1346,"What returns a new tensor. Computes the Kronecker product, denoted by otimes?",Flip tensor in the up/down direction,5516
1347,"Computes what product, denoted by otimes, of input and other?",Kronecker,5516
1348,Computes the GCD of input and other. Computes the histogram of a tensor.,element-wise greatest common divisor,5516
1349,"Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create what?",NNN N-dimensional grids,5516
1350,What does NNN N-dimensional grids do?,Compute,5516
1351,Returns what of input and other?,cross product of vectors in dimension dim,5516
1352,What returns the cumulative maximum of elements of input in the dimension dim?,a namedtuple,5516
1353,What happens to the outer-product of vectors vec1 and vec2?,adds it to the matrix input,4768
1354,Solves a linear system of equations with what to be inverted given its Cholesky factor matrix uuu?,positive semidefinite matrix,4768
1355,What does the batch matrix-matrix product return?,matrix product of the NNN 2-D tensors,4771
1356,What is the name of the function that calculates the log determinant of a square matrix or batches of square matrices?,torch.linalg.slogdet(),4771
1357,Alias for torch.linalg.det() Calculates what of a square matrix or batches of square matrices?,log determinant,5596
1358,What is the name of the function that calculates the log determinant of a square matrix?,torch.linalg.slogdet(),5596
1359,What returns the matrix product of the NNN 2-D tensors?,matrix product of the NNN 2-D tensors,5596
1360,What decomposition of a symmetric positive-definite matrix AAA is computed?,Cholesky,5596
1361,What does Alias for torch.linalg.det() calculate of a square matrix or batches of square matrices?,log determinant,1061
1362,What is the LU solve of the linear system?,Ax=bAx = bAx=b,1061
1363,What is the result of the LU factorization of a tensor into tensors L and U and a permut,"LU_data, LU_pivots",1061
1364,What does Alias for torch.linalg.matrix_power() return?,numerical rank of a 2-D tensor,1061
1365,What is the name of the Alias that computes the dot product for 1D tensors?,torch.outer(),1061
1366,"What does LU_data, LU_pivots return?",Matrix product of two tensors,1061
1367,Computes what of a square matrix or of each square matrix in a batch?,the matrix exponential,1061
1368,"What does LU_data, LU_pivots = (P @ L @ U)) calculate?",Matrix product of two tensors,1705
1369,Computes the dot product for 1D tensors. Alias of what function?,torch.outer(),7601
1370,What function returns the LU solve of the linear system Ax=bAx = b using the partially pivoted LU factorization of A,torch.lu(),7601
1371,Computes the what of a matrix or batches of matrices A?,LU factorization,7601
1372,Performs what of the matrices input and mat2?,matrix multiplication,7601
1373,Performs a what product of the matrix input and the vector vec?,matrix-vector,7601
1374,What is the function that computes the dot product for 1D tensors?,Computes the dot product for 1D tensors,7601
1375,What is the result of lu()?,Matrix product of two tensors,7601
1376,What does matrix_power return?,numerical rank of a 2-D tensor,7601
1377,Outer product of input and what?,vec2,7601
1378,What returns a QR decomposition of a matrix or a batch of matrices input?,namedtuple,7601
1379,"What does LU_data, LU_pivots = (P @ L @ U)) represent?",Matrix product of two tensors,1715
1380,What is the name of the low-level function for calling LAPACK's geqrf directly?,torch.outer(),7600
1381,"What is returned by LU_data, LU_pivots?",Matrix product of two tensors,7600
1382,What does torch.outer() do?,Computes the dot product for 1D tensors,5961
1383,Computes the factorization of a matrix or batches of matrices A. Returns the LU solve of the linear system Ax,LU,5961
1384,What is the LU factorization of a matrix or batches of matrices A?,LU factorization of a tensor,5961
1385,What is the result of LU factorization of a tensor?,Matrix product of two tensors,5961
1386,What is returned by a 2-D tensor?,numerical rank,5961
1387,What is the name of the function that computes the matrix-matrix?,torch.linalg.householder_product(),5961
1388,Computes what matrix-matrix multiplication?,matrix-matrix multiplication,5961
1389,What release is FX under?,Beta,7508
1390,What does the intermediate representation consist of?,Nodes,7508
1391,What is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph,GraphModule,7508
1392,What is the pipeline of components of FX?,Python-to-Python transformation pipeline,7508
1393,Code generation can be used for what?,programmatically generating models,7508
1394,What can be used separately?,components,7508
1395,Where can several example transformations be found?,examples repository,7508
1396,What can symbolic tracing be used in isolation for?,to capture a form of the code for analysis (and not transformation) purposes,7508
1397,How many uses does FX have?,many uses,7508
1398,What does the intermediate representation of a GraphModule constitute?,Python-to-Python transformation pipeline,7508
1399,What can be used for programmatically generating models?,Code generation,7508
1400,For each what can we create valid Python code matching the Graph's semantics?,Graph IR,7508
1401,What are recorded on Proxies?,Operations,2385
1402,Where can more information about the IR be found?,documentation for Graph,7150
1403,What can we create valid Python code matching the Graph's semantics?,Graph IR,5010
1404,What can be used in isolation to capture a form of the code for analysis (and not transformation) purposes?,symbolic tracing,5010
1405,How many uses are there for FX?,many uses,5010
1406,Where can you find examples of FX transformations?,examples repository,5010
1407,What does an FX transform take in?,torch.nn.Module,5010
1408,What is the name of the FX transform that can be passed to another FX transform?,TorchScript,5010
1409,How can components be used in the Python-to-Python transformation pipeline of FX?,components can be used separately,5009
1410,Where can you find examples of FX transforms?,examples repository,5878
1411,What can you pass the torch.nn.Module to?,TorchScript,8687
1412,What does your transform take in?,torch,8687
1413,What is the name of the FX transform that you can pass it to?,TorchScript,8331
1414,What is a data structure that represents a method on a GraphModule?,Graph,4504
1415,What method does the module MyModule call to print out a table showing the nodes of the Graph?,Graph.print_tabular(),4504
1416,What is an example of a Graph?,a short example,4386
1417,What is an example of a GraphModule?,a short example,3928
1418,What are the inputs to a GraphModule?,operations,2626
1419,A Graph is a data structure that represents a method on what?,GraphModule,2626
1420,What is the ‘dim’ of a GraphModule?,-1,2626
1421,What is the return value from a GraphModule?,output,2724
1422,What is the output value from a GraphModule?,return,2724
1423,What method is used to print out a table showing the nodes of a Graph?,Graph.print_tabular(),2724
1424,What is the output (i.e. return) value from the method?,operations,8329
1425,All three concepts are represented by what?,Node instances,8329
1426,What module is defined for demonstration purposes?,MyModule,1095
1427,What is the name of the module we define for demonstration purposes?,MyModule,2793
1428,What do we want to replace with torch.mul() calls?,torch.add() calls,2793
1429,What is the return value of the method?,return value,2793
1430,What does FX have to aid in transforming the graph?,utility functions,2793
1431,What does opcode name target args kwargs placeholder x x?,opcode name target args kwargs placeholder x x,10306
1432,What can we explore now that we know the basics of how code is represented in FX?,how we would edit a Graph,10306
1433,What is one approach to building a new Graph?,directly manipulate your old one,10306
1434,What are the inputs to a method specified via special placeholder nodes?,inputs,71
1435,Where can a full treatment of the semantics of all of the nodes be found?,Node documentation,71
1436,What is the target of a single placeholder node?,x,10573
1437,What do the placeholder nodes represent within a method?,operations,10573
1438,Which nodes represent the operations in the method?,"get_attr, call_function, call_module, and call_method",10573
1439,What is the return value of a method?,return value,10573
1440,The return value in a Graph is specified by what?,a special output node,10573
1441,Where can a full treatment of the semantics of all of the operations in a method be found?,Node documentation,10573
1442,What are the methods specified via special placeholder nodes?,inputs,10573
1443,What do we now know about how code is represented in FX?,basics,10573
1444,What is an example of a torch.mul() call?,append,10573
1445,What would you want to replace torch.add() calls with?,torch.mul(),10573
1446,What do the placeholder nodes represent in a method?,operations,9452
1447,"What do the get_attr, call_function, call_module, and call_method nodes represent?",operations in the method,11475
1448,What is the return value of the method specified by a special output node?,Graph,190
1449,What is the _weight of the built-in function add?,linear,190
1450,What is the return value of a method specified by a special output node?,return value,10427
1451,What is get_attr linear_weight?,linear.weight,9453
1452,"What is the term for x, linear_weight?",linear.weight,9796
1453,What are the operations within the method?,operations,61
1454,What is x?,linear_weight,72
1455,What is the name of the add_1 method?,linear_1,9798
1456,What is built-in method sum...> (relu_1)?,call_function sum_1,68
1457,What is the name of the linear_1 method?,call_method relu_1 relu,9006
1458,Which nodes represent the operations in a method?,"get_attr, call_function, call_module, and call_method",10572
1459,What is one way to build a new graph?,directly manipulate your old one,4653
1460,What can we do with the Graph we obtain from symbolic tracing?,modify it,4653
1461,What is the name of the call we want to replace torch.add() with?,torch.mul(),4653
1462,What is an example of using the APIs to append a call to a Graph?,torch.relu(),4653
1463,FX provides another level of what on top of direct graph manipulation?,automation,4654
1464,What provides another level of automation on top of direct graph manipulation?,FX,4654
1465,What can we do to aid in building a new Graph?,modify it,4654
1466,What is another name for torch.add()?,torch.mul(),4654
1467,How can tedious graph manipulation code become as the transformations get more complex?,can get unwieldy,4654
1468,What is the replace_pattern API used for?,Basic usage Quantization Invert Transformation,4654
1469,What API is essentially a “find/replace” tool for editing Graphs?,replace_pattern(),2380
1470,What can happen to tedious graph manipulation code as the transformations get more complex?,can get unwieldy,2380
1471,What does the replace_pattern() API do?,trace through those functions,8244
1472,How can tedious graph manipulation code get as the transformations get more complex?,unwieldy,8244
1473,What is the usage of replace_pattern?,Basic,8244
1474,What can replace_pattern() greatly automate?,graph manipulation code,2572
1475,How many op Conv/Batch Norm fusions does replace_pattern() replace?,one,2572
1476,What is used in symbolic tracing?,Proxy machinery,1146
1477,What would a transformation write that would transform every F.relu(x) call into (x > 0) * x?,decomposed PyTorch functions into smaller operations,1146
1478,What would a transformation that decomposed PyTorch functions into smaller operations do?,every F.relu(x) call into (x > 0) * x,1146
1479,What do Proxy objects do?,capture the operations that are performed on them and append them to the Graph,1146
1480,Proxys allow you to specify your rewrite rules as what?,native Python code,1146
1481,Where can you find an example of using Proxys for Graph manipulation?,here,1146
1482,What would we do with the graph rewriting?,insert the comparison and multiplication after the F.relu,1146
1483,What can be used to automate the rewriting process?,Proxy objects,1146
1484,Proxy objects are called what?,arugments,1146
1485,What would be performed to insert comparison and multiplication after the F.relu?,graph rewriting,1145
1486,What can be used to automate the process of rewriting a graph?,Proxy objects,1145
1487,What are Proxy objects called?,arugments,1145
1488,What do Proxy objects do with the operations that are performed on them?,append them to the Graph,1145
1489,What do we use to invoke PyTorch code?,Proxy objects,7928
1490,What do we want to run and record the torch.Tensor shape and dtype properties on the nodes?,GraphModule,882
1491,What is not that complicated but it can be very useful?,full interpreter,883
1492,What do we want to run and record the torch.Tensor shape and dtype properties on?,GraphModule,883
1493,What class encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides,Interpreter,883
1494,What is the Transformer class called?,Shape Propagation Performance Profiler,883
1495,What class does the Interpreter class include?,Transformer,1322
1496,What is the name of the class that can be used to create a new GraphModule?,Shape Propagation Performance Profiler,1322
1497,What do we do when our code is not quite right?,debugging,4585
1498,What did we try to check with the == equality operator?,equality of the values of two deep learning models,1426
1499,What should comparison of floating point values use to account for the non-commutativity of floating point operations?,margin of error,1426
1500,What gives us an approximate comparison taking into account a relative and absolute tolerance threshold?,torch.allclose(),3596
1501,What is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation?,torch.allclose(),1427
1502,Why is using traditional debugging techniques like print statements or pdb not as straightfoward?,FX generates the forward() function on GraphModules,2801
1503,What is used to step into the GraphModules when the forward pass is invoked?,pdb,7688
1504,"If you want to run the same code multiple times, what can be a bit tedious to step to the right code with?",pdb,7688
1505,Why is using traditional debugging techniques like print statements or pdb not as straightforward?,FX generates the forward() function on GraphModules,1417
1506,What is used to step into the GraphModule when the forward pass is invoked?,pdb,1417
1507,"If you want to run the same code multiple times, it can be a bit tedious to step to the right code with what?",pdb,1417
1508,What is an easier way to examine modules and parameters using GraphModule.to_folder()?,to_folder,1417
1509,How can we modify the code within foo/module.py?,adding print statements or using pdb,1417
1510,What is a way to examine modules and parameters in GraphModule?,to_folder,2744
1511,What is the goal after we verify that tracing is working as expected?,figuring out what went wrong during our GraphModule transformation,2744
1512,What do we want to see in the print_tabular method?,input_nodes and users,2744
1513,What method can we edit to print different attributes of the Nodes in the Graph?,print_tabular,2744
1514,What is a quick answer to the question of what went wrong in GraphModule?,Writing Transformations,2744
1515,What debugger is used to find out what went wrong?,pdb,2744
1516,What is another way to examine modules and parameters in GraphModule?,to_folder,2743
1517,Where can we look at the code within GraphModule?,foo/module.py,2743
1518,Where can we find a quick answer to the question of what went wrong in GraphModule?,Writing Transformations,2743
1519,What section in the documentation will we check first?,Limitations of Symbolic Tracing,3866
1520,What is a quick answer to the question of what went wrong during a GraphModule transformation?,Writing Transformations,3866
1521,"When the forward pass is invoked, what is used to step into the Graph?",pdb,3866
1522,What does a pdb session break on?,transform_graph(traced),3866
1523,What can we compare our traced Module before and after we've applied our transformations?,utility functions,8121
1524,What debugger is used to find what goes wrong?,pdb,8121
1525,What is broken on a pdb session?,transform_graph(traced),8121
1526,A simple visual comparison is enough to trace down what?,a bug,8122
1527,What do we start to find what goes wrong using a debugger?,pdb session,8122
1528,What is the name of the debugger we want to break on?,transform_graph(traced),8122
1529,What is a breakpoint set when you start pdb?,b LINE-NUMBER,8122
1530,How can your program start in debug mode when you run it?,type python FILENAME.py into the command line,8122
1531,"If you add pdb.set_trace(), your program will automatically start in debug mode when you run it?",Once,8122
1532,What can be a good next step if it's still not clear what's going wrong?,pdb,4541
1533,What do we want to see in a Node?,input_nodes and users,4541
1534,What is an example of a program that uses symbolic tracing?,let’s examine the following program,2393
1535,"What works for most neural net code, but has some limitations?",symbolic tracing,1114
1536,What method walks back through your code to show you where this situation happens?,The traceback,1114
1537,What happens when you pass a new input tensor to the traced function?,x can change,7167
1538,What is the term for loops or if statements whose value cannot change across invocations?,static control flow,7167
1539,What is supported by symbolic tracing?,a valid pattern,7167
1540,What type of control flow does symbolic tracing not currently support?,loops or if statements,7167
1541,What does symbolic tracing use to show you where a situation can happen?,The traceback,7167
1542,Why does static control flow arise in PyTorch programs?,for code making decisions about a model’s architecture based on hyper-parameters,7167
1543,What are examples of dynamic control flow?,loops or if statements,7166
1544,What do PyTorch programs use to make decisions about a model's architecture?,hyper-parameters,7000
1545,What is the name of the method that shows you where a situation happens?,The traceback,7000
1546,What is a loops or if statements whose value cannot change across invocations?,static control flow,7000
1547,What is the name of the function that walks back through your code to show you where this situation happens?,The traceback,6999
1548,What type of control flow is supported by PyTorch?,static control flow,6999
1549,What is the if-statement if?,self.do_activation,6999
1550,What is supported by PyTorch?,static control flow,4634
1551,What is a valid pattern that is supported by?,symbolic tracing,4634
1552,What is a method that can be traced as calls to the Method?,Customizing Tracing with the Tracer class,4634
1553,What is a loop that cannot change across invocations?,if statements,4634
1554,How can static control flow be made to support symbolic tracing?,by removing the data dependencies on input values,4633
1555,How can semantically static control flow be made to support symbolic tracing?,by removing the data dependencies on input values,7111
1556,What does not depend on any function inputs?,if-statement if self.do_activation,7111
1557,What is the name of the method that can be traced as calls to?,Customizing Tracing with the Tracer class,4205
1558,What function is not supported by __torch_function__?,built-in function len,4205
1559,What is an example of a function that is not covered by __torch_function__?,For example:,4204
1560,What functions are not covered by __torch_function__?,builtin Python functions or those in the math module,2389
1561,"Nondeterministic constructors (rand, randn) will have a single what type of value embedded in the trace?",random,6190
1562,What is the result of nondeterministic constructors having a single random value embedded in the trace?,likely not the intended behavior,6190
1563,What do arguments to deterministic constructors refer to?,dynamic input sizes,6190
1564,What does the torch.fx.wrap function contain?,Type annotations,4071
1565,What is used to preserve Python 3-style type annotations?,symbolic tracing,7275
1566,What type of function can typically not trace through concrete_args due to the presence of control flow?,FX,9109
1567,What can we use concrete_args to do?,to specialize on the value of b to trace through this,9109
1568,What is a module or function to be traced and converted into?,Graph representation,9109
1569,What representation is a module or function to be traced and converted into?,Graph,9109
1570,What enable C-level patching of functions?,enable_cpatching,9109
1571,What values will be ignored by concrete_args?,different values of b,2718
1572,What type of function can typically not trace through this because of the presence of control flow?,FX,2718
1573,What is a Module or function to be traced and converted into?,Graph representation,6169
1574,"What can be passed in, but they will be ignored?",different values of b,2540
1575,What can concrete_args do to our function?,eliminate data-structure handling,2383
1576,What is a Module created from the recorded operations from root?,GraphModule,2383
1577,What can a GraphModule function be used as?,a decorator,4463
1578,What can be called at module-level scope to register fn_or_name as a “leaf function”?,GraphModule,8248
1579,What is an nn.Module generated from?,fx.Graph,2737
1580,What should you construct if you edit the contents of the graph without reassigning the graph attribute itself?,GraphModule,2737
1581,"When a graph is reassigned, what will be automatically regenerated?",code and forward,8249
1582,What is the name of the function that can be called at module-level scope to register fn_or_name as a “lea,GraphModule,8249
1583,What is the function or name of the global function to insert into the graph when it’s called GraphModule?,fn_or_name,9369
1584,"In the case that root is a Module, any references to what will be copied over from the respective place within root’s Module hierarchy into the",Module-based objects,9369
1585,What will report as originating from GraphModule if unset?,all error messages,9369
1586,What root can be an nn.Module instance or a Dict mapping strings to any attribute type?,root,9369
1587,For what purpose does class_name denote the name of this GraphModule?,debugging purposes,9369
1588,Installs empty Modules where none exist yet if they are subpaths of target. what does this do?,Adds the given submodule to self,9369
1589,What is the name of the submodule?,m,9369
1590,What is the name of the function that can be used as a decorator?,GraphModule,7518
1591,What is a function that can be used as a decorator?,GraphModule,7514
1592,What is the name of the module that is created when a graph is reassigned?,GraphModule,9368
1593,What will be automatically regenerated when graph is reassigned?,code and forward,8756
1594,What will be copied over into the appropriate place within the GraphModule's module hierarchy?,The object mapped to by the Dict,8756
1595,What is the name of the module created from the recorded operations from root?,GraphModule,8756
1596,What type of module does root belong to?,Module,8756
1597,"If root is a Module, the qualified name found in a Node’s target will be looked up directly in what?",dict,8756
1598,What is a graph that contains the nodes this GraphModule should use for code generation?,graph,8756
1599,What should you construct to update the generated code?,GraphModule,8208
1600,What can be an nn.Module instance or a Dict mapping strings to any attribute type?,root,886
1601,What is class_name denoted the name of this GraphModule for?,debugging purposes,886
1602,What should the name of the GraphModule be set to?,root’s original name or a name that makes sense within the context of your transform,886
1603,What can be thought of as a “leaf function”?,A wrapped function,886
1604,"If root is a what, any references to Module-based objects will be copied over from the respective place within root’s Module hierarchy into the",Module,886
1605,"If unset, all error messages will report as originating from GraphModule. If it’s unset, all error messages will report",root’s original name,886
1606,"If root is a GraphModule, what attribute type will references to Module-based objects be copied over from the respective place within the",Module,8362
1607,What is the name of the nodes this GraphModule should use for code generation?,graph,8362
1608,What is the root of a GraphModule?,dict,1830
1609,What will be copied over into the appropriate place within the GraphModule’s module hierarchy?,The object mapped to by the Dict,1830
1610,Root can either be an nn.Module instance or a Dict mapping strings to what?,any attribute type,1830
1611,"If root is a GraphModule, what type of object will references to Module-based objects be copied over from the respective place within root",Module,1830
1612,What contains the nodes this GraphModule should use for code generation?,graph,1830
1613,"When graph is reassigned, code and forward will be what?",automatically regenerated,8363
1614,What will be copied over from the respective place within root's Module hierarchy into the GraphModule's module hierarchy?,Module-based objects,8363
1615,"In the case that root is a what, the qualified name found in a Node’s target will be looked up directly in the dict",dict,8363
1616,The object mapped to by the Dict will be copied over into the appropriate place within what?,GraphModule’s module hierarchy,8363
1617,What must you call to update the generated code if you edit the contents of the graph without reassigning the graph attribute itself?,recompile(),8363
1618,What does this install?,empty Modules,8363
1619,"If unset, all error messages will report as originating from GraphModule. It may be helpful to set this to what?",root’s original name,8363
1620,This installs what where none exist yet if they are subpaths of target. target – The fully-qualified string name of the new sub,empty Modules,8363
1621,Which root can be an nn.Module instance or a Dict mapping strings to any attribute type?,root,8363
1622,Any references to what will be copied over from the respective place within root’s Module hierarchy into the GraphModule’s module hierarchy?,Module-based objects,8363
1623,What will report as originating from GraphModule?,all error messages,8363
1624,What does Delete do to the Python code generated from the Graph underlying this GraphModule?,Delete,8363
1625,What is a GraphModule?,GraphModule,9106
1626,Root can be either an nn.Module instance or a Dict mapping strings to what?,any attribute type,10619
1627,"If root is a Module, the qualified name found in a Node's target will be looked up directly in what?",dict,10619
1628,What name denotes the name of this GraphModule for debugging purposes?,class_name,10619
1629,What do error messages report as originating from GraphModule?,root’s original name or a name that makes sense within the context of your transform,9486
1630,"If it's unset, all error messages will report as originating from what?",GraphModule,9486
1631,Installs what if they are subpaths of target?,empty Modules,9081
1632,A Module is considered “used” if any of the following is true: 1. It has what that are used?,children,9864
1633,What does delete_submodule do to the given submodule from self?,Deletes,7588
1634,What does delete_submodule do to an nn.Module?,Deletes all unused submodules,7588
1635,Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?,bool,7588
1636,A Module is considered “used” if any one of the following is true: 1. It has what that are used?,children,7588
1637,When should this be called?,after editing the contained graph,7588
1638,What does a return value of False mean?,the target was not a valid reference to a submodule,10909
1639,What is the return value of False that means that the target was not a valid reference to a submodule?,bool,10909
1640,What does bool Return the Python code generated from the Graph underlying this GraphModule?,Deletes all unused submodules,8978
1641,A Module is considered used if any of the following is true: 1. It has what that are used 2. Its forward is called directly via a,children,8978
1642,What does return the Python code generated from the underlying this GraphModule?,bool Return the Graph,8978
1643,What does the Python code generated from the GraphModule do?,Deletes all unused submodules from self,5187
1644,If any one of the following is true: 1. it has children that are used 2. Its forward is called directly via a call_module no,if any one of the following is true: 1.,5187
1645,Return what Python code generated from the Graph underlying this GraphModule?,Python code generated from the Graph underlying this GraphModule,5187
1646,Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target – The,delete_submodule,5187
1647,"A call_function node represents a call to a Python callable, specified by what?",the_function,5187
1648,What happens to a module when it is deleted?,if target is not a valid target,2100
1649,What is the name of the submodule we want to delete?,delete,769
1650,This method can be called to clean up an nn.Module without manually calling what on each unused submodule?,delete_submodule,7730
1651,What are the positional arguments to be passed to the called function?,kwargs,7730
1652,What does delete_submodule dump out to folder with?,module_name,7729
1653,What does the module dump out to folder with?,module_name,2103
1654,Dumps out module to folder with what name so that it can be imported with from folder> import module_name> folder,module_name,7182
1655,What do we want to do with the submodule we want to delete?,delete,7183
1656,Target – what is the name of the new submodule we want to delete?,The fully-qualified string name of the new submodule,7183
1657,What module will not be deleted if target is not a valid target?,module,7183
1658,What will the generated code of this GraphModule be if it is not recompiled?,out of date,10807
1659,Dumps out module to folder with what name so that it can be imported from folder> import module_name> folder?,module_name,10776
1660,What will the generated code of this GraphModule be if it is not recompiled after editing the contained graph?,out of date,10776
1661,Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of what?,Node s,10776
1662,Graph is the main data structure used in the FX Intermediate Representation. It consists of what?,a series of Node s,10777
1663,"The list of Node s, taken together, constitute a valid what?",Python function,10777
1664,"For the semantics of operations represented in the Graph, please see what?",Node,10777
1665,Dumps out module to folder with what name?,module_name,10777
1666,Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after,bool,10777
1667,Construct an what?,empty Graph,10777
1668,What does a call_function node represent?,a call to a Python callable,10777
1669,"What can be the_function (Callable[.., Any]) – The function to be called?",the_function,10777
1670,What can the function to be called be?,"PyTorch operator, Python function, or member of the builtins or operator namespaces",10777
1671,"args (Optional[Tuple[Argument,..]]) – The what?",positional arguments to be passed to the called function,10777
1672,"Insert a what Node into the Graph. A call_function node represents a call to a Python callable, specified by the",call_function,10777
1673,kwargs (Optional[Any]) – An optional type annotation representing the Python type the output of this node will have.,type_expr,10777
1674,Returns The what?,newly created and inserted call_function node,10777
1675,The same insertion point and type expression rules apply for this method as what?,Graph.create_node(),10777
1676,Insert a Which Node into the Graph?,call_method,10777
1677,"The list of Node s, taken together, constitute what?",a valid Python function,10808
1678,What is the main data structure used in the FX Intermediate Representation?,a series of Node s,10808
1679,When should the GraphModule be recompiled?,after editing the contained graph,10808
1680,Insert what into the Graph?,a call_function Node into the Graph,10808
1681,"kwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed to the",type_expr,10808
1682,Return value of what means that the target was not a valid reference to a submodule?,False,10808
1683,What is an example of a tensor split?,Example,6040
1684,What language does setuptools.Extension work for?,C++,2023
1685,What may cause the extension to need to be recompiled?,a new card is installed,2023
1686,Example Creates a setuptools.Extension for what language?,CUDA/C++,2022
1687,What is an example of a setuptools.Extension for CUDA/C++?,Compute capabilities,2022
1688,What extension does setuptools.Extension build?,CUDA/C++,1896
1689,When will the extension need to be recompiled?,a new card is installed,1896
1690,What does Pytorch make nvcc fall back to building kernels with?,PTX,1896
1691,What is used to override the default behavior?,TORCH_CUDA_ARCH_LIST,1896
1692,What is the extension compiled to run on all archs of the cards visible during the building process of the extension?,Compute capabilities,1896
1693,Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvc,nvcc,1896
1694,"Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus",custom,1633
1695,This improves your binary’s what?,forward compatibility,1633
1696,What happens if a new card is installed?,extension may need to be recompiled,1633
1697,"By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX.",Compute capabilities,1633
1698,What is a compute capability that's newer than the newest version for which your nvcc can build fully-compiled binaries,CC,1076
1699,Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a,Creates a setuptools.Extension for CUDA/C++,2025
1700,When does the extension need to be recompiled?,a new card is installed,1483
1701,How can you override the default behavior?,TORCH_CUDA_ARCH_LIST,1483
1702,How can you override the default behavior of Pytorch?,TORCH_CUDA_ARCH_LIST,1483
1703,What will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support,Pytorch,1483
1704,"Along with archs of the cards visible during the building process of the extension, what else will the extension be compiled to run on?",PTX,1483
1705,What does PTX improve your binary's?,forward compatibility,1483
1706,"The slower the building process will be, because it will build a separate kernel image for each arch?",the more archs get included,1483
1707,What is used to build a separate kernel image for each arch?,custom setuptools,1483
1708,Pytorch will make nvcc fall back to building kernels with the newest version of what?,PTX,1074
1709,"The extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus what?",PTX,1897
1710,Convenience method that creates what with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension?,setuptools.Extension,1897
1711,What does CC stand for?,compute capability,1897
1712,What are the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension?,"CUDA include path, library path and runtime library",1897
1713,What can modestly reduce performance on those newer CCs?,relying on older PTX to provide forward compat,1897
1714,What GPUs do you want your extension to run on?,8.0 and 8.6,1897
1715,If what is installed the extension may need to be recompiled?,a new card,1482
1716,What is used to override the default behavior of Pytorch?,TORCH_CUDA_ARCH_LIST,1482
1717,What option causes extension kernel binaries to include PTX instructions for the specified CC?,+PTX,6905
1718,What is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC?,PTX,6905
1719,What does the +PTX option improve your binary's?,forward compatibility,6905
1720,What can reduce performance on newer CCs?,relying on older PTX to provide forward compat by runtime-compiling for newer CCs,6905
1721,"If you know exact CC(s) of the GPUs you want to target, you're always better off doing what?",specifying them individually,6905
1722,What build extension?,custom setuptools,6905
1723,"If you know exact CC(s) of the GPUs you want to target, you’re always better off doing what?",specifying them individually,2026
1724,What creates a CUDA/C++ extension?,setuptools,2026
1725,What GPUs would you want your extension to run on?,8.0 and 8.6,6906
1726,What can relying on older PTX to provide forward compat by runtime-compiling for newer CCs modestly reduce,performance,6906
1727,BuildExtension is allowed to supply a dictionary for what?,extra_compile_args,6906
1728,How many resources does the Ninja backend use to build the extension?,One can,6906
1729,"If you know exact CC(s) of the GPUs you want to target, you’re always better off what?",specifying them individually,6906
1730,"The slower the building process will be, because it will build a separate kernel image for each arch.",the more archs get included,6906
1731,What compiler does setuptools.build_ext support?,CUDA,784
1732,What does the standard distutils backend do if Ninja is not available?,Note,784
1733,What is the name of the backend that builds using the Ninja backend?,use_ninja,4503
1734,What is Ninja's advantage over setuptools.build_ext?,greatly speeds up compilation,4503
1735,What does the setuptools.build_ext subclass support in general?,CUDA files,7806
1736,How much resources does #CPUS + 2 workers use on some systems?,too many resources,7806
1737,What does Ninja greatly speed up compared to the standard setuptools.build_ext?,compilation,11322
1738,What does using #CPUS + 2 workers on some systems use up?,too many resources,11322
1739,What does nvcc support with mixed compilation?,CUDA,7872
1740,What can be passed along with other sources?,CUDA source files,7872
1741,What compiler will CUDA source files be detected and compiled with instead of the C++ compiler?,nvcc,7872
1742,What is passed as a library directory?,CUDA lib64 directory,7872
1743,How can you pass additional flags to nvcc?,extra_cuda_cflags,7872
1744,What is used to find the CUDA install directory?,Various heuristics,7872
1745,What is the safest option for finding the CUDA install directory?,setting the CUDA_HOME environment variable,7872
1746,What is the optional list of linker flags to forward to the build?,extra_ldflags,9311
1747,What is an optional list of include directories to forward to the build?,extra_include_paths,9311
1748,"If True, turns on verbose logging of load steps. with_cuda – Determines whether CUDA headers and",verbose,9311
1749,What determines whether CUDA headers and libraries are added to the build?,with_cuda,9311
1750,When does with_cuda determine whether CUDA headers and libraries are added to the build?,If set to None,9311
1751,Set it to what value to force CUDA headers and libraries to be included?,True,9311
1752,"If False, load the constructed extension into the process as a plain dynamic library. If True, build a standalone executable.",is_standalone,9311
1753,What does is_standalone do if False?,build a standalone executable,9311
1754,What does is_standalone return as a Python module?,PyTorch extension,9311
1755,What imports the produced shared library as a Python module?,is_python_module,9718
1756,"The same shape as input, except along what axis?",axis,5154
1757,What type of input must other be a real number?,FloatTensor or DoubleTensor,971
1758,What is returned when each element of the tensor other is multiplied by the scalar alpha and added to each element of,resulting tensor,971
1759,"What is the scalar multiplier for other out (Tensor, optional) – the output tensor?",Example,971
1760,"If one tensor has fewer dimensions than the other, it is unsqueezed until what?",it has the same number of dimensions,3422
1761,What types of inputs does the Kronecker function support?,real-valued and complex-valued inputs,3422
1762,This function generalizes the typical definition of what for two matrices to two tensors?,Kronecker product,3422
1763,What type of operations will act deterministically when mode=True?,normally-nondeterministic,7083
1764,What is the torch.nn.Conv2d when called on?,CUDA tensor torch,11124
1765,When is torch.nn.ConvTranspose1d called?,CUDA tensor torch,11129
1766,What is torch.nn.ConvTranspose2d called on?,CUDA tensor torch,11131
1767,When is ConvTranspose3d called?,CUDA tensor torch,11133
1768,What is index_add() called on?,CUDA tensor torch,10975
1769,What type of operations will throw a RuntimeError when mode=True?,normally-nondeterministic,7087
1770,What is AdaptiveAvgPool3d when attempting to differentiate?,CUDA tensor torch,11113
1771,"When attempting to differentiate a CUDA tensor torch, what is MaxPool3d?",CUDA tensor torch,11144
1772,What is AdaptiveMaxPool2d when attempting to differentiate?,CUDA tensor torch,11117
1773,"When attempting to differentiate a CUDA tensor torch, what is FractionalMaxPool2d?",CUDA tensor torch,11138
1774,"When attempting to differentiate a CUDA tensor torch, what is FractionalMaxPool3d?",CUDA tensor torch,11141
1775,What does median() have when called on a CUDA tensor torch?,indices output,8959
1776,When input dimension is larger than one and called on a CUDA tensor that requires what?,grad,11156
1777,"When attempting to differentiate a CUDA tensor torch, what does ReplicationPad2d do?",CUDA tensor torch,11161
1778,When is NLLLoss called on a CUDA tensor torch?,CUDA tensor torch,11150
1779,"When mode='max' torch, what is EmbeddingBag?",CUDA tensor,11135
1780,When is torch.Tensor.scatter_add_() called on a CUDA tensor torch?,CUDA,10980
1781,What documentation describes nondeterministic CUDA operations?,CUDA,10980
1782,What is used to split indices_or_sections?,dimension,9635
1783,What is the default dimension along which to split the tensor?,0,9635
1784,How many dices_or_sections are there?,n,9614
1785,What is the default value of indices_or_sections?,0,9614
1786,What is dim?,dimension along which to split the tensor,6029
1787,"Default: if what, defaults to the dtype of input. device (torch.device, optional) – the",None,5484
1788,"If None, what does torch.rand_like(input) do?",defaults,5484
1789,"If unbiased is True, Bessel's correction will be used. Otherwise, the sample deviation is calculated, without any correction.",If unbiased is True,3539
1790,"What is calculated, without any correction?",the sample deviation,3539
1791,Whether to use Bessel’s correction (N=1delta N = 1N=1)?,unbiased,3539
1792,"If unbiased is True, Bessel’s correction will be used. Otherwise, what is calculated, without any correction?",the sample deviation,3539
1793,What beta is Quantization currently in?,beta,8178
1794,Quantization refers to techniques for performing computations and storing tensors at what?,lower bitwidths than floating point precision,8178
1795,A quantized model executes some or all of the operations on tensors with integers rather than floating point values. This allows for what,more compact model representation,8178
1796,What quantization does PyTorch support?,INT8,8178
1797,Quantization is primarily a technique to what?,speed up inference,8178
1798,What does PyTorch support that models quantization errors in both the forward and backward passes using fake-quantization modules?,quantization aware training,8178
1799,What does PyTorch convert the trained model into at the end of quantization aware training?,lower precision,8178
1800,A quantized model executes some or all of the operations on tensors with what instead of floating point values?,integers,5060
1801,How much faster is hardware support for INT8 computations compared to FP32?,2 to 4 times faster,5060
1802,When is quantization in beta?,in beta,5058
1803,What platform supports multiple approaches to quantizing a deep learning model?,PyTorch,4995
1804,"In most cases, the model is trained in what?",FP32,4995
1805,What can PyTorch perform operations with quantized tensors?,directly construct models,4995
1806,Where is the entire computation carried out in PyTorch?,floating point,4996
1807,"At lower level, PyTorch provides a way to represent quantized what?",tensors,4996
1808,Where should a model be moved to test quantized functionality?,CPU,1354
1809,What must be ensured when preparing a quantized model?,qconfig and the engine used for quantized computations match the backend on which the model will be executed,11432
1810,What backend does Quantization currently support?,fbgemm,8374
1811,What is the name of the backend used for quantization on ARM?,qnnpack,8374
1812,How do you set the qconfig for a model to run on ARM?,by calling:,11431
1813,Where should the model be moved in order to test the quantized functionality?,CPU,7015
1814,What is the qconfig for a model to run on ARM?,torch.quantization.get_default_qconfig,902
1815,What is the qconfig for post training quantization?,torch.quantization.get_default_qconfig,1348
1816,Quantization-aware training supports both what?,CPU and CUDA,5067
1817,What is the backend set to qnnpack?,torch.backends.quantized.engine = 'qnnpack',5066
1818,fbgemm is for use on what platform?,x86,8373
1819,What is the name of the backend used to quantize a model to run on ARM?,qnnpack,4448
1820,What is the qconfig for quantization aware training?,torch.quantization.get_default_qat_qconfig('qnnpack'),9378
1821,FX Graph Mode Quantization is a new automated quantization framework in what?,PyTorch,2379
1822,What domain library will FX Graph Mode Quantization integrate into?,torchvision,2379
1823,What does FX Graph Mode Quantization improve upon?,Eager Mode Quantization,3700
1824,What do users need to know to make FX Graph Mode Quantization work?,torch.fx,2377
1825,"To make FX Graph Mode Quantization work, users might need to familiarize themselves with what?",torch.fx,2377
1826,What is another option if FX Graph Mode Quantization does not work?,eager mode quantization,4311
1827,What is Eager Mode Quantization compared to?,FX Graph Mode Quantization,4310
1828,What is Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support?,Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules,4673
1829,What type of Quantization is supported in Post Training Quantization?,Dynamic,6098
1830,What does Manual Automatic Support for Customization Limited Support support?,Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization,4201
1831,What is Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization?,Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization,1381
1832,"Static, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization: Static, Dynamic, Weight Only",Dynamic,6093
1833,What is the name of the post training quantization mode supported by Limited Support Fully Supported Quantization Mode Support?,Dynamic,4110
1834,"Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization",Dynamic,5055
1835,What type of quantization does Eager Mode Quantization support?,static quantization,4884
1836,What does torch.nn.Module need some refactors to make compatible with?,FX Graph Mode Quantization,11146
1837,What is an example of quantization in Eager Mode Quantization?,API,7408
1838,What is the term for weights quantized with activations read/stored in floating point and quantized for compute?,dynamic quantization,9229
1839,What is used in situations where the model execution time is dominated by?,loading weights from memory,9229
1840,What is PTQ also known as?,static quantization,9229
1841,What is another name for quantization aware training?,static quantization,9228
1842,What type of models with small batch size are LSTM and dynamic quantization used for?,Transformer type models,4883
1843,What blog post provides a more comprehensive overview of the tradeoffs between static and dynamic quantization?,Pytorch,10757
1844,When are activations dynamically quantized?,inference,10538
1845,What is the model execution time dominated by?,loading weights from memory,4857
1846,What is the name of the technique that quantizes the weights and activations of a model?,static quantization,7903
1847,What quantizes the weights and activations of the model?,Static quantization,6074
1848,What does static quantization require to determine optimal quantization parameters for activations?,calibration with a representative dataset,6074
1849,What is used when memory bandwidth and compute savings are important?,Post Training Quantization,6074
1850,What does static quantization fuse activations into where possible?,preceding layers,2129
1851,What is another name for Post Training Quantization?,PTQ,2129
1852,What is the term for quantization aware training?,static quantization,2128
1853,What is also known as QAT?,Quantization Aware Training,2128
1854,What is the name of the quantization aware training tutorial?,QAT,2128
1855,What model allows for higher accuracy compared to other quantization methods?,Quantization Aware Training,5054
1856,"During training, all calculations are done in what?",floating point,5054
1857,What are quantized after model conversion?,weights and activations,5054
1858,What is Quantization Aware Training commonly used with?,CNNs,5054
1859,What is Quantization Aware Training also known as?,QAT,5054
1860,How many ways can quantization types supported by FX Graph Mode be classified?,two,5062
1861,What simulates quantization during training?,Quantization Aware Training,5052
1862,What is an example of a quantization type in FX Graph Mode Quantization?,API,5052
1863,PyTorch supports both per tensor and per channel what?,asymmetric linear quantization,4991
1864,"What means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are scaled and",Per channel,4992
1865,What does this allow for in converting tensors to quantized values?,lesser error,4992
1866,What is the mapping performed by converting?,floating point tensors,4992
1867,In what programming language do we need to be able to represent quantized data in Tensors?,PyTorch,7171
1868,Quantized Tensors allow for what in a quantized format?,serialization of data,7171
1869,Many operations for quantized tensors are available under the same API as full float version in what?,torch,173
1870,What is the name of the fused version of NN modules that impact quantization?,torch.nn.intrinsic.quantized,4479
1871,Where are many operations for quantized tensors available under the same API as full float version?,torch,4479
1872,What does the user need to do before quantization?,Specify which parts of the model need to be quantized,3946
1873,What does setting None mean that the model.conv layer will not be quantized?,model.conv,3946
1874,What is used to convert operations that require output requantization to module form?,torch.nn.ReLU,3946
1875,What does the user need to do to quantize activations?,Specify where activations are quantized and de-quantized,3946
1876,What do static quantization techniques do?,quantize activations,3945
1877,What does None mean that the model.conv layer will not be quantized?,model.conv,6016
1878,What API does the user use to combine operations/modules into a single module?,torch.quantization.fuse_modules(),2575
1879,What is a common workaround to quantize the tensor?,torch.quantization.QuantStub,3581
1880,What is an example of an error similar to: If you see an error similar to: This means that you are trying to pass a quantized Ten,e2e,3581
1881,In what mode is torch.quantization.DeQuantStub used?,Eager mode quantization,3581
1882,What type of example does torch.quantization.DeQuantStub provide?,e2e,3581
1883,What are some helper functions for quantizing the input to your model and performing critical fusions?,conv+relu,11208
1884,What are the combined (fused) modules implemented by torch.nn.intrinsic?,conv + relu,11208
1885,What combined modules are implemented by torch.nn.intrinsic?,conv + relu,11207
1886,In what platform are the key nn modules Conv2d() and Linear() implemented?,FP32,7752
1887,What format does this module convert your model from?,FP32,7752
1888,What are the combined (fused) modules that can then be quantized?,conv + relu,7752
1889,What module implements versions of those fused operations needed for quantization aware training?,torch.nn.intrinsic.qat,11166
1890,What quantization effect does the rounding simulate?,INT8,11166
1891,Computes and returns what with respect to the inputs?,the sum of gradients of outputs,1654
1892,What function computes the Jacobian of a given function?,functional.hessian,1654
1893,What does functional.jacobian function do?,computes the Jacobian of a given function,1653
1894,What can you use to capture arguments that are not Tensors or Tensors that don’t have requires_grad set?,lambda,1655
1895,What does this section contain?,higher level API,8193
1896,What does functional.vjp compute?,the dot product between a vector v and the Jacobian of the given function at the point given by the inputs,7456
1897,What does this API work with that take only Tensors as input and return only Tensors?,user-provided functions,7456
1898,What is the name of the group that disabled gradient calculation?,Context-manager,9422
1899,Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each ten,use_cuda,9260
1900,What is the most likely skew for bottom most events?,negligible,9260
1901,What does use_kineto do?,enable profiling with Kineto profiler,10446
1902,"What is bool, optional, default=False?",record_shapes,10563
1903,How will arguments be listed in the nvtx range wrapping each autograd op?,in the order they are received by the backend op,10563
1904,Which side of the nvtx range may not match the order in which arguments were passed?,Python,10563
1905,"Getting-Started Use what to visualize data and model training. Interpretability,Getting-Started,TensorBoard Finet",TensorBoard,2714
1906,What is the third and final tutorial on doing “NLP From Scratch”?,Second,4079
1907,What library is used to build the dataset and classify text?,torchtext library,4079
1908,Learn how to correctly format a dataset?,audio,4079
1909,What is a tool that can be used as an initial step for debugging bottlenecks in your program?,torch.utils.bottleneck,11240
1910,What does torch.utils.bottleneck summarize runs of your script with?,autograd profiler,11240
1911,What does torch.utils.bottleneck run on the command line with?,script.py,11240
1912,What is the name of the command line where [args] are any number of arguments?,script.py,5718
1913,Ops that do synchronize appear to be what?,Ops that do synchronize appear to be extremely expensive,8146
1914,What are the two types of autograd profiler outputs?,CPU-only-mode or CUDA-mode,5719
1915,"If your script spends most of its time executing on the GPU, then it makes sense to start looking for what in the output of the CU",responsible CUDA operators,2218
1916,"If your script spends most of its time executing on the GPU, what should you look for in the output of the CUDA-mode auto",CUDA operators,2218
1917,What is the reality of the CUDA-mode autograd profiler?,more complicated,11371
1918,What does cProfile include when profiling CUDA code?,CUDA startup time,11371
1919,"If deterministic output compared to non-checkpointed passes is not required, supply what to checkpoint or checkpoint_sequ",preserve_rng_state=False,1573
1920,What does not save intermediate activations of the entire computation graph for computing backward?,the checkpointed part,1568
1921,Checkpointing currently only supports what function?,torch.autograd.backward(),1571
1922,What is not supported by checkpointing?,torch.autograd.grad(),1571
1923,"If function invocation during backward does anything different than the one during forward, the checkpointed version won't be equivalent, and unfortunately it",global variable,8143
1924,What is LU factorization with pivot = False?,not available for CPU,4388
1925,Why does this function not check if the factorization was successful or not if get_infos is True?,the status of the factorization is present in the third element of the return tuple,4417
1926,"What is the tensor to factor of size (,m,n)(*, m, n)(,m",A (Tensor),4410
1927,"What is done when the tensor to factor of size (,m,n)(*, m, n)(",pivoting,4047
1928,Where does the AlexNet end-to-end from PyTorch to?,ONNX,2311
1929,Tracing vs what?,Scripting Type Annotations,7984
1930,What is the name of the field used to write a PyTorch model?,.data field,8443
1931,What field does numpy avoid?,.data field,1392
1932,What Operator Export Type is ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTH,Operator Export Type ONNX,2707
1933,What is the name of the exporter that exports a pretrained AlexNet into ONNX?,ONNX,4568
1934,What types of exporters can the ONNX exporter be?,trace-based and script-based exporter,4642
1935,What type of exporter will unroll loops and if conditions?,trace-based exporter,7994
1936,Functions Here is a simple script which exports what into ONNX?,a pretrained AlexNet,2657
1937,How many ways are there to handle models that consist of named parameters or keyword arguments as inputs?,two,7412
1938,What is a dictionary always the last argument in a tuple args?,last argument in the args tuple,7412
1939,"In cases where the last input is also of a dictionary type, it is mandatory to have what as the last argument in the args t",empty dictionary,7412
1940,What is provided as the last input in the args tuple args?,an empty dictionary,7412
1941,"If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class.",ATen operator,3510
1942,What type of functions are Symbolic functions?,Symbolic functions,3510
1943,What must EXACTLY match the names in VariableType.h?,Parameter names,7064
1944,How do you create a symbolic function?,Create a symbolic function named symbolic in the corresponding Function class,7064
1945,Parameter names except the first must what?,EXACTLY match the names in forward,7064
1946,Which parameter is always the exported ONNX graph?,first,7064
1947,"In the symbolic function, if the operator is already standardized in what language, we only need to create a node to represent the ONN",ONNX,7064
1948,Parameter names except the first must EXACTLY match the names in what?,forward,7064
1949,"In the symbolic function, if the operator is already standardized in what language, we just need to create a node to represent the ONN",ONNX,7064
1950,What is an example of handling missing symbolic function for?,elu operator,7064
1951,What does the export fail to export because PyTorch does not support exporting elu operator?,virtual Tensor elu,7064
1952,What is the name of the virtual Tensor elu?,virtual Tensor elu,7064
1953,Inputs are always what?,"first, then non-tensor arguments",4733
1954,"If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding what?",PyTorch Function class,4733
1955,What operator is elu?,ATen operator,4733
1956,What must EXACTLY match the names in forward and output tuple size to match the outputs of forward?,elu,4733
1957,How do you create a symbolic function in the corresponding Function class?,Create a symbolic function named symbolic,3794
1958,What do we check to confirm that Elu is standardized in ONNX?,ONNX operator list,3794
1959,Where can you find examples of PyTorch's exporting elu operator?,symbolic_opset9.py,3794
1960,Python methods are implemented via what?,C++-Python bindings,4730
1961,What is checked to confirm that Elu is standardized in ONNX?,ONNX operator list,4730
1962,What are examples of PyTorch exporting elu operator?,"symbolic_opset9.py, symbolic_opset10.py",4730
1963,Is the interface for specifying operator definitions experimental or experimental?,experimental,4730
1964,"In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent what operator",ONNX operator,4730
1965,What operator does an example handle missing symbolic function for?,elu operator,4730
1966,What is the name of the operator that elu is an ATen operator?,virtual Tensor elu,4730
1967,Where can you export your custom ops implementation?,ONNX,4730
1968,What can you export the custom operator as?,one or a combination of existing ONNX ops,4730
1969,What can you export a custom operator as in ONNX?,custom op,4730
1970,What does custom opset specify?,custom domain and version,4730
1971,What do custom ONNX ops need to extend?,backend,4730
1972,What operator does this example handle missing symbolic function for?,elu operator,3495
1973,How do you add a symbolic function in the corresponding Function class?,Create a symbolic function named symbolic,3495
1974,"In the symbolic function, if the operator is already standardized in ONNX, we just need to what?",create a node,3495
1975,What is checked to confirm that elu is standardized in ONNX?,ONNX operator list,3495
1976,"If certain named argument is not present in the dictionary, it is assigned what?",default value,7138
1977,What value does Model() k return?,x m,7138
1978,"Training (enum, default) – TrainingMode.EVAL: export the model in inference mode in what mode?",TrainingMode.EVAL,7138
1979,What does TrainingMode.EVAL stand for?,Training,7138
1980,What would a case in which dictionary input is the last input of the args tuple cause when a dictionary of named parameters is,a conflict,9925
1981,What is the return value of the model to be exported?,x m,9925
1982,What is placed to prevent this from being an issue?,constraint,9925
1983,The last value of a tuple consisting of non-keyword arguments is structured as what?,key-value pairs,8887
1984,What function assumes that the x input is intended to represent the optional dictionary consisting of named arguments?,the export function,8887
1985,What is another name for a dictionary consisting of named arguments (optional))?,torch.Tensor,8887
1986,The new call to export API would now assume that the x input is intended to represent the optional dictionary consisting of named parameters and the corresponding input,The new call,8887
1987,What would the call to export API look like?,"torch.onnx.export(model, (k, x), ‘test.onnx’)",9861
1988,What does torch.onnx.export represent?,a file-like object,9861
1989,"What = Model() k = torch.randn(2, 3) x = torch.tensor(1",m,9861
1990,Names to assign to the output nodes of the graph in order what?,output_names,9861
1991,What is another name for a TUPLE OF ARGUMENTS?,torch.Tensor,4561
1992,What are the corresponding inputs in a tuple?,key-value pairs,4561
1993,What type of object has to implement fileno that returns a file descriptor?,a file-like object,4561
1994,What is a binary Prototype?,A binary Proto,4561
1995,What is provided as the last input in the tuple args in such cases?,empty dictionary,3790
1996,What is intended to represent the optional dictionary consisting of named arguments?,the x input,3790
1997,"If using aten mode, what is exported by the functions in symbolic_opsetversion?",all the ops original exported,3790
1998,"If specified, all parameters will be exported. Set this to False if you want to export an untrained model.",export_params,9324
1999,What will we print out of the trace being exported?,debug description,9324
2000,In what mode is the model exported in?,aten mode,9324
2001,A file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name,f,9324
2002,What operator does OperatorExportTypes.ONNX_ATEN use?,Operator,9324
2003,Set this to what if you want to export an untrained model?,False,9299
2004,What is not supported in ONNX or its symbolic is missing?,ATen op,9299
2005,"What : export the model in a training friendly mode. input_names (list of strings, default empty list) – names to assign",TrainingMode.TRAINING,11254
2006,In what mode is the model exported?,aten mode,11254
2007,"Export_raw_ir (bool, default False) – [DEPRECATED. use operator_export_type",internal IR,11254
2008,"If model.training is what, export the model in inference mode if model.training is what?",False,11254
2009,What is the name of the exporter that exports the internal IR directly instead of converting it to ONNX ops?,export_raw_ir,11254
2010,OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). Operator,ONNX,11254
2011,Which OperatorExportTypes.RAW : Export raw ir?,OperatorExportTypes.RAW,11254
2012,What is the name of the operator that is not supported in ONNX?,ONNX_FALLTHROUGH,11254
2013,"What is bool, default False?",export_raw_ir,10363
2014,What does torch.onnx.export do?,torch.onnx.export,11184
2015,What does OperatorExport do?,OperatorExportTypes,11184
2016,OperatorExportTypes.ONNX: All ops are exported as regular what?,ONNX ops,4676
2017,"OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall",custom ONNX op,4676
2018,What can the user use to implement the op?,runtime backend,4676
2019,"What is not supported, hence exporter falls through?",prim::ListConstruct,4676
2020,What must be _onnx_main_opset or in _onnx_stable_opset?,opset_version,4676
2021,By default we export to what version of the ONNX submodule?,one stable opset version,4676
2022,"Right now, supported stable opset version is what?",9.,4676
2023,What does example_outputs stand for?,example_outputs,4676
2024,By default we export to how many stable opset versions?,one stable opset version,3768
2025,What is dynamic_axes?,a dictionary to specify dynamic axes of input/output,9231
2026,A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes,(1),9231
2027,What specifies the dynamic axes of provided input?,A list of integers,9231
2028,What will be generated and applied to dynamic axes during export?,automated names,9231
2029,An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to,(2),9231
2030,What specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on,An inner dictionary,9231
2031,What is an example of an inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output to the,Example,9231
2032,"If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph.",If False,9231
2033,"If False, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as input",better optimizations,9231
2034,Who executes these graphs?,backends/runtimes,9231
2035,If unspecified (what is the default value of dynamic_axes) then the behavior is chosen automatically as follows?,default None,9231
2036,"If opset_version is set to what, this argument will be set to what?",8 or lower,9231
2037,What is the name of the MIXED MODE of (1) and (2)?,keep_initializers_as_inputs,4565
2038,Who executes graphs?,backends/runtimes,4565
2039,What counterpart allows you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0?,CUDA,3921
2040,Returns True if the input is a what?,single element tensor,3921
2041,Creation ops are listed under Random sampling and include: torch.rand() torch.rand() torch.rand_like() torch.r,Random sampling,3921
2042,Returns a tensor of the same size as input with each element sampled from a what distribution?,Poisson,2208
2043,What is the name of the in-place version of torch.bernoulli()?,torch.Tensor.bernoulli_(),2208
2044,Draws binary random numbers (0 or 1) from what distribution?,Bernoulli distribution,2208
2045,What type of distribution is the tensor sampled from?,Poisson,2208
2046,Random integers generated uniformly between low (inclusive) and high (exclusive) return a tensor with the same shape as Tens,exclusive,5445
2047,Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the what?,standard normal distribution,5445
2048,Returns a tensor with what size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1,same size,5445
2049,Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance,random permutation of integers,5445
2050,What is the in-place version of torch.bernoulli() torch?,torch.Tensor.bernoulli_(),5445
2051,Performs what of tensor1 by tensor2 multiply the result by the scalar value and add it to input?,element-wise multiplication,4815
2052,Alias for torch.asinh(). Returns a new tensor with the what of the elements of input?,arctangent,4815
2053,What function computes the element-wise conjugate of the given input tensor?,torch.clamp(),4815
2054,The arctangent of inputi/otheri/textinputi /otheri is considered with consideration of what?,quadrant,4815
2055,What divides each element of the input?,Divides each element of the input,4815
2056,Computes the given input tensor's what?,element-wise angle,1734
2057,Computes the bitwise AND of input and other. Computes the bitwise XOR of input and other. Computes the bit,Computes the bitwise AND of input and other,1734
2058,What does Clamp all elements in input into the range of?,Clamps all elements in input into the range,1734
2059,"Create a new what with the magnitude of input and the sign of other, elementwise?",floating-point tensor,1734
2060,Computes the bitwise OR of input and other. Computes the bitwise XOR of input and other. Computes the bit,Computes the bitwise OR of input and other,1734
2061,What are the elements of input converted from?,angles in degrees to radians,1734
2062,With consideration of what is the arctangent of inputi/otheri / textother_iinputi,quadrant,1734
2063,Computes the element-wise conjugate of the given input tensor. Create a new floating-point tensor with the,torch.clamp(),1734
2064,What does each element of the input input by the corresponding element of other do?,Divides,1734
2065,Computes the logarithmic derivative of the gamma function on input?,torch.div(),1734
2066,Computes the what of the gamma function on input?,logarithmic derivative,1734
2067,What is the name of the function that computes the logarithmic derivative of the gamma function on input?,torch.special.erf(),1734
2068,What does Alias for torch.special.erf(). Alias for what?,torch.special.erfc(),1734
2069,Alias for torch.special.erf(). Alias for torch.special.erfc(). What is the name of the,Alias for torch.special.erf,1734
2070,Alias for torch.asinh(). Returns a new tensor with what of the elements of input?,arctangent,1734
2071,Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of input and other. Computes,Computes the bitwise NOT of the given input tensor,4827
2072,"What is created with the magnitude of input and the sign of other, elementwise?",floating-point tensor,4827
2073,Returns a new tensor with the cosine of the elements of input. Returns a new tensor with what,hyperbolic,4827
2074,What function returns a new tensor with the arctangent of the elements of input?,torch.asinh(),4827
2075,Multiply the result by what?,scalar value,4827
2076,How does each element of the input input by the corresponding element of other?,Divides,4827
2077,Computes the logarithmic derivative of the gamma?,torch.div(),4827
2078,Computes what derivative of the gamma function?,logarithmic derivative,4827
2079,Adds what to each element of the input input and returns a new resulting tensor?,scalar,5387
2080,Broadcasts input to what?,shape shape,5271
2081,Returns what of elements of input in the dimension dim?,cumulative product,5271
2082,What of the buckets are set by boundaries?,boundaries,5271
2083,Compute combinations of what?,length rrr of the given tensor,5271
2084,What of the given sequence of tensors?,Do cartesian product,5271
2085,What does the namedtuple return?,cumulative sum of elements of input in the dimension dim,5271
2086,Returns what of vectors in dimension dim of input and other?,cross product,5271
2087,What is similar to broadcast_tensors() but for shapes?,broadcast_tensors(),1464
2088,Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim. Returns a namedtuple,values,1464
2089,"Returns the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Do cartesian product of",indices,5539
2090,Returns a partial view of input with its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the,2-D square tensor,5539
2091,Compute combinations of length rrr of the given tensor. Returns the cross product of vectors in dimension dim of input and,Compute combinations of length rrr of the given tensor,5539
2092,"In the left/right direction, returning a new tensor, what do you do?",Flip tensor,5539
2093,Computes batched the distance between each pair of the two collections of row vectors. Returns a copy of input. Compute combinations of,p-norm,2171
2094,"Returns what (values, indices) where values is the cumulative maximum of elements of input in the dimension dim?",a namedtuple,2171
2095,Compute what of the given tensor?,combinations of length rrr,2171
2096,Computes the difference along the given dimension. Sums the product of the elements of the input operands along dimensions specified using a notation,n-th forward,2171
2097,Computes the element-wise greatest common divisor of input and other. Computes the histogram of a tenss.,GCD,2171
2098,Compute combinations of length rrr of the given tensor. Returns a copy of input. Returns a copy of,Compute combinations of length rrr of the given tensor,5263
2099,What does reshaping input into a one-dimensional tensor do?,Flattens input,5263
2100,How large is the view of each input tensor with zero dimensions?,2-dimensional,5263
2101,Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim. Returns the cumulative product of elements of,values,1658
2102,Computes what difference along the given dimension?,n-th forward,1658
2103,Computes batched the distance between each pair of the two collections of row vectors. Returns a copy of input. Computes what,p-norm,1658
2104,Computes the input and other. Rotate a n-D tensor by 90 degrees in the plane specified by dims,Kronecker,1658
2105,What does GCD stand for?,element-wise greatest common divisor,1658
2106,What are NNN tensors?,Take NNN tensors,1658
2107,Compute combinations of length rrr of the given tensor?,Compute combinations of length rrr of the given tensor,1930
2108,"In the left/right direction, return a new tensor. What does Flip tensor do?",Flip tensor,1930
2109,"Flip tensor in the left/right direction, returning a new tensor?",Flip tens,1930
2110,What is similar to broadcasts input to the shape shape?,broadcast_tensors(),1462
2111,Rotate a n-D Rotate a n-D Rotate a n-D Rotate a n-,Rotate a n-D,1462
2112,Broadcasts input to what shape?,shape,1462
2113,Similar to broadcast_tensors() but for what?,shapes,5907
2114,Computes the along the given dimension. Sums the product of the elements of the input operands along dimensions specified using a notation,n-th forward difference,5907
2115,How do you rotate a n-D tensor?,Rotate a n-D tensor by 90 degrees,5907
2116,Returns what of the buckets to which each value in the input belongs?,indices,5907
2117,What happens to input by reshaping it into a one-dimensional tensor?,Flattens,1636
2118,Compute what combinations of the given tensor?,combinations of length rrr,1636
2119,Computes the difference along the given dimension?,n-th forward,1636
2120,What does flip tensor in the up/down direction return a new tensor?,Flip tensor in the up/down direction,1636
2121,Compute combinations of length rrr of the given tensor. Returns a copy of input.,Compute combinations of length rrr of the given tensor,5294
2122,Performs a matrix multiplication of the matrices what?,mat1 and mat2,4791
2123,Unpacks the data?,Unpacks the data,4791
2124,Returns what matrix product of the NNN 2-D tensors?,matrix product of the NNN 2-D tensors,5597
2125,Computes the inverse of a symmetric positive-definite matrix AAA using what?,Cholesky factor uuu,5597
2126,Computes the what decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices,Cholesky,5597
2127,What is the numerical rank of a 2-D tensor?,Compute,5597
2128,Performs what product of the matrix mat and the vector vec?,matrix-vector product,4799
2129,Positive semidefinite matrix to be inverted given what?,Cholesky factor matrix uuu,4799
2130,Computes the what factorization of a matrix or batches of matrices A?,LU,4799
2131,What does the LU factorization of a tensor do?,Unpacks the data and pivots,4799
2132,Performs a what product of matrices in batch1 and batch2?,batch matrix-matrix,11009
2133,What is the matrix product of matrices stored in input and mat2?,Mat,11009
2134,What does a positive semidefinite matrix have to be inverted given?,Cholesky factor matrix uuu,1777
2135,Computes the what product of two 1D tensors?,dot,1777
2136,What is computed by LU factorization of a tensor into tensors?,Matrix product of two tensors,1777
2137,Performs a what product of the matrix input and mat2. Performs a matrix multiplication of the matrices input and mat2. Perform,matrix-vector,1777
2138,What does the matrix product of two tensors produce?,Matrix product of two tensors,4772
2139,What is the name of the Alias for torch.linalg.matrix product of two tensors?,torch.linalg.matrix,4772
2140,What does matrix_power return for a 2-D tensor?,numerical rank,1706
2141,Computes the what of two 1D tensors?,dot product,1706
2142,What product of input and vec2?,Outer product,1706
2143,Who is responsible for Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix,Alias,1706
2144,What is computed by lu()?,Matrix product of two tensors,1716
2145,Computes what for 1D tensors?,dot product,1716
2146,What does matrix_power() return the numerical rank of?,2-D tensor,1716
2147,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Computes,Outer product,1716
2148,What decomposition does pinv compute?,QR decomposition of a matrix,1716
2149,What is used to create Python code matching the Graph's semantics?,Graph IR,2386
2150,Where can some examples of FX transformations be found?,examples repository,2386
2151,What does the symbolic tracer perform of the Python code?,symbolic execution,7317
2152,What does the intermediate representation of the GraphModule constitute?,Python-to-Python transformation pipeline,7317
2153,Where can some examples of transformations be found?,examples repository,7317
2154,Why can symbolic tracing be used in isolation?,to capture a form of the code for analysis (and not transformation) purposes,7151
2155,What can we create for each Graph IR?,Python code matching the Graph’s semantics,7151
2156,What should your FX transform return as identical to a regular torch?,torch,7151
2157,What is it possible to do to an existing GraphModule instead of creating a new one?,modify an existing GraphModule,7151
2158,All three concepts are represented with what?,Node instances,1096
2159,Where can a full treatment of the semantics of all three concepts be found?,Node documentation,1096
2160,In what language is code represented?,FX,1096
2161,What type of rewrites can be done?,Graph rewrites,1096
2162,What is the target of the placeholder node?,x,10801
2163,What specifies the return value in a Graph?,a special output node,10801
2164,Where can a full treatment of the semantics of the operations in the method be found?,Node documentation,10801
2165,"To aid in building a new Graph, we can simply take the Graph we obtain from symbolic tracing and what?",modify it,10801
2166,Simple transformations that only consist of substitutions can also make use of what?,subgraph rewriter,10801
2167,In what programming language is code represented?,FX,9963
2168,Name target args kwargs placeholder what ()  get_attr linear_weight linear.weight (),x x,9963
2169,What is an example of using APIs to append a call to a Graph?,torch.relu(),9963
2170,"For simple transformations that only consist of substitutions, you can make use of what?",subgraph rewriter,9963
2171,What is the return value specified by in a Graph?,a special output node,8889
2172,What is the call_method relu_1 relu (linear_1)?,linear_1,9799
2173,What is the name of the function that is built-in method sum?,call_function sum_1,11472
2174,What happens in the course of authoring transformations?,our code will not be quite right,4586
2175,"In the course of authoring transformations, our code may not be quite right. In this case, we may need to do what?",debugging,4586
2176,What is the first tool in our toolbox to check if transformed modules are behaving as we expect?,torch.allclose(),4586
2177,"If you want to run the same code multiple times, it can be a bit tedious to step to the right code with what tool?",pdb,7689
2178,What is an easier way to examine modules and parameters?,to_folder,7689
2179,What is the first step in identifying that a transformation is creating incorrect code?,debug the transformation itself,7689
2180,What section in the documentation will we check first to see if a transformation is creating incorrect code?,Limitations of Symbolic Tracing,7689
2181,What is the goal of tracing once we verify that tracing is working as expected?,figuring out what went wrong during our GraphModule transformation,7689
2182,What can be a good next step if it's not clear what's going wrong?,pdb,7689
2183,"If you’d like to run the same code multiple times, it can be a bit tedious to step to the right code with what?",pdb,3595
2184,How do you examine modules and parameters?,to_folder,3595
2185,"Now that we’ve identified that a transformation is creating incorrect code, it’s time to what?",debug the transformation itself,3595
2186,What is the goal of debugging if tracing is working as expected?,GraphModule transformation,3595
2187,"Sometimes, a simple visual comparison is enough to trace down what?",a bug,3595
2188,What debugger can be a good next step if it’s still not clear what’s going wrong?,pdb,3595
2189,Where can you find a quick answer to this question?,Writing Transformations,3595
2190,What do we start if we want to find what goes wrong using a debugger?,pdb session,3595
2191,What does a debugger break on?,transform_graph(traced),3595
2192,What do we want to see?,Node’s,3595
2193,What is the most common Python debugger?,pdb,7187
2194,What is the name of the file you want to debug?,python -m pdb FILENAME.py,7187
2195,How do pdb debugger commands move through your running program?,stepwise,7187
2196,What do you need to use to get to the part of the code you want to examine?,s or n,7187
2197,"If you add what, your program will automatically start in debug mode when you run it?",pdb.set_trace(),7187
2198,What can you do with the pdb debugger commands?,move through your running program stepwise,7187
2199,What does b LINE-NUMBER do when you start pdb?,set a breakpoint,7187
2200,What commands do you need to use to get to the part of the code you want to examine?,s or n,7187
2201,What does import pdb call before the line you want to break at?,pdb.set_trace(),7187
2202,What can you type into the command line instead of python -m pdb FILENAME.py?,python FILENAME.py,7187
2203,What can you do once you're running your file in debug mode?,step through the code and examine your program’s internal state using certain commands,7187
2204,What is an excellent tutorial on pdb?,RealPython’s “Python Debugging With Pdb”,7187
2205,How can you use pdb in VSCode?,a) use pdb by pulling up a terminal window in your IDE,7187
2206,What can you write before the line you want to break at?,import pdb; pdb.set_trace(),2728
2207,What is dynamic control flow?,x can change,1115
2208,What does symbolic tracing use to show you where a situation happens?,The traceback,1115
2209,Static control flow arises for code making decisions about a model's architecture based on what?,hyper-parameters,2394
2210,What is not supported by __torch_function__?,built-in function len,7112
2211,Many instances of dynamic control flow are what?,semantically static control flow,7112
2212,What are some functions that are not covered by __torch_function__?,builtin Python functions or those in the math module,2390
2213,What function is not supported?,built-in function len,2390
2214,The default set of leaf modules is the set of standard what?,torch.nn module instances,2390
2215,What constructors are currently not traceable?,Tensor constructors,2390
2216,The value produced by deterministic constructors will be embedded in the trace as what?,a constant,2390
2217,What do arguments to constructors refer to?,dynamic input sizes,2390
2218,What may be a viable substitute?,ones_like or zeros_like,2390
2219,"Nondeterministic constructors (rand, randn) will have a single what?",random value embedded in the trace,2390
2220,What is wrapped in a torch.fx.wrap function?,torch.randn,2390
2221,Python 3-style type annotations are supported and will be preserved by what?,symbolic tracing,2390
2222,What Python 2-style comment type annotations are supported and will be preserved by symbolic tracing?,Python 2-style comment type annotations,2390
2223,What type of tracing is used?,Symbolic tracing,6170
2224,What can typically not trace through this due to the presence of control flow?,FX,6170
2225,"If you pass in different values of b, they will be what?",ignored,2719
2226,Concrete_args will use what to flatten your input?,pytrees,2719
2227,What will this function return if an nn.Module or function instance root is present?,GraphModule,2719
2228,What will a “leaf function” be preserved as?,a CallFunction node in the FX trace,7519
2229,"When graph is reassigned, what will be automatically regenerated?",code and forward,7519
2230,Where will the object mapped to by the Dict be copied over?,the appropriate place within the GraphModule’s module hierarchy,7519
2231,"If it’s unset, what happens to the name of this GraphModule for debugging purposes?",If it’s unset,7519
2232,What is the name of the module that can be called at module-level scope to register fn_or_name as a “lea,GraphModule,9319
2233,"What is a root (Union[torch.nn.Module, Callable]) a module or function to be trace",Graph representation,10617
2234,In the case that root is a what?,dict,10617
2235,What is the name of the global function to insert into the graph when it’s called GraphModule?,fn_or_name,9255
2236,What is the name of the function you must call if you edit the contents of the graph without reassigning the graph attribute itself?,recompile(),9255
2237,Root can either be an nn.Module instance or a Dict mapping strings to any attribute type?,nn.Module instance or a Dict mapping strings to any attribute type,9255
2238,What is the name of the GraphModule created from the recorded operations from root?,GraphModule,9255
2239,"In the case that root is a what, any references to Module-based objects will be copied over from the respective place within root’s Module hierarchy",Module,9255
2240,The object mapped to by the Dict will be copied over into the appropriate place within what Module’s module hierarchy?,Graph,9255
2241,What is the name of the graph that contains the nodes this GraphModule contains?,graph (Graph) – graph contains the nodes this GraphModule,9255
2242,What is the name of the function to update the generated code if you edit the contents of the graph without reassigning the graph attribute itself,recompile(),7515
2243,What can a function equivalently be used as?,decorator,7515
2244,"If it’s unset, what will happen to all error messages?",all error messages will report as originating from GraphModule,7515
2245,Does this install empty Modules?,Adds the given submodule to self,7515
2246,Installs empty Modules where none exists?,This installs empty Modules where none,7515
2247,Name denotes the name of this GraphModule for what purpose?,debugging purposes,1831
2248,Installs what when none exist yet if they are subpaths of target?,empty Modules,1831
2249,Target – What is the name of the new submodule?,fully-qualified string name of the new submodule,1831
2250,Name denotes the name of this GraphModule for what?,debugging purposes,9487
2251,"If it’s unset, what will happen if it’s unset?",all error messages will report as originating from GraphModule,9487
2252,What is the name that makes sense within the context of your transform?,root’s original name,9487
2253,Does this install empty Modules where none exist yet if they are subpaths of target?,Adds the given submodule to self,9487
2254,A Module is considered “used” when which of the following is true?,if any one of the following is true: 1.,9487
2255,What should be called when the GraphModule Recompile this GraphModule from its graph attribute?,This should be called,9487
2256,Target – The name of the new submodule (See example in nn.Module.get_submodule for how to,fully-qualified string,10809
2257,What underlying the GraphModule Recompile this GraphModule from its graph attribute?,Graph,10809
2258,What happens to all unused submodules from self?,Deletes all unused submodules from self,9865
2259,"The submodule itself; the actual object we want to install in the current Module which method to return True, each object in the chain denoted",m,9865
2260,A Module is considered “used” if any of the following is true: 1. It has what children that are used 2?,children that are used 2,2101
2261,Target – What is the name of the new submodule we want to delete?,The fully-qualified string name of the new submodule,2101
2262,How is a Module considered “used”?,if any one of the following is true: 1.,2101
2263,Deletes the given submodule from self. The module will not be deleted what if target is not a valid target?,if target is not a valid target,2101
2264,Where does delete the given submodule from?,self,2101
2265,"What can be the_function (Callable[.., Any])?",the_function,2101
2266,Deletes the given submodule from self. The module will not be deleted if target is not a valid target?,delete_submodule,770
2267,What type of PyTorch can the function to be called be?,PyTorch,770
2268,Deletes the given submodule from what?,self,2104
2269,"If target is not a valid target, the module will not be deleted?",if target is not a valid target,2104
2270,What does the Graph consist of?,a series of Node s,2104
2271,What does bool Return?,Graph,965
2272,Installs what where none exist yet if they are subpaths of target?,empty Modules,965
2273,"If unset, all error messages will report as originating from what?",GraphModule,9082
2274,What is a name that makes sense within the context of your transform?,root’s original name,9082
2275,This installs what where none exist yet if they are subpaths of target?,empty Modules,9082
2276,A Module is considered “used” when what is true?,if any one of the following is true: 1.,9082
2277,bool Return the what underlying this GraphModule Recompile this GraphModule from its graph attribute?,Graph,9082
2278,When should the GraphModule Recompile this GraphModule from its graph attribute be called?,after editing the contained graph,10910
2279,What does the Python code generate from the GraphModule do?,Deletes all unused submodules from self,10910
2280,Dumps out module to folder with what?,module_name,10910
2281,What underlying this GraphModule?,Graph,10910
2282,"What is the return value of each object in the chain denoted by target must either a) not exist yet, or b) reference an",return True,10910
2283,What will produce the following Graph?,the following code,10910
2284,What is the setuptools.Extension for?,CUDA/C++,1895
2285,"If a new card is installed, the extension may need to be recompiled.",extension may need to be recompiled,1077
2286,You can override the default behavior using what to explicitly specify which CCs you want the extension to support?,TORCH_CUDA_ARCH_LIST,1077
2287,What do you want to target?,GPUs,1077
2288,What happens when more archs are included?,slower the building process,1075
2289,What is used to specify which CCs you want the extension to support?,TORCH_CUDA_ARCH_LIST,8587
2290,"If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them what?",individually,8587
2291,What will the building process be if more archs are included?,slower,8587
2292,SummaryWriter writes entries directly to what in the log_dir to be consumed by TensorBoard?,event files,11242
2293,How does the SummaryWriter class update the file contents?,asynchronously,11242
2294,What does the asynchronous update of a file do without?,slowing down training,11242
2295,What class writes entries directly to event files in the log_dir to be consumed by TensorBoard?,SummaryWriter,11242
2296,What does the SummaryWriter class do?,updates the file contents asynchronously,11242
2297,How can a training program use the SummaryWriter class to add data to a file?,without slowing down training,11242
2298,What creates a SummaryWriter that will write out events and summaries to the event file?,__init__ method,8702
2299,Where does the SummaryWriter write events and summaries?,event file,8702
2300,What does the SummaryWriter do?,write out events and summaries,8702
2301,What does the __init__ method create?,SummaryWriter,8702
2302,What adds scalar data to summary?,add_scalar method,8827
2303,What method adds scalar data to summary?,add_scalar,8827
2304,"Parameters of what method are tag (string) - Data identifier, scalar_value (float or string/blob",add_scalar method,4747
2305,What could a new style lead to in add_scalar method?,faster data loading,4747
2306,What is the scalar_value in add_scalar method?,float or string/blobname,4747
2307,New style could lead to what in add_scalar method?,faster data loading,4747
2308,What method adds many scalar data to summary?,add_scalars,8828
2309,What is the dtype of walltime in add_pr_curve method?,float,4746
2310,Which parameter feeds the probability that an element be classified as true in add_pr_curve method?,true,4746
2311,What is the name of the method used to add a histogram to a summary?,add_histogram,8821
2312,What does the add_histogram method add a histogram to?,summary,8821
2313,What is the Parameter of add_histogram method?,"tag, values, global_step, bins, walltime ",4741
2314,What parameter determines how the bins are made in add_histogram method?,bins,4741
2315,Which optional override default walltime (time.time()) seconds after epoch of event in add_histogram method?,walltime,4741
2316,What is the name of the global step value to record parameter in add_images method?,global_step,4744
2317,What is the Parameter of the add_images method?,"tag, img_tensor, global_step, walltime, dataformats",4744
2318,What method adds image data to summary?,add_image,8823
2319,What does the add_image method require?,pillow package,8823
2320,What is the global step value to record parameter in add_image method?,global_step,4743
2321,How many image data does add_images method require?,batched,8824
2322,What does add_images require?,pillow package,8824
2323,What method Render matplotlib figure into an image and add it to summary?,add_figure,8819
2324,what does walltime override in add_figure method?,default walltime seconds after epoch of event,4739
2325,What adds video data to summary?,add_video method,8830
2326,What method add video data to summary?,add_video,8830
2327,What package does add_video require?,moviepy,8830
2328,What is another name for Frames per second?,fps,4748
2329,What is the Parameter of add_video method?,"vid_tensor, global_step, fps, walltime",4748
2330,What adds audio data to summary?,add_audio method,8816
2331,After what event does add_audio override default walltime?,walltime seconds after epoch of event,4737
2332,What adds text data to summary?,add_text method,8829
2333,What is the name of the method used to add graph data to a summary?,add_graph method,8820
2334,What does verbose (bool) do?,Whether to print graph structure in console,4740
2335,What does the add_embedding method add to the summary?,embedding projector data,8818
2336,What adds embedding projector data to summary?,add_embedding method,8818
2337,What is the name for the embedding in add_embedding method?,tag,4738
2338,What does each row of a matrix represent in add_embedding method?,feature vector,4738
2339,What is the shape of some parameters of add_embedding method?,"mat - (N,D) where N is number of data and D is feature dimension",5880
2340,What method adds precision recall curve?,add_pr_curve,8826
2341,What lets you understand your model's performance under different threshold settings?,Plotting a precision-recall curve,8826
2342,What lets you choose the threshold interactively in add_pr_curve method?,TensorBoard UI,8826
2343,What does T/F stand for in add_pr_curve method?,the ground truth labeling,8826
2344,What method create special chart by collecting charts tags in scalars?,add_custom_scalars,8817
2345,How many times can the add_custom_scalars function be called for each SummaryWriter() object?,once,8817
2346,What does the add_custom_scalars method provide to tensorboard?,metadata,8817
2347,What adds meshes or 3D point clouds to TensorBoard?,add_mesh method,8825
2348,What is TensorBoard's visualization based on?,Three.js,8825
2349,What are some basic definitions users can provide to add_mesh method?,"vertices, faces",8825
2350,What method add meshes or 3D point clouds to TensorBoard?,add_mesh,8825
2351,The visualization in add_mesh method is based on what?,Three.js,8825
2352,What other parameters can a user provide to add_mesh method?,"camera parameter, lighting condition, etc.",8825
2353,Where can you find a manual for advanced usage of TensorBoard?,https://threejs.org/docs/index.html,8825
2354,What are the Parameter of add_mesh method?,"tag, vertices, colors, faces, ",4745
2355,What is the name of the parameter which is a dictionary with ThreeJS classes names and configuration?,config_dict,4745
2356,What is the name of the parameter which are 3D coordinates of vertices?,vertices,4745
2357,some of the shape parameters of add_mesh method are what?,"vertices , colors , faces",5881
2358,Where should the values for colors lie in for type uint8 in add_mesh method?,"[0, 255]",5881
2359,What is the shape of vertices parameters of add_mesh method?,"(batch, number_of_vertices, channels)",5881
2360,What method adds a set of hyperparameters to be compared in TensorBoard?,add_hparams,8822
2361,Where can a set of hyperparameters be compared?,TensorBoard,8822
2362,What is the name of the hyper parameter and it's corresponding value in add_hparams method?,hparam_dict,4742
2363,What is the name of the metric parameter and it's corresponding value in add_hparams method?,metric_dict,4742
2364,"What is the name of the run parameter, to be included as part of the logdir in add_hparams method?",run_name,4742
2365,"What does add_hparams use as run name, if it's unspecified?",current timestamp,4742
2366,What should the key used in metric_dict parameter of add_hparams method be?,unique,4742
2367,What method flushes the event file to disk?,flush method,9361
2368,What is the purpose of the flush method?,to make sure that all pending events have been written to disk,9361
2369,What methods are used to get a new tensor with the data in inputfake quantized per channel?,"scale,zero_point,quant_minandquant_max",9329
2370,"The new tensor returns a new tensor with the data in inputfake quantized per channel usingscale,zer",byaxis,9329
2371,"What returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant",fake_quantize_per_channel_affine,9329
2372,What methods are used to fake the quantization of a tensor?,"scale,zero_point,quant_minandquant_max",9330
2373,"What returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_min",fake_quantize_per_tensor_affine,9330
2374,What is fix Alias for ?,Alias for torch.trunc(),9344
2375,What does fix Alias for torch.trunc() do?,fix Alias for torch.trunc(),9344
2376,"float_power Raises input to the power of exponent, element wise, in what precision?",double precision,9358
2377,"What Raises input to the power of exponent, element wise?",float_power,9358
2378,What returns a new tensor with the floor of the elements of input?,floor,9359
2379,What is the term for a floor division?,floor_divide,9360
2380,What does fmod Compute?,element-wise remainder,9364
2381,What computes the element-wise remainder of division?,remainder,10578
2382,What does remainder do?,Computes the element-wise remainder of division,10578
2383,frac Computes what portion of each element in input?,fractional portion,9391
2384,frac Computes the what portion of each element in input?,fractional,9391
2385,What computes the fractional portion of each element in input?,frac,9391
2386,What Decomposes input into mantissa and exponent tensors?,frexp,9395
2387,What is function that is analogous to NumPy's gradient function?,gradient,9484
2388,The gradient function is analogous to what other function?,NumPy,9484
2389,What is the gradient function analogous to?,NumPy’s gradient function,9484
2390,imag Returns a new tensor containing what?,imaginary values of the self tensor,9593
2391,What is ldexp?,Multiplies input by 2**:attr:other,9777
2392,What does Multiplies input by 2**:attr:other?,ldexp,9777
2393,What does a linear interpolation of two tensors start(given by input) and end based on a s,lerp,9785
2394,What does lerp base its interpolation of two tensors on?,a scalar or tensor weight,9785
2395,What does lerp do a linear interpolation of two tensors start(given by input) and end based on?,a scalar or tensor weight,9785
2396,What computes the natural logarithm of the absolute value of the gamma function on input?,lgamma,9788
2397,lgamma Computes the natural what of the value of the gamma function on input?,logarithm,9788
2398,What Computes the natural logarithm of the absolute value of the gamma function on input?,lgamma,9788
2399,What does lgamma compute?,the natural logarithm of the absolute value of the gamma function on input,9788
2400,What returns a new tensor with the natural logarithm of the elements of input?,log,9809
2401,The new tensor returns a new tensor with what?,logarithm,9810
2402,What returns a new tensor with the natural logarithm of (1 + input)?,log1p,9811
2403,What is the natural logarithm of p?,1,9811
2404,What returns a new tensor with the logarithm to the base 2 of the elements of input?,log2,9812
2405,What is the logarithm of the sum of exponentiations of the inputs?,logaddexp,9814
2406,What is the base of the logaddexp2 logarithm of the sum of exponentiations of the inputs in?,base-2,9815
2407,What is function that computes the element-wise logical AND of the given input tensors?,logical_and Computes the element-wise logical AND of the given input tensors,9818
2408,What computes the element-wise logical AND of the given input tensors?,logical_and,9818
2409,logical_and Computes the element-wise logical AND of the given what?,given input tensors,9818
2410,What does not compute the element-wise logical NOT of the given input tensor?,logical_not Computes the element-wise logical NOT of the given input tensor,9819
2411,What computes the element-wise logical NOT of the given input tensor?,logical_not,9819
2412,What computes the element-wise logical OR of the given input tensors?,logical_or,9820
2413,logical_or Computes the element-wise logical OR of the given what?,given input tensors,9820
2414,What computes the element-wise logical XOR of the given input tensors?,logical_xor,9821
2415,What is logit?,Alias for torch.special.logit(),9822
2416,What is the Alias for torch.special.logit()?,logit,9822
2417,What is leg of a right triangle?,hypotenuse,9554
2418,What do the legs of a right triangle return?,hypotenuse,9554
2419,i0 Computes the modified Bessel function of the first kind for each element of input?,zeroth order,9568
2420,What computes the zeroth order modified Bessel function of the first kind for each element of input?,i0,9568
2421,What computes the regularized lower incomplete gamma function?,igamma,9589
2422,What compiler computes the regularized upper incomplete gamma function?,igammac,9590
2423,What Computes the regularized upper incomplete gamma function?,igammac,9590
2424,What does mul do with the scalarotherand return a new tensor?,mul Multiplies each element of the input input,9942
2425,What Multiplies each element of the input input with the scalarotherand returns a new resulting tensor?,mul,9942
2426,What is Alias fortorch?,Alias for torch.mul,9948
2427,What is Alias for torch.mul()?,multiply Alias for torch.mul(),9948
2428,mvlgamma Computes themultivariate log-gamma function) with what -wise?,dimensionpppelement,9950
2429,What does mvlgamma compute?,multivariate log-gamma function,9950
2430,What is given by mvlgamma?,dimensionpppelement-wise,9950
2431,What Computes themultivariate log-gamma function?,mvlgamma,9950
2432,What are the values specified by nan_to_num?,"bynan,posinf, andneginf",9967
2433,"What replacesNaN, positive infinity, and negative infinity values in input with the values specified bynan,posinf,",nan_to_num,9967
2434,What returns a new tensor with the negative of the elements of input?,neg,9972
2435,What is website that is negative for Alias for torch.neg?,Alias for torch.neg,9973
2436,Is Alias for torch.neg() positive or negative?,negative,9973
2437,What is Alias for torch.neg stand for?,Alias for torch.neg,9973
2438,Afterinputtowardsother Return the next what?,floating-point value,9975
2439,What is function that computes the digamma function on input?,polygamma,10435
2440,What is function that computes the secondthnthnthderivative of the digamma function onin,polygamma,10435
2441,What is the result of the Returnsinput?,positive Returnsinput,10436
2442,What is input?,positive Returnsinput,10436
2443,What does pow return with the result?,tensor,10437
2444,What takes the power of each element in inputwithexponentand returns a tensor with the result?,pow,10437
2445,rad2deg Returns a new tensor with each of the elements of input converted from angles in what?,radians,10547
2446,What returns a new tensor with each element of input converted from angles in radians to degrees?,rad2deg,10547
2447,What returns a new tensor with each of the elements of input converted from angles in radians to degrees?,rad2deg,10547
2448,real Returns a new tensor containing real values of what?,the self tensor,10558
2449,reciprocal Returns a new what with the reciprocal of the elements of input?,tensor,10559
2450,round Returns a new what?,tensor,10621
2451,What returns a new tensor with each of the elements of inputrounded to the closest integer?,round,10621
2452,What is sigmoid?,Alias for torch.special.expit(),10682
2453,What does sign return with the signs of the elements of input?,tensor,10685
2454,What does sign Return with the signs of the elements of input?,tensor,10685
2455,sgn is an extension of what function?,torch.sign(),10679
2456,What is an extension of torch.sign() to complex tensors?,sgn,10679
2457,What is the sign bit set of each element of input?,less than zero,10686
2458,What tests determine if each element of input has its sign bit set (is less than zero) or not?,signbit Tests,10686
2459,The new tensor returns a new tensor with what of the elements of input?,sine,10689
2460,Sin Returns a new what with the sine of the elements of input?,tensor,10689
2461,What does sinc do to the normalized sinc of input?,Computes,10690
2462,What Computes the normalized sinc of input?,sinc,10690
2463,What type of sine does sinh return?,hyperbolic,10693
2464,What returns a new tensor with the square-root of the elements of input?,sqrt,10743
2465,What returns a new tensor with the square of the elements of input?,square,10744
2466,What Returns a new tensor with the square of the elements of input?,square,10744
2467,What is sub that is scaled byalpha?,sub Subtracts other,10772
2468,How is sub Subtracts other scaled?,byalpha,10772
2469,Where is sub Subtracts other scaled byalpha?,from input,10772
2470,What is the function that subtracts Alias for torch.sub()?,subtract Alias for torch.sub(),10778
2471,What is subtract?,Alias for torch.sub(),10778
2472,tan Returns a new tensor with what of the elements of input?,tangent,10796
2473,What returns a new tensor with the tangent of the elements of input?,tan,10796
2474,What returns a new tensor with the hyperbolic tangent of the elements of input?,tanh,10798
2475,tanh Returns a new tensor with what element?,hyperbolic tangent,10798
2476,What does true_divide do?,Alias for torch.div(),11268
2477,What is the Alias for torch.div() with rounding_mode=None?,true_divide,11268
2478,What returns a new tensor with the truncated integer values of the elements of input?,trunc,11269
2479,What does trunc return with the truncated integer values of the elements of input?,tensor,11269
2480,What is xlogy?,Computesinput*log,11434
2481,Computes input*log(other) ?,xlogy,11433
2482,What returns the indices of the minimum value(s) of the flattened tensor?,argmin,8881
2483,What returns the indices of the minimum value(s) of the flattened tensor or along a dimension?,argmin,8881
2484,What does amax return?,the maximum value of each slice of the input tensor,8858
2485,What happens if all elements in inputevaluate toTrue?,all Tests,8846
2486,max Returns what value of all elements in the input tensor?,maximum value,9883
2487,max Returns the maximum value of all elements in what?,the input tensor,9883
2488,min Returns the minimum value of all elements in what?,the input tensor,9914
2489,What does dist return?,p-norm,9187
2490,What returns the mean value of all elements in the input tensor?,mean,9894
2491,mean Returns what?,the mean value of all elements in the input tensor,9894
2492,What returns the median of the values in input?,nanmedian,9968
2493,What Returns the median of the values in input?,median,9896
2494,What does values represent in the given dimension dim?,the mode value of each row of the input tensor,9922
2495,"What returns a named tuple(values,indices)where values is the mode value of each row of the input tensor",mode,9922
2496,What is the return value of a given tensor?,matrix  norm or vector norm,10263
2497,What type of norm returns a given tensor?,matrix  norm,10263
2498,Returns the matrix  norm or what else of a given tensor?,vector norm,10263
2499,What does nansum treat Not a Numbers as?,zero,9970
2500,What returns the sum of all elements?,nansum,9970
2501,prod Returns the product of all elements in what?,the input tensor,10444
2502,what does prod Return?,the product of all elements in the input tensor,10444
2503,The quantile computes the q-th quantiles of each row of the input tensor along what?,the dimension dim,10535
2504,What computes the q-th quantiles of each row of the input tensor along the dimension dim?,quantile,10535
2505,"What is a variant of of torch.quantile() that ""ignores""NaNvalues?",nanquantile,9969
2506,What is computed if NaN values in input didnot exist?,quantiles q as,9969
2507,Which correction will be used if std if unbiased is True?,Bessel’s,10758
2508,What will Bessel’s correction be used to calculate?,standard deviation,10759
2509,sum Returns the sum of all elements in what?,the input tensor,10781
2510,sum Returns what?,the sum of all elements in the input tensor,10781
2511,What returns the unique elements of the input tensor?,unique,11283
2512,What eliminates all but the first element from every group of equivalent elements?,unique_consecutive,11284
2513,What eliminates all but the first element from every consecutive group of equivalent elements?,unique_consecutive,11284
2514,What type of correction will be used?,Bessel’s,11332
2515,What Counts the number of non-zero values in the tensor input along the given dim?,count_nonzero,9133
2516,What is the tensor input along?,the given dim,9133
2517,What condition does allclose check?,allinput and othersatisfy,8848
2518,What returns the indices that sort a tensor along a given dimension in ascending order by value?,argsort,8898
2519,argsort Returns the indices that sort a tensor along a given dimension in what order by value?,ascending,8898
2520,argsort Returns the indices that sort a tensor along a given dimension in what order?,ascending order,8898
2521,What does eq compute?,element-wise equality,9274
2522,eq Computes what?,element-wise equality,9274
2523,What Computes element-wise equality?,eq,9274
2524,What method returns  true if two tensors have the same size and elements?,equal,9275
2525,What is anothertextinput geq textotherelement-wise?,Computesinput,9445
2526,What is greater_equal?,Alias for torch.ge(),9494
2527,What is Alias for torch.ge()?,greater_equal,9494
2528,What is gt?,Computesinput,9497
2529,What is anothertextinput > textotherinput>otherelement-wise?,gt Computesinput,9497
2530,What is Alias for torch.gt?,Alias for torch.gt(),9493
2531,What is Alias for torch.gt()?,greater Alias for torch.gt(),9493
2532,What does isclose return a new tensor with?,boolean elements,9725
2533,What does the isclose return a new tensor with boolean elements representing?,if each element of input is “close” to the corresponding element of other,9725
2534,What does isfinite return if each element isfiniteor?,boolean elements,9726
2535,What does isfinite return if each element isfiniteor not?,boolean elements,9726
2536,What does isfinite return a new tensor with boolean elements represent?,if each element isfiniteor not,9726
2537,Is each element of input infinite or negative infinity?,infinite,9727
2538,What Tests if each element ofinput is infinite?,isinf,9727
2539,What tests determine if each element of input is positive infinity or not?,isposinf Tests,9732
2540,What test determines if each element of input is positive infinity or not?,isposinf,9732
2541,What is used to determine if each element of input is negative infinity or not?,isneginf Tests,9731
2542,What test determines if each element of input is negative infinity or not?,isneginf,9731
2543,isnan Returns a new tensor with what?,boolean elements,9730
2544,Isnan Returns a new tensor with boolean elements representing what?,if each element of input is NaN or not,9730
2545,isnan Returns a new tensor with what elements?,boolean elements,9730
2546,What does isreal return a new tensor with?,boolean elements,9733
2547,isreal Returns a new tensor with what elements?,boolean elements,9733
2548,Isreal Returns a new tensor with boolean elements representing?,if each element of input is real-valued or not,9733
2549,"What returns a named tuple(values,indices)?",kthvalue,9762
2550,"What Returns a named tuple(values,indices)where values is thekth smallest element of each row of the",kthvalue,9762
2551,What is othertextinput leq textotherelement-wise?,Computesinput,9778
2552,What is less_equal?,Alias for torch.le(),9787
2553,What is the difference between Alias for torch.le() and Alias for torch.le()?,less_equal,9787
2554,What is anothertextinput textotherinputotherelement-wise?,Computesinput,9853
2555,What is othertextinput  textotherelement-wise?,Computesinput,9853
2556,What is less?,Alias for torch.lt(),9786
2557,What is Alias for torch.lt ?,less,9786
2558,maximum Computes what?,the element-wise maximum of input and other,9891
2559,What computes the element-wise minimum of input and other?,fmin,9363
2560,What Computes the element-wise minimum of input and other?,fmin,9363
2561,minimum Computes what of input and other?,element-wise minimum,9915
2562,What computes the element-wise maximum of input and other?,fmax,9362
2563,What is Alias for torch.ne()?,not_equal,10271
2564,What is not_equal?,Alias for torch.ne(),10271
2565,Sort Sorts what along a given dimension in ascending order by value?,the elements of the input tensor,10719
2566,Sort Sorts the elements of the input tensor along a given dimension in what order?,ascending order by value,10719
2567,Sorts the elements of the input tensor along a given dimension in what order?,ascending order,10719
2568,What Sorts the elements of the input tensor along a given dimension in ascending order by value?,sort,10719
2569,What does topk return along a given dimension?,the k largest elements of the given input tensor,10935
2570,topk Returns what element of the given input tensor along a given dimension?,the k largest,10935
2571,msort Sorts the elements of what?,the input tensor,9941
2572,msort Sorts the elements of the input tensor along its first dimension in what order?,ascending order,9941
2573,What is istft Inverse short time?,Fourier Transform,9735
2574,What is the Fourier Transform?,Inverse short time,9735
2575,What is Bartlett window function?,evaluates bartlett_window,8943
2576,What is Blackman window function?,evaluates blackman_window,8971
2577,What does hamming_window function do?,evaluates Hamming window function,9500
2578,What is hamming_window?,evaluates Hamming window function,9500
2579,What is Hann window function?,evaluates hann_window,9501
2580,What does hann_window do?,evaluates Hann window function,9501
2581,What does kaiser_window compute the Kaiser window with?,window length window_length and shape parameter beta,9758
2582,What computes the Kaiser window with window length window_length and shape parameter beta?,kaiser_window,9758
2583,What Computes the Kaiser window with window length window_length and shape parameter beta?,kaiser_window,9758
2584,Atleast_1d Returns a 1-dimensional view of each input tensor with what dimensions?,zero,8917
2585,At least_1d Returns a view of each input tensor with zero dimensions that is what?,1-dimensional,8917
2586,What returns a 1-dimensional view of each input tensor with zero dimensions?,atleast_1d,8917
2587,Atleast_2d Returns a 2-dimensional view of each input tensor with what dimensions?,zero,8918
2588,Atleast_3d Returns a 3-dimensional view of each input tensor with what dimensions?,zero,8919
2589,What counts the frequency of each value in an array of non-negative ints?,bincount,8966
2590,What creates a block diagonal matrix  from provided tensors?,block_diag,8972
2591,block_diag Create a block diagonal matrix  from what?,provided tensors,8972
2592,What is broadcast_tensors based on?,Broadcasting semantics,8985
2593,What shape is broadcast_to Broadcasts input to?,shape shape,8986
2594,What does broadcast_to the shape shape?,Broadcasts input,8986
2595,What type of broadcast is similar to broadcast_tensors?,shapes,8984
2596,What are broadcast_shapes similar to?,broadcast_tensors,8984
2597,What are broadcast_tensors?,shapes,8984
2598,How are the boundaries of the buckets set by bucketize?,"the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries",8990
2599,Do cartesian product of the given sequence of what?,given sequence of tensors,9026
2600,What does cartesian_prod do of the given sequence of tensors?,cartesian product,9026
2601,What does cdist compute between each pair of the two collections of row vectors?,p-norm distance,9049
2602,What Computes batched the p-norm distance between each pair of two collections of row vectors?,cdist,9049
2603,What returns a copy of input?,clone,9086
2604,Compute combinations of lengthrrof the given tensor?,combinations,9096
2605,cross Returns the cross product of what?,vectors,9134
2606,What returns a named tuple?,cummin,9144
2607,"What Returns a named tuple(values,indices)where values is the cumulative minimum of elements ofinput in the dimensiond",cummin,9144
2608,"What Returns a named tuple(values,indices)where values is the cumulative maximum of elements ofinput in the dimensiond",cummax,9143
2609,What does cumprod return?,cumulative product,9145
2610,What returns the cumulative product of elements ofinput in the dimension dim?,cumprod,9145
2611,What returns the cumulative sum of elements ofinput in the dimension dim?,cumsum,9146
2612,"diag Ifinputis a vector (1-D tensor), then returns a what tensor?",2-D square,9174
2613,"diag Ifinputis a vector (1-D tensor), then returns a what?",2-D square tensor,9174
2614,How are the diagonals of certain 2D planes filled?,by input,9175
2615,What creates a tensor whose diagonals of certain 2D planes are filled by input?,diag_embed,9175
2616,diag_embed Creates a tensor whose diagonals of certain 2D planes are filled what?,by input,9175
2617,"diagflat Ifinputis a vector (1-D tensor), then returns a what tensor?",2-D square,9176
2618,"Ifinputis a vector (1-D tensor), then returns a 2-D square tensor?",diagflat,9176
2619,"Ifinputis a vector (1-D tensor), then returns a what?",2-D square tensor,9176
2620,What does diagonal Return return?,a partial view,9177
2621,What is the forward difference along a given dimension?,n-th,9178
2622,Einsum Sums the product of the elements of what dimension specified using a notation based on the Einstein summation convention?,input operands along dimensions,9242
2623,Einsum Sums the product of the elements of the input operands along dimensions specified using a notation based on what convention?,Einstein summation convention,9242
2624,Einsum Sums the product of the elements of what dimensions specified using a notation based on the Einstein summation convention?,input operands along,9242
2625,What does Flattens input do by reshaping it into a one-dimensional tensor?,flatten,9354
2626,What kind of tensor is Flattens input?,one-dimensional,9354
2627,How does flatten Flattens input work?,reshaping it into a one-dimensional tensor,9354
2628,flip Reverse the order of a n-D tensor along given axis in what?,dims,9355
2629,What is the order of a tensor along given axis in dims?,n-D,9355
2630,In what order does flip Reverse the order of a n-D tensor along given axis?,dims,9355
2631,What flips tensor in the left/right direction?,fliplr,9356
2632,What do fliplr Flip in the left/right direction?,tensor,9356
2633,What flips tensor in the up/down direction?,flipud,9357
2634,"What computes the Kronecker product, denoted byotimes, of input and other?",kron,9761
2635,What denotes the Kronecker product?,kron,9761
2636,rot90 Rotate a tensor by 90 degrees in the plane specified by dims axis?,n-D,10620
2637,What specifies the plane of a n-D tensor?,dims axis,10620
2638,Rotate a n-D tensor by 90 degrees in the plane specified by what axis?,dims,10620
2639,What does gcd stand for?,element-wise greatest common divisor,9444
2640,What does histc do?,Computes the histogram of a tensor,9518
2641,What does histc compute of a tensor?,histogram,9518
2642,histc Computes the histogram of what?,tensor,9518
2643,What is defined by expanding the iiith input over dimensions defined by other inputs?,theiiithgrid,9904
2644,What can each of the Take NNN tensors be?,scalar or 1-dimensional vector,9904
2645,What are Take NNN tensors?,meshgrid,9904
2646,What computes the element-wise least common multiple (LCM) of input and other?,lcm,9776
2647,What does lcm stand for?,least common multiple,9776
2648,What returns the logarithm of the cumulative summation of the exponentiation of elements ofinput in the dimension dim?,logcumsumexp,9816
2649,ravel Return a contiguous flattened what?,tensor,10557
2650,What animal returns a contiguous flattened tensor?,ravel,10557
2651,What is the term for a contiguous flattened tensor?,ravel,10557
2652,Thep-norm of the sub-tensor is lower than what?,the value max norm,10581
2653,What of the sub-tensor is lower than the value max norm?,thep-norm,10581
2654,What is repeat elements of a tensor?,repeat_interleave,10582
2655,What is the term for rolling the tensor along a given dimension?,roll,10614
2656,What roll the tensor along the given dimension(s)?,roll,10614
2657,What would be preserved if the corresponding values invalueswere inserted before the indices?,the order of the correspondinginnermostdimension withinsorted_sequence,10653
2658,What returns a contraction of a and b over multiple dimensions?,tensordot,10817
2659,tensordot Returns a contraction of what?,a and b  over multiple dimensions,10817
2660,What returns the lower triangular part of the matrix ?,tril,11260
2661,tril Returns the lower what part of the matrix ?,triangular,11260
2662,What returns the indices of the lower triangular part of arow-by-colmatrix  in a 2-by-N,tril_indices,11261
2663,What does the second row of tril_indices contain?,column coordinates,11261
2664,tril_indices Returns the indices of the lower triangular part of what?,arow-by-colmatrix,11261
2665,What returns the other elements of the result?,tensoroutare,11266
2666,How many tensors does triu return?,2,11266
2667,What returns the upper triangular part of a matrix ?,triu,11266
2668,What returns the upper triangular part of a matrix  or batch of matrices?,triu,11266
2669,What returns the indices of the upper triangular part of arowbycolmatrix  in a 2-by-N Tens,triu_indices,11267
2670,What does the first row of triu_indices contain?,row coordinates,11267
2671,What type of matrix  does vander generate?,Vandermonde,11331
2672,What generates a Vandermonde matrix ?,vander,11331
2673,What returns a view of iinput as a real tensor?,view_as_real,11349
2674,What real value does view_as_real return?,a real tensor,11349
2675,Returns a view of iinput as a complex what?,tensor,11348
2676,What returns a view of iinput as a complex tensor?,view_as_complex,11348
2677,What type of complex does view_as_complex return?,tensor,11348
2678,What performs a batch matrix -matrix  product of matrices stored in batch1 and batch2?,addbmm,8831
2679,What performs a batch matrix -matrix  product of matrices stored in batch1 and batch2 with a reduced add,addbmm,8831
2680,What does addmm perform on the matricesmat1 and mat2?,matrix  multiplication,8834
2681,What performs a matrix  multiplication of the matricesmat1 and mat2?,addmm,8834
2682,addmm performs what of the matricesmat1 and mat2?,matrix  multiplication,8834
2683,addmv performs what of the matrix  mat and the vector vec?,matrix -vector product,8837
2684,What performs a matrix -vector product of the matrix  mat and the vector vec?,addmv,8837
2685,addmv Performs a matrix -vector product of what?,matrix  mat and,8837
2686,What performs the outer-product of vectorsvec1 andvec2and adds it to the matrix input?,addr,8838
2687,What performs a batch matrix -matrix  product of matrices in batch1 and batch2?,baddbmm,8941
2688,What performs a batch matrix -matrix  product of matrices stored in input and mat2?,bmm,8976
2689,bmm Performs a batch matrix -matrix  product of what?,matrices stored in input and mat2,8976
2690,What returns the matrix  product of the NNN2-D tensors?,chain_matmul,9058
2691,What computes the Cholesky decomposition of a symmetric positive-definite matrix  AAA orfor batches of symmetric positive-definite,cholesky,9073
2692,What computes the inverse of a symmetric positive-definite matrix ?,cholesky_inverse,9075
2693,What computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu?,cholesky_inverse,9075
2694,What does cholesky_inverse return?,matric inv,9075
2695,cholesky_solve Solves a linear system of equations with a positive semidefinite matrix  to be inverted given what?,Cholesky factor matrix uuu,9076
2696,What is a linear system of equations with a positive semidefinite matrix  called?,cholesky_solve Solves,9076
2697,What Solves a linear system of equations with a positive semidefinite matrix  to be inverted given its Cholesky factor matrix uu,cholesky_solve,9076
2698,What does dot compute of two 1D tensors?,dot product,9197
2699,What computes the dot product of two 1D tensors?,dot,9197
2700,How many 1D tensors does dot Compute?,two,9197
2701,eig Computes what of a real square matrix ?,eigenvalues and eigenvectors,9234
2702,What Computes the eigenvalues and eigenvectors of a real square matrix ?,eig,9234
2703,What is function that calls geqrf?,LAPACK,9448
2704,What is geqrf a low-level function for?,calling LAPACK’s geqrf directly,9448
2705,What is geqrf?,low-level function,9448
2706,What is ger?,Alias of torch.outer,9449
2707,What does ger Alias do?,of torch.outer(),9449
2708,inner Computes the dot product for what?,1D tensors,9620
2709,What is the inverse of what?,Alias for torch.linalg.inv,9691
2710,What is the inverse ?,Alias for torch.linalg.inv(),9691
2711,What calculates the log determinant of a square matrix  or batches of square matrices?,logdet,9817
2712,What does logdet calculate?,log determinant,9817
2713,What does slogdet stand for?,Alias for torch.linalg.slogdet,10702
2714,What is slogdet Alias?,for torch.linalg.slogdet,10702
2715,What computes the solution to the least squares and least norm problems for a full rank matirx AAA ?,lstsq,9852
2716,What computes the solution to the least squares and least norm problems for a full rank matirx AAA of size(mn)(m,lstsq,9852
2717,lu Computes what of a matrix  or batches of matricesA?,LU factorization,9855
2718,lu Computes what factorization of a matrix  or batches of matricesA?,LU,9855
2719,What does lu_solve return?,LU solve,9856
2720,What returns the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromt,LU solve,9856
2721,What returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factor,lu_solve,9856
2722,lu_unpack Unpacks the data and pivots from what of a tensor into tensorsLand,LU factorization,9857
2723,What does lu_unpack pivot from a LU factorization of a tensor into a permutation ten,LU_pivots,9857
2724,What is the product of two tensors?,matmul matrix,9875
2725,What is the matmul matrix  product of?,two tensors,9875
2726,What is the matrix  product of two tensors?,matmul,9875
2727,What does matrix _power stand for?,Alias for torch.linalg.matrix _power,9878
2728,What is the Alias for torch.linalg.matrix _power()?,matrix _power,9878
2729,What computes the matrix  exponential of a square matrix  or of each square matrix  in a batch?,matrix _exp,9876
2730,What does matrix _exp compute?,matrix  exponential,9876
2731,What performs a matrix  multiplication of the matricesinput and mat2?,mm,9917
2732,What does mm perform a matrix  multiplication of?,matricesinput and mat2.,9917
2733,What does mv perform of the matrix input and the vector vec?,matrix -vector product,9949
2734,mv Performs a matrix -vector product of the matrix input and what?,vector vec,9949
2735,mv Performs what product of the matrix input and the vector vec?,matrix -vector,9949
2736,What is Alias for torch.linalg.householder_product()?,orgqr,10329
2737,What is orgqr Alias?,for torch.linalg.householder_product(),10329
2738,What computes the matrix -matrix  multiplication of a product of Householder matrices with a general matrix ?,ormqr,10330
2739,What is the multiplication of a product of Householder matrices with a general matrix ?,matrix -matrix,10330
2740,What is the outer product of input andvec2?,Outer product of input andvec2,10359
2741,What is file that contains the name Alias for torch.linalg.pinv?,Alias for torch.linalg.pinv,10416
2742,What is Alias for torch.linalg.pinv()?,pinverse,10416
2743,What computes the QR decomposition of a matrix  or a batch of matricesinput?,qr,10532
2744,What does qr return of tensors?,a named tuple,10532
2745,What Computes the QR decomposition of a matrix  or a batch of matricesinput?,qr,10532
2746,What is the factorization of A?,LU,10715
2747,What does solve This function return the solution to?,the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A,10715
2748,What computes the singular value decomposition of either a matrix  or batch of matricesinput?,svd,10786
2749,"What returns the singular value decomposition(U,S,V)of a matrix , batches of matrices, or a spar",svd_lowrank,10787
2750,What performs linear Principal Component Analysis on a low-rank matrix ?,pca_lowrank,10400
2751,What does symeig return eigenvalues and eigenvectors of a real symmetric or complex Hermit,a named tuple,10791
2752,symeig returns eigenvalues and eigenvectors of what kind of real Hermitian matrix input,symmetric or complex,10791
2753,What function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor,symeig,10791
2754,symeig returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian,a batch,10791
2755,What does a named tuple return?,"eigenvalues, eigenvectors",10791
2756,What does lobpcg find in a symmetric positive defined generalized eigenvalue problem?,k largest (or smallest) eigenvalues,9807
2757,What type of LOBPCG methods are used to find the k largest (or smallest) eigenvalues?,matrix -free,9807
2758,lobpcg Find the k largest (or smallest) what?,eigenvalues,9807
2759,"What rule is used to Estimateydxint y,dxydxalongdim?",trapezoid rule,11257
2760,What Solves a system of equations with a triangular coefficient matirx AAA ?,triangular_solve,11258
2761,What _solve Solves a system of equations with a triangular coefficient matirx AAA and multiple right-hand sidesb,triangular,11258
2762,What does vdot compute of two 1D tensors?,dot product,11333
2763,What Computes the dot product of two 1D tensors?,vdot,11333
2764,What does compiled_with_cxx11_abi return if PyTorch was built with?,_GLIBCXX_USE_CXX11_ABI=1,9102
2765,What returns The torch.dtype that would result from performing an arithmetic operation on the provided input tensors?,result_type,10601
2766,What determines if a type conversion is allowed under PyTorch casting rules?,can_cast,9024
2767,What document describes PyTorch casting rules?,type promotiondocumentation,9024
2768,What kind of type does The torch.dtype have?,smallest size and scalar kind,10463
2769,The torch.dtype returns the smallest size and scalar kind that is not smaller nor lower than what?,eithertype1ortype2,10463
2770,What returns true if the global deterministic flag is turned on?,are_deterministic_algorithms_enabled,8879
2771,How many times per process may PyTorch warnings appear?,once,10676
2772,"If set_warn_always is what, then some PyTorch warnings may only appear once per process?",False,10676
2773,"When set_warn_always is False, some PyTorch warnings may only appear how many times per process?",once,10676
2774,If set_warn_always is set to what flag (default) then some PyTorch warnings may only appear once per process?,False,10676
2775,What happens if the global warn_always flag is turned on?,Returns True,9724
2776,What returns true if the global warn_always flag is turned on?,is_warn_always_enabled,9724
2777,_assert A wrapper around Python's assert which is what?,symbolically traceable,8703
2778,conv1d Applies what type of convolution over an input signal composed of several input planes?,1D,9117
2779,What type of convolution does conv2d apply?,2D,9118
2780,What Applies a 2D convolution over an input image composed of several input planes?,conv2d,9118
2781,What type of convolution does conv3d apply?,3D,9119
2782,What applies a 3D convolution over an input image composed of several input planes?,conv3d,9119
2783,What is a 1D transposed convolution operator?,deconvolution,9120
2784,Conv_transpose1d Applies a transposed convolution operator over an input signal composed of several input planes?,1D,9120
2785,What is conv_transpose1d?,deconvolution,9120
2786,What applies a 1D transposed convolution operator over an input signal composed of several input planes?,conv_transpose1d,9120
2787,What is conv_transpospos2d?,deconvolution,9121
2788,What is conv_transpose2d?,deconvolution,9121
2789,What is conv_transpospos3d?,deconvolution,9122
2790,What type of transposed convolution operator does conv_transpose3d use?,3D,9122
2791,What is conv_transpose3d?,deconvolution,9122
2792,What type of input tensor does unfold extract local blocks from?,batched input tensor,11281
2793,What does unfold extract from a batched input tensor?,sliding local blocks,11281
2794,What do unfold Extracts sliding from a batched input tensor?,local blocks,11281
2795,fold Combines an array of sliding local blocks into a large containing what?,tensor,9370
2796,avg_pool1d Applies what kind of pooling over an input signal composed of several input planes?,1D,8926
2797,What Applies a 1D average pooling over an input signal composed of several input planes?,nn.AvgPool1d,9989
2798,nn.AvgPool1d Applies what type of pooling over an input signal?,1D,9989
2799,What Applies 2D average-pooling operation inkHkWkH times kWkHkWregions by step,avg_pool2d,8928
2800,What Applies 3D average-pooling operation inkTkHkWkT times kH times,avg_pool3d,8931
2801,How does avg_pool3d apply?,3D,8931
2802,max_pool1d Applies a max pooling over an input signal composed of several input planes?,1D,9886
2803,max_pool1d Applies what over an input signal?,a 1D max pooling,9886
2804,max_pool3d Applies a what kind of max pooling over an input signal composed of several input planes?,3D,9887
2805,max_unpool1d Computes a partial what of MaxPool1d?,inverse,9888
2806,max_unpool2d Computes a partial what of MaxPool2d?,inverse,9889
2807,max_unpool3d Computes a partial what of MaxPool3d?,inverse,9890
2808,lp_pool1d Applies a power-average pooling over an input signal composed of several input planes?,1D,9827
2809,How large is adaptive_max_pool1d?,1D,8809
2810,What type of pooling does adaptive_max_pool2d apply?,2D,8810
2811,What kind of pooling does adaptive_max_pool3d apply?,3D,8811
2812,How large is adaptive_avg_pool1d?,1D,8802
2813,What type of pooling does adaptive_avg_pool2d apply?,2D,8805
2814,What kind of pooling does adaptive_avg_pool3d apply?,3D,8807
2815,What type of fractional max pooling does fractional_max_pool2d apply?,2D,9392
2816,What applies 2D fractional max pooling over an input signal composed of several input planes?,fractional_max_pool2d,9392
2817,What kind of fractional max pooling does fractional_max_pool3d apply?,3D,9393
2818,What applies 3D fractional max pooling over an input signal composed of several input planes?,fractional_max_pool3d,9393
2819,What is the threshold for each element of the input Tensor?,Thresholds,10916
2820,What is the term for each element of the input Tensor?,nn.Threshold Thresholds,10210
2821,nn.Threshold Thresholds each element of what input?,Tensor,10210
2822,What Thresholds each element of the input Tensor?,Threshold,10210
2823,What is function that determines the threshold?,threshold_ In-place version ofthreshold(),10917
2824,What version of ofthreshold() is used?,threshold_ In-place,10917
2825,What does relu apply element-wise?,rectified linear unit function,10574
2826,Relu Applies the rectified linear unit function what?,element-wise,10574
2827,What does relu_ In-place version ofrelu() do?,relu_ In-place version ofrelu(),10576
2828,What Applies the HardTanh function element-wise?,hardtanh,9504
2829,What function does hardtanh apply element-wise?,HardTanh,9504
2830,How does hardtanh apply the HardTanh function?,element-wise,9504
2831,What does hardtanh do?,hardtanh_ In-place version ofhardtanh(),9505
2832,What is In-place version of hardtanh()?,hardtanh,9505
2833,How is the hardswish function described in the paper?,element-wise,10074
2834,What does nn.Hardswish Apply?,hardswish function,10074
2835,"nn.Hardswish Applies what function, element-wise?",hardswish,10074
2836,"What applies the hardswish function, element-wise?",hardswish,9503
2837,What type of function does relu6 apply?,element-wise function,10575
2838,"What Applies element-wise,ELU(x)=max(0,x)+min(0,(exp",elu,9245
2839,What is version of elu?,In-place,9246
2840,What is version of ofelu()?,elu_ In-place,9246
2841,What Applies element-wise?,tanh,10797
2842,tanh(x)=tanh(x)=exp(x)exp(x)+ex,x,10797
2843,"What Applies element-wise,SELU(x)=scale(max(0,x)+min(0,(",selu,10663
2844,"What Applies element-wise,CELU(x)=max(0,x)+min(0,(exp",celu,9051
2845,What applies element-wise?,leaky_relu,9779
2846,What is In-place version of leaky_relu()?,leaky_relu,9780
2847,What is in-place version of leaky_relu()?,leaky_relu,9780
2848,What is the leaky_relu_ In-place version of?,leaky_relu(),9780
2849,"What Applies element-wise the functionPReLU(x)=max(0,x)+weightmin(0,x",prelu,10440
2850,"Prelu Applies element-wise the functionPReLU(x)=max(0,x)+ what?",weight,10440
2851,What is rrelu?,Randomized leaky ReLU,10626
2852,What is rrelu_ In-place version ofrrelu()?,rrelu_ In-place version ofrrelu(),10627
2853,What is version of ofrrelu()?,rrelu_ In-place,10627
2854,What is glu?,gated linear unit,9477
2855,What type of unit is glu?,gated linear unit,9477
2856,What is the value of the functionGELU?,x,9446
2857,What Applies element-wiseLogSigmoid(xi)=log(11+exp(xi))text,logsigmoid,9823
2858,How does logsigmoid Applies?,element-wise,9823
2859,What function does hardshrink apply?,hard shrinkage,9502
2860,What applies the hard shrinkage function element-wise?,hardshrink,9502
2861,hardshrink Applies what function element-wise?,hard shrinkage,9502
2862,What is tanhshrink(x)=xTanh(x)textTanhshrink,x,10799
2863,What is the element-wise value of tanhshrink?,x,10799
2864,What does the functionSoftSign(x)=x1+xtextSoftSign(x) = frac,softsign,10713
2865,"What Applies element-wise, the functionSoftplus(x)=1log(1+exp(x))",softplus,10711
2866,What does softmin Apply?,softmin function,10710
2867,What Applies a softmin function?,softmin,10710
2868,What does softmax Apply?,softmax function,10709
2869,What function does softshrink apply?,soft shrinkage,10712
2870,What applies the soft shrinkage function element wise?,softshrink,10712
2871,What is a sample of the Gumbel-Softmax distribution?,gumbel_softmax Samples,9498
2872,What is distribution that contains samples from the Gumbel-Softmax distribution?,gumbel_softmax,9498
2873,What Applies a softmax followed by a logarithm?,log_softmax,9813
2874,What is followed by a log_softmax?,logarithm,9813
2875,What is followed by a softmax?,logarithm,9813
2876,What type of function does sigmoid apply?,element-wise function,10683
2877,How does silu apply the Sigmoid Linear Unit function?,element-wise,10687
2878,"What Applies the Sigmoid Linear Unit (SiLU) function, element-wise?",silu,10687
2879,How does silu apply the SiLU function?,element-wise,10687
2880,What does mish apply element-wise?,Mish function,9916
2881,What aspect of the Mish function is mish applied?,element-wise,9916
2882,How does mish apply the Mish function?,element-wise,9916
2883,What function does mish apply element-wise?,Mish,9916
2884,What does batch_norm apply for each channel across a batch of data?,Batch Normalization,8948
2885,What Applies Batch Normalization for each channel across a batch of data?,batch_norm,8948
2886,What Applies Group Normalization for last certain number of dimensions?,group_norm,9496
2887,group_norm Applies what for last certain number of dimensions?,Group Normalization,9496
2888,What Applies Instance Normalization for each channel in each data sample in a batch?,instance_norm,9683
2889,What applies Instance Normalization for each channel in each data sample in a batch?,instance_norm,9683
2890,What Applies Layer Normalization for last certain number of dimensions?,layer_norm,9771
2891,What does layer_norm apply for last certain number of dimensions?,Layer Normalization,9771
2892,What applies Layer Normalization for last certain number of dimensions?,layer_norm,9771
2893,Layer_norm Applies what for last certain number of dimensions?,Layer Normalization,9771
2894,What does local_response_norm apply over an input signal composed of several input planes?,local response normalization,9808
2895,What does local_response_norm apply to an input signal composed of several input planes?,channels occupy the second dimension,9808
2896,What is the second dimension of the input planes?,channels occupy the second dimension,9808
2897,What normalizes the normalization of inputs over a specified dimension?,PerformsLpL_pLp,10265
2898,What does PerformsLpL_pLp normalization of inputs over specified dimension?,normalize,10265
2899,What type of transformation applies a linear transformation to the incoming data?,linear,9794
2900,y = xAT + by= xAT+b. linear Applies a linear transformation to the incoming data:y,xAT+b,9794
2901,What type of transformation applies a bilinear transformation to the incoming data?,bilinear,8962
2902,What does alpha_dropout apply to the input?,alpha dropout,8856
2903,What applies alpha dropout to the input?,alpha_dropout,8856
2904,What is a channel a part of?,feature map,9331
2905,What is program that randomly zeros out entire channels?,nn.Dropout3d,10047
2906,embedding A simple lookup table that looks up embeddings in what?,a fixed dictionary and size,9247
2907,A simple lookup table that looks up what in a fixed dictionary and size?,embeddings,9247
2908,"The embedding_bag computes sums, means and maxes of embeddings without doing what?",instantiating the intermediate embeddings,9248
2909,"What Computes sums, means or maxes ofbagsof embeddings, without instantiating the intermediate embeddings",embedding_bag,9248
2910,"What takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_class",one_hot,10299
2911,How many _hot takes LongTensor with index values of shape(*)and returns a tensor of shape(*,one,10299
2912,What does Seetorch.nn.PairwiseDistancefor details?,pairwise_distance,10374
2913,What does cosine_similarity return?,cosine similarity,9129
2914,What returns cosine similarity between x1 and x2?,cosine_similarity,9129
2915,How is the cosine similarity between x1 and x2 computed?,computed along dim,9129
2916,What is the cosine similarity computed along?,dim,10036
2917,What does nn.CosineSimilarity return?,cosine similarity,10036
2918,What returns cosine similarity betweenx1x_1x1 andx2x_2x2?,nn.CosineSimilarity,10036
2919,CosineSimilarity Returns cosine similarity betweenx1x_1x1 andx2x_2x2,computed along dim,10036
2920,What does pdist compute between every pair of row vectors in the input?,p-norm distance,10401
2921,pdist Computes the distance between every pair of row vectors in the input?,p-norm,10401
2922,What is function that measures the Binary Cross Entropy between the target and the output?,binary_cross_entropy Function,8964
2923,What function measures the Binary Cross Entropy between the target and the output?,binary_cross_entropy,8964
2924,What function measures Binary Cross Entropy between target and output logits?,binary_cross_entropy_with_logits,8965
2925,What is poisson_nll_loss?,Poisson negative log likelihood loss,10433
2926,What is a cosine embedding loss?,cosine_embedding_loss,9128
2927,What is loss caused by cosineEmbeddingLoss?,cosine_embedding_loss,9128
2928,What type of loss is ctc_loss?,Connectionist Temporal Classification,9135
2929,What is ctc_loss?,The Connectionist Temporal Classification,9135
2930,What type of loss is gaussian_nll_loss?,negative log,9443
2931,What is gaussian_nll_loss?,negative log likelihood loss,9443
2932,What are the details of hinge_embedding_loss?,SeeHingeEmbeddingLossfor,9517
2933,What does SeeHingeEmbeddingLossfor details?,hinge_embedding_loss,9517
2934,What is the divergence of kl_div TheKullback-Leibler?,Loss,9760
2935,What is TheKullback-Leibler divergence Loss?,kl_div,9760
2936,What measures the element-wise mean squared error?,mse_loss,9938
2937,What do you see for details?,triplet_margin_with_distance_loss,11265
2938,What does triplet_margin_with_distance_loss refer to?,triplet_margin_with_distance_loss,11265
2939,What does marginRankingLoss stand for?,margin_ranking_loss,9873
2940,What is multilabel_margin_loss?,multilabel_margin_loss,9945
2941,What is file that is used for labels?,multilabel_soft_margin_loss,9946
2942,What does MultiLabelSoftMarginLoss stand for?,multilabel_soft_margin_loss,9946
2943,What is the weight of the multi_margin_loss?,weight=None,9944
2944,"What is the term for multi_margin_loss(input, target, p=1, margin=1, weight=None,",multi_margin_loss,9944
2945,What uses a squared term if the absolute element-wise error falls below delta?,huber_loss Function,9553
2946,What does the huber_loss function use a squared term for?,if the absolute element-wise error falls below delta,9553
2947,What function uses a squared term if the absolute element-wise error falls below delta?,huber_loss Function,9553
2948,What uses a squared term if the absolute element-wise error falls below beta?,smooth_l1_loss Function,10705
2949,What term does the smooth_l1_loss function use if the absolute element-wise error falls below beta?,squared term,10705
2950,What is loss that occurs when you lose a lot of money?,soft_margin_loss,10708
2951,What is the name of a triplet?,triplet_margin_loss,11264
2952,What is triplet_margin_loss?,triplet_margin_loss,11264
2953,What rearranges elements in a tensor of shape?,pixel_shuffle,10423
2954,"What Rearranges elements in a tensor of shape(,Cr2,H,W)(*, C",pixel_shuffle,10423
2955,What is pad?,Pads tensor,10367
2956,What is pad pad?,pad Pads tensor,10367
2957,What is the term for pad Pads?,pad Pads tensor,10367
2958,What does the interpolate Down/up samples the input to?,the givensizeor the givenscale_factor,9688
2959,What does the interpolate Down/up sample the input to?,the givenscale_factor,9688
2960,What is an upsample?,the input to either the givensizeor the givenscale_factor,11298
2961,What upsamples the input to either the givensize or the givenscale_factor?,upsample,11298
2962,What does upsample_nearest use?,nearest neighbours’ pixel values,11300
2963,"What upsamples the input, using nearest neighbours' pixel values?",upsample_nearest,11300
2964,Upsample_nearest Upsamples the input using what pixel values?,nearest neighbours,11300
2965,What type of upsampling does upsample?,bilinear,11299
2966,What upsamples the input using bilinear upsampling?,upsample_bilinear,11299
2967,What computes theoutputusinginputvalues and pixel locations from a flow-fieldgrid?,grid_sample,9495
2968,What generates a 2D or 3D flow field?,affine_grid,8839
2969,What is the batch of affine_grid given?,affine matricestheta,8839
2970,How many bits does float32ortorch.float torch have?,32,146
2971,What is a 32-bit torch?,floating point torch,146
2972,What is 32-bit floating point torch?,*.FloatTensor,146
2973,What is the 32-bit version of the float32ortorch?,floating point torch,146
2974,What is floating point torch?,*,146
2975,What is the version of the floating point torch?,64-bit,164
2976,What is 64-bit floating point torch?,*.DoubleTensor,164
2977,What is a floating point torch?,double torch,164
2978,What is the floating point torch?,64-bit,164
2979,What is float64ortorch?,DoubleTensor,164
2980,What is the version of the complex torch?,64-bit,160
2981,What is the version of complex torch.complex64ortorch.cfloat?,64-bit,160
2982,What is a 64-bit complex torch?,cfloat,160
2983,How many bits is the complex torch?,128,115
2984,What is torch.complex128ortorch.cdouble?,complex,115
2985,What is 128-bit complex torch?,cdouble,115
2986,How many bits does a floating point1 torch have?,16,121
2987,What is 16-bit floating point1 torch?,*.HalfTensor,121
2988,What is a 16-bit floating point1 torch?,half torch,121
2989,What is the 16-bit version of a torch?,floating point,121
2990,What is a 16-bit floating point torch?,HalfTensor,121
2991,How many bits does a bfloat16 torch have?,16,125
2992,What is 16-bit floating point2 torch?,*.BFloat16Tensor,125
2993,What is the 16-bit torch?,floating point2,125
2994,How many bits is a torch?,8,176
2995,Int8 torch.cuda.CharTensor torch.cuda.CharTensor torch.cuda.,8-bit,176
2996,What is an 8-bit integer?,unsigned,183
2997,What is the unsigned torch?,8-bit integer,183
2998,How many bits is the torch?,8,183
2999,What is 8-bit integer (unsigned) torch?,*.ByteTensor,180
3000,What is 8-bit integer?,unsigned,180
3001,What is 8-bit integer (signed) torch?,*.CharTensor,175
3002,What is short torch?,ShortTensor,130
3003,How many bit integers does torch.int16ortorch.short torch have?,16,130
3004,What is 32-bit integer (signed) torch?,*.IntTensor,152
3005,How many bits are in a long torch?,64,167
3006,What is 64-bit integer (signed) torch?,*.LongTensor,167
3007,What is a long torch?,LongTensor,167
3008,Int64ortorch.long torch is what?,64-bit,167
3009,What is a bool torch?,Boolean torch,1452
3010,What is a Boolean torch?,*.BoolTensor,1452
3011,What is group that adds a param group to theOptimizersparam_groups?,Optimizer,4677
3012,What add a param group to theOptimizersparam_groups?,Optimizer.add_param_group,4677
3013,What is dict that loads the optimizer state?,load_state_dict,4680
3014,What is function that loads the optimizer state?,Optimizer.load_state_dict,4680
3015,What grad sets the gradients of all optimizedtorch.Tensors to zero?,zero,4685
3016,What does Adadelta implement?,Adadelta algorithm,935
3017,What implements the Adadelta algorithm?,Adadelta,935
3018,Adadelta Implements what?,Adadelta algorithm,935
3019,What does Adagrad implement?,Adagrad algorithm,936
3020,What implements the Adam algorithm?,Adam,937
3021,What does Adam implement?,Adam algorithm,937
3022,What implements the AdamW algorithm?,AdamW,939
3023,What does AdamW implement?,AdamW algorithm,939
3024,What type of Adam algorithm does SparseAdam implement?,lazy,6006
3025,What implements a lazy version of Adam algorithm?,SparseAdam,6006
3026,What implements lazy version of Adam algorithm?,SparseAdam,6006
3027,SparseAdam implements lazy version of Adam algorithm suitable for what?,sparse tensors,6006
3028,What implements the Adamax algorithm?,Adamax,940
3029,What is the Adamax algorithm based on?,infinity norm,940
3030,Adamax Implements is a variant of Adam based on what?,infinity norm,940
3031,What does ASGD implement?,Averaged Stochastic Gradient Descent,904
3032,What does LBFGS implement?,L-BFGS algorithm,4054
3033,LBFGS is heavily inspired by what algorithm?,minFunc,4054
3034,What algorithm does LBFGS implement?,L-BFGS,4054
3035,What does RMSprop implement?,RMSprop algorithm,5082
3036,What implements RMSprop algorithm?,RMSprop,5082
3037,What implements the resilient backpropagation algorithm?,Rprop,5714
3038,What does Rprop implement?,resilient backpropagation algorithm,5714
3039,What does SGD implement?,stochastic gradient descent,5724
3040,SGD Implements stochastic gradient descent (optionally with what?,momentum,5724
3041,SGD implements stochastic gradient descent (optionally with what?,momentum,5724
3042,What does MultiplicativeLR use to multiply the learning rate of each parameter group by the factor given in the specified function?,lr_scheduler,9841
3043,What is lr_scheduler.MultiplicativeLR?,Multiply the learning rate of each parameter group by the factor given in the specified function,9841
3044,What is program that Decays the learning rate of each parameter group by gamma every step_size ep,lr_scheduler,9848
3045,MultiStepLR Decays the learning rate by gamma once the number of epoch reaches one of the milestones,lr_scheduler,9838
3046,What does lr_scheduler.ExponentialLR Decay the learning rate of each parameter group by every epoch,gamma,9833
3047,What does lr_scheduler.CosineAnnealingLR Set?,the learning rate,9828
3048,What is program that reduces learning rate when a metric has stopped improving?,lr_scheduler,9846
3049,What does lr_scheduler.ReduceLROnPlateau Reduce when a metric has stopped improving?,learning rate,9846
3050,What sets the learning rate of each parameter group according to cyclical learning rate policy?,lr_scheduler.CyclicLR,9831
3051,lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what policy?,1cycle learning rate policy,9844
3052,What is the learning rate policy for each parameter group?,1cycle,9844
3053,What is used to set the learning rate of each parameter group using a cosine annealing schedule?,lr_scheduler,9830
3054,Parameter A kind of Tensor that is to be considered what type of parameter?,module,4726
3055,Parameter A kind of what is to be considered a module parameter?,Tensor,4726
3056,What is a Sequential A?,sequential container,5815
3057,ModuleList holds what in a list?,submodules,4255
3058,register_module_forward_pre_hook Registers a forward pre-hook common to what?,all modules,10569
3059,What registers a global forward hook for all the modules?,register_module_forward_hook,10568
3060,What does register_module_forward_hook register a global forward hook for?,all the modules,10568
3061,What registers a backward hook common to all the modules?,register_module_backward_hook,10567
3062,What does register_module_backward_hook register?,backward hook,10567
3063,register_module_backward_hook Registers a backward hook common to what?,all the modules,10567
3064,nn.Conv1d Applies what type of convolution over an input signal?,1D,10015
3065,Conv2d Applies a 2D convolution over an input signal composed of several input planes?,nn,10019
3066,Conv3d Applies a 3D convolution over an input signal composed of several input planes?,nn,10022
3067,What is 1D transposed convolution operator?,nn,10025
3068,What Applies a 1D transposed convolution operator over an input image composed of several input planes?,nn.ConvTranspose1d,10025
3069,What is 2D transposed convolution operator?,nn,10028
3070,What Applies a 2D transposed convolution operator over an input image composed of several input planes?,nn.ConvTranspose2d,10028
3071,What is 3D transposed convolution operator?,nn,10031
3072,What is the lazy initialization of of theConv1d that is inferred from the input.size(1)?,thein_channelsargument,10112
3073,What is the lazy initialization of thein_channelsargument of theConv1d that is inferred from?,the input.size,10112
3074,What is thein_channelsargument of theConv1d that is inferred from the input.size(1)?,lazy initialization,10112
3075,What is the lazy initialization of of theConv2d that is inferred from the input.size(1)?,thein_channelsargument,10115
3076,What is the lazy initialization of thein_channelsargument of theConv2d that is inferred from?,the input.size,10115
3077,What is inferred from the input.size(1)?,thenum_featuresargument of theBatchNorm3d,10109
3078,The lazy initialization of thenum_featuresargument of theBatchNorm3d is inferred from what?,the input.size(1),10109
3079,What is the lazy initialization of of theConv3d that is inferred from the input.size(1)?,thein_channelsargument,10118
3080,What is thein_channelsargument of theConv3d that is inferred from the input.size(1)?,lazy initialization,10118
3081,The lazy initialization of thein_channelsargument of theConvTranspose1d is inferred from what?,the input.size,10120
3082,The lazy initialization of thein_channelsargument of theConvTranspose2d is inferred from what?,the input.size,10122
3083,What is the lazy initialization of of theConvTranspose3d that is inferred from the input.size(1)?,thein_channelsargument,10124
3084,The lazy initialization of thein_channelsargument of theConvTranspose3d is inferred from what?,the input.size,10124
3085,Unfold Extracts sliding what from a batched input tensor?,local blocks,10217
3086,What combine an array of sliding local blocks into a large containing tensor?,nn.Fold,10055
3087,nn.MaxPool1d Applies a max pooling over an input signal composed of several input planes?,1D,10138
3088,What applies a 1D max pooling over an input signal composed of several input planes?,nn.MaxPool1d,10138
3089,nn.MaxPool2d Applies a what kind of max pooling over an input signal?,2D,10142
3090,MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes?,nn,10144
3091,What is the value of the inverse of MaxUnpool1d?,nn,10147
3092,nn.MaxUnpool1d Computes a partial what of MaxPool1d?,inverse,10147
3093,What is the value of the inverse of MaxUnpool2d?,nn,10150
3094,nn.MaxUnpool2d Computes a partial what of MaxPool2d?,inverse,10150
3095,What is the value of the inverse of MaxUnpool3d?,nn,10152
3096,What Applies a 2D average pooling over an input signal composed of several input planes?,nn.AvgPool2d,9991
3097,What Applies a 3D average pooling over an input signal composed of several input planes?,nn.AvgPool3d,9994
3098,What kind of fractional max pooling does nn.FractionalMaxPool2d apply?,2D,10056
3099,What kind of pooling does nn.FractionalMaxPool3d apply?,3D,10058
3100,nn.LPPool1d Applies a power-average pooling over an input signal composed of several input planes?,1D,10097
3101,nn.LPPool2d Applies a power-average pooling over an input signal composed of several input planes in,2D,10099
3102,What is the value of AdaptiveMaxPool1d?,nn,9983
3103,What is the value of AdaptiveMaxPool2d?,nn,9985
3104,What is the value of AdaptiveMaxPool3d?,nn,9987
3105,AdaptiveAvgPool1d Applies what kind of pooling over an input signal?,1D,9979
3106,What is the value of AdaptiveAvgPool2d?,nn,9980
3107,What type of pooling does AdaptiveAvgPool3d apply?,3D,9981
3108,What does nn.ReflectionPad1d Pad the input tensor using?,reflection of the input boundary,10182
3109,What does nn.ReflectionPad2d Pads the input tensor using?,reflection of the input boundary,10184
3110,What are the input tensor boundaries with a constant value?,nn.ConstantPad3d Pads,10014
3111,What type of function does nn.ELU apply?,element-wise,10048
3112,nn.ELU Applies what?,element-wise function,10048
3113,What function does Hardshrink apply?,hard shrinkage,10069
3114,nn.Hardshrink Applies the hard shrinkage function what?,element-wise,10069
3115,What does nn.Hardsigmoid apply?,element-wise function,10072
3116,Which function does nn.Hardtanh apply element-wise?,HardTanh,10075
3117,How does HardTanh apply the HardTanh function?,element-wise,10075
3118,What function does nn.Hardtanh apply element-wise?,HardTanh,10075
3119,What kind of function does nn.LeakyReLU apply?,element-wise,10126
3120,What does nn.LeakyReLU apply?,element-wise function,10126
3121,What does nn.LogSigmoid apply?,element-wise function,10130
3122,nn.PReLU Applies what?,element-wise function,10164
3123,What does nn.ReLU apply element-wise?,rectified linear unit function,10180
3124,nn.ReLU Applies the rectified linear unit function what?,element-wise,10180
3125,What kind of function does nn.ReLU6 apply?,element-wise,10181
3126,nn.ReLU6 Applies what?,element-wise function,10181
3127,How is the randomized leaky rectified liner unit function described in the paper?,element-wise,10178
3128,How does nn.RReLU apply the randomized leaky rectified liner unit function?,element-wise,10178
3129,What is nn.SELU?,Applied element-wise,10187
3130,How is nn.SELU applied?,element-wise,10187
3131,What kind of function does nn.CELU apply?,element-wise,10010
3132,What does nn.CELU apply?,element-wise function,10010
3133,What function does nn.GELU apply?,Gaussian Error Linear Units,10060
3134,What does nn.Sigmoid apply?,element-wise function,10190
3135,How does nn.SiLU apply the Sigmoid Linear Unit function?,element-wise,10189
3136,How does nn.Mish apply the Mish function?,element-wise,10154
3137,What kind of function does nn.Softplus apply?,element-wise function,10203
3138,What function does Softshrink apply?,soft shrinkage,10204
3139,What function does nn.Softshrink apply element wise?,soft shrinkage,10204
3140,How does nn.Softshrink apply the soft shrinkage function?,element wise,10204
3141,What kind of function does nn.Softsign apply?,element-wise,10205
3142,What type of function does nn.Tanh apply?,element-wise function,10208
3143,nn.Tanhshrink Applies what?,the element-wise function,10209
3144,What Applies the Softmin function to an n-dimensional input Tensor?,nn.Softmin,10200
3145,What is the range of the elements of the n-dimensional output Tensor?,"0,1",10196
3146,How does nn.Softmax2d apply SoftMax to each spatial location?,over features,10198
3147,nn.Softmax2d Applies what over features to each spatial location?,SoftMax,10198
3148,What type of input Tensor does nn.LogSoftmax apply the function to?,n-dimensional,10131
3149,Who wrote the Efficient softmax approximation for GPUs?,"Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou",9982
3150,"Along with Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and what other person did they",Hervé Jégou,9982
3151,What is the value of the number that applies Batch Normalization over a 2D or 3D input?,nn,10004
3152,The lazy initialization of thenum_featuresargument of theBatchNorm1d is inferred from what?,the input.size(1),10104
3153,The lazy initialization of thenum_featuresargument of theBatchNorm2d is inferred from what?,the input.size(1),10107
3154,What Applies Group Normalization over a mini-batch of inputs?,nn.GroupNorm,10067
3155,InstanceNorm1d Applies Instance Normalization over a 3D input?,nn,10084
3156,What is a 3D input?,a mini-batch of 1D inputs with optional additional channel dimension,10084
3157,What is paper that describes Instance Normalization over a 4D input?,Instance Normalization: The Missing Ingredient for Fast Stylization,10087
3158,nn.InstanceNorm2d Applies Instance Normalization over what input?,4D,10087
3159,What is paper that describes Instance Normalization over a 5D input?,Instance Normalization: The Missing Ingredient for Fast Stylization,10089
3160,nn.InstanceNorm3d Applies Instance Normalization over what input?,5D,10089
3161,What document describes Layer Normalization over a mini-batch of inputs?,paperLayer Normalization,10103
3162,What does nn.LayerNorm apply over a mini-batch of inputs?,Layer Normalization,10103
3163,What does nn.LocalResponseNorm apply over an input signal composed of several input planes?,local response normalization,10129
3164,What does local response normalization apply to an input signal composed of several input planes?,channels occupy the second dimension,10129
3165,What occupy the second dimension of the input plane?,channels,10129
3166,What dimension do channels occupy?,second dimension,10129
3167,What is enzyme that is responsible for the RNNBase?,nn.RNNBase,10175
3168,What is the name for nn.RNNBase?,nn.RNNBase,10175
3169,nn.RNN Applies a multi-layer Elman RNN withtanhtanhtanhorReLU,input sequence,10174
3170,What is multi-layer long short-term memory (LSTM) RNN that is applied to an input sequence?,nn,10101
3171,nn.LSTM Applies a what type of long short-term memory RNN to an input sequence?,multi-layer,10101
3172,What is GRU that applies a multi-layer gated recurrent unit to an input sequence?,nn,10062
3173,What is nn.GRU?,multi-layer gated recurrent unit,10062
3174,What type of non-linearity does an Elman RNN cell have?,tanh or ReLU non-linearity,10177
3175,What is LSTMCell?,long short-term memory,10102
3176,What is gated recurrent unit cell?,GRU,10063
3177,What type of transformer is a transformer?,transformer,10211
3178,What is TransformerEncoder?,nn,10214
3179,What is nn.TransformerEncoder a stack of?,N encoder layers,10214
3180,What is the stack of N decoder layers?,nn,10213
3181,How many decoder layers does TransformerDecoder have?,N,10213
3182,What is a stack of N decoder layers called?,TransformerDecoder,10213
3183,Is nn.Identity a placeholder identity operator?,argument-insensitive,10082
3184,nn.Identity A placeholder identity operator that is what?,argument-insensitive,10082
3185,What type of identity operator is argument-insensitive?,placeholder,10082
3186,What is y=?,xAT+b,10128
3187,What does nn.LazyLinear a torch.nn.Linearmodule do?,wherein_featuresis inferred,10125
3188,What type of Atorch is nn.LazyLinear Atorch?,Linearmodule,10125
3189,What is program that randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a,nn.Dropout,10043
3190,What randomly zeroes some elements of the input tensor with probabilitypusing samples from a Bernoulli distribution?,nn.Dropout,10043
3191,What does Embedding store?,embeddings of a fixed dictionary and size,10050
3192,A simple lookup table that stores what of a fixed dictionary and size?,embeddings,10050
3193,What is the value of the EmbeddingBag?,nn,10053
3194,What Computes sums or means of ‘bags’ of embeddings without instantiating the intermediate embeddings?,nn.EmbeddingBag,10053
3195,What does nn.PairwiseDistance use to compute the batchwise pairwise distance between vectorsv1v_1v1,p-norm,10165
3196,"What is the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2 using the p",nn.PairwiseDistance Computes,10165
3197,"What is used to calculate the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2?",p-norm,10165
3198,"What computes the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2 using the",nn.PairwiseDistance,10165
3199,What creates a criterion that measures the mean absolute error?,nn.L1Loss,10094
3200,What creates a criterion that measures the mean squared error?,nn.MSELoss,10132
3201,What is the criterion that measures the mean squared error?,squared L2 norm,10132
3202,What is nn.CTCLoss?,The Connectionist Temporal Classification,10011
3203,What type of distribution is the target?,Poisson distribution,10171
3204,What is nn.PoissonNLLLoss?,Negative log likelihood loss,10171
3205,What is the result of nn.GaussianNLLLoss Gaussian?,negative log likelihood loss,10064
3206,What is nn.GaussianNLLLoss Gaussian?,negative log likelihood loss,10064
3207,What is Kullback-Leibler divergence loss measure?,nn.KLDivLoss,10091
3208,What is the divergence loss measure?,Kullback-Leibler,10091
3209,What does nn.BCELoss measure between the target and the output?,Binary Cross Entropy,9996
3210,What does nn.BCELoss create that measures the Binary Cross Entropy between the target and the output?,criterion,9996
3211,What is loss that combines aSigmoidlayer and theBCELossin?,nn.BCEWithLogitsLoss,10000
3212,What two classes does nn.BCEWithLogitsLoss combine?,aSigmoidlayer and theBCELossin,10000
3213,What label contains 1 or -1?,1D mini-batch tensoryyy,10135
3214,What is the label of the 1D mini-batch tensoryyy?,1 or -1,10135
3215,What does the label 1D mini-batch tensoryyyy contain?,1 or -1),10135
3216,What measures the loss given an input tensorxxxand a labels tensoryyy?,nn.HingeEmbeddingLoss,10076
3217,What is the value of MultiLabelMarginLoss?,nn,10155
3218,What does nn.HuberLoss use if the absolute element-wise error falls below delta?,a squared term,10079
3219,What does nn.HuberLoss create?,criterion,10079
3220,What does nn.SmoothL1Loss use if the absolute element-wise error falls below beta?,a squared term,10192
3221,What creates a criterion that uses a squared term if the absolute element-wise error falls below beta?,nn.SmoothL1Loss,10192
3222,What term does nn.SmoothL1Loss use if the absolute element-wise error falls below beta?,squared term,10192
3223,What Loss creates a criterion that uses a squared term if the absolute element-wise error falls below beta?,L1,10192
3224,What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target,nn.SoftMarginLoss,10194
3225,What does nn.SoftMarginLoss create?,criterion,10194
3226,What does nn.MultiLabelSoftMarginLoss create a criterion that optimizes?,multi-label one-versus-all loss based on max-entropy,10157
3227,What does nn.CosineEmbeddingLoss create?,criterion,10033
3228,What is the value of MultiMarginLoss?,nn,10159
3229,What is outputyyy?,a 1D tensor of target class indices,10159
3230,What does nn.TripletMarginWithDistanceLoss measure?,triplet loss,10215
3231,What does nn.TripletMarginWithDistanceLoss create?,criterion,10215
3232,What type of factor is r?,upscale,10166
3233,What is the value of the PixelUnshuffle?,nn,10169
3234,"What is thePixelShuffleoperation by rearranging elements in a tensor of shape(,C,H",r,10169
3235,What is Upsample?,nn,10218
3236,What type of data does nn.Upsample Upsample?,multi-channel,10218
3237,What does UpsamplingNearest2d do?,upsampling,10221
3238,nn.UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input,nearest neighbor,10221
3239,What type of upsampling does nn.UpsamplingBilinear2d apply?,bilinear,10220
3240,What Applies a 2D bilinear upsampling to an input signal composed of several input channels?,nn.UpsamplingBilinear2d,10220
3241,At what level does nn.DataParallel implement data parallelism?,module level,10041
3242,What does nn.DataParallel implement at the module level?,data parallelism,10041
3243,What is nn.parallel.DistributedDataParallel based on?,ontorch.distributedpackage,10223
3244,At what level is distributed data parallelism based?,module level,10223
3245,What is clip that shows the gradient norm of an iterable of parameters?,clip_grad_norm,9084
3246,What is clip that shows the gradient of an iterable of parameters at specified value?,clip_grad_value,9085
3247,What is clips gradient of an iterable of parameters at specified value?,clip_grad_value,9085
3248,At what value does clip_grad_value represent a gradient of an iterable of parameters?,specified value,9085
3249,How many vectors do parameters_to_vector convert to?,one,10383
3250,What does _to_vector Convert parameters to one vector?,parameters,10383
3251,parameters_to_vector Convert parameters to how many vectors?,one,10383
3252,How many vectors does vector_to_parameters convert to the parameters?,one,11335
3253,What is the conversion of one vector to the parameters?,vector_to_parameters,11335
3254,What is prune.BasePruningMethod used for?,new pruning techniques,10464
3255,What is the abstract base class for creation of new pruning techniques?,prune.BasePruningMethod,10464
3256,What is container holding a sequence of methods for iterative pruning?,prune,10482
3257,For what type of pruning is prune.PruningContainer used?,iterative pruning,10482
3258,What does prune.Identity Utility generate the pruning parametrization with?,mask of ones,10471
3259,What is pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?,prune,10471
3260,What does the prune.Identity pruning method generate?,pruning parametrization,10471
3261,What is currently unpruned?,RandomUnstructured Prune,10490
3262,At what frequency are prune.RandomUnstructured Prune units in a tensor?,random,10490
3263,How can prune.L1Unstructured Prune units in a tensor be pruned?,zeroing out the ones with the lowest L1-norm,10475
3264,What are currently unpruned units in a tensor?,prune.L1Unstructured Prune,10475
3265,What is prune.L1Unstructured Prune units in?,tensor,10475
3266,What does RandomStructured Prune?,prune,10486
3267,At what frequency do prune.RandomStructured Prune entire channels in a tensor?,random,10486
3268,What tensor is prune.LnStructured based on?,Ln-norm,10479
3269,What is entire (currently unpruned) channels in a tensor based on their Ln-,prune.LnStructured Prune,10479
3270,What type of tree is a CustomFromMask?,prune,10467
3271,What is prune?,CustomFromMask,10467
3272,What is prune tensor?,random,10510
3273,What does prune.l1_unstructured Prunes tensor do by removing the specifiedamountof?,removing the specifiedamountof,10502
3274,What type of prunes tensors?,global_unstructured,10495
3275,What does prune.global_unstructured globally prune?,tensors,10495
3276,Prunes tensor corresponding to parameter callednameinmodule by applying what?,pre-computed mask inmask,10494
3277,What type of mask inmask does prune.custom_from_mask use?,pre-computed,10494
3278,What Prunes tensor corresponding to parameter callednameinmodule by applying the pre-computed mask inmask?,prune.custom_from_mask,10494
3279,Where does prune.remove remove the pruning method from?,forward hook,10514
3280,What does weight_norm apply to a parameter in a given module?,weight normalization,11362
3281,What applies weight normalization to a parameter in the given module?,weight_norm,11362
3282,weight_norm Applies what to a parameter in a given module?,weight normalization,11362
3283,What removes the weight normalization reparameterization from a module?,remove_weight_norm,10580
3284,What does spectral_norm apply to a parameter in a given module?,spectral normalization,10741
3285,What Applies spectral normalization to a parameter in the given module?,spectral_norm,10741
3286,What removes the spectral normalization reparameterization from a module?,remove_spectral_norm,10579
3287,What does remove_spectral_norm remove from a module?,spectral normalization reparameterization,10579
3288,What applies spectral normalization to a parameter in a given module?,parametrizations,10384
3289,What does spectral_norm do?,parametrizations,10384
3290,In what type of module does parametrize.register_parametrization add a parametrization to a tensor?,module,10388
3291,What does remove the parametrizations on a tensor in a module?,parametrize,10389
3292,What type of module does parametrize.remove_parametrizations remove the parametrizations on a tensor in?,module,10389
3293,The parametrize.cached Context manager enables the caching system within what?,parametrizations,10386
3294,What does the parametrize.cached Context manager enable the caching system within?,parametrizations registered withregister_parametrization(),10386
3295,What is Context manager that enables the caching system within parametrizations registered withregister_parametrization()?,parametrize.cached,10386
3296,What ReturnsTrue if module has an active parametrization?,parametrize.is_parametrized,10387
3297,What is container that holds and manages the originalparameter or buffer of a parametrizedtorch.nn.,parametrize.ParametrizationList,10385
3298,What is a parametrize.ParametrizationList a sequential container that holds and manages theoriginalparameter or buffer of?,parametrizedtorch,10385
3299,What holds the data and list ofbatch_sizesof a packed sequence?,nn.utils.rnn.PackedSequence,10224
3300,What type of sequence contains padded sequences of variable length?,Tensor,10225
3301,What type of sequences are padded sequences?,variable length,10225
3302,What type of sequences does nn.utils.rnn.pack_padded_sequence Pack a Tensor,padded sequences,10225
3303,Pads a packed batch of what?,variable length sequences,10227
3304,What type of Tensors does nn.utils.rnn.pad_sequence Pad a list of?,variable length,10228
3305,Pad a list of variable length what?,Tensors,10228
3306,What type of Tensors does nn.utils.rnn.pack_sequence pack?,variable length,10226
3307,What is the variable length of a pack?,Tensors,10226
3308,nn.Flatten Flattens a contiguous range of what into a tensor?,dims,10054
3309,nn.Flatten Flattens a contiguous range of dims into what?,tensor,10054
3310,What does nn.Unflatten unflatten?,tensor dim,10216
3311,What does Unflatten do to a tensor dim?,expanding it to a desired shape,10216
3312,What is modules that lazily initialize parameters?,lazy modules,10222
3313,What is another name for modules that lazily initialize parameters?,lazy modules,10222
3314,What is False sorted_sequence[i-1]values[m][n]...[l][x,1-D,91
3315,What is sorted_sequence[i-1]values[m][n]...[l][x]=,False,91
3316,sorted_sequence[i-1]=values[m][n]?,True,93
3317,What is true sorted_sequence[i-1]=values[m][n]?,1-D,93
3318,What is the default value for sorted_sequence[m][n]?,False,4294
3319,What is the value of sorted_sequence[m]?,True,4295
3320,What is the latest version of Microsoft Visual Studio?,Visual Studio 2019,105
3321,10.1 Visual Studio 2019 (16.X) (_MSC_VER 1930)  1.7.0?,1.3.0,105
3322,What is the version number of Visual Studio 2019?,10.2,106
3323,What is the latest version of Visual Studio?,Visual Studio 2019,107
3324,fft Computes the one dimensional discrete what of input?,Fourier transform,9332
3325,ifft Computes the one dimensional inverse discrete what of input?,Fourier transform,9585
3326,What computes the 2 dimensional inverse discrete Fourier transform of input?,ifft2,9586
3327,What Computes the 2 dimensional inverse discrete Fourier transform of input?,ifft2,9586
3328,fftn Computes the N dimensional discrete what?,Fourier transform,9334
3329,What does fftn compute?,N dimensional discrete Fourier transform,9334
3330,fftn Computes the N dimensional discrete what of input?,Fourier transform,9334
3331,ifftn Computes the N dimensional inverse discrete what?,Fourier transform,9587
3332,What does ifftn compute?,N dimensional inverse discrete Fourier transform,9587
3333,What computes the N dimensional inverse discrete Fourier transform of input?,ifftn,9587
3334,rfft Computes what kind of transform of real-valuedinput?,one dimensional Fourier transform,10606
3335,What Computes the one dimensional Fourier transform of real-valuedinput?,rfft,10606
3336,rfft Computes the one dimensional what of real-valuedinput?,Fourier transform,10606
3337,What Computes the inverse ofrfft()?,irfft,9696
3338,What Computes the 2-dimensional discrete Fourier transform of realinput?,rfft2,10607
3339,rfft2 Computes the 2-dimensional discrete Fourier transform of what?,realinput,10607
3340,What Computes the inverse ofrfft2()?,irfft2,9697
3341,What does rfftn compute?,N-dimensional discrete Fourier transform,10609
3342,rfftn Computes the N-dimensional discrete Fourier transform of what?,realinput,10609
3343,What Computes the inverse ofrfftn()?,irfftn,9698
3344,hfft Computes the one dimensional discrete what transform of a Hermitian symmetricinputsignal?,Fourier,9511
3345,hfft Computes the one dimensional discrete what of a Hermitian symmetricinputsignal?,Fourier transform,9511
3346,hfft Computes the one dimensional discrete Fourier transform of what?,Hermitian symmetricinputsignal,9511
3347,What Computes the inverse ofhfft()?,ihfft,9592
3348,What computes the discrete Fourier Transform sample frequencies for a signal of sizen?,fftfreq,9333
3349,fftfreq Computes the discrete what?,Fourier Transform,9333
3350,rfftfreq Computes the sample frequencies forrfft()with a signal of what?,sizen,10608
3351,What computes the sample frequencies forrfft?,rfftfreq,10608
3352,What Computes the sample frequencies forrfft() with a signal of sizen?,rfftfreq,10608
3353,fftshift reorders what dimension of FFT data?,n-dimensional,9335
3354,What does fftshift order FFT data to have first?,negative frequency terms,9335
3355,fftshift Reorders n-dimensional FFT data as provided by what?,byfftn(),9335
3356,What is function that does offftshift?,ifftshift Inverse offftshift(),9588
3357,What is ifftshift Inverse?,offftshift,9588
3358,What does i-1 mean?,True boundaries,8001
3359,What does [i-1]=input[m][n]...[l][x]boundaries[i]?,True boundaries,8001
3360,What does [i-1]input[m][n]...[l][x]=boundaries[i]?,False boundaries,2401
3361,What does ornn.Module return?,aScriptModuleorScriptFunction,10650
3362,What type of compilation is used to optimize a function?,just-in-time compilation,11245
3363,What will return an executable orScriptFunction that will be optimized using just-in-time compilation?,trace Trace,11245
3364,What does script_if_tracing call when it is first called during tracing?,Compilesfn,10652
3365,What compilesfn when it is first called during tracing?,script_if_tracing,10652
3366,What does Compilesfn do when it is first called during tracing?,script_if_tracing,10652
3367,What is module that returns an executableScriptModule that will be optimized using just-in-time compilation?,trace_module,11247
3368,what does fork create?,an asynchronous task executingfunc,9388
3369,What creates an asynchronous task executingfuncand a reference to the value of the result of this execution?,fork,9388
3370,What is the name of a.Future[T]asynchronous task?,a torch.jit,11353
3371,What does a torch.jit.Future[T]asynchronous wait for?,Forces,11353
3372,ScriptModule A wrapper around what?,C++torch::jit::Module,5751
3373,What is a wrapper around C++torch?,ScriptModule,5751
3374,What is ScriptModule a wrapper around?,C++torch::jit::Module,5751
3375,What is ScriptFunction functionally equivalent to?,aScriptModule,5750
3376,What does freeze Freezing aScriptModule do?,clone it,9394
3377,"What does freeze Freezing attempt to inline the cloned module's submodules, parameters, and attributes as constants in",TorchScript IR Graph,9394
3378,How do you save an offline version of this module for use in a separate process?,save,10643
3379,What kind of version of this module can be saved for use in a separate process?,offline,10643
3380,What did load Load aScriptModuleorScriptFunctionpreviously save?,with torch.jit.save,9804
3381,What was previously saved with torch.jit.saved?,Load aScriptModuleorScriptFunction,9804
3382,What did load Load aScriptModuleorScriptFunctionpreviously saved?,with torch.jit.save,9804
3383,What language does ignore indicate to the compiler that a function or method should be ignored and left as?,Python,9591
3384,What does the decorator indicate to the compiler that should be ignored and left as a Python function?,a function or method,9591
3385,The decorator indicates to the compiler that a function or method should be what and left as a Python function?,ignored,9591
3386,What indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception?,decorator,11287
3387,The isinstance function provides for conatiner type refinement in what?,TorchScript,9728
3388,What does isinstance provide for in TorchScript?,conatiner type refinement,9728
3389,What function provides for conatiner type refinement in TorchScript?,isinstance,9728
3390,isinstance provides for conatiner type refinement in what?,TorchScript,9728
3391,What does Attribute This method indicate to the TorchScript compiler that the left-hand side expression is a class instance of?,attribute,1368
3392,What compiler uses Attribute This method to indicate that the left-hand side expression is a class instance attribute with type oftype?,TorchScript,1368
3393,Attribute This method is used to indicate to the TorchScript compiler that the left-hand side expression is a what?,class instance attribute,1368
3394,Attribute This method is mostly used to indicate to whom that the left-hand side expression is a class instance attribute with type oftype?,the TorchScript compiler,1368
3395,Attribute This method is mostly used to indicate to the TorchScript compiler that the left-hand side expression is what?,a class instance attribute with type oftype,1368
3396,Who uses the annotate method to hint at the type of the value?,TorchScript compiler,8868
3397,What is a pass-through function that returns the_value?,annotate,8868
3398,What compiler uses the annotate method to hint at the type of the_value?,TorchScript,8868
3399,What is 'fro'?,Frobenius norm,11489
3400,What is the term for 'fro'?,Frobenius norm,11489
3401,What is the definition of a nuclear standard?,nuclear norm,11486
3402,What is ‘nuc’?,nuclear norm,11486
3403,What is the sum of abs(x)**ord)**(1./ord)?,Number,4550
3404,What is a number?,sum(abs(x)**ord)**(1./ord),4550
3405,What is Linear / Identity 111?,Linear / Identity 111,4111
3406,What is the definition of Linear / Identity 111?,Linear / Identity 111,4111
3407,"How many Conv1,2,3D are there?",111,1911
3408,"How many conv1,2,3D?",111,1911
3409,What is Sigmoid 111?,Sigmoid 111,5899
3410,What is Sigmoid?,Sigmoid 111,5899
3411,What is the term for 53frac5335?,Tanh,6186
3412,How many lines does Leaky Relu have?,21,4072
3413,Who is responsible for 21+negative_slope2sqrtfrac21 + textnegative_s,Leaky Relu,4072
3414,What is negative slope2sqrtfrac21 + textnegative_slope,Leaky Relu 21,4072
3415,What is a float32ortorch?,32-bit floating point torch,148
3416,How many bits is float32ortorch?,32,148
3417,How many bits are in a floating point1 torch?,16,122
3418,What torch is a cuda?,HalfTensor,122
3419,What type of torch is the bfloat16 torch?,16-bit floating point2 torch,126
3420,What is bfloat16 torch?,16-bit floating point2 torch,126
3421,What type of torch is complex32?,32-bit,144
3422,What is 32-bit complex torch?,complex32,144
3423,What is the bit count for a complex torch?,64,157
3424,What is 64-bit version of the torch?,complex torch.complex64,157
3425,What kind of complex torch is complex64?,64-bit,157
3426,What is 64-bit complex torch?,complex64,157
3427,How many bit integers are in the torch?,32,154
3428,Int32ortorch.int torch.cuda.IntTensor torch.cuda.IntT,32-bit,154
3429,What type of integer is the torch?,64-bit,171
3430,What is the torch.cuda.LongTensor?,LongTensor,171
3431,What is the BoolTensor torch?,Boolean torch,1454
3432,What is torch?,Boolean torch,1454
3433,What is a quantized torch?,4-bit integer,10542
3434,What type of torch is unsigned?,quantized 4-bit integer,10542
3435,What is a quantized 4-bit integer (unsigned)3 torch?,ByteTensor,10542
3436,What is quantized 4-bit integer (unsigned)3 torch?,quantized 4-bit integer (unsigned)3 torch,10542
3437,What is a torch that is unsigned?,quantized 8-bit integer,10544
3438,What is torch.qint8?,quantized 8-bit integer,10543
3439,What is torch.qfint32?,32-bit integer,10541
3440,What is torch.IntTensor?,quantized 32-bit integer,10541
3441,What returns a new Tensor withdataas the tensor data?,Tensor.new_tensor,6580
3442,Tensor.new_full returns a Tensor of what size?,size filled,6571
3443,What returns a Tensor of  size filled with fill_value?,Tensor.new_full,6571
3444,What type of Tensor does new_empty return?,size filled with uninitialized data,6569
3445,What returns a Tensor of  size filled with uninitialized data?,Tensor.new_empty,6569
3446,What type of data does Tensor.new_empty return?,uninitialized data,6569
3447,What type of Tensor is returned by Tensor.new_ones?,size filled,6574
3448,What is the Tensor returned by Tensor.new_ones?,size filled with1,6574
3449,What type of Tensor returns a Tensor?,size filled,6582
3450,What is device where the Tensor is located?,thetorch,6337
3451,What is first call to compute gradients for itself?,tobackward(),10963
3452,What attribute is None bydefault?,torch.Tensor.grad,10963
3453,What attribute is None byby default?,Tensor.grad,6407
3454,What is Tensor.ndim Alias fordim?,Tensor.ndim Alias fordim,6559
3455,What is Alias fordim?,Tensor.ndim,6559
3456,What is.abs Seetorch.abs()?,Tensor,6199
3457,What is Tensor.abs?,Seetorch,6199
3458,What is Seetorch.abs() function?,Tensor.abs,6199
3459,What does Tensor.abs_ In-place version ofabs() do?,Tensor.abs_ In-place version ofabs(),6202
3460,What is version ofabs()?,Tensor.abs_ In-place,6202
3461,What is program that creates Alias forabs?,Tensor,6206
3462,What type of Alias are forabs?,Tensor.absolute Alias,6206
3463,What is the Tensor.absolute_ In-place version of?,absolute()Alias forabs_(),6210
3464,What is in-place version of absolute()Alias forabs_()?,Tensor.absolute_ In-place version,6210
3465,What is Seetorch.acos?,Tensor.acos,6213
3466,What is Tensor.acos?,Seetorch.acos,6213
3467,What does Tensor.acos_ In-place version ofacos() do?,Tensor.acos_ In-place version ofacos(),6214
3468,What is in-place version ofacos?,Tensor.acos,6214
3469,What is Tensor.arccos?,Seetorch.arccos,6238
3470,What is arccos function?,Tensor.arccos_ In-place version ofarccos(),6239
3471,What is In-place version ofarccos?,Tensor.arccos,6239
3472,What is a scalar or tensor toselftensor?,Tensor,6217
3473,What is a tensor toselftensor?,scalar,6217
3474,What is add function?,Tensor.add_ In-place version ofadd(),6218
3475,What is version of add()?,Tensor.add_ In-place,6218
3476,What is the Seetorch?,Tensor,6225
3477,What is the term for a Seetorch?,Tensor,6225
3478,What is Tensor.addbmm?,Seetorch,6219
3479,What is in-place version of addbmm()?,Tensor.addbmm_ In-place version ofaddbmm(),6220
3480,What version ofaddbmm() is used?,Tensor.addbmm_ In-place,6220
3481,What is file?,Tensor.trunc Seetorch.trunc,6734
3482,What is Tensor.trunc?,Seetorch.trunc(),6734
3483,What is the name of Seetorch.trunc()?,Tensor.trunc,6734
3484,What does Seetorch.addcdiv() call?,Tensor.addcdiv,6221
3485,What is in-place version of addcdiv()?,Tensor.addcdiv_ In-place version ofaddcdiv(),6222
3486,What is version ofaddcdiv()?,Tensor.addcdiv_ In-place,6222
3487,What is program?,Tensor.addcmul Seetorch.addcmul,6223
3488,What does Seetorch.addcmul() do?,Tensor.addcmul,6223
3489,What is Tensor?,Seetorch,6754
3490,What is Tensor.xlogy?,Tensor.xlogy Seetorch.xlogy,6754
3491,What is Seetorch.xlogy?,Tensor.xlogy,6754
3492,What is in-place version of addcmul()?,Tensor.addcmul_ In-place version ofaddcmul(),6224
3493,What version ofaddcmul() is used?,Tensor.addcmul_ In-place,6224
3494,What is in-place version of addmm()?,Tensor.addmm_ In-place version ofaddmm(),6226
3495,What is version ofaddmm()?,Tensor.addmm_ In-place,6226
3496,What does Tensor.addmv Seetorch.addmv do?,Tensor.addmv Seetorch.addmv(),6227
3497,What is Tensor.addmv?,Seetorch,6227
3498,What is Seetorch.addmv() function?,Tensor.addmv,6227
3499,What is function used by addmv()?,Tensor.addmv_ In-place version ofaddmv(),6228
3500,What version ofaddmv() is used?,Tensor.addmv_ In-place,6228
3501,What does Tensor.addr do?,Tensor.addr Seetorch.addr(),6229
3502,What is Tensor.addr?,Seetorch.addr,6229
3503,What is Seetorch.addr?,Tensor.addr,6229
3504,What is in-place version ofaddr()?,Tensor.addr_ In-place version ofaddr(),6230
3505,What is version ofaddr()?,Tensor.addr_ In-place,6230
3506,What is company that owns the Seetorch?,Tensor.allclose Seetorch.allclose,6232
3507,What does Tensor.allclose?,Seetorch,6232
3508,What is Tensor.amax Seetorch.amax?,Tensor.amax Seetorch.amax,6233
3509,What is Seetorch.amin?,Tensor.amin,6234
3510,What is Tensor.amin Seetorch.amin?,Tensor.amin Seetorch.amin,6234
3511,What is Tensor.angle Seetorch.angle?,Tensor.angle Seetorch.angle,6235
3512,What is a Tensor.angle?,Tensor.angle Seetorch.angle,6235
3513,What does Tensor.apply_ apply to each element in the tensor?,functioncallable,6237
3514,What applies the functioncallableto each element in the tensor?,Tensor.apply_,6237
3515,What is Tensor.argmax Seetorch.argmax?,Tensor.argmax Seetorch.argmax,6250
3516,What is Seetorch.argmax() function?,Tensor.argmax,6250
3517,What is Tensor.argmin?,Seetorch,6251
3518,What is Seetorch.argmin?,Tensor.argmin,6251
3519,What does Tensor.argsort Seetorch.argsort() do?,Tensor.argsort Seetorch.argsort(),6252
3520,What is In-place version ofasin()?,Tensor.asin,6255
3521,What does Seetorch.arcsin call?,Tensor.arcsin,6242
3522,What is Tensor.arcsin?,Seetorch.arcsin,6242
3523,What is Seetorch.arcsin?,Tensor.arcsin,6242
3524,What is in-place version of arcsin()?,Tensor.arcsin_ In-place version ofarcsin(),6243
3525,What is In-place version ofarcsin()?,Tensor.arcsin,6243
3526,What does Tensor.as_strided Seetorch.as_strided() do?,Tensor.as_strided Seetorch.as_strided(),6253
3527,What is a Tensor.as_strided?,Seetorch,6253
3528,What is Tensor.atan Seetorch.atan?,Tensor.atan Seetorch.atan,6258
3529,What is Tensor.atan?,Tensor.atan Seetorch.atan,6258
3530,What is in-place version of atan?,Tensor.atan_ In-place version ofatan(),6261
3531,What is In-place version of atan?,Tensor.atan,6261
3532,What is Seetorch.arctan?,Tensor.arctan,6246
3533,What is in-place version of arctan?,Tensor.arctan_ In-place version ofarctan(),6247
3534,What is In-place version ofarctan?,Tensor.arctan,6247
3535,What does Tensor.atan2 Seetorch.atan2() do?,Tensor.atan2 Seetorch.atan2(),6259
3536,What is Tensor.atan2 Seetorch.atan2()?,Tensor.atan2 Seetorch.atan2(),6259
3537,What is in-place version of atan2()?,Tensor.atan2_ In-place version ofatan2(),6260
3538,What is version of atan2()?,Tensor.atan2_ In-place,6260
3539,What is Tensor.all Seetorch?,Tensor.all Seetorch.all,6231
3540,What is a Seetorch?,Tensor,6236
3541,What computes the gradient of current tensor w.r.t?,Tensor,6264
3542,What computes the gradient of current tensor w.r.t.?,Tensor.backward,6264
3543,What is a baddbmm Seetorch?,Tensor,6265
3544,What is In-place version ofbaddbmm()?,Tensor.baddbmm_ In-place version ofbaddbmm(),6266
3545,What is version ofbaddbmm()?,Tensor.baddbmm_ In-place,6266
3546,Eachresult[i]textttresult[i]result[i]is what?,independently sampled,6267
3547,What fills each location ofselfwith an independent sample fromBernoulli(p)textBernoulli(,Tensor.bernoulli,6268
3548,What is equivalent to self.to(torch.bfloat16)?,self.bfloat16,6271
3549,What is self.bfloat16 equivalent to?,self.to(torch.bfloat16),6271
3550,What is Tensor.bincount?,Seetorch.bincount(),6272
3551,What is Seetorch.bincount function?,Tensor.bincount,6272
3552,What is component that makes a bitwise_not Seetorch.bitwise_not()?,Tensor,6275
3553,What is in-place version ofbitwise_not?,Tensor,6276
3554,What is In-place version ofbitwise_not()?,Tensor.bitwise_not,6276
3555,What is a bitwise and Seetorch?,Tensor,6273
3556,What is the.bitwise_and_ In-place version ofbitwise_and?,Tensor,6274
3557,What version of bitwise_and() is used?,Tensor.bitwise_and_ In-place,6274
3558,What is function?,Tensor.diagonal Seetorch.diagonal(),6342
3559,What is a Tensor.diagonal?,Seetorch,6342
3560,What is Seetorch.bitwise_or?,Tensor.bitwise_or,6277
3561,What is In-place version ofbitwise_or?,Tensor.bitwise_or_ In-place version ofbitwise_or(),6278
3562,What is the In-place version ofbitwise_or?,Tensor.bitwise_or,6278
3563,What is function that determines a bitwise xor?,Tensor.bitwise_xor,6279
3564,What is Tensor.bitwise_xor?,Seetorch,6279
3565,What is In-place version ofbitwise_xor?,Tensor.bitwise_xor,6280
3566,What is the In-place version ofbitwise_xor?,Tensor.bitwise_xor,6280
3567,What is equivalent toself.to(torch.bool)?,self.bool(),6281
3568,What is the equivalent of self.bool()?,self.to(torch.bool),6281
3569,What is tensor.byte self.byte() equivalent to?,self.to(torch.uint8),6283
3570,What is program that broadcasts to Seetorch?,Tensor,6282
3571,What does Tensor.broadcast_to?,Seetorch,6282
3572,What does Seetorch.broadcast_to() do?,Tensor.broadcast_to,6282
3573,What distribution does Tensor.cauchy come from?,Cauchy,6284
3574,Tensor.cauchy_ Fills the tensor with numbers drawn from what distribution?,Cauchy,6284
3575,What is a tent?,Tensor.ceil Seetorch.ceil,6285
3576,What is Tensor.ceil?,Seetorch,6285
3577,What is in-place version of the in-place version of the in-place version of the in-place version of the in,Tensor.t_ In-place version oft(),6706
3578,What is version of oft?,Tensor.t_ In-place,6706
3579,What is version ofceil()?,Tensor.ceil_ In-place,6286
3580,What is equivalent to self.to(torch.int8)?,self.char(),6287
3581,What is tensor.char self.char() equivalent to?,self.to(torch.int8),6287
3582,What is cholesky?,Tensor.cholesky Seetorch.cholesky,6288
3583,What is the name of Seetorch.cholesky?,Tensor.cholesky,6288
3584,What is inverse of the Seetorch.cholesky_inverse?,Tensor.cholesky_inverse,6289
3585,What is Seetorch.cholesky_inverse?,Tensor.cholesky_inverse,6289
3586,What does Tensor.cholesky_solve?,Seetorch.cholesky_solve,6290
3587,What does Seetorch.cholesky_solve do?,Tensor.cholesky_solve,6290
3588,What does Seetorch.cholesky_solve() do?,Tensor.cholesky_solve,6290
3589,What is.chunk Seetorch.chunk()?,Tensor,6291
3590,What is a clam?,Tensor.clamp Seetorch.clamp,6292
3591,What is Tensor.clamp?,Seetorch.clamp,6292
3592,What is Seetorch.clamp() function?,Tensor.clamp,6292
3593,What does Tensor.clamp_ In-place version ofclamp() do?,Tensor.clamp_ In-place version ofclamp(),6293
3594,What version ofclamp() does Tensor.clamp_ In-place?,Tensor.clamp_ In-place,6293
3595,What is Alias forclamp()?,Tensor.clip,6294
3596,What is Alias forclamp?,Tensor.clip_ Alias forclamp_(),6295
3597,What is forclamp_()?,Tensor.clip_ Alias,6295
3598,What is clone?,Tensor.clone Seetorch.clone,6296
3599,What does Tensor.clone?,Seetorch,6296
3600,What is clone Seetorch.clone()?,Tensor,6296
3601,What does Tensor.contiguous return?,a contiguous in memory tensor,6304
3602,Tensor.contiguous Returns a contiguous in memory tensor containing the same data as what?,selftensor,6304
3603,What returns a contiguous in memory tensor containing the same data asselftensor?,Tensor.contiguous,6304
3604,Which element copies the elements fromsrcintoselftensor and returnsself?,Tensor,6305
3605,What _ Copies the elements fromsrcintoselftensor and returnsself?,Tensor.copy,6305
3606,What does Tensor.conj Seetorch.conj do?,Tensor.conj Seetorch.conj(),6303
3607,What is Seetorch.conj?,Tensor.conj,6303
3608,What is Tensor.conj?,Seetorch.conj,6303
3609,What is.copysign Seetorch.copysign() function?,Tensor,6306
3610,What is Seetorch.copysign function?,Tensor.copysign,6306
3611,What is program that does the copysign function?,Tensor,6307
3612,What is version ofcopysign()?,Tensor.copysign_ In-place,6307
3613,What is Seetorch.cos?,Tensor.cos,6308
3614,What is Tensor.cos?,Seetorch.cos,6308
3615,What is the name of Seetorch.cos?,Tensor.cos,6308
3616,What does Tensor.cos do?,Tensor.cos_ In-place version ofcos(),6309
3617,What is in-place version of ofcos?,Tensor.cos,6309
3618,What is website?,Tensor.symeig Seetorch.symeig,6704
3619,What is Tensor.symeig?,Seetorch.symeig,6704
3620,What is Seetorch.symeig?,Tensor.symeig,6704
3621,What is Tensor.cosh?,Seetorch.cosh,6310
3622,What is Seetorch.cosh function?,Tensor.cosh,6310
3623,What does Tensor.cosh do?,Tensor.cosh_ In-place version ofcosh(),6311
3624,What is In-place version ofcosh()?,Tensor.cosh,6311
3625,What does Seetorch.count_nonzero do?,Tensor.count_nonzero,6312
3626,What is Tensor.acosh Seetorch.acosh?,Tensor.acosh Seetorch.acosh,6215
3627,What is Seetorch.acosh function?,Tensor.acosh,6215
3628,What is in-place version of acosh?,Tensor.acosh_ In-place version ofacosh(),6216
3629,What is In-place version ofacosh()?,Tensor.acosh,6216
3630,What does arccosh acosh() stand for?,Tensor,6240
3631,What does arccosh acosh() -> Tensor?,Tensor,6240
3632,What is Tensor.arccosh?,acosh,6240
3633,What does arccosh_ acosh_() stand for?,Tensor,6241
3634,What is the name of Tensor.arccosh?,acosh_(),6241
3635,What does arccosh_ acosh_() -> Tensor?,Tensor,6241
3636,What does Tensor.arccosh_?,acosh,6241
3637,What returns a copy of this object in CPU memory?,Tensor.cpu,6313
3638,What type of memory does Tensor.cpu return a copy of?,CPU,6313
3639,What is Tensor.cross Seetorch.cross?,Tensor.cross Seetorch.cross,6314
3640,What is Tensor.cross?,Seetorch,6314
3641,What is Seetorch.cross function?,Tensor.cross,6314
3642,What returns a copy of this object in CUDA memory?,Tensor.cuda,6318
3643,Tensor.cuda returns a copy of this object in what?,CUDA memory,6318
3644,What is Tensor.logcumsumexp?,Seetorch.logcumsumexp,6503
3645,What does Seetorch.logcumsumexp use?,Tensor.logcumsumexp,6503
3646,What is Tensor.cummax Seetorch.cummax?,Tensor.cummax Seetorch.cummax,6319
3647,What is Tensor.cummax?,Seetorch.cummax,6319
3648,What is Seetorch.cummax() function?,Tensor.cummax,6319
3649,What is Tensor.cummin?,Seetorch,6320
3650,What is Seetorch.cummin?,Tensor.cummin,6320
3651,What is Seetorch.cumprod?,Tensor.cumprod,6321
3652,What is in-place version ofcumprod()?,Tensor.cumprod_ In-place version ofcumprod(),6322
3653,What is version ofcumprod?,Tensor.cumprod_ In-place,6322
3654,What is Tensor.cumsum Seetorch.cumsum?,Tensor.cumsum Seetorch.cumsum,6323
3655,What is Seetorch.cumsum?,Tensor.cumsum,6323
3656,What is In-place version ofcumsum()?,Tensor.cumsum_ In-place version ofcumsum(),6324
3657,What is version ofcumsum()?,Tensor.cumsum_ In-place,6324
3658,What returns the address of the first element of selftensor?,Tensor.data_ptr,6326
3659,What does Tensor.deg2rad do?,Tensor.deg2rad Seetorch.deg2rad(),6327
3660,What is Tensor.deg2rad?,Seetorch.deg2rad,6327
3661,What is Seetorch.deg2rad() function?,Tensor.deg2rad,6327
3662,What can you do with a quantized Tensor?,dequantize it and return the dequantized float Tensor,6333
3663,What do you do with a quantized Tensor?,dequantize it,6333
3664,What is Tensor.det?,Seetorch.det,6334
3665,What is Seetorch.det?,Tensor.det,6334
3666,What returns the number of dense dimensions in asparse tensorself?,Tensor.dense_dim,6332
3667,"Tensor.detach Returns a new Tensor, what from the current graph?",detached,6335
3668,"Detaches the Tensor from the graph that created it, making it what?",leaf,6336
3669,What detaches the Tensor from the graph that created it?,torch,10960
3670,What does the Tensor become?,leaf,10960
3671,What does torch.Tensor.detach_ Detaches the Tensor from the graph that created it?,leaf,10960
3672,What is component in the Seetorch?,Tensor,6410
3673,What is.greater_equal()?,Seetorch,6410
3674,What is Tensor.diag Seetorch.diag?,Tensor.diag Seetorch.diag,6339
3675,What does Tensor.diag_embed Seetorch.diag_embed() do?,Tensor.diag_embed Seetorch.diag_embed(),6340
3676,What does Tensor.diagflat Seetorch.diagflat() do?,Tensor.diagflat Seetorch.diagflat(),6341
3677,What is Tensor.diagflat?,Seetorch,6341
3678,What is Seetorch.diagflat() function?,Tensor.diagflat,6341
3679,What is the minimum dimension of a tensor?,2-dimensions,6374
3680,What is Tensor.fmax Seetorch.fmax()?,Tensor.fmax Seetorch.fmax(),6388
3681,What is Seetorch.fmin?,Tensor.fmin,6389
3682,What does Tensor.diff Seetorch.diff do?,Tensor.diff Seetorch.diff(),6343
3683,What is Seetorch.diff() function?,Tensor.diff,6343
3684,What is Tensor.digamma?,Seetorch.digamma,6344
3685,What is.digamma Seetorch?,Tensor,6344
3686,What is In-place version ofdigamma?,Tensor.digamma_ In-place version ofdigamma(),6345
3687,What is In-place version ofdigamma()?,Tensor.digamma,6345
3688,What returns the number of dimensions of selftensor?,Tensor,6346
3689,Tensor.dim returns the number of dimensions of what?,selftensor,6346
3690,What does Tensor.dist Seetorch.dist do?,Tensor.dist Seetorch.dist(),6347
3691,What is Seetorch.dist?,Tensor.dist,6347
3692,What is.div Seetorch.div?,Tensor,6348
3693,What does Tensor.div Seetorch.div() do?,Tensor.div Seetorch.div(),6348
3694,What is div function?,Tensor.div_ In-place version ofdiv(),6349
3695,What is version ofdiv()?,Tensor.div_ In-place,6349
3696,What is component that creates Seetorch.divide()?,Tensor,6350
3697,What does Seetorch.divide() do?,Tensor.divide,6350
3698,What is in-place version ofdivide()?,Tensor.divide_ In-place version ofdivide(),6351
3699,What is version ofdivide()?,Tensor.divide_ In-place,6351
3700,What does Tensor.dot Seetorch.dot() do?,Tensor.dot Seetorch.dot(),6352
3701,What is Tensor.dot?,Seetorch.dot,6352
3702,What is Seetorch.dot() function?,Tensor.dot,6352
3703,What is tensor.double self.double() equivalent to?,self.to(torch.float64),6353
3704,What is Tensor.dsplit?,Seetorch.dsplit,6354
3705,What is Seetorch.dsplit() function?,Tensor.dsplit,6354
3706,What is file that is used to create a Seetorch?,Tensor.eig Seetorch.eig(),6355
3707,What is Tensor.eig?,Seetorch,6355
3708,What is Seetorch.eig?,Tensor.eig,6355
3709,What returns the size in bytes of an individual element?,Tensor.element_size,6356
3710,Tensor.element_size Returns the size in what?,bytes,6356
3711,What does Tensor.eq Seetorch.eq do?,Tensor.eq Seetorch.eq(),6357
3712,What is Seetorch.eq() function?,Tensor.eq,6357
3713,What does Tensor.eq_ In-place version ofeq() do?,Tensor.eq_ In-place version ofeq(),6358
3714,What is version ofeq()?,Tensor.eq_ In-place,6358
3715,What is Tensor.equal Seetorch.equal?,Tensor.equal Seetorch.equal,6359
3716,What is Seetorch.erf?,Tensor.erf,6360
3717,What is version oferf()?,Tensor.erf_ In-place,6361
3718,What does Tensor.erfc do?,Tensor.erfc Seetorch.erfc(),6362
3719,What is Tensor.erfc?,Seetorch.erfc,6362
3720,What is Seetorch.erfc?,Tensor.erfc,6362
3721,What is function that is used by the Tensor.erfc?,Tensor.erfc_ In-place version oferfc(),6363
3722,What is version of oferfc()?,Tensor.erfc_ In-place,6363
3723,What is Tensor.erfinv?,Seetorch.erfinv,6364
3724,What is Seetorch.erfinv?,Tensor.erfinv,6364
3725,What is In-place version of oferfinv()?,Tensor.erfinv,6365
3726,What is.exp Seetorch.exp?,Tensor,6366
3727,What is Tensor.exp?,Seetorch,6366
3728,What is Seetorch.exp?,Tensor.exp,6366
3729,What does Tensor.exp_ In-place version ofexp() do?,Tensor.exp_ In-place version ofexp(),6367
3730,What version ofexp() does Tensor.exp_ In-place?,Tensor.exp_ In-place,6367
3731,What is component in the Seetorch.expm1 function?,Tensor,6370
3732,What is Seetorch.expm1?,Tensor.expm1,6370
3733,What does Tensor.expm1_ In-place version ofexpm1() do?,Tensor.expm1_ In-place version ofexpm1(),6371
3734,What is version ofexpm1()?,Tensor.expm1_ In-place,6371
3735,What is the size of the selftensor?,singleton,6368
3736,What returns a new view of the self tensor with singleton dimensions expanded to a larger size?,Tensor.expand,6368
3737,What does Tensor.expand_as do?,Expand this tensor to the same size asother,6369
3738,What does the tensor do?,expand,6369
3739,What fillsselftensor with elements drawn from the exponential distribution?,Tensor.exponential,6372
3740,What type of tensor is filled with elements drawn from the exponential distribution?,exponential,6372
3741,What is program that fixes Seetorch.fix()?,Tensor,6375
3742,What does Tensor.fix Seetorch.fix() do?,Tensor.fix Seetorch.fix(),6375
3743,What does Tensor.fix_ In-place version offix() do?,Tensor.fix_ In-place version offix(),6376
3744,What is version offix()?,Tensor.fix_ In-place,6376
3745,What does it do with the specified value?,Tensor.fill_ Fillsselftensor,6373
3746,What does fillsselftensor with the specified value?,Tensor.fill,6373
3747,What does Tensor.flatten stand for?,seetorch.flatten,6377
3748,What is the term for a seetorch?,Tensor,6377
3749,What does Tensor.flip?,Seetorch,6378
3750,What does Seetorch.flip() do?,Tensor.flip,6378
3751,What is Tensor.fliplr?,Seetorch.fliplr,6379
3752,What is Tensor.flipud?,Seetorch.flipud,6380
3753,What is Seetorch.flipud?,Tensor.flipud,6380
3754,What is equivalent toself.to(torch.float32)?,self.float(),6381
3755,What is the equivalent of Tensor.float self.float()?,self.to(torch.float32),6381
3756,What is float_power Seetorch.float_power()?,Tensor,6382
3757,What is Tensor.float_power?,Seetorch,6382
3758,What does Tensor.float_power Seetorch.float_power() do?,Tensor.float_power Seetorch.float_power(),6382
3759,What is Tensor.float_power_ In-place version offloat_power()?,Tensor.float_power_ In-place version offloat_power(),6383
3760,What version of float_power() is used?,Tensor.float_power_ In-place,6383
3761,What is.floor Seetorch.floor?,Tensor,6384
3762,What is Tensor.floor?,Seetorch.floor,6384
3763,What is Seetorch.floor?,Tensor.floor,6384
3764,What is floor()?,Tensor.floor_ In-place version offloor(),6385
3765,What is version of floor?,Tensor.floor_ In-place,6385
3766,What is component that creates the Seetorch.floor_divide?,Tensor,6386
3767,What is Seetorch.floor_divide() function?,Tensor.floor_divide,6386
3768,What is in-place version of floor_divide?,Tensor,6387
3769,What is floor_divide?,floor_divide_ In-place version,6387
3770,What is the in-place version of Tensor.floor_divide?,floor_divide(),6387
3771,Floor_divide_ In-place version offloor_divide()?,Tensor,6387
3772,What does Tensor.fmod Seetorch.fmod call?,Tensor.fmod Seetorch.fmod(),6390
3773,What is Tensor.fmod?,Seetorch.fmod,6390
3774,What is Seetorch.fmod?,Tensor.fmod,6390
3775,What does Tensor.fmod_ In-place version offmod() do?,Tensor.fmod_ In-place version offmod(),6391
3776,What is version offmod()?,Tensor.fmod_ In-place,6391
3777,What is Tensor.frac?,Seetorch,6392
3778,What is Seetorch.frac()?,Tensor.frac,6392
3779,What is in-place version of frac()?,Tensor.frac_ In-place version offrac(),6393
3780,What is version of frac()?,Tensor.frac_ In-place,6393
3781,What does Seetorch.frexp use?,Tensor.frexp,6394
3782,What is Seetorch.frexp?,Tensor.frexp,6394
3783,What is Tensor.gather Seetorch.gather?,Tensor.gather Seetorch.gather,6395
3784,What does Seetorch.gather do?,Tensor.gather,6395
3785,What is.gcd Seetorch.gcd?,Tensor,6396
3786,What does Tensor.gcd stand for?,Seetorch.gcd,6396
3787,What is Seetorch.gcd?,Tensor.gcd,6396
3788,What is gcd function?,Tensor.gcd_ In-place version ofgcd(),6397
3789,What is version of gcd()?,Tensor.gcd_ In-place,6397
3790,What does Tensor.ge Seetorch.ge do?,Tensor.ge Seetorch.ge(),6398
3791,What is Tensor.ge Seetorch.ge?,Tensor.ge Seetorch.ge,6398
3792,What does Tensor.ge do?,Tensor.ge_ In-place version ofge(),6399
3793,What is In-place version ofge()?,Tensor.ge,6399
3794,What is In-place version ofgreater_equal()?,Tensor,6411
3795,What is the In-place version ofgreater_equal()?,Tensor.greater_equal,6411
3796,What type of tensor is filled with elements drawn from the geometric distribution?,geometric,6400
3797,What is fillsselftensor?,Tensor.geometric,6400
3798,What is Seetorch.geqrf?,Tensor.geqrf,6401
3799,What does Tensor.ger Seetorch.ger do?,Tensor.ger Seetorch.ger(),6402
3800,What is Tensor.ger?,Seetorch.ger,6402
3801,What is Seetorch.ger function?,Tensor.ger,6402
3802,What returns the device ordinal of the GPU on which the tensor resides?,Tensor.get_device,6403
3803,What function returns the device ordinal of the GPU on which the tensor resides?,Tensor.get_device,6403
3804,What device does Tensor.get_device return the device ordinal of?,the GPU,6403
3805,What is Seetorch.gt()?,Tensor.gt,6412
3806,What is gt function?,Tensor.gt_ In-place version ofgt(),6413
3807,What is version ofgt()?,Tensor.gt_ In-place,6413
3808,What is component that makes a great Seetorch?,Tensor,6408
3809,What is In-place version ofgreater()?,Tensor.greater,6409
3810,What is the equivalent of self.to(torch.float16)?,self.half,6414
3811,What is tensor.half self.half() equivalent to?,self.to(torch.float16),6414
3812,What does Seetorch.nn.functional.hardshrink() do?,Tensor.hardshrink,6415
3813,What type of hardshrink Seetorch.nn.functional.hardshrink()?,Tensor,6415
3814,What is Tensor.heaviside Seetorch.heaviside?,Tensor.heaviside Seetorch.heaviside,6416
3815,What is Tensor.heaviside?,Seetorch,6416
3816,What does Tensor.histc do?,Tensor.histc Seetorch.histc(),6417
3817,What is Tensor.histc?,Seetorch.histc,6417
3818,What is Seetorch.histc?,Tensor.histc,6417
3819,What is Tensor.hsplit?,Seetorch.hsplit,6418
3820,What is Seetorch.hsplit() function?,Tensor.hsplit,6418
3821,What is hypnotist?,Tensor.hypot Seetorch.hypot,6419
3822,What is Tensor.hypot?,Seetorch.hypot,6419
3823,What is Seetorch.hypot() function?,Tensor.hypot,6419
3824,What is In-place version ofhypot()?,Tensor.hypot,6420
3825,What does Tensor.i0 Seetorch.i0 do?,Tensor.i0 Seetorch.i0(),6421
3826,What does Tensor.i0_ In-place version ofi0() do?,Tensor.i0_ In-place version ofi0(),6422
3827,What is version ofi0()?,Tensor.i0_ In-place,6422
3828,What is Tensor.igamma Seetorch.igamma?,Tensor.igamma Seetorch.igamma,6423
3829,What is Tensor.igamma?,Seetorch,6423
3830,What is Seetorch.igamma?,Tensor.igamma,6423
3831,What is In-place version ofigamma?,Tensor.igamma_ In-place version ofigamma(),6424
3832,What is In-place version ofigamma()?,Tensor.igamma_ In-place version ofigamma(),6424
3833,What is Tensor.igammac Seetorch.igammac?,Tensor.igammac Seetorch.igammac,6425
3834,What does Tensor.igammac_ In-place version ofigammac() do?,Tensor.igammac_ In-place version ofigammac(),6426
3835,What is version ofigammac()?,Tensor.igammac_ In-place,6426
3836,What does Tensor.index_add_ Accumulate the elements of?,attr:alphatimestensorinto the self tensor,6430
3837,How do you add the elements of attr:alphatimestensorinto the self tensor?,by adding to the indices,6430
3838,What is the Out-of-place version of of torch.Tensor?,Tensor,6433
3839,What version of of torch.Tensor.index_fill_()?,Tensor.index_fill Out-of-place,6433
3840,What is version of Tensor.index_add?,Out-of-place,6429
3841,What version of of torch.Tensor.index_add_()?,Tensor.index_add Out-of-place,6429
3842,How are the elements of a selftensor copied?,the indices in the order given inindex,6432
3843,What _ Copies the elements oftensorinto the self tensor by selecting the indices in the order given inindex?,Tensor.index_copy,6432
3844,What is version of Tensor.index_copy?,Out-of-place,6431
3845,What version of of torch.Tensor.index_copy_()?,Tensor.index_copy Out-of-place,6431
3846,What does Tensor.index_fill_ select to fill the elements of the self tensor with valuevalue?,the indices in the order given inindex,6434
3847,What fills the elements of the self tensor with valuevalue?,Tensor.index_fill,6434
3848,What does Tensor.index_put_ put values from the tensorvaluesinto the tensorselfusing?,indices,6436
3849,What puts values from the tensorvaluesinto the tensorselfusing the indices specified inindices?,Tensor.index_put,6436
3850,What is out-place version of index_put_()?,Tensor,6435
3851,What is the Out-place version of index_put_()?,index_put,6435
3852,What version of index_put_() is used?,Tensor.index_put Out-place,6435
3853,What does Tensor.index_select?,Seetorch,6437
3854,What does Tensor.index_select Seetorch.index_select() do?,Tensor.index_select Seetorch.index_select(),6437
3855,What indices return the indices tensor of asparse COO tensor?,Tensor,6441
3856,What returns the indices tensor of asparse COO tensor?,indices,6441
3857,What is equivalent toself.to(torch.int32)?,self.int,6442
3858,What is self.int equivalent to?,self.to(torch.int32),6442
3859,What is the underlying uint8_t value of the given Tensor?,uint8_t,6443
3860,What returns a CPU Tensor with uint8_t as data type?,self.int_repr(),6443
3861,What is the data type that stores the underlying uint8_t values of the given Tensor?,uint8_t,6443
3862,What is inverse Seetorch?,Tensor,6444
3863,What is Tensor.inverse Seetorch.inverse()?,Tensor.inverse Seetorch.inverse(),6444
3864,What is component that closes Seetorch.isclose?,Tensor,6468
3865,What does Tensor.isclose?,Seetorch,6468
3866,What is.isfinite?,Seetorch,6469
3867,What is Seetorch.isfinite?,Tensor.isfinite,6469
3868,What is Seetorch.isinf function?,Tensor.isinf,6470
3869,What is Tensor.isposinf?,Seetorch.isposinf,6473
3870,What is Seetorch.isposinf function?,Tensor.isposinf,6473
3871,What does Seetorch.isneginf do?,Tensor.isneginf,6472
3872,What is Tensor.isneginf?,Seetorch.isneginf,6472
3873,What is Seetorch.isneginf function?,Tensor.isneginf,6472
3874,What is Tensor.isnan Seetorch.isnan?,Tensor.isnan Seetorch.isnan,6471
3875,What happens if selftensor is contiguous in memory in the order specified by memory format?,Tensor.is_contiguous Returns True,6451
3876,What Returns True if selftensor is contiguous in memory in the order specified by memory format?,Tensor.is_contiguous,6451
3877,What returns true if the data type ofselfis a complex data type?,Tensor.is_complex,6450
3878,What Returns True if the data type ofselfis a complex data type?,Tensor.is_complex,6450
3879,What returns true if the data type ofselfis a floating point data type?,Tensor.is_floating_point,6453
3880,Tensor.is_floating_point Returns what if the data type ofselfis a floating point data type?,True,6453
3881,What Returns True if the data type ofselfis a floating point data type?,Tensor.is_floating_point,6453
3882,What are all Tensors that have grad which is False?,leaf Tensors,6454
3883,What returns true if the tensor resides in pinned memory?,Tensor.is_pinned,6458
3884,Where does the tensor reside?,pinned memory,6458
3885,"If this tensor resides in pinned memory, what does Tensor.is_pinned return?",true,6458
3886,What do both tensors point to?,exact same memory,6460
3887,"If both tensors are pointing to the exact same memory, what does Tensor.is_set_to Returns True?",if both tensors are pointing to the exact same memory,6460
3888,Tensor.is_shared Checks if tensor is in what type of memory?,shared memory,6461
3889,What check if tensor is in shared memory?,Tensor,6461
3890,What checks if tensor is in shared memory?,Tensor.is_shared,6461
3891,Tensor.is_signed Returns what if the data type ofselfis a signed data type?,True,6462
3892,What returns true if the data type ofselfis a signed data type?,Tensor.is_signed,6462
3893,What Returns True if the data type ofselfis a signed data type?,Tensor.is_signed,6462
3894,What type of storage layout does the Tensor use?,sparse storage layout,6466
3895,Is the Tensor true if the Tensor uses sparse storage layout?,sparse,6466
3896,What does Tensor.istft do?,Tensor.istft Seetorch.istft(),6475
3897,What is Tensor.istft?,Seetorch.istft,6475
3898,What is Seetorch.istft?,Tensor.istft,6475
3899,What does Seetorch.isreal do?,Tensor.isreal,6474
3900,What is the real name for Tensor?,Seetorch,6474
3901,What is the name of Seetorch?,Tensor.isreal,6474
3902,What language does Tensor.item come from?,Python,6476
3903,What language does Tensor.item return the value of this tensor as a standard number?,Python,6476
3904,What is value?,Tensor.kthvalue Seetorch.kthvalue,6477
3905,What is a Tensor.kthvalue?,Seetorch,6477
3906,What is Seetorch.kthvalue?,Tensor.kthvalue,6477
3907,What is Seetorch.lcm()?,Tensor.lcm,6478
3908,What is in-place version of the in-place version oflcm()?,Tensor.lcm_ In-place version oflcm(),6479
3909,What is version oflcm()?,Tensor.lcm_ In-place,6479
3910,What does Seetorch.ldexp do?,Tensor.ldexp,6480
3911,What is Tensor.ldexp?,Seetorch.ldexp,6480
3912,What is Seetorch.ldexp?,Tensor.ldexp,6480
3913,What does Tensor.ldexp_ In-place version ofldexp() do?,Tensor.ldexp_ In-place version ofldexp(),6481
3914,What is version ofldexp()?,Tensor.ldexp_ In-place,6481
3915,What is Tensor.le Seetorch.le?,Tensor.le Seetorch.le,6482
3916,What does Tensor.le_ In-place version ofle do?,Tensor.le_ In-place version ofle(),6483
3917,What is version ofle()?,Tensor.le_ In-place,6483
3918,What is component that makes Seetorch less_equal?,Tensor,6488
3919,What is Tensor.less_equal?,Seetorch,6488
3920,What is In-place version ofless_equal()?,Tensor,6489
3921,What is the In-place version ofless_equal()?,Tensor.less_equal,6489
3922,What is Seetorch.lerp?,Tensor.lerp,6484
3923,What does Tensor.lerp_ In-place version oflerp do?,Tensor.lerp_ In-place version oflerp(),6485
3924,What is version oflerp()?,Tensor.lerp_ In-place,6485
3925,What is Tensor.lgamma?,Seetorch.lgamma,6490
3926,What is Seetorch.lgamma?,Tensor.lgamma,6490
3927,What is in-place version oflgamma?,Tensor.lgamma_ In-place version oflgamma(),6491
3928,What is In-place version oflgamma()?,Tensor.lgamma,6491
3929,What does Tensor.log Seetorch.log do?,Tensor.log Seetorch.log(),6492
3930,What is Tensor.log Seetorch.log()?,Tensor.log Seetorch.log(),6492
3931,What does Tensor.log_ In-place version oflog() do?,Tensor.log_ In-place version oflog(),6499
3932,What is version oflog?,Tensor.log_ In-place,6499
3933,What is Tensor.logdet?,Seetorch.logdet,6504
3934,What is Seetorch.logdet?,Tensor.logdet,6504
3935,What does Tensor.log10 Seetorch.log10 call?,Tensor.log10 Seetorch.log10(),6493
3936,What is Tensor.log10?,Seetorch.log10,6493
3937,What is Seetorch.log10?,Tensor.log10,6493
3938,What does Tensor.log10_ In-place version oflog10() do?,Tensor.log10_ In-place version oflog10(),6494
3939,What is version oflog10()?,Tensor.log10_ In-place,6494
3940,What does Seetorch.log1p do?,Tensor.log1p,6495
3941,What is Tensor.log1p?,Seetorch.log1p,6495
3942,What is Seetorch.log1p() function?,Tensor.log1p,6495
3943,What does Tensor.log1p_ In-place version oflog1p() do?,Tensor.log1p_ In-place version oflog1p(),6496
3944,What is version oflog1p()?,Tensor.log1p_ In-place,6496
3945,What does Tensor.log2 Seetorch.log2() do?,Tensor.log2 Seetorch.log2(),6497
3946,What is Tensor.log2?,Seetorch.log2,6497
3947,What is Seetorch.log2() function?,Tensor.log2,6497
3948,What is In-place version oflog2()?,Tensor.log2_ In-place version oflog2(),6498
3949,What is version oflog2()?,Tensor.log2_ In-place,6498
3950,What fillsselftensor with numbers samples from the log-normal distribution parameterized by the given meanmuand standard deviations,Tensor.log_normal,6500
3951,What does Seetorch.logaddexp call?,Tensor.logaddexp,6501
3952,What is Tensor.logaddexp?,Seetorch.logaddexp,6501
3953,What is Seetorch.logaddexp() function?,Tensor.logaddexp,6501
3954,What does Tensor.logaddexp2 do?,Tensor.logaddexp2 Seetorch.logaddexp2(),6502
3955,What is Tensor.logaddexp2 Seetorch.logaddexp2()?,Tensor.logaddexp2 Seetorch.logaddexp2(),6502
3956,What does Seetorch.logsumexp use?,Tensor.logsumexp,6515
3957,What is Tensor.logsumexp?,Seetorch.logsumexp,6515
3958,What is Seetorch.logsumexp function?,Tensor.logsumexp,6515
3959,What are two examples of what?,Tensor.logical_and Seetorch.logical_and,6505
3960,What is Tensor.logical_and_ In-place version oflogical_and()?,Tensor.logical_and_ In-place version oflogical_and(),6506
3961,What is version oflogical_and()?,Tensor.logical_and_ In-place,6506
3962,What is Tensor.logical_not?,Seetorch,6507
3963,What is Seetorch.logical_not()?,Tensor.logical_not,6507
3964,What is In-place version of logical_not()?,Tensor.logical_not_ In-place version oflogical_not(),6508
3965,What is In-place version oflogical_not()?,Tensor.logical_not,6508
3966,What is a Tensor.logical_or?,Seetorch,6509
3967,What is Seetorch.logical_or?,Tensor.logical_or,6509
3968,What is In-place version oflogical_or?,Tensor.logical_or_ In-place version oflogical_or(),6510
3969,What is the In-place version oflogical_or?,Tensor.logical_or,6510
3970,What is logical xor?,Tensor.logical_xor Seetorch.logical_xor,6511
3971,What is Tensor.logical_xor?,Seetorch,6511
3972,What is Seetorch.logical_xor?,Tensor.logical_xor,6511
3973,What is In-place version of logical_xor?,Tensor.logical_xor_ In-place version oflogical_xor(),6512
3974,What is In-place version oflogical_xor()?,Tensor.logical_xor,6512
3975,What does Tensor.logit Seetorch.logit do?,Tensor.logit Seetorch.logit(),6513
3976,What is Tensor.logit?,Seetorch.logit,6513
3977,What is Seetorch.logit function?,Tensor.logit,6513
3978,What does Tensor.logit_ In-place version oflogit() do?,Tensor.logit_ In-place version oflogit(),6514
3979,What is version oflogit()?,Tensor.logit_ In-place,6514
3980,What is tensor.long self.long() equivalent to?,self.to,6516
3981,What is Seetorch.lstsq?,Tensor.lstsq,6517
3982,What is Tensor.lstsq?,Seetorch.lstsq,6517
3983,What is company that owns the company?,Tensor.lt Seetorch.lt,6518
3984,What is Tensor.lt?,Seetorch.lt,6518
3985,What is the name of Seetorch.lt?,Tensor.lt,6518
3986,What is version oflt()?,Tensor.lt_ In-place,6519
3987,What is less lt(other) -> Tensor?,Tensor,6486
3988,What is lt(other) -> Tensor?,Tensor.less,6486
3989,What is Tensor.less_ In-place version ofless()?,Tensor.less_ In-place version ofless(),6487
3990,What is version ofless()?,Tensor.less_ In-place,6487
3991,What is Tensor.lu Seetorch.lu?,Tensor.lu Seetorch.lu,6520
3992,What is Tensor.lu?,Seetorch.lu,6520
3993,What is Seetorch.lu?,Tensor.lu,6520
3994,What does Seetorch.lu_solve do?,Tensor.lu_solve,6521
3995,What does Tensor.lu_solve do?,Seetorch.lu_solve,6521
3996,What does Tensor.lu_solve?,Seetorch.lu_solve(),6521
3997,What does Seetorch.lu_solve() do?,Tensor.lu_solve,6521
3998,What makes aclsinstance with the same data pointer as self?,Tensor.as_subclass,6254
3999,What makes aclsinstance with the same data pointer asself?,Tensor.as_subclass,6254
4000,What is used for each element in selftensor and the giventensorand stores the results in selftensor?,Tensor.map_ Appliescallable,6522
4001,What _ Appliescallablefor each element inselftensor and the giventensorand stores the results inselftensor?,Tensor.map,6522
4002,What does Tensor.masked_scatter_ Copies elements fromsourceintoselftensor?,Tensor.masked_scatter_ Copies elements fromsourceintoselftensor,6526
4003,What is the default value of Tensor.masked_scatter_Copies elements fromsourceintoselftensor?,themaskis True,6526
4004,What does.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True?,Tensor,6526
4005,What is the Out-of-place version of of torch.Tensor.masked_scatter_()?,Tensor.masked_scatter,6525
4006,What version of of torch.Tensor.masked_scatter_()?,Tensor.masked_scatter Out-of-place,6525
4007,What does Tensor.masked_fill_fill elements of?,selftensor,6524
4008,What is Out-of-place version of of torch.Tensor.masked_fill?,Tensor.masked_fill Out-of-place version of torch.Tensor.masked_fill_(),6523
4009,What version of of torch.Tensor.masked_fill_()?,Tensor.masked_fill Out-of-place,6523
4010,What does Tensor.masked_select do?,Seetorch.masked_select,6527
4011,What does Tensor.masked_select Seetorch.masked_select() do?,Tensor.masked_select Seetorch.masked_select(),6527
4012,What is Tensor.matmul Seetorch.matmul?,Tensor.matmul Seetorch.matmul,6528
4013,What is Tensor.matmul?,Seetorch.matmul,6528
4014,What is Seetorch.matmul?,Tensor.matmul,6528
4015,What is the replacement for Tensor.matrix _power?,usetorch.linalg.matrix _power(),6530
4016,What does Tensor.matrix _exp do?,Tensor.matrix _exp Seetorch.matrix _exp(),6529
4017,What is Seetorch.matrix _exp() function?,Tensor.matrix _exp,6529
4018,What does Tensor.max Seetorch.max() do?,Tensor.max Seetorch.max(),6531
4019,What is the maximum Seetorch.maximum?,Tensor,6532
4020,What is the Tensor.maximum?,Seetorch.maximum,6532
4021,What is the maximum of a Tensor?,Seetorch,6532
4022,What does Tensor mean?,Seetorch.mean,6533
4023,What does Seetorch.mean mean?,Tensor.mean,6533
4024,What is.median Seetorch.median?,Tensor,6534
4025,What is Seetorch.median?,Tensor.median,6534
4026,What does Seetorch.nanmedian stand for?,Tensor.nanmedian,6552
4027,What is Tensor.nanmedian?,Seetorch.nanmedian,6552
4028,What is Seetorch.nanmedian?,Tensor.nanmedian,6552
4029,What does Tensor.min Seetorch.min do?,Tensor.min Seetorch.min(),6535
4030,What is Seetorch.min function?,Tensor.min,6535
4031,What is the minimum value for a Seetorch?,Tensor.minimum Seetorch.minimum,6536
4032,What is the minimum of a Tensor?,Seetorch,6536
4033,What is Tensor.mm Seetorch.mm?,Tensor.mm Seetorch.mm,6537
4034,What is Tensor.smm?,Seetorch.smm,6663
4035,What is function that determines the mode Seetorch.mode?,Tensor,6538
4036,What is Tensor.mode?,Seetorch.mode,6538
4037,What is Tensor.mode Seetorch.mode?,Tensor.mode Seetorch.mode,6538
4038,What is component that moves the Seetorch?,Tensor,6540
4039,What does Tensor.movedim do?,Seetorch,6540
4040,What is Tensor.movedim?,Seetorch,6540
4041,What is movedim Seetorch.movedim()?,Tensor,6540
4042,What is Tensor.moveaxis?,Seetorch,6539
4043,What is the name of Seetorch.moveaxis()?,Tensor.moveaxis,6539
4044,What does Tensor.msort?,Tensor.msort Seetorch.msort(),6541
4045,What is Tensor.msort?,Seetorch.msort,6541
4046,What is Seetorch.msort function?,Tensor.msort,6541
4047,What is mule?,Tensor.mul Seetorch.mul,6542
4048,What is mul function?,Tensor.mul_ In-place version ofmul(),6543
4049,What is version ofmul()?,Tensor.mul_ In-place,6543
4050,What is component that is used to multiply Seetorch.multiply()?,Tensor,6545
4051,What does Tensor.multiply?,Seetorch,6545
4052,What is Tensor.multiply_ In-place version ofmultiply()?,Tensor.multiply_ In-place version ofmultiply(),6546
4053,What is version of multiply()?,Tensor.multiply_ In-place,6546
4054,What is Tensor.multinomial Seetorch.multinomial?,Tensor.multinomial Seetorch.multinomial,6544
4055,What is a Tensor.multinomial?,Seetorch.multinomial,6544
4056,What is Seetorch.multinomial()?,Tensor.multinomial,6544
4057,What is Tensor.mv?,Seetorch,6547
4058,What is Seetorch.mv?,Tensor.mv,6547
4059,What is Tensor.mvlgamma?,Seetorch,6548
4060,What is Seetorch.mvlgamma?,Tensor.mvlgamma,6548
4061,What is mvlgamma version of Tensor?,Tensor.mvlgamma_ In-place version ofmvlgamma,6549
4062,What is In-place version ofmvlgamma()?,Tensor.mvlgamma,6549
4063,What is Tensor.nansum Seetorch.nansum?,Tensor.nansum Seetorch.nansum,6554
4064,What is Tensor.nansum?,Seetorch.nansum,6554
4065,What is Seetorch.nansum?,Tensor.nansum,6554
4066,What does Seetorch.narrow do?,Tensor.narrow,6555
4067,Tensor.narrow_copy returns a copy instead of what?,shared storage,6556
4068,What does Tensor.narrow() return instead of?,shared storage,6556
4069,What is the same as Tensor.narrow_copy?,Tensor.narrow(),6556
4070,What is Tensor.ndimension Alias fordim?,Tensor.ndimension Alias fordim,6561
4071,What is Alias fordim()?,Tensor.ndimension,6561
4072,What does Seetorch.nan_to_num() do?,Tensor.nan_to_num,6550
4073,What is In-place version ofnan_to_num()?,Tensor.nan_to_num_ In-place version ofnan_to_num(),6551
4074,What is the In-place version of ofnan_to_num()?,Tensor.nan_to_num,6551
4075,What is Tensor.ne Seetorch.ne?,Tensor.ne Seetorch.ne,6562
4076,What is function used by Tensor.ne?,Tensor.ne_ In-place version ofne(),6563
4077,What is version of ofne()?,Tensor.ne_ In-place,6563
4078,What is tensor that is not equal to Seetorch?,Tensor,6588
4079,What is Tensor.not_equal()?,Seetorch,6588
4080,What is not_equal Seetorch.not_equal()?,Tensor,6588
4081,What is In-place version of not_equal()?,Tensor,6589
4082,What is the In-place version of not_equal()?,Tensor.not_equal,6589
4083,What is Tensor.neg?,Seetorch.neg,6564
4084,What is Seetorch.neg?,Tensor.neg,6564
4085,What is function that is used by Tensor.neg?,Tensor.neg_ In-place version ofneg(),6565
4086,What is version ofneg()?,Tensor.neg_ In-place,6565
4087,What is a negative Seetorch?,Tensor,6566
4088,What is negative?,Tensor.negative_ In-place version ofnegative,6567
4089,What is version of negative?,Tensor.negative_ In-place,6567
4090,What is Tensor.nelement?,Alias fornumel,6568
4091,What does Tensor.nelement Alias fornumel do?,Tensor.nelement Alias fornumel(),6568
4092,What is Alias fornumel?,Tensor.nelement,6568
4093,What is next after Seetorch?,Tensor,6583
4094,What is In-place version ofnextafter()?,Tensor.nextafter_ In-place version ofnextafter(),6584
4095,What is In-place version of ofnextafter()?,Tensor.nextafter,6584
4096,What does Seetorch.nonzero do?,Tensor.nonzero,6585
4097,What is Tensor.nonzero?,Seetorch.nonzero,6585
4098,What is the name of Seetorch.norm?,Tensor.norm,6586
4099,What parameter does Tensor.normal_ Fillsselftensor with elements samples from?,bymeanandstd,6587
4100,What is filled with elements from the normal distribution parameterized bymeanandstd?,Tensor.normal_ Fillsselftensor,6587
4101,What fillsselftensor with elements samples from the normal distribution parameterized bymeanandstd?,Tensor.normal,6587
4102,Tensor.numpy Returns selftensor as what?,NumPyndarray,6590
4103,What returns selftensor as a NumPyndarray?,Tensor.numpy,6590
4104,What is Tensor.orgqr?,Seetorch.orgqr,6591
4105,What is the name of Seetorch.orgqr?,Tensor.orgqr,6591
4106,What is Tensor.ormqr?,Seetorch.ormqr,6592
4107,What is Seetorch.ormqr?,Tensor.ormqr,6592
4108,What does Tensor.outer do?,Tensor.outer Seetorch.outer(),6593
4109,What is the name of Seetorch.outer()?,Tensor.outer,6593
4110,What is Tensor.permute?,Seetorch.permute,6594
4111,What is the name of Seetorch.permute()?,Tensor.permute,6594
4112,What copies the tensor to pinned memory?,Tensor.pin_memory,6595
4113,What does Tensor.pin_memory copy the tensor to?,pinned memory,6595
4114,What is inverse of the Seetorch?,Tensor.pinverse Seetorch.pinverse(),6596
4115,What is Seetorch.pinverse()?,Tensor.pinverse,6596
4116,What is Tensor.polygamma?,Seetorch.polygamma,6597
4117,What is Seetorch.polygamma?,Tensor.polygamma,6597
4118,What is in-place version of polygamma?,Tensor.polygamma,6598
4119,What is a positive Seetorch?,Tensor,6599
4120,What is Tensor.pow Seetorch.pow?,Tensor.pow Seetorch.pow,6600
4121,What is Tensor.pow?,Seetorch,6600
4122,What is Seetorch.pow() function?,Tensor.pow,6600
4123,What does Tensor.pow_ In-place version ofpow() do?,Tensor.pow_ In-place version ofpow(),6601
4124,What is version ofpow()?,Tensor.pow_ In-place,6601
4125,What does Tensor.prod do?,Tensor.prod Seetorch.prod(),6602
4126,What is Tensor.prod?,Seetorch.prod,6602
4127,What is Seetorch.prod?,Tensor.prod,6602
4128,What copies the elements fromsourceinto the positions specified byindex?,Tensor.put_,6603
4129,What does Tensor.qr stand for?,Tensor.qr Seetorch.qr(),6609
4130,What is Seetorch.qr?,Tensor.qr,6609
4131,What returns the quantization scheme of a given QTensor?,Tensor.qscheme,6610
4132,Tensor.qscheme returns the quantization scheme of a given what?,QTensor,6610
4133,What does Tensor.quantile Seetorch.quantile() do?,Tensor.quantile Seetorch.quantile(),6611
4134,What is Seetorch.quantile()?,Tensor.quantile,6611
4135,What does Seetorch.nanquantile do?,Tensor.nanquantile,6553
4136,What is Tensor.nanquantile?,Seetorch,6553
4137,How is a Tensor quantized?,linear(affine) quantization,6607
4138,What returns the scale of the underlying quantizer()?,Tensor.q_scale,6607
4139,What does Tensor.q_ return?,zero_point,6608
4140,What is a Tensor quantized by?,linear(affine) quantization,6608
4141,What is the value of Tensor.q_?,zero_point,6608
4142,What is an affine quantization of a Tensor?,linear,6605
4143,What is the term for affine per-channel quantization?,linear,6606
4144,What is the tensor of the underlying quantizer?,zero,6606
4145,What is an affine per-channel quantization?,linear,6606
4146,What is another term for linear quantization?,affine,6606
4147,Tensor.q_per_channel_axis Given a Tensor quantized by what type of per-channel quantization?,linear,6604
4148,What does Tensor.q_per_channel_axis return?,index of dimension,6604
4149,Tensor.q_per_channel_axis Given a Tensor quantized by what?,linear,6604
4150,What returns the index of dimension on which per-channel quantization is applied?,Tensor.q_per_channel_axis,6604
4151,What does Seetorch.rad2deg do?,Tensor.rad2deg,6612
4152,What is Tensor.rad2deg?,Seetorch,6612
4153,What is Seetorch.rad2deg?,Tensor.rad2deg,6612
4154,Tensor.random_ Fillsselftensor with numbers sampled from what kind of distribution?,uniform distribution,6613
4155,"What type of tensor is filled with numbers sampled from the discrete uniform distribution over[from,to-1]?",Tensor.random,6613
4156,What does seetorch.ravel do?,Tensor.ravel,6614
4157,What is Tensor.ravel?,seetorch,6614
4158,What is reciprocal Seetorch?,Tensor,6617
4159,What is a Tensor.reciprocal?,Seetorch,6617
4160,What is reciprocal function?,Tensor.reciprocal_ In-place version ofreciprocal(),6618
4161,What is version ofreciprocal()?,Tensor.reciprocal_ In-place,6618
4162,When is the tensor memory not reused for another tensor?,all current work queued onstreamare complete,6619
4163,What ensures that the tensor memory is not reused for another tensor?,Tensor.record_stream,6619
4164,What type of hook does Tensor register?,backward hook,6621
4165,What registers a backward hook?,Tensor.register_hook,6621
4166,What does Tensor.remainder do?,Seetorch.remainder(),6622
4167,What does Seetorch.remainder do?,Tensor.remainder,6622
4168,What is Tensor.remainder?,Seetorch.remainder,6622
4169,What is Seetorch.remainder?,Tensor.remainder,6622
4170,What is in-place version ofmainder()?,Tensor.remainder_ In-place version ofremainder(),6623
4171,What is version ofremainder()?,Tensor.remainder_ In-place,6623
4172,What is Tensor.renorm?,Seetorch,6624
4173,What is Seetorch.renorm?,Tensor.renorm,6624
4174,What is in-place version ofrenorm?,Tensor.renorm_ In-place version ofrenorm(),6625
4175,What is In-place version ofrenorm?,Tensor.renorm,6625
4176,What repeats the tensor along the specified dimensions?,Tensor,6626
4177,What is tensor that repeats along the specified dimensions?,Tensor,6626
4178,What is component that performs the repeat interleave of Seetorch?,Tensor,6627
4179,What does Tensor.repeat_interleave?,Seetorch,6627
4180,What is.repeat_interleave Seetorch.repeat_interleave()?,Tensor,6627
4181,What type of gradients need to be computed for the Tensor?,is True if,6628
4182,What should record operations on this tensor?,autograd,6629
4183,How does autograd record operations on a tensor?,sets this tensor’srequires_gradattribute in-place,6629
4184,Tensor.reshape Returns a tensor with the same data and number of elements as selfbut with what?,the specified shape,6630
4185,What returns a tensor with the same data and number of elements asselfbut with the specified shape?,Tensor.reshape,6630
4186,What shape is returned to the tensor?,same shape,6631
4187,What returns this tensor as the same shape asother?,Tensor.reshape_as,6631
4188,What does Tensor.resize_ do?,Tensor.resize_ Resizesselftensor to the specified size,6632
4189,What _ Resizesselftensor to the specified size?,Tensor.resize,6632
4190,What does Tensor.resize_as_ Resize the self tensor to be?,the same size as the specifiedtensor,6633
4191,What resizes the self tensor to be the same size as the specifiedtensor?,Tensor.resize_as,6633
4192,Tensor.retain_grad Enables.grad attribute for what?,non-leaf Tensors,6634
4193,Tensor.retain_grad Enables.grad attribute for what type of Tensors?,non-leaf,6634
4194,What is Tensor.roll Seetorch.roll?,Tensor.roll Seetorch.roll,6635
4195,What is Tensor.roll Seetorch.roll()?,Tensor.roll Seetorch.roll(),6635
4196,What is rot90 Seetorch.rot90?,Tensor,6636
4197,What is Tensor.rot90?,Seetorch.rot90,6636
4198,What is Seetorch.rot90?,Tensor.rot90,6636
4199,What is Tensor.round Seetorch.round?,Tensor.round Seetorch.round,6637
4200,What is Tensor.round?,Seetorch.round,6637
4201,What is In-place version ofround()?,Tensor.round_ In-place version ofround(),6638
4202,What is In-place version ofround?,Tensor.round,6638
4203,What does Tensor.rsqrt do?,Tensor.rsqrt Seetorch.rsqrt(),6639
4204,What is Tensor.rsqrt?,Seetorch.rsqrt,6639
4205,What is Seetorch.rsqrt?,Tensor.rsqrt,6639
4206,What is in-place version of ofrsqrt?,Tensor.rsqrt_ In-place version ofrsqrt(),6640
4207,What is In-place version of ofrsqrt()?,Tensor.rsqrt,6640
4208,What is version of of torch.Tensor.scatter_()?,Tensor.scatter Out-of-place,6641
4209,Where are the indices specified?,theindextensor,6642
4210,What does Tensor.scatter_ write from the tensorsrcintoselfat the indices specified in the,all values,6642
4211,What _ Writes all values from the tensorsrcintoselfat the indices specified in the indextensor,Tensor.scatter,6642
4212,How does Tensor.scatter_add_ add all values from the tensorotherintoselfat the indices specified,asscatter_(),6644
4213,How does Tensor.scatter_add_ Add all values from the tensorotherintoselfat the indices specified,asscatter_(),6644
4214,What is the version of of torch.Tensor.scatter_add_()?,Tensor.scatter_add Out-of-place,6643
4215,What does Tensor.select do?,Slices the self tensor along the selected dimension at the given index,6645
4216,What does Tensor.select along the selected dimension at the given index?,Slices the self tensor,6645
4217,"What sets the underlying storage, size, and strides?",Tensor.set_,6646
4218,What moves the underlying storage to shared memory?,Tensor.share_memory,6649
4219,What _ Moves the underlying storage to shared memory?,Tensor.share_memory,6649
4220,What is equivalent to self.to(torch.int16)?,self.short,6650
4221,What is tensor.short self.short() equivalent to?,self.to,6650
4222,What is sigmoid Seetorch?,Tensor,6651
4223,What is Tensor.sigmoid?,Seetorch,6651
4224,What is In-place version ofsigmoid()?,Tensor.sigmoid_ In-place version ofsigmoid(),6652
4225,What version ofsigmoid() does Tensor.sigmoid_ In-place?,Tensor.sigmoid_ In-place,6652
4226,What does Seetorch.sign() stand for?,Tensor,6653
4227,What does Tensor.sign Seetorch.sign() do?,Tensor.sign Seetorch.sign(),6653
4228,What does Tensor.sign_ In-place version ofsign() do?,Tensor.sign_ In-place version ofsign(),6654
4229,What is version ofsign()?,Tensor.sign_ In-place,6654
4230,What is.signbit Seetorch.signbit() function?,Tensor,6655
4231,What does Tensor.signbit do?,Seetorch.signbit(),6655
4232,What does Tensor.signbit Seetorch.signbit() do?,Tensor.signbit Seetorch.signbit(),6655
4233,What is Tensor.sgn?,Seetorch.sgn,6647
4234,What is Seetorch.sgn?,Tensor.sgn,6647
4235,What is function used by Tensor.sgn?,Tensor.sgn_ In-place version ofsgn(),6648
4236,What is version ofsgn()?,Tensor.sgn_ In-place,6648
4237,What is In-place version ofsin()?,Tensor.sin_ In-place version ofsin(),6656
4238,What is version ofsin()?,Tensor.sin_ In-place,6656
4239,What is Tensor.sinc?,Seetorch.sinc,6657
4240,What is Seetorch.sinc?,Tensor.sinc,6657
4241,What does Tensor.sinc_ In-place version ofsinc() do?,Tensor.sinc_ In-place version ofsinc(),6658
4242,What is In-place version ofsinc()?,Tensor.sinc,6658
4243,What is Tensor.sinh?,Seetorch.sinh,6659
4244,What is Seetorch.sinh function?,Tensor.sinh,6659
4245,What does Tensor.sinh_ In-place version ofsinh() do?,Tensor.sinh_ In-place version ofsinh(),6660
4246,What is version ofsinh()?,Tensor.sinh_ In-place,6660
4247,What is Tensor.asinh?,Seetorch.asinh,6256
4248,What is Seetorch.asinh?,Tensor.asinh,6256
4249,What does Tensor.asinh do?,Tensor.asinh_ In-place version ofasinh(),6257
4250,What is In-place version ofasinh()?,Tensor.asinh,6257
4251,What is Tensor.arcsinh?,Seetorch.arcsinh,6244
4252,What is Seetorch.arcsinh function?,Tensor.arcsinh,6244
4253,What is in-place version of arcsinh?,Tensor.arcsinh_ In-place version ofarcsinh(),6245
4254,What is In-place version ofarcsinh()?,Tensor.arcsinh,6245
4255,What returns the size of the selftensor?,Tensor,6661
4256,What does Tensor.size return?,the size of the self tensor,6661
4257,What returns the size of the self tensor?,Tensor.size,6661
4258,What does Seetorch.slogdet do?,Tensor.slogdet,6662
4259,What does Tensor.slogdet stand for?,Seetorch.slogdet,6662
4260,What is Tensor.slogdet?,Seetorch.slogdet,6662
4261,What is Seetorch.slogdet() function?,Tensor.slogdet,6662
4262,What does Seetorch.solve() do?,Tensor.solve,6664
4263,What is Tensor.solve?,Seetorch.solve,6664
4264,What is.sort Seetorch.sort() function?,Tensor,6665
4265,What does Tensor.sort Seetorch.sort() do?,Tensor.sort Seetorch.sort(),6665
4266,What is component that splits a Seetorch?,Tensor,6682
4267,What is Tensor.split?,Seetorch.split,6682
4268,What is Seetorch.split() function?,Tensor.split,6682
4269,What is used to filter the strided tensor?,indices of the sparse tensormask,6671
4270,What is the number of in the asparse tensorself?,sparse dimensions,6670
4271,What returns the number of sparse dimensions in asparse tensorself?,Tensor.sparse_dim,6670
4272,What does Tensor.sqrt do?,Tensor.sqrt Seetorch.sqrt(),6683
4273,What is Tensor.sqrt?,Seetorch.sqrt,6683
4274,What is in-place version ofsqrt()?,Tensor.sqrt_ In-place version ofsqrt(),6684
4275,What is version ofsqrt()?,Tensor.sqrt_ In-place,6684
4276,What is Tensor.square Seetorch.square?,Tensor.square Seetorch.square,6685
4277,What is Tensor.square Seetorch.square()?,Tensor.square Seetorch.square(),6685
4278,What is In-place version of square()?,Tensor.square_ In-place version ofsquare(),6686
4279,What is version of square()?,Tensor.square_ In-place,6686
4280,What is Seetorch.squeeze() function?,Tensor.squeeze,6687
4281,What does Tensor.squeeze_ In-place version ofsqueeze() do?,Tensor.squeeze_ In-place version ofsqueeze(),6688
4282,What is version ofsqueeze()?,Tensor.squeeze_ In-place,6688
4283,What is Tensor.std?,Seetorch.std,6689
4284,What is Tensor.stft?,Seetorch.stft,6690
4285,What is Seetorch.stft?,Tensor.stft,6690
4286,What returns the underlying storage?,storage,6691
4287,What type of storage returns the underlying storage?,Tensor,6691
4288,Tensor.storage_offset Returns selftensor's offset in the underlying storage in terms of what?,number of storage elements,6692
4289,What is the offset in selftensor's underlying storage?,bytes,6692
4290,What returns the type of the underlying storage?,Tensor.storage_type,6693
4291,What returns the stride of selftensor?,stride,6694
4292,What is sub Seetorch.sub()?,Tensor,6695
4293,What is the name of Seetorch.sub()?,Tensor.sub,6695
4294,What version of sub() does Tensor.sub_ In-place?,Tensor.sub_ In-place,6696
4295,What does Seetorch.subtract() do?,Tensor,6697
4296,What does Tensor.subtract do?,Seetorch.subtract,6697
4297,What is the term for Seetorch?,Tensor,6697
4298,What does Tensor.subtract_ In-place version ofsubtract() do?,Tensor.subtract_ In-place version ofsubtract(),6698
4299,What is version of subtract()?,Tensor.subtract_ In-place,6698
4300,What does Tensor.sum Seetorch.sum do?,Tensor.sum Seetorch.sum(),6699
4301,What is the Tensor.sum Seetorch.sum?,Tensor.sum Seetorch.sum,6699
4302,What is a Sumthistensor?,Tensor,6700
4303,What is a Tensor.sum_to_size?,Sumthistensor,6700
4304,Tosize what is a Tensor.sum_to_size?,Sumthistensor,6700
4305,What is the Sumthistensor tosize?,Tensor.sum_to_size,6700
4306,What is Tensor.svd?,Seetorch.svd,6701
4307,What is Seetorch.svd?,Tensor.svd,6701
4308,What does Seetorch.swapaxes do?,Tensor.swapaxes,6702
4309,What is Tensor.swapaxes?,Seetorch.swapaxes,6702
4310,What does Seetorch.swapdims do?,Tensor.swapdims,6703
4311,What is Tensor.swapdims?,Seetorch.swapdims,6703
4312,What does Tensor.t Seetorch.t do?,Tensor.t Seetorch.t(),6705
4313,What is Tensor.t Seetorch?,Tensor.t Seetorch.t,6705
4314,What is Tensor.t Seetorch.t?,Tensor.t Seetorch.t,6705
4315,What is Tensor.tensor_split?,Seetorch.tensor_split,6713
4316,What does Seetorch.tensor_split call?,Tensor.tensor_split,6713
4317,What is function that is used to determine the type of tile?,Tensor.tile Seetorch.tile(),6714
4318,What is Tensor.tile?,Seetorch.tile,6714
4319,What is Seetorch.tile?,Tensor.tile,6714
4320,What performs Tensor dtype and/or device conversion?,Tensor.to,6715
4321,What does Tensor.to perform?,Tensor dtype and/or device conversion,6715
4322,What does Tensor.to_mkldnn return a copy of?,intorch.mkldnnlayout,6718
4323,What does Tensor.to_mkldnn return?,a copy,6718
4324,What is function that takes Seetorch.take()?,Tensor,6707
4325,What is Tensor.take Seetorch.take?,Tensor.take Seetorch.take,6707
4326,Take_along_dim Seetorch.take_along_dim() what?,Tensor,6708
4327,What is Tensor.tan Seetorch.tan?,Tensor.tan Seetorch.tan,6709
4328,What is Tensor.tan?,Tensor.tan Seetorch.tan,6709
4329,What is Tensor.tan_ In-place version of oftan?,Tensor.tan_ In-place version oftan(),6710
4330,What is version of oftan?,Tensor.tan_ In-place,6710
4331,What is Tensor.tanh Seetorch.tanh?,Tensor.tanh Seetorch.tanh,6711
4332,What is Tensor.tanh?,Seetorch.tanh,6711
4333,What is Seetorch.tanh?,Tensor.tanh,6711
4334,What does Tensor.tanh_ In-place version of oftanh do?,Tensor.tanh_ In-place version oftanh(),6712
4335,What is In-place version of oftanh()?,Tensor.tanh,6712
4336,What is Tensor.atanh Seetorch.atanh?,Tensor.atanh Seetorch.atanh,6262
4337,What is in-place version of atanh?,Tensor.atanh_ In-place version ofatanh(),6263
4338,What is In-place version of atanh()?,Tensor.atanh,6263
4339,What is Tensor.arctanh?,Seetorch,6248
4340,What is Seetorch.arctanh function?,Tensor.arctanh,6248
4341,What is in-place version of arctanh?,Tensor.arctanh_ In-place version ofarctanh(),6249
4342,What is In-place version ofarctanh?,Tensor.arctanh,6249
4343,What returns the tensor as a list?,Tensor,6722
4344,What returns the tensor as a (nested) list?,Tensor.tolist,6722
4345,What is Tensor.topk Seetorch.topk?,Tensor.topk Seetorch.topk,6723
4346,What is Tensor.topk?,Seetorch.topk,6723
4347,What is Seetorch.topk?,Tensor.topk,6723
4348,What is Tensor.trace?,Seetorch.trace,6724
4349,What is Seetorch.trace?,Tensor.trace,6724
4350,What is.transpose Seetorch.transpose() function?,Tensor,6725
4351,What is Tensor.transpose?,Seetorch.transpose,6725
4352,What is Seetorch.transpose() function?,Tensor.transpose,6725
4353,What does Tensor.transpose_ In-place version oftranspose() do?,Tensor.transpose_ In-place version oftranspose(),6726
4354,What version oftranspose() does Tensor.transpose_ In-place?,Tensor.transpose_ In-place,6726
4355,What does Seetorch.triangular_solve do?,Tensor.triangular_solve,6727
4356,What is Tensor.triangular_solve?,Seetorch.triangular_solve(),6727
4357,What is Seetorch.triangular_solve?,Tensor.triangular_solve,6727
4358,What does Tensor.tril Seetorch.tril do?,Tensor.tril Seetorch.tril(),6728
4359,What is Tensor.tril?,Seetorch.tril,6728
4360,What is Seetorch.tril?,Tensor.tril,6728
4361,What is In-place version oftril()?,Tensor.tril_ In-place version oftril(),6729
4362,What is version oftril()?,Tensor.tril_ In-place,6729
4363,What is Tensor.triu Seetorch.triu?,Tensor.triu Seetorch.triu,6730
4364,What is Tensor.triu?,Seetorch.triu,6730
4365,What is Seetorch.triu?,Tensor.triu,6730
4366,What is In-place version oftriu()?,Tensor.triu_ In-place version oftriu(),6731
4367,What is version oftriu()?,Tensor.triu_ In-place,6731
4368,What is component that does Seetorch.true_divide()?,Tensor,6732
4369,What does Seetorch.true_divide() use?,Tensor.true_divide,6732
4370,What is In-place version oftrue_divide_()?,Tensor.true_divide_ In-place version oftrue_divide_(),6733
4371,What version oftrue_divide_() is used?,Tensor.true_divide_ In-place,6733
4372,What does Tensor.trunc_ In-place version of oftrunc() do?,Tensor.trunc_ In-place version oftrunc(),6735
4373,What is version of oftrunc()?,Tensor.trunc_ In-place,6735
4374,What type does Tensor.type return?,ifdtypeis not provided,6736
4375,What does Tensor.type_as return?,the type of the given tensor,6737
4376,What returns this tensor cast to the type of the given tensor?,Tensor.type_as,6737
4377,What does Tensor.unbind Seetorch.unbind() do?,Tensor.unbind Seetorch.unbind(),6738
4378,What does Tensor.unbind?,Seetorch,6738
4379,What does Seetorch.unbind() do?,Tensor.unbind,6738
4380,The original tensor contains all slices of sizesizefromselftensor in what dimension?,dimension dimension,6739
4381,The original tensor contains all slices of what?,sizesizefromselftensor,6739
4382,What type of tensor is filled with numbers sampled from the continuous uniform distribution?,uniform,6740
4383,What is Fillsselftensor?,Tensor.uniform,6740
4384,What does Tensor.unique return?,unique elements,6741
4385,What does Tensor.unsqueeze?,Seetorch,6742
4386,What does Seetorch.unsqueeze() do?,Tensor.unsqueeze,6742
4387,What is In-place version ofunsqueeze()?,Tensor.unsqueeze_ In-place version ofunsqueeze(),6743
4388,What value returns the values tensor of asparse COO tensor?,Tensor,6747
4389,What does Tensor.values do?,Return the values tensor of asparse COO tensor,6747
4390,What does Tensor.var Seetorch.var do?,Tensor.var Seetorch.var(),6748
4391,What is Tensor.var Seetorch.var?,Tensor.var Seetorch.var,6748
4392,What is Tensor.vdot?,Seetorch.vdot,6749
4393,What is Seetorch.vdot?,Tensor.vdot,6749
4394,Tensor.view Returns a new tensor with the same data as the self tensor but of what shape?,differentshape,6750
4395,Tensor.view Returns a new tensor with the same data as what?,the self tensor,6750
4396,What returns a new tensor with the same data as the self tensor but of a different shape?,Tensor.view,6750
4397,What does view_as view as the same size asother?,Tensor,6751
4398,What does Tensor.vsplit Seetorch.vsplit() do?,Tensor.vsplit Seetorch.vsplit(),6752
4399,What is Tensor.vsplit?,Seetorch.vsplit,6752
4400,What is Seetorch.vsplit() function?,Tensor.vsplit,6752
4401,What is the equivalent of a totorch?,Tensor,6753
4402,What does Tensor.xlogy_ In-place version ofxlogy() do?,Tensor.xlogy_ In-place version ofxlogy(),6755
4403,What is In-place version ofxlogy()?,Tensor.xlogy,6755
4404,What does Tensor.zero_ Fillsselftensor with?,zeros,6756
4405,What function does no M[sparse_coo]@V[strided]->V[strided] use,torch.mv(),11102
4406,What does no M[sparse_coo]@V[strided]->V[strided]?,torch.mv(),11102
4407,What does no M[sparse_csr] do?,torch.mv(),11105
4408,What does no M[sparse_csr]@V[strided]->V[strided]?,torch.mv(),11105
4409,What function does no M[sparse_coo]@M[strided]->M[strided] use,torch.mm(),11097
4410,What function does no M[sparse_coo]@M[strided]->M[strided]?,torch.matmul(),11092
4411,What function does no M[sparse_csr]@M[strided]->M[strided],torch.matmul(),11094
4412,What function does no M[sparse_coo]@M[strided]->M[sparse_co,torch.smm(),11214
4413,What does no M[sparse_coo]@M[strided]->M[sparse_coo,torch.smm(),11214
4414,What function does no M[sparse_coo]@M[strided]->M[hybridspar,torch.hspmm(),11069
4415,What does no M[sparse_coo]@M[strided]->M[hybridspars,torch.hspmm(),11069
4416,What function does no T[sparse_coo]@T[strided]->T[strided] use,torch.bmm(),11016
4417,What does no T[sparse_coo]@T[strided]->T[strided]?,torch.bmm(),11016
4418,f*M[strided]+f*(M[sparse_coo]@M[strided,yes,11222
4419,What does torch.lobpcg() do?,no,11089
4420,What function does PCA(M[sparse_coo]) use?,torch.pca_lowrank(),11203
4421,"What does PCA(M[sparse_coo])->M[strided],M[strided],",torch.pca_lowrank(),11203
4422,"SVD(M[sparse_coo])->M[strided],M[strided],M",yes,11229
4423,What does torch.svd_lowrank() do?,SVD,11229
4424,What converts a tensor to compressed row storage format?,Tensor._to_sparse_csr,6194
4425,Convert a tensor to compressed row storage format?,Tensor,6194
4426,What format does tensor._to_sparse_csr convert a tensor to?,compressed row storage format,6194
4427,What is a tensorto?,the desired size,6675
4428,What resizes itselfsparse tensorto the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,6675
4429,What does Tensor.sparse_resize_and_clear_ resize itself to?,the desired size,6681
4430,What removes all specified elements from asparse tensorselfand resizes itself to the desired size and the number of spar,Tensor.sparse_resize_and_clear,6681
4431,What is tensorthat?,coalesced,6447
4432,Tensor.to_dense Creates a strided copy of what?,self,6717
4433,What returns the compressed row indices of the self tensor whenselfis a sparse CSR tensor of layouts,tensor,6316
4434,What returns the column indices of the self tensor whenselfis a sparse CSR tensor of layoutspar,tensor,6301
4435,Where are the specified values for the asparse tensor?,givencrow_indicesandcol_indices,8705
4436,What is the Compressed Sparse Row?,CSR,8705
4437,What returns the sum of each row of the sparse tensorinputin the given dimensionsdim?,sparse.sum,10738
4438,What direction does sparse.addmm support?,backward,10732
4439,What type of matrix mat performs a matrix  multiplication of the sparse matrix mat1 and the (sparse or strided,sparse,10734
4440,What type of matrix mat1 performs a matrix  multiplication?,sparse,10734
4441,sspaddmm matrix  multiplies what tensormat1 with a dense tensormat2?,sparse,10747
4442,sspaddmm matrix  multiplies a sparse tensormat1with a what?,dense,10747
4443,hspmm performs a matrix  multiplication of asparse COO matrix mat1 and what else?,a strided matrix mat2,9531
4444,What performs a matrix  multiplication of the sparse matrix input with the dense matrix mat?,smm,10704
4445,smm Performs a matrix  multiplication of the sparse matrix input with what?,dense matrix mat,10704
4446,What does smm perform a matrix  multiplication of?,sparse matrix input,10704
4447,Softmax Applies a softmax function that is what?,sparse,10736
4448,What is followed by a softmax function?,logarithm,10733
4449,What applies a softmax function followed by logarithm?,sparse.log_softmax,10733
4450,What type of function does.log_softmax Applies a softmax function followed by logarithm?,sparse,10733
4451,What does norm compute?,a vector or matrix  norm,10262
4452,What does norm Compute?,a vector or matrix  norm,10262
4453,What does vector_norm compute?,vector norm,11334
4454,What Computes a vector norm?,vector_norm,11334
4455,What does matrix _norm compute?,matrix  norm,9877
4456,What Computes a matrix  norm?,matrix _norm,9877
4457,What does det do with the determinant of a square matrix ?,Computes,9162
4458,What computes the determinant of a square matrix ?,det,9162
4459,What computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ?,slogdet,10703
4460,What does slogdet compute of the absolute value of the determinant of a square matrix ?,the sign and natural logarithm,10703
4461,What Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ?,slogdet,10703
4462,What does cond compute?,the condition number of a matrix  with respect to a matrix  norm,9111
4463,What computes the condition number of a matrix  with respect to a matrix  norm?,cond,9111
4464,cond Computes the condition number of a matrix  with respect to what?,matrix  norm,9111
4465,What computes the numerical rank of a matrix ?,matrix,9880
4466,What computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix ?,cholesky_ex,9074
4467,cholesky_ex Computes the Cholesky decomposition of a complex what?,Hermitian,9074
4468,cholesky computes the Cholesky decomposition of a complex what?,Hermitian,9072
4469,What computes the QR decomposition of a matrix ?,qr,10533
4470,eig Computes the decomposition of a square matrix  if it exists?,eigenvalue,9233
4471,What computes the eigenvalue decomposition of a square matrix  if it exists?,eig,9233
4472,eigvals Computes what of a square matrix ?,eigenvalues,9240
4473,What computes the eigenvalues of a square matrix ?,eigvals,9240
4474,What does eigh do?,Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix,9239
4475,What computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ?,eigh,9239
4476,eigh Computes the eigenvalue decomposition of a complex what?,Hermitian,9239
4477,eigvalsh Computes what of a complex Hermitian or real symmetric matrix ?,eigenvalues,9241
4478,eigvalsh Computes the eigenvalues of a complex Hermitian or real what type of matrix ?,symmetric,9241
4479,What computes the eigenvalues of a complex Hermitian or real symmetric matrix ?,eigvalsh,9241
4480,eigvalsh Computes the eigenvalues of a complex what?,Hermitian,9241
4481,What computes the singular value decomposition of a matrix ?,svd,10785
4482,What is svd?,SVD,10785
4483,What computes the singular values of a matrix ?,svdvals,10788
4484,What Computes the singular values of a matrix ?,svdvals,10788
4485,What does solve the solution of a square system of linear equations with a unique solution?,Computes,10714
4486,What does a unique solution do to a square system of linear equations?,solve Computes the solution of a square system of linear equations,10714
4487,What computes a solution to the least squares problem of a system of linear equations?,lstsq,9851
4488,lstsq Computes a solution to what problem of a system of linear equations?,least squares,9851
4489,inv Computes the what of a square matrix  if it exists?,inverse,9689
4490,What computes the inverse of a square matrix  if it exists?,inv,9689
4491,What does pinv compute?,pseudoinverse,10415
4492,What does pinv compute of a matrix ?,the pseudoinverse,10415
4493,What computes the then-th power of a square matrix  for an integern?,matrix _power,9879
4494,What computes then-th power of a square matrix  for an integern?,matrix _power,9879
4495,What multiplies two or more matrices by reordering the multiplications?,multi_dot,9943
4496,multi_dot multiplies two or more matrices by what?,reordering,9943
4497,What computes the firstncolumns of a product of Householder matrices?,householder_product,9527
4498,householder_product Computes what of a product of Householder matrices?,firstncolumns,9527
4499,householder_product Computes the firstncolumns of a product of what?,Householder matrices,9527
4500,What computes the multiplicative inverse of torch.tensordot()?,tensorinv,10818
4501,What Computes the multiplicative inverse of torch.tensordot()?,tensorinv,10818
4502,What does tensorsolve compute the solutionXto?,systemtorch,10820
4503,What Computes the solutionXto the systemtorch?,tensorsolve,10820
4504,What computes the inverse of a square matrix  if it is invertible?,inv_ex,9690
4505,inv_ex computes what of a square matrix  if it is invertible?,inverse,9690
4506,What is program that selects a given stream?,StreamContext Context-manager,6079
4507,What is manager that selects a given stream?,StreamContext Context-manager,6079
4508,Can_device_access_peer Checks if what is possible between two devices?,peer access,9025
4509,What checks if peer access between two devices is possible?,can_device_access_peer,9025
4510,Can_device_access_peer Checks if what is possible?,peer access between two devices,9025
4511,what pointer does current_blas_handle return?,cublasHandle_t,9147
4512,current_blas_handle Returns cublasHandle_t pointer to what?,current cuBLAS handle,9147
4513,What returns cublasHandle_t pointer to current cuBLAS handle?,current_blas_handle,9147
4514,What returns the index of a currently selected device?,current_device,9148
4515,What does current_stream return?,current_stream Returns the currently selectedStreamfor a given device,9149
4516,What returns the current selectedStream for a given device?,current_stream,9149
4517,What returns the defaultStream for a given device?,default_stream,9153
4518,What does default_stream return for a given device?,defaultStream,9153
4519,What returns the default stream for a given device?,default_stream,9153
4520,What is device that changes the selected device?,Context-manager,9166
4521,What returns the number of GPUs available?,device_count,9171
4522,What is Context-manager that changes the current device to that of given object?,device,9172
4523,What returns the CUDA architectures this library was compiled for?,get_arch_list,9450
4524,What does get_arch_list return?,CUDA architectures,9450
4525,What Returns list CUDA architectures this library was compiled for?,get_arch_list,9450
4526,What does get_device_capability get from a device?,cuda capability,9455
4527,What gets the cuda capability of a device?,get_device_capability,9455
4528,What does get_device_name get?,the name of a device,9456
4529,What gets the name of a device?,get_device_name,9456
4530,What does get the properties of a device?,properties,9457
4531,What gets the properties of a device?,get_device_properties,9457
4532,What gencode flags does get_gencode_flags return?,NVCC,9458
4533,What returns the NVCC gencode flags this library was compiled with?,get_gencode_flags,9458
4534,What returns NVCC gencode flags this library was compiled with?,get_gencode_flags,9458
4535,What does init do?,Initialize PyTorch’s CUDA state,9617
4536,Init Initialize PyTorch's state what?,CUDA,9617
4537,Who released the GPU memory for ipc_collect Force?,CUDA IPC,9695
4538,What does ipc_collect Force collect after it has been released by CUDA IPC?,GPU memory,9695
4539,What is currently available?,CUDA,9709
4540,What type of return does is_available return?,bool,9709
4541,is_available Returns what type of value indicating if CUDA is currently available?,a bool,9709
4542,What has been initialized?,PyTorch’s CUDA state,9714
4543,What returns whether PyTorch’s CUDA state has been initialized?,initialized,9714
4544,What sets the current device?,set_device,10666
4545,What does set_device set?,current device,10666
4546,what does set_device set?,current device,10666
4547,What is stream that wraps around the Context-manager StreamContext that selects a given stream?,Wrapper,10770
4548,What is the stream Wrapper around that selects a given stream?,Context-manager StreamContext,10770
4549,On what device should Waits for all kernels in all streams be synchronized?,CUDA device,10792
4550,What is synchronized for all kernels in all streams on a CUDA device?,Waits,10792
4551,What returns the random number generator state of the specified GPU as a ByteTensor?,get_rng_state,9463
4552,Get_rng_state Returns the random number generator state of the specified GPU as a what?,ByteTensor,9463
4553,What does get_rng_state return?,random number generator state,9463
4554,What does get_rng_state_all return?,a list of ByteTensor,9464
4555,What sets the random number generator state of the specified GPU?,set_rng_state,10673
4556,set_rng_state Sets what state of the specified GPU?,random number generator state,10673
4557,What sets the random number generator state of all devices?,set_rng_state_all,10675
4558,What does set_rng_state_all set?,random number generator state,10675
4559,What sets the seed for generating random numbers for the current GPU?,manual_seed,9867
4560,What sets the seed for generating random numbers on all GPUs?,manual_seed_all,9869
4561,Manual_seed_all Sets the seed for generating what on all GPUs?,random numbers,9869
4562,What is seed used for?,generating random numbers,10656
4563,What sets the seed for generating random numbers to a random number for the current GPU?,seed,10656
4564,On which GPUs does seed_all set the seed for generating random numbers to a random number?,all GPUs,10658
4565,What sets the seed for generating random numbers to a random number on all GPUs?,seed_all,10658
4566,The seed_all Sets the seed for generating random numbers to a random number on all what?,GPUs,10658
4567,What returns the current random seed of the current GPU?,initial_seed,9618
4568,What is a tensor to specified GPU devices?,comm.broadcast Broadcasts,9097
4569,comm.broadcast Broadcasts a tensor to specified what?,GPU devices,9097
4570,comm.broadcast_coalesced Broadcasts a sequence tensors to the specified what?,GPUs,9098
4571,What does comm.reduce_add Sums tensors do from multiple GPUs?,comm.reduce_add Sums tensors,9100
4572,What does comm.reduce_add from multiple GPUs?,Sums tensors,9100
4573,What does comm.scatter Scatters tensor do across multiple GPUs?,comm.scatter Scatters tensor,9101
4574,comm.scatter Scatters tensor across multiple what?,GPUs,9101
4575,What does comm.gather gather from multiple GPU devices?,tensors,9099
4576,Stream Wrapper wraps around what stream?,CUDA,6078
4577,Stream Wrapper around what stream?,CUDA,6078
4578,What wrapper is used around a CUDA stream?,Stream Wrapper,6078
4579,What is wrapper that wraps around a CUDA event?,Event Wrapper,2265
4580,What wrapper is used around a CUDA event?,Event Wrapper,2265
4581,What does empty_cache release?,unoccupied cached memory,9250
4582,What returns a human-readable printout of the running processes and their GPU memory use for a given device?,list_gpu_processes,9802
4583,memory_stats Returns a dictionary of what memory allocator statistics for a given device?,CUDA,9902
4584,What does memory_stats return for a given device?,a dictionary of CUDA memory allocator statistics,9902
4585,What returns a dictionary of CUDA memory allocator statistics for a given device?,memory_stats,9902
4586,memory_stats Returns a dictionary of what for a given device?,CUDA memory allocator statistics,9902
4587,What does memory_summary return?,a human-readable printout of the current memory allocator statistics,9903
4588,What returns a human-readable printout of the current memory allocator statistics for a given device?,memory_summary,9903
4589,What memory allocator does memory_snapshot return a snapshot of?,CUDA,9901
4590,What returns a snapshot of the CUDA memory allocator state across all devices?,memory_snapshot,9901
4591,memory_allocated Returns the current GPU memory occupied by what?,tensors,9898
4592,What returns the current GPU memory occupied by tensors in bytes for a given device?,memory_allocated,9898
4593,What returns the maximum GPU memory occupied by tensors in bytes for a given device?,max_memory_allocated,9884
4594,What does max_memory_allocated return?,tensors,9884
4595,What resets the starting point in tracking maximum GPU memory occupied by tensors for a given device?,reset_max_memory_allocated,10590
4596,Reset_max_memory_allocated Resets the starting point in tracking maximum GPU memory occupied by what?,tensors,10590
4597,What does memory_reserved return?,the current GPU memory,9900
4598,memory_reserved Returns the current GPU memory managed by the caching allocator in what format for a given device?,bytes,9900
4599,What returns the current GPU memory managed by the caching allocator in bytes for a given device?,memory_reserved,9900
4600,memory_reserved Returns the current GPU memory managed by the caching allocator in what for a given device?,bytes,9900
4601,What returns the maximum GPU memory managed by the caching allocator in bytes for a given device?,max_memory_reserved,9885
4602,How many bytes does max_memory_reserved return for a given device?,bytes,9885
4603,What is setting that sets memory fraction for a process?,set_per_process_memory_fraction,10671
4604,What is set memory fraction for a process?,set_per_process_memory_fraction,10671
4605,What is memory_cached Deprecated?,seememory_reserved(),9899
4606,What resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device?,reset_max_memory_cached,10591
4607,Reset_max_memory_cached Resets the starting point in tracking maximum GPU memory managed by what?,caching allocator,10591
4608,What resets the “peak” stats tracked by the CUDA memory allocator?,reset_peak_memory_stats,10592
4609,What does reset_peak_memory_stats do?,CUDA memory allocator,10592
4610,What type of event occurs at some point?,instantaneous,10285
4611,What type of event does nvtx.mark describe?,instantaneous,10285
4612,What pushes a range onto a stack of nested range span?,nvtx.range_push,10287
4613,nvtx.range_push Pushes a range onto what?,stack,10287
4614,nvtx.range_pop Pops a range off of a stack of what?,nested range spans,10286
4615,nvtx.range_pop Pops a range off of what?,a stack of nested range spans,10286
4616,What is feature that reduces the amount of scatter?,reduce_scatter,10566
4617,What is a placeholder for?,placeholder x x (),10424
4618,What is function that determines the weight of a linear weight?,get_attr linear_weight linear.weight,9451
4619,What is a linear.weight?,get_attr linear_weight,9451
4620,What is call_function?,add_1,9000
4621,What is built-in function add?,call_function add_1,9000
4622,What is call_module linear_1 linear?,add_1,9007
4623,What is _1 linear?,call_module linear,9007
4624,what is call_module?,linear_1,9007
4625,call_module is what?,linear,9007
4626,What is relu_1 relu (linear_1)?,call_method,9004
4627,What is the decimal value of the call_function sum_1?,-1,9001
4628,‘dim’: what?,-1,9001
4629,What is built-in method sum...>?,call_function sum_1,9001
4630,What is built-in method topk?,call_function topk_1,9003
4631,What is the output output output?,topk_1,10360
4632,What is FX Graph Mode Quantization?,Eager Mode Quantization FX Graph Mode Quantization,2244
4633,What is the term for FX Graph Mode Quantization?,Eager Mode Quantization,2244
4634,What is Eager Mode Quantization FX?,Graph Mode Quantization,2244
4635,What is the status of the beta prototype?,Release Status beta prototype,5135
4636,What is the status of the release?,beta prototype,5135
4637,What is the release status beta?,prototype,5135
4638,What type of manual is used?,Operator Fusion Manual,4672
4639,What type of manual is the operator Fusion?,Automatic,4672
4640,What is manual?,Operator Fusion Manual Automatic,4672
4641,What type of manual is the Quant/DeQuant Placement Manual?,Automatic,5050
4642,Quant/DeQuant Placement Manual is what?,Automatic,5050
4643,What is Automatic Placement Manual?,Quant/DeQuant Placement Manual,5050
4644,What is Supported Supported?,Quantizing Modules,5076
4645,What are Quantizing Modules Supported Supported?,Quantizing Modules Supported Supported,5076
4646,What type of Modules are Supported?,Quantizing Modules Supported,5076
4647,What type of manual is automatic?,Quantizing Functionals/Torch Ops Manual,5075
4648,What type of manual is Quantizing Functionals/Torch Ops Manual?,Automatic,5075
4649,Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quant,Torch Ops Manual Automatic,5075
4650,What does Torch Ops Manual Automatically do?,Quantizing Functionals,5075
4651,What is limited support for?,Customization,6092
4652,What type of Support does Customization Limited Support offer?,Fully Supported,6092
4653,Support for what type of support?,Customization Limited Support Fully Supported,6092
4654,"Quantization Mode Support Post Training Quantization: Dynamic, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training Quantization",Static,5056
4655,"Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight Only, Quantiztion Aware Training: Static Post Training Quant",Weight Only,5056
4656,"What is the term for Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training?",Quantization Mode Support,5056
4657,What type of torch may need some refactoring to make the model compatible with FX Graph Mode Quantization?,Module,3852
4658,What may need some refactoring to make the model compatible with?,FX Graph Mode Quantization,3852
4659,What does the torch need some refactors to make it compatible with?,FX Graph Mode Quantization,3852
4660,What are the weights converted to?,int8,11209
4661,What modules does torch.nn.intrinsic implement?,conv + relu,11165
4662,What can be done with the combined modules conv + relu?,quantized,11165
4663,What is module that implements the versions of the fused operations needed for quantization aware training?,torch.nn.intrinsic.qat,11168
4664,This module implements the versions of those fused operations needed for what?,quantization aware training,11168
4665,What are some of the quantized implementations of fused operations?,conv + relu,11170
4666,In what platform do the nn modulesConv2d() andLinear() run?,FP32,11171
4667,What effect does the rounding of the nn modules simulate?,INT8 quantization,11171
4668,In what platform do the modulesConv2d() andLinear() run?,FP32,11171
4669,What quantization does torch.nn.qat simulate?,INT8,11171
4670,What module implements the quantized versions of the nn layers?,torch.nn.quantized,11172
4671,What does torch.nn.Conv2d module implement ?,torch.nn.Conv2d module implements  a quantized version of the nn layers,11172
4672,How is torch.nn.quantized.dynamically?,"Dynamically quantized Linear,LSTM,LSTMCell,GRUCell, andRNNCell",11173
4673,"Dynamically quantized Linear,LSTM,LSTMCell,GRUCell, andRNNCell?",torch.nn.quantized,11173
4674,With respect to what  the backward Computes the sum of gradients of given tensors ?,graph leaves,8940
4675,What computes the sum of gradients of given tensors with respect to graph leaves?,backward,8940
4676,What returns the sum of gradients of outputs with respect to the inputs?,grad Computes,9481
4677,What computes and returns the sum of gradients of outputs with respect to the inputs?,grad,9481
4678,What type of function does functional.jacobian Function compute?,Jacobian,9420
4679,What does functional.jacobian Function do?,computes the Jacobian of a given function,9420
4680,What is a functional.hessian function?,computes the Hessian of a given scalar function,9414
4681,What computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs?,functional.vjp Function,9431
4682,A functional.vjp Function computes the dot product between a vector vand and what of the given function?,the Jacobian,9431
4683,What does functional.vjp compute between a vector vand the Jacobian of the given function at the point given by the inputs?,the dot product,9431
4684,What type of function computes the dot product between a vector vand the Jacobian of the given function at the point given by the input,functional,9431
4685,The functional.vjp Function computes the dot product between a vector vand and what?,the Jacobian,9431
4686,What is the dot product between a given function and a vector v?,the Jacobian,9424
4687,Where is the dot product computed between the Jacobian of the given function?,the point given by the inputs and a vector v,9424
4688,What computes the dot product between a vector vand the Hessian of a given scalar function at the point given by,functional.vhp Function,9429
4689,What is the vector vand of a given scalar function?,the Hessian,9429
4690,What does functional.vhp compute between a vector vand the Hessian of a given scalar function at the point given,the dot product,9429
4691,What computes the dot product between the Hessian of a given scalar function and a vector vat the point given by,functional.hvp Function,9419
4692,What is the dot product between a given scalar function and a vector vat?,the Hessian,9419
4693,What does functional.hvp compute between the Hessian of a given scalar function and a vector vat the point given,the dot product,9419
4694,What requires gradients to be computed for?,Tensor,10978
4695,What are all Tensors that require grad which is False?,leaf Tensors,10974
4696,What type of Tensor is a torch?,leaf,10974
4697,What does torch.Tensor.detach remove a new Tensor from?,current graph,10959
4698,What is detached from the current graph?,Tensor,10959
4699,What type of hook does torch.Tensor.register_hook register?,backward hook,10976
4700,What does Function.forward do?,Function.forward Performs the operation,2652
4701,How are tensors modified in function._ContextMethodMixin.mark_dirty?,in-place operation,9410
4702,In what operation are tensors modified?,in-place operation,9410
4703,Function._ContextMethodMixin.mark_non_differentiable Marks outputs as what?,non-differentiable,9411
4704,What mark does function._ContextMethodMixin.mark_non_differentiable?,function._ContextMethodMixin.mark_non_differentiable,9411
4705,To what direction does function._ContextMethodMixin.save_for_backward save given tensor,tobackward,9412
4706,What does function._ContextMethodMixin.save_for_backward save?,given tensors,9412
4707,What does function._ContextMethodMixin.set_materialize_grads set whether to materialize output?,grad tensors,9413
4708,What does function._ContextMethodMixin.set_materialize_grads Set?,whether to materialize output grad tensors,9413
4709,What function sets whether to materialize output grad tensors?,function._ContextMethodMixin,9413
4710,What is computed via small finite differences against analytical gradients w.r.t?,gradcheck Check gradients,9482
4711,What is check gradients computed via small finite differences against analytical gradients w.r.t.,gradcheck,9482
4712,What does gradgradcheck do?,gradgradcheck Check gradients of gradients computed via small finite differences against analytical gradients,9483
4713,What check gradients of gradients computed via small finite differences against analytical gradients?,gradgradcheck,9483
4714,What does profiler.profile.export_chrome_trace export?,EventList,10450
4715,What does export an EventList as a Chrome tracing tools file?,profiler.profile.export_chrome_trace,10450
4716,What does profiler.profile.total_average mean?,average of all events,10457
4717,What is the name of all events?,profiler.profile.total_average,10457
4718,What operating system is ppc64le?,Linux,4118
4719,What is the name of Linux's GPU?,ppc64le,4118
4720,What version of of Linux for pytorch?,aarch64,4117
4721,Which  Linux's CPU is for pytorch?,aarch64,4117
4722,What kind of support does NumPy have?,GPU support,10937
4723,What is a Tensor library similar to?,NumPy,10937
4724,What is torch.autograd?,tape-based automatic differentiation library,10988
4725,What does torch.autograd support?,all differentiable Tensor operations,10988
4726,What is tape-based automatic differentiation library that supports all differentiable Tensor operations in torch?,torch.autograd,10988
4727,What is torch.nn deeply integrated with?,autograd,11108
4728,What is torch.nn designed for?,maximum flexibility,11108
4729,What is torch.nn a neural networks library deeply integrated with?,autograd,11108
4730,What language does torch.multiprocessing use?,Python,11100
4731,What is torch.multiprocessing useful for?,data loading and Hogwild training,11100
4732,What do torch.multiprocessing haeve for torch Tensors  ?,magical memory sharing,11100
4733,What magical memory sharing does torch.multiprocessing Python multiprocessing have?,torch Tensors across processes. Useful for data loading and Hogwild training,11100
4734,What is DataLoader and other utility function ?,torch.utils DataLoader,11239
4735,What is the purpose of torch.utils DataLoader?,convenience,11239
4736,InferenceMode is an analogous to what?,no_grad,3847
4737,Code run under InferenceMode gets better performance by disabling what?,by disabling view tracking and version counter bumps,3846
4738,InferenceMode functions as what?,decorator,3846
4739,What do you make sure to make  InferenceMode work as decorator?,instantiate with parenthesis,3846
4740,What does InferenceMode do?,enable or disable gradients locally,3846
4741,Whataracteristic the  context manager  InferenceMode have?,thread local,1877
4742,What does the context manager function as?,decorator,7482
4743,What does mode(bool) do?,Flag whether to enable or disable inference mode,7482
4744,What does torch.trapz do?,"Estimate ∫y dx along dim, using trapezoid rule",2264
4745,what torch.trapz return?,"A Tensor with the same shape as the input, each element being integral along dim",2264
4746,"for the dim  parameter to torch.trapz- By default, what is the dimension along which to integrate?",the last dimension,2264
4747,for the dx parameter to torch.trapz What is the point at which the functiony is sampled?,x(Tensor),11435
4748,"for the x  parameter to torch.trapz -If the functionyis not in ascending order, what contributes negatively to the estimated integral?",Ifxis not in ascending order,11435
4749,"If inputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should",integer,3619
4750,for torch.mul Each element of the tensorinputis multiplied by the corresponding element of what?,Tensor other,3619
4751,for torch.mul The shapes of input and other must be what?,be broadcastable,3619
4752,for torch.mul What ore the parameters and out?,"input , other, out where out i = other X inout i",3427
4753,in torch.logsumexp How is the computation handled?,numerically stabilized,5591
4754,"in torch.logsumexp If keepdim is True, the output tensor has how many fewer dimension(s)?",1,5591
4755,"in torch.logsumexp, If the output tensor is of the same size as input, how many dimension(s) does it have?",1,2577
4756,torch.max Returns the maximum value of all elements in what?,the input tensor,5607
4757,for torch.max What is the maximum value of each row of the inputtensor in the given dimensiondim?,a namedtuple,5607
4758,"for  torch.max If keepdimisTrue, the output tensors have what?",1 fewer dimension thaninput,5321
4759,"for torch.min If the output tensors are of the same size asinput, what is the default?",IfkeepdimisTrue,5321
4760,What is the value of the minimum value of each row of the inputtensor in the given dimensiondim?,Note,5321
4761,What returns the minimum value of each row of theinputtensor in the given dimensiondim?,a namedtuple,5321
4762,"By  torch.max, What is returned If there are multiple maximal values in a reduced row ?",the indices of the first maximal value,2322
4763,"for torch.max , output tensors are of the same size as input, in what  default condition?",IfkeepdimisTrue,5608
4764,in torch.max  What are returned if there are multiple maximal values in a reduced row?,indices of the first maximal value,5608
4765,By torch.max What is returned for  in the given dimensiondim?,the maximum value of each rpw of the input  tensor ,5320
4766,"For torch.max, If keepdimisTrue, the output tensors are of what size?",size 1,3624
4767,For torch.max What happens if there are multiple maximal values in a reduced row?,the indices of the first maximal value are returned,3624
4768,for torch.max What default value indicates that the output tensor hasdimretained?,False,3624
4769,"for torch.min, What happens if the output tensors are squeezed?",1 fewer dimension thaninput,5639
4770,If the output tensors are of the same size as input except in the dimensiondimwhere they are of size 1 what is the default,IfkeepdimisTrue,5639
4771,"If keepdimisTrue, the output tensors are of the same size as input except in the dimensiondimwhere they are",1 fewer dimension,5639
4772,What are returned if there are multiple minimal values in a reduced row?,the indices of the first minimal value,5639
4773,"for torch.max, What is the default value of the output tensor hasdimretained?",False,4371
4774,What does move totorch.hub do?,Loads the Torch serialized object at the given URL,4272
4775,"in torch.utils.model_zoo.load_url where the object should already be  present ,when it's deserialized and returned?",model_dir,4133
4776,What is the default value of the hash?,False,4133
4777,for torch.mean How many dimension(s) smaller is the output tensor?,1,5621
4778,in torch.mean What do you do ifdimis a list of dimensions?,reduce,2326
4779,for torch.mean How many dimension(s) fewer dimension(s) does the output tensor have?,1,9657
4780,in torch.mean What do you do ifdimis a list of dimensions over all of them?,reduce,5622
4781,in torch.quantile How is the result computed if the quantile lies between two data points?,linear interpolation,7873
4782,in torch.quantile What do we map to compute the quantile?,"q in [0, 1] to the range of indices [0, n]",7873
4783,in torch.quantile What is fraction?,fractional part of the computed quantile index,8782
4784,"in torch.quantile If the first dimension of the output represents quantiles and has size equal to the size ofq, what is the size of the remaining dimensions?",Ifqis a 1D tensor,8782
4785,in torch.quantile What resulted in the inputtensor being flattened before computation?,defaultdimisNone,3633
4786,in torch.quantile What does the first dimension of the output represent?,the first dimension of the output represents the quantiles,3633
4787,What does torch.vdot do?,Computes the dot product of two 1D tensors,1712
4788,"What function handles complex numbers differently than dot(a, b)?","vdot(a, b)",1712
4789,What intentionally only supports computing the dot product of two 1D tensors with the same number of elements?,torch.vdot,1712
4790,for torch.vdot When is the conjugate of input(Tensor) used?,if it’s complex,1712
4791,"What is the same as totorch.hann_window(L+1,periodic=False)?",havetorch.hann_window,7145
4792,"for torch.hann_window ,what is the use of periodic parameter?","If True, returns a window to be used as periodic function. If False, return a symmetric window.",7145
4793,What does the returned solutionin torch.lstsq()store the residuals of the solution in?,ncolumns,8213
4794,The returnedsolutionintorch.lstsq()stores the residuals of the solution in the lastm - what in the,ncolumns,11091
4795,What documentation provides more details about the global deterministic flag?,totorch.use_deterministic_algorithms(),5237
4796,What documentation does totorch.use_deterministic_algorithms() refer to for more details?,totorch.use_deterministic_algorithms(),5237
4797,What kind of distribution is the fillsselftensor drawn from?,geometric distribution,2429
4798,What is filled with elements drawn from the geometric distribution?,Fillsselftensor,2429
4799,What type of distribution does fillselftensor with elements drawn from with torch.Tensor.geometric_?,geometric distribution,2429
4800,What kind of distribution is the fillsselftensor?,uniform,2431
4801,What is the name of the function that uses numbers from the continuous uniform distribution?,Fillsselftensor,2431
4802,How are numbers sampled from the continuous uniform distribution?,using torch.Tensor.uniform_,2431
4803,What is filled with numbers sampled from the continuous uniform distribution?,self.tensor,2431
4804,What does torch.flipudis do to return a new tensor?,Flip the entries in each column in the up/down direction,2474
4805,In what direction does the flipud return a new tensor?,Flip the entries in each column in the up/down direction,2474
4806,"What are preserved, but appear in a different order than before when flipud is used?",Rows,2474
4807,What is requirement on tensor when using flipud?,2-D,2474
4808,What is the difference between torch.flipudis and numpy.flipud?,torch.flipud is slower than numpy.flipud,2474
4809,What does flip tensor in the up/down direction return?,torch.flipud,2474
4810,In which direction do the entries in each column flip?,up/down,2474
4811,What is required for the tensor to be at least?,1-D,2474
4812,What makes a copy of input's data?, torch.flip,5700
4813,What program returns a view in constant time?,NumPy,5700
4814,What is the order of a n-D tensor along a given axis in?,dims,5700
4815,What is torch.flip expected to be?,slower thannp.flip,5700
4816,What does dims(a listortuple) flip on?,axis,5700
4817,What returns a view in constant time?,NumPy’snp.flip,5700
4818,What is the order of a tensor along a given axis in dims?,n-D,5700
4819,Why is copying a tensor's data slower than numPy'snp.flip?,more work,5700
4820,What does dims(a listortuple) represent?,axis to flip on,5700
4821,Where is the tensor's dtype inferred?,fromfill_value,2030
4822,How is the tensor's dtype inferred with torch.full?,from fill_value,2030
4823,What is inferred from fill_value?,datatype,2030
4824,What is the name of the generator state?,atorch.ByteTensor,2703
4825,What does atorch.ByteTensor get?,current device,2703
4826,What does the torch.Generator do?,Sets the seed for generating random numbers,6187
4827,What does the Tensor return?,atorch.Generatorobject,6187
4828,What is the name of the object that returns the initial seed for generating random numbers?,atorch.Generatorobject,2325
4829,What is the name of the function that returns the initial seed for generating random numbers?,torch.Generator Returns the initial seed for generating random numbers,2325
4830,What Sets the seed for generating random numbers return?,a torch.Generator object,2325
4831,What is recommended to set a number that has a good balance of 0 and 1 bits?,large seed,2325
4832,What does it return for generating random numbers?,Returns the initial seed,2325
4833,What is a number that has a good balance of 0 and 1 bits?,a large seed,5578
4834,a good balance of what in a seed needs to be considered?,0 and 1  bits,5578
4835,What  returns the initial seed for generating random numbers?,a torch.Generator object,5578
4836,What should you avoid in the seed?,Avoid having many 0 bits,5578
4837,As What is the  Generator state is retured by generator?,atorch.ByteTensor,5510
4838,What does Atorch.ByteTensor contain?,Tensor,2324
4839,What is the pupose of initial seed for generating random numbers?,Sets the seed for generating random numbers,2324
4840,What does Atorch.ByteTensor contain.,Tensor all the necessary bits to restore a Generator to a specific point in time,1356
4841,What does Atorch.ByteTensor return?,atorch.Generatorobject,1356
4842,What does Atorch.ByteTensor do?,Sets the seed for generating random numbers,1356
4843,for  torch.is_inference_mode_enabled Returns what if inference mode is currently enabled?,True,5208
4844,What is returned if inference mode is enabled?,True,5208
4845,for torch.var_mean What contains the variance and mean?,tuple,3643
4846,What is the name of correction that will be used to calculate the variance?,Bessel's correction,3643
4847,What is the name of the function that determines whether the output tensor is hasdimretained or not?,keepdim,9660
4848,What is the function that calculates the variance and mean of all elements in the inputtensor?,Calculates the variance and mean of all elements in theinputtensor,9660
4849,What is calculated by a tuple containing the variance and mean of all elements in the inputtensor with torch.var_mean?,the variance and mean of all elements in theinputtensor with torch.var_mean,9660
4850,"in torch.var_mean A tuple (var, mean) contains what?",variance and mean,9759
4851,What is used to determine whether to use Bessel's correction?,unbiased as True,9759
4852,What indicates that Bessel's correction will be used?,IfunbiasedisTrue,11278
4853,What type of storage does Everytorch.Tensorcast to?,bfloat16,1357
4854,What type of storage does this storage cast to?,float type,1357
4855,What type of storage does Casts this storage to Casts this storage to bool type Casts this storage to byte type Casts this storage,bfloat16,1534
4856,What type of storage does Casts this storage to Casts this storage to byte type Casts this storage to char type Casts this storage to,bool,1537
4857,Casts this storage to what type of type?,bfloat16,1554
4858,What does device(int) contain?,destination GPU id,1539
4859,What type of storage does Casts this storage to byte type Return?,a copy,1539
4860,What type of storage does Casts this storage to Returns a copy of?,char,1542
4861,What does device(int) – refer to ?,destination GPU id,1542
4862,What does Casts this storage return?,a copy,1542
4863,What does Casts this storage to complex double type Casts this storage to complex float type Returns if it's not already on the CPU,a CPU copy,5304
4864, storage to complex is casted  of what type?,float,1544
4865,Casts this storage to complex what type?,float,1544
4866,What does cast this storage to complex float type return if it's not already on the CPU?,a CPU copy,1546
4867,"If the storage is already on the CPU, where is the CPU copy of the storage returned?",if it’s not already on the CPU,5280
4868,The destination GPU id Defaults to what?,current device,5280
4869,"If the source is in pinned memory, the copy will be what with respect to the host?",asynchronous,1548
4870,What is the file name to map shared?,filename(str),1548
4871,"If the storage is already of the correct type, what is performed and the original object is returned?",no copy,1548
4872,What has no effect if the source is in pinned memory?,argument,9168
4873,What is the name of the argument that asynchronous with respect to the host?,non_blocking(bool),9168
4874,Where is the original object stored?,CUDA memory,5301
4875,Where does the object return a copy of?,CUDA memory,5301
4876,What is the name of the function that returns a copy of the object in CUDA memory?,non_blocking(bool),5301
4877,"If the source is in pinned memory, the argument has what effect?",no effect,5300
4878,"If the object is already in CUDA memory and on the correct device, what is performed?",no copy,5300
4879,What does kwargs contain for compatibility?,keyasyncin place of thenon_blockingargument,1541
4880,What is the size of the storage?,sizeis the number of elements in the storage,1541
4881,"If the object is already in what, then no copy is performed and the original object is returned?",CUDA memory,3531
4882,What type of storage is cast to double type?,float type,3531
4883,"If the object is already in what memory and on the correct device, then no copy is performed and the original object is returned?",CUDA memory,3531
4884,"If the source is in pinned memory, the copy will be asynchronous with respect to the host, what happens?",the argument has no effect,1543
4885,What type of storage does not affect the file?,IfsharedisFalse,1543
4886,What does Casts this storage to char type return?,a copy,1543
4887,What type of storage does the keyasyncin cast to?,double type,9167
4888,What type of storage does double type cast?,float type,9167
4889,"If the source is in pinned memory, the copy will be asynchronous with respect to the host, what effect does the argument have?",no effect,10255
4890,What type of argument has no effect if the source is in pinned memory?,non_blocking(bool),10255
4891,What type of storage is used to share memory between all processes?,IfsharedisTrue,1545
4892,What type of storage does Casts this storage to?,float,1545
4893,What happens if the storage is cast to double type?,All changes are written to the file,1549
4894,What type of storage is casts?,float type,1549
4895,What does filename(str) - file name to map?,filename(str) – file name to map,1549
4896,What may kwargs contain for compatibility?,keyasyncin place of thenon_blockingargument,7606
4897,Which storage does not need to be moved for sharing across processes?,CUDA storages,7606
4898,What cannot be resized?,Storages in shared memory,7606
4899,"Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, otherwise casts this object to the",self Casts this storage to short type,7606
4900,"IfTrue and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",non_blocking(bool),7606
4901,What will be created if needed?,IfsharedisTruethe file,10701
4902,If the file must contain at leastsize * sizeof(Type)bytes (Typeis the type of storage)?,IfsharedisFalse,10701
4903,What happens if the storage is cast to float type IfsharedisTrue?,All changes are written to the file,1552
4904,"If the changes on the storage do not affect the file, what is the name of the type of storage?",IfsharedisFalse,1552
4905,What happens when a float type is shared between all processes?,All changes are written to the file,1552
4906,What does filename(str) – file name to map shared(bool) – whether to share memory?,filename(str) – file name to map shared(bool) – whether to share memory,1552
4907,"IfsharedisTrue, the file must contain what?",at leastsize * sizeof(Type)bytes,1553
4908,What does Casts this storage to if it's not already pinned?,Copies the storage to pinned memory,1553
4909,What type of storage does the changes on the storage do not affect the file?,IfsharedisFalse,1553
4910,What means that memory is shared between all processes?,IfsharedisTrue,3638
4911,What happens if memory is shared between all processes?,All changes are written to the file,3638
4912,for storage What does self return?,self Casts this storage to short type Returns a list containing the elements of this storage,3638
4913,What happens if sharedisTrue?,memory is shared between all processes,3638
4914,Returns a list containing the elements of what storage?,self Casts this storage to short type,3638
4915,What means that the changes on the storage do not affect the file?,IfsharedisFalse,3637
4916,What is the size of a file?,sizeis the number of elements in the storage,3637
4917,What is shared between all processes?,memory,3637
4918,What does size mean?,sizeis the number of elements in the storage,10699
4919,"If the file must contain at leastsize * sizeof(Type)bytes, what is it called?",IfsharedisFalse,10699
4920,What returns the storage to shared memory?,self,9339
4921,What type of return does CUDA return?,self,9339
4922,Is shared memory able to be resized?,Storages in shared memory cannot be resized,1557
4923,What is the name of the storage that casts it to short type?,self,1557
4924,What does self do to a storage?,self Casts this storage to short type,10695
4925,Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what type of storage?,CUDA,10695
4926,What type of storage is cast to pinned memory if it's not already pinned?,Copies,1556
4927,Copies the storage to what if it's not already pinned?,pinned memory,1556
4928,"If this is already of the correct type, no copy is performed and what is returned?",the original object is returned,1556
4929,What happens if the copy is performed asynchronously with respect to the host?,the argument has no effect,1556
4930,What type of storage does not need to be moved for sharing across processes?,CUDA storages,1916
4931,What does the storage do if it's not already pinned?,Copies,1916
4932,What type of storage is the async arg deprecated?,bfloat16,1916
4933,What type of storage is cast to pinned memory?,Copies,1559
4934,What type does self cast the storage to?,ifdtypeis not provided,1558
4935,What is performed if the storage is already of the correct type?,no copy,4277
4936,What is the desired type of a storage?,dtype(typeorstring),7605
4937,What is the desired type of the storage?,dtype(typeorstring),9340
4938,What happens if the type of the storage is not provided?,ifdtypeis not provided,1560
4939,What returns the type of the storage?,ifdtypeis not provided,5310
4940,What has no effect if the source is in pinned memory and destination is on the GPU?,argument,1555
4941,What type does return the type of the object?,ifdtypeis not provided,5681
4942,What happens to the argument if the source is in pinned memory and destination is on the GPU?,no effect,5681
4943,What happens if the object is already of the correct type?,no copy is performed and the original object is returned,5302
4944,"If the source is in pinned memory and destination is on the GPU, the copy is performed asynchronously with respect to the host, what happens",argument has no effect,3529
4945,What type of storage does Theasyncarg cast to?,bfloat16,4278
4946,What type of memory is shared between all processes?,IfsharedisTrue,4278
4947,"If the argument is true, the copy is performed asynchronously with respect to the host, and the source is in pinned memory and destination is",no effect,10253
4948,"If the source is in pinned memory and destination is on the GPU, how is the copy performed with respect to the host?",asynchronously,10253
4949,"IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy performed?",asynchronously,10253
4950,What is the name of the function that performs asynchronously with respect to the host?,non_blocking(bool),10253
4951,What type of storage does the bool type cast to?,byte type,10252
4952,Solves a system of equations with what?,triangular coefficient,5968
4953,What does solve AX = b assume A to be?,upper-triangular,5968
4954,What does solve AX = b  solve?,a system of equations with a triangular coefficient matrix,5968
4955,What does solvesAAA assume is?,upper-triangular,3751
4956,What is the value of * of more batch dimensions?,zero,3751
4957,Supports input of what data types?,"float, double, cfloat and cdouble",6120
4958,"What supports multiple right-hand sides of size(,m,k)(*, m, k)(,m,k",b(Tensor),6120
4959,What is the default value of the upper-triangular system of equations?,True,891
4960,"What is the default value of the transpose(bool,optional)?",False,891
4961,What is the default value for whether the AAA should be transposed before being sent into the solver?,transpose,891
4962,"What is the name of the multiple right-hand sides of size(,m,k)?",b(Tensor),8936
4963,"What is the input triangular coefficient matrix of size(,m,m)(*, m, k)(,m",b(Tensor),8936
4964,What is the default value of seetorch.set_default_tensor_type()?,ifNone,5442
4965,What defines the shape of the output tensor with torch.ones?,a sequence of integers,5442
4966,What is a boolean tensor that is True whereinputis greater thanother?,Computesinput,1813
4967,What represents the principal directions of AAA?,AAAis a data matrix withmsamples andnfeatures theVVVcolumns,4808
4968,What is a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions?,AAAis,893
4969,"What is the relation of(U,S,V)to PCA?",AAAis a data matrix withmsamples andnfeatures theVVVcolumns,7251
4970,What is the size of returned matrices?,UUUis m x q matrix,7251
4971,The size of returned matrices depend on the specified rank and what other value?,q,10880
4972,What is the default q value?,"By default,q=min(6,m,n)",2141
4973,What is the default setting to center the input tensor?,if True,5726
4974,"What is the default value of the q(int,optional)?","By default,q=min(6,m,n)",5726
4975,What is a reference to the number of subspace iterations to conduct?,References,5726
4976,What is the starting dimension length?,the distance to the ending dimension,5348
4977,What is the name of the inputtensor that returns a new tensor that is a narrowed version ofinputten,dimensiondimis input fromstarttostart+length,5348
4978,Input(Tensor) – the tensor to narrow dim(int) – the dimension along which to narrow what,start,5348
4979,Andindicesis what of each minimum value found (argmin)?,index location,2323
4980,"If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondim where they",1 fewer dimension,2323
4981,When are the output tensors of the same size as input?,IfkeepdimisTrue,3626
4982,What is the tuple of two output tensors?,out,4373
4983,What kind of function is the call to geqrf?,low-level,7603
4984,What is the return value of the low-level function for calling LAPACK's geqrf?,namedtuple,7603
4985,What is the function called for calling LAPACK's geqrf directly?,low-level function,7603
4986,What does the function compute?,QR decomposition ofinput,1640
4987,Where are the elements ofRare stored?,on and above the diagonal,1640
4988,Where is the matrix stored?,below the diagonal,1640
4989,What is the result of withtorch.ormqr()?,matrix-matrix multiplication,1640
4990,What is the name of the LAPACK function that computes Q and R matrices?,geqrf,5798
4991,What can solve matrix equations using?,a QR decomposition,5798
4992,"What is the default value for the input matrix out(tuple,optional)?",Default:None,5798
4993,"What is the input matrix out(tuple,optional)?",input(Tensor),7604
4994,What returns a number of samples from the multinomial probability distribution located in the corresponding row of tensorinput?,tensor,5464
4995,What must the rows ofinputdo be?,"non-negative, finite and have a non-zero sum",5464
4996,"Ifinput is a vector, what is it?",Ifinputis a vector,5464
4997,What must the rows ofinput be?,"non-negative, finite and have a non-zero sum",5464
4998,What is ordered from left to right according to when each was sampled?,Indices,3844
4999,"Ifinputis a matrix,outis a matrix of shape(mnum_samples)?",matrix withmrows,3844
5000,What is the name of the variable that is used when a sample index is drawn for a row?,Note,3844
5001,How are samples ordered from left to right?,first samples are placed in first column,3844
5002,What is the difference between a vector and a matrix of sizenum_samples?,Note,3844
5003,What must the rows of inputdo be?,"non-negative, finite and have a non-zero sum",7262
5004,What must the rows of inputdo not need to sum to one?,"non-negative, finite and have a non-zero sum",7262
5005,"Ifinputis a matrix withmrows,outis a matrix of shape(mnum_samples)(m",Ifinputis a vector,7262
5006,"Ifinputis a matrix, outis a matrix of shape(m times textnum_samples",matrix withmrows,4413
5007,How many rows of inputdo not need to sum to one?,rows ofinputdo not need to sum to one,4413
5008,"Ifinputis a vector,outis a matrix of what?",matrix withmrows,4413
5009,"If what isTrue, samples are drawn with replacement. If not, samples are drawn without replacement.",replacement,3618
5010,"Ifinputis a matrix withmrows, outis a matrix of shape(mnum_samples)?",matrix withmrows,3618
5011,What is the name of a vector that is a matrix of sizenum_samples?,Note,3618
5012,"If a sample index is drawn for what, it cannot be drawn again for that row?",a row,3618
5013,What is outputis a vector of sizenum_samples?,Ifinputis a vector,3618
5014,"If replacement isTrue, samples are drawn with replacement. If not, what happens to them?",they are drawn without replacement,3474
5015,"If a sample index is drawn for a row, it cannot be drawn again for that row?",If replacement isTrue,3474
5016,What does input(Tensor) contain probabilities?,input tensor,3474
5017,"If what is true, samples are drawn with replacement. If not, they are drawn without replacement?",If replacement isTrue,3474
5018,What must num_samples be lower than when drawn without replacement?,number of non-zero elements ininput,4414
5019,"What is drawn for a row, it cannot be drawn again for that row?",a sample index,3463
5020,"If num_samples is lower than the min number of non-zero elements in each row of input, what is it?",matrix,3463
5021,What do modules make it easy to specify for PyTorch's Optimizers to update?,learnable parameters,7853
5022,What are so fundamental to PyTorch?,modules,7853
5023,How do modules work with PyTorch's autogradsystem?,Easy to work with and transform,7852
5024,What is one of the advantages of using modules?,Easy to work with and transform,2247
5025,What is a Simple Custom Module Modules a part of?,Building Blocks Neural Network Training,2247
5026,How are modules to save and restore?,straightforward,2247
5027,What are the Building Blocks of Neural Network Training?,Modules Module State,2247
5028,Why are many topics in this note elaborated on in other notes or tutorials?,modules are so fundamental to PyTorch,7768
5029,What does the Linearmodule module apply to its input?,an affine transformation,7768
5030,What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?,Simple Custom Module Modules,7768
5031,What is a simple custom module?,Module State Module Hooks Advanced Features,7767
5032,What type of training does A Simple Custom Module Modules provide?,Neural Network Training,7767
5033,What is a module that can be used for Neural Network Training?,Module State Module Hooks Advanced Features,4256
5034,What does this module apply to its input?,an affine transformation,7899
5035,What is the name of the module that applies an affine transformation to its input?,"a simpler, custom version of PyTorch’sLinearmodule",7899
5036,What is each of the random-initializedweightandbiastensors defined as?,aParameter,7899
5037,What module applies an affine transformation to its input?,PyTorch’sLinearmodule,7899
5038,What is the basic feature of the Linearmodule module?,module has the following fundamental characteristics of modules,974
5039,What is a custom version of PyTorch'sLinearmodule called?,Advanced Features,974
5040,What is an example of a module that can be used to train a neural network?,Neural Network Training with Modules Module State Module Hooks Advanced Features,4304
5041,What is the name of the advanced features of Neural Network Training?,Neural Network Training with Modules Module State Module Hooks Advanced Features,4304
5042,What is the basic feature of the Linearmodule?,module has the following fundamental characteristics of modules,7898
5043,What is the name of the module that lets us look at a custom version of PyTorch'sLinearmodule?,Module State Module Hooks Advanced Features,4254
5044,What is the name of the feature that lets us look at a custom version of PyTorch'sLinearmodule?,Module Hooks Advanced Features,4252
5045,What does the simple module inherit from the base Module class?,module has the following fundamental characteristics of modules,4252
5046,What defines some that is used in computation?,state,3907
5047,"What can be considered the ""learnable"" aspects of the module's computation?",Parameters,3908
5048,Modules are not required to have what?,state,3908
5049,What performs arbitrary computation involving any number of inputs and outputs?,theforward()implementation,3908
5050,What is each of the randomly-initializedweightandbiastensors defined as?,aParameter,3908
5051,Parameters can be considered what?,the “learnable” aspects of the module’s computation,3908
5052,What can theforward()implementation for a module perform?,arbitrary computation involving any number of inputs and outputs,3908
5053,What can be constructed and called in this simple module?,Instances,7813
5054,"What are aspects of a module's computation that should be ""learned""?",the parameters registered by a module,3728
5055,What program can be used to update the parameters of a module?,PyTorch,3729
5056,Modules can contain other what?,modules,3729
5057,What are aspects of a module's computation that should be learned?,the parameters registered by a module,3729
5058,What does PyTorch use to update parameters?,Optimizers,3729
5059,What is the simplest way to do this?,using theSequentialmodule,4258
5060,What does Sequential feed the output of the first MyLinearmodule into?,theReLU,4258
5061,What does Sequential feed the output of the first MyLinearmodule as input into?,theReLU,4258
5062,What does Sequential automatically feed the output of the first MyLinearmodule as?,input into theReLU,4508
5063,"Sequentialautomatically feeds the output of the firstMyLinearmodule as input into the ReLU, and the output of that as input",in-order chaining of modules,4508
5064,What is recommended to define for anything beyond the simplest use cases?,a custom module,3718
5065,What does s() andnamed_modules()recursivelyiterate through a module and its child modules?,module,7901
5066,What register submodules from a list or dict?,TheModuleListandModuleDictmodules,5978
5067,What provides a large library of performant modules within thetorch.nnnamespace?,PyTorch,5978
5068,What is it sometimes necessary for a module to do?,dynamically define submodules,5979
5069,What do TheModuleListandModuleDictmodules do?,register submodules from a list or dict,5979
5070,"Sometimes, it’s necessary for a module to do what?",dynamically define submodules,5979
5071,What do calls toparameters() andnamed_parameters() recursively include?,child parameters,2503
5072,What do we give in the next section of training a neural network?,a full example,2503
5073,Where can PyTorch provide a large library of performant modules?,thetorch.nnnamespace,7436
5074,What is a full example of?,training a neural network,7436
5075,What do you want to know about PyTorch?,more information,7434
5076,What can you find on PyTorch's website?,more information,7434
5077,What is an example of in the next section?,training a neural network,3782
5078,What can be used to optimize a neural network?,PyTorch’s Optimizers,3782
5079,What is an example of how to train a neural network?,Recursivelyapply()a function,7435
5080,What is an example of a function that can be added to a module and its submodules?,Recursivelyapply()a function,2566
5081,What can be used to optimize a network's parameters?,PyTorch’s Optimizers,2566
5082,What is a function that can be added to a module and its submodules?,Recursivelyapply()a function,5110
5083,What is one of PyTorch's Optimizers?,Defining neural net modules,2095
5084,What are present in this simplified example?,key parts of training,2094
5085,What happens when a network is built?,it has to be trained,4638
5086,What does the network learn to output in this simplified example?,zero,3806
5087,What is the optimizer in this example?,stochastic gradient descent optimizer,3806
5088,What does the optimizer do to the network's gradients?,zeros,823
5089,What is an optimizer in this case?,stochastic gradient descent optimizer,1131
5090,What is the name of the optimizer that is created?,stochastic gradient descent optimizer,1131
5091,What does loss.backward() do to update the parameters' gradients?,zeros,8792
5092,What happens after the above snippet is run?,the network’s parameters have changed,8792
5093,The value ofl1'sweightparameter is closer to what value?,0,8792
5094,What does the network do?,computes a loss,8792
5095,What is an example of?,training a neural network,3783
5096,What happens after the above snippet has been run?,the network’s parameters have changed,3784
5097,What is an example of training a neural network?,Using Optimizers,3784
5098,What shows that the network's values are closer to 0?,examining the value ofl1’sweightparameter,4639
5099,What is the name of the article that explains how to train neural networks?,Using Optimizers,4639
5100,What is the term for training neural networks?,Neural network training,988
5101,What is the value ofl1’sweightparameter now closer to?,0,988
5102,What type of training can be tricky?,Neural network training,988
5103,What is the name of a module's dict that affects its computation?,state,816
5104,What is included in a module'sstate_dict?,module’s parameters,816
5105,"For some modules, it may be useful to have state that affects module computation but is not learnable?",beyond parameters,816
5106,What provides the concept of buffers?,PyTorch,816
5107,Parameters are what type of state a module can have?,learnable aspects of computation,816
5108,What does a module'sstate_dict contain?,state that affects its computation,816
5109,"A module'sstate_dict includes state that affects its computation. This includes, but is not limited to, what?",the module’s parameters,816
5110,"For some modules, it may be useful to have state beyond parameters that affects module computation but is what?",not learnable,816
5111,What does PyTorch provide for state beyond parameters that affects module computation but is not learnable?,buffers,816
5112,Parameters are contained within what?,thestate_dict,816
5113,What do all hooks allow the user to return that will be used throughout the remaining computation?,updated value,1398
5114,When are backward hooks called?,when the backward for this Module has been computed,1398
5115,What provides advanced features that are designed to work with modules?,PyTorch,4952
5116,What type of discussion of PyTorch's advanced features can be found in the links below?,In-depth,4952
5117,What are the functionalities of PyTorch's advanced features called when writing a new module?,inherited,4951
5118,Where can an in-depth discussion of PyTorch's advanced features be found?,links below,4951
5119,What is the name of the feature that is inherited when writing a new module?,Profiling,4951
5120,What company provides several more advanced features that are designed to work with modules?,PyTorch,4951
5121,What happens to PyTorch's functionalities when writing a new module?,inherited,4951
5122,What is another name for profiling?,Pruning,4951
5123,Exporting modules to what is a good way to use C++?,TorchScript,2565
5124,What language is a good example of a language that can be exported to TorchScript?,C++,2565
5125,Exporting modules to what?,TorchScript,2565
5126,What language is TorchScript used for?,C++,2565
5127,What is the result oftorch.FloatTensor.abs()?,a new tensor,4242
5128,What is the result of torch.FloatTensor.abs()?,a new tensor,4242
5129,What does usingto()method on a tensor do?,Warning,4242
5130,What are some of the attributes of atorch.Tensor?,"thetorch.dtype,torch.device, andtorch.layoutattributes",2564
5131,"For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what",atorch.Tensor,2564
5132,What kind of memory usage could be caused by the current implementation oftorch.Tensor?,unexpectedly high,4432
5133,What does the new Tensor return withdataas?,tensor data,6581
5134,Returns a new Tensor withdataas what?,tensor data,5337
5135,What returns the reduced singular value decomposition?,IfsomeisTrue,3639
5136,What is the default value of the method that returns the reduced singular value decomposition?,IfsomeisTrue,3639
5137,"If the last two dimensions ofinputaremandn, the returnedUandVmatrices will contain what?","onlymin(n, m)orthonormal columns",3641
5138,What does torch.svd() return ifcompute_uvisFalse?,zero-filled tensors forUandVh,3641
5139,"What should be replaced with _,S,_=torch.svd(A,some=some,compute_uv","U,S,V",3641
5140,What is the default value for both oftorch.linalg.svd()'sfull_matrices?,default value for both isTrue,3641
5141,What types of data types does torch support?,"float, double, cfloat and cdouble data types",6141
5142,The dtypes ofUandVare the same asinput's.Swill always be what?,real-valued,6141
5143,What should be replaced with torch.linalg.svd()?,Note Differences,6141
5144,"Along with cfloat and cfloat, what data type is supported by Torch?",cdouble,6141
5145,The dtypes ofUandVare will always be what?,real-valued,6141
5146,What should be replaced with Note Differences?,withtorch.linalg.svd(),6141
5147,What is the name of the batch of matrices that returns the singular values of each matrix in the batch in descending order?,Ifinputis,3609
5148,The Tensor can only be used to do what?,compute gradients ifcompute_uvisTrue,3609
5149,"Ifcompute_uvisFalse, what does torch.svd() return?",zero-filled tensors,3609
5150,What can TheStensor be used to compute gradients ifcompute_uvisTrue?,Note,3609
5151,What does theStensor can only be used to compute gradients ifcompute_uvisTrue?,Note,11228
5152,"When someisFalse, the gradients onU[..., :, min(m, n):]andV[...",the backward pass,8401
5153,What is the name of the algorithm used in the implementation oftorch.linalg.svd() on CPU?,LAPACK,8401
5154,What program uses routinegesddon earlier versions of CUDA?,MAGMA,8401
5155,What does the implementation oftorch.linalg.svd()on GPU use?,Note,8401
5156,The gradients with respect toUandVwill only be what when the input does not have zero or repeated singular values?,finite,7261
5157,"When the matrix has what, the gradients also depend onS1?",small singular values,7261
5158,What is the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values is close to,Warning,8184
5159,What is the warning that the gradients with respect toUandVwill be numerically unstable when the distance between any two singular values is close to zero,Warning,7100
5160,When do the gradients with respect toUandVbe numerically unstable?,when the matrix has small singular values,7100
5161,What will be numerically unstable if the distance between any two singular values is close to zero?,the gradients with respect toUandV,3489
5162,What may be multiplied byUandV?,arbitrary phase factor,3489
5163,What is a warning when the distance between any two singular values is close to zero?,Warning,8161
5164,What is the warning if the distance between singular values is close to zero?,Warning,8161
5165,What is the name of the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values,Warning,3488
5166,When do gradients depend onS1?,when the matrix has small singular values,3488
5167,For what type of input is the singular value decomposition not unique?,complex-valuedinput,2507
5168,When does the same happen for complex-valuedinput?,wheninputhas repeated singular values,2507
5169,What is used to multiply the columns of the spanning subspace inUandV?,a rotation matrix,2507
5170,What is the function that converts a DLPack to a tensor?,Decodes a DLPack to a tensor,2083
5171,What is the dlpack?,PyCapsule object,2083
5172,How many times can a dlpack be consumed?,once,2083
5173,What is the name of the tensor to be exported?,tensor,2083
5174,How many times can each dlpack be consumed?,once,2083
5175,Decodes a DLPack to what?,a tensor,2083
5176,What is a PyCapsule object with the dltensor?,dlpack,2083
5177,How many times can a DLPack be consumed?,once,2083
5178,What is the tensor represented by?,DLPack,2083
5179,What type of tensor is input(Tensor)?,float 1D tensor,1910
5180,What is input(Tensor) to quantize scales?,float tensor,1910
5181,What has to be the desired data type of returned tensor?,one of the quantized dtypes,1910
5182,What does input(Tensor) use to quantize scales?,float tensor,9647
5183,What is input(Tensor) used to quantize scales?,float tensor,9647
5184,What is an example of a quantized tensor Tensor?,newly quantized tensor Tensor Example:,9647
5185,What is the function that computes the eigenvalues and eigenvectors of a real square matrix?,Computes the eigenvalues and eigenvectors of a real square matrix,1725
5186,"Wheninputis on CUDA,torch.eig()causes what?",host-device synchronization,1728
5187,"What should be replaced with L,V=torch.eig(A,eigenvectors=True)?","L,_=torch.eig(A)",1728
5188,What is the eigenvalue of input?,the first element is the real part and the second element is the imaginary part,1728
5189,Backward pass is supported only if eigenvalues and eigenvectors are all real valued?,eigenvalues and eigenvectors,5923
5190,What should be replaced with something else?,"L,_=torch.eig(A)",5923
5191,What does torch.eig() cause wheninputis on CUDA?,host-device synchronization,8400
5192,"What should be replaced with L,_=torch.eig(A)?","L,V=torch.eig(A,eigenvectors=True)",8399
5193,What is the square matrix of shape?,input(Tensor),9675
5194,Where is each row an eigenvalue of input?,the first element is the real part and the second element is the imaginary part,9675
5195,"What is a namedtuple containing eigenvalues, eigenvectors?",output tensors,9237
5196,What is the first element of an eigenvalue of input?,the first element is the real part and the second element is the imaginary part,9237
5197,Are eigenvalues ordered or ordered?,not necessarily ordered,9235
5198,What is the second element of an eigenvalue of input?,imaginary part,9235
5199,"Out(tuple,optional) - the output what?",tensors,10358
5200,What is the real part of an eigenvalue of input?,the first element is the real part and the second element is the imaginary part,10358
5201,Are eigenvalues ordered?,not necessarily,820
5202,What is a namedtuple containing eigenvalues(Tensor)?,Shape,820
5203,What is another name for Tensor?,Tensor,63
5204,What happens to the dimension(s) of input?,Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination,4276
5205,What happens to other dimensions of input that are not explicitly moved?,remain in their original order,4276
5206,What are the original positions of the dims to move?,source,4276
5207,What must be the original positions of the dims to move?,unique,4276
5208,What is the name of the destination position for each of the original dims?,destination,4276
5209,Destination positions for each of the original dims must also be what?,unique,4276
5210,What must be unique for each of the original dims to move?,Examples,4276
5211,What are scaling factors on matrix-vector product betweenmat1 andmat2?,alphaandbetaare scaling factors,8857
5212,What are the arguments for inputs of typeFloatTensororDoubleTensor?,argumentsbetaandalphamust be real numbers,8857
5213,What is the second matrix to be matrix multiplied?,mat2(Tensor),8857
5214,What do alphaandbetaare scaling factors on?,matrix-vector product,8857
5215,What do argumentsbeta andalphamust be?,argumentsbetaandalphamust be real numbers,3602
5216,What is the first matrix to be matrix multiplied?,mat1(Tensor),3602
5217,Returns what tensor of sizeendstartstepleftlceil fractextend -,1-D tensor,1855
5218,What returns the product of all elements in the inputtensor?,Returns the product of all elements in theinputtensor,5658
5219,Returns the product of each row of the inputtensor in the given dimensiondim.,None,5658
5220,What is returned by the inputtensor?,the product of each row of theinputtensor in the given dimensiondim,5658
5221,"If specified, the input tensor is casted before the operation is performed.",todtype,9662
5222,Why is the input tensor casted todtype before the operation is performed?,data type overflows,9662
5223,What is returned in the given dimensiondim?,the product of each row of theinputtensor,9216
5224,This is useful for preventing what?,data type overflows,9216
5225,What is the function that returns the product of each row of the inputtensor in the given dimensiondim?,Returns the product of each row of theinputtensor in the given dimensiondim,2272
5226,What is returned by the output tensor if keepdimisTrue?,Returns the product of each row of theinputtensor in the given dimensiondim,2272
5227,"If the output tensor is of the same size as input, what does it do?",IfkeepdimisTrue,2272
5228,What natural function is computed of the absolute value of the gamma function oninput?,logarithm,2272
5229,What is an example of a Computesinput*log1p?,Computesinput*log1p(other)with the following cases,2272
5230,What does input(NumberorTensor) represent?,Multiplier,2272
5231,What indicates that the output tensor is of the same size asinput?,IfkeepdimisTrue,3622
5232,What makes the output tensor of the same size as input?,IfkeepdimisTrue,3622
5233,"IfkeepdimisTrue, the output tensor has how much less dimension than input?",1,3622
5234,What does the output tensor have if keepdimisTrue?,1 fewer dimension thaninput,5659
5235,When is the output tensor of the same size asinput?,IfkeepdimisTrue,5659
5236,What returns the product of each row of theinputtensor in the given dimensiondim?,Returns the product of each row of theinputtensor in the given dimensiondim,5659
5237,What returns the output tensor of the same size asinput except in the dimensiondim where it is of size 1?,IfkeepdimisTrue,5659
5238,"If keepdimisTrue, the output tensor has how many dimension fewer than input?",1,5659
5239,What returns whether the output tensor hasdimretained or not?,keepdim(bool),9656
5240,See its documentation for the exact what of this method?,semantics,9656
5241,What is the second value returned bytorch.max()?,Returns the indices of the maximum values of a tensor across a dimension,9656
5242,What returns the argmax of the flattened input?,IfNone,9656
5243,Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned?,bytorch.max(),9656
5244,What does Alias fortorch.special.exp2() do?,Alias fortorch.special.exp2(),1058
5245,What type of element does the function return?,tensor,5182
5246,What is the operation defined as?,"tensorscondition,x,ymust bebroadcastable",5182
5247,What is the current valid scalar and tensor combination?,1,5182
5248,What is the Scalar of presently valid scalar and tensor combination?,integral dtype and torch,5182
5249,What is the scalar of dtype and torch?,x,5182
5250,"Return what of elements selected from eitherxory, depending oncondition?",a tensor,5182
5251,What are two examples of scalar and tensor combination?,floating dtype and torch,5182
5252,What condition returns a tensor of elements selected from eitherxory or bothxory?,BoolTensor,5182
5253,What are two examples of valid scalar and tensor combination?,floating dtype and torch,5182
5254,What is the scale of dtype and torch.complex128 condition(BoolTensor) – When True (,x,5182
5255,"What function returns a tensor of shape equal to the broadcasted shape of condition,x,y?",alsotorch.nonzero(),5182
5256,What is a valid scalar and tensor combination?,floating dtype and torch,2050
5257,What is the Scalar of floating dtype and torch?,integral dtype and torch,2050
5258,What condition(BoolTensor) is nonzero?,True,2050
5259,What type of dtype and torch are presently valid scalar and tensor combination?,complex,2050
5260,Where are values selected?,indices,7322
5261,What are two examples of a valid scalar and tensor combination?,integral dtype and torch,7322
5262,What are bebroadcastable?,"tensorscondition,x,ymust",7322
5263,What must bebroadcastable?,"tensorscondition,x,y",7322
5264,What are two examples of current valid scalar and tensor combination?,Currently valid scalar and tensor combination,4367
5265,What condition(BoolTensor) yields x?,True,4367
5266,What type of dtype and torch?,complex,4367
5267,What ield x if condition(BoolTensor) is True?,y,9114
5268,"What does torch.where(condition,as_tuple=True) refer to?",alsotorch.nonzero,9114
5269,What does condition(BoolTensor) ield x?,y,9114
5270,"What is another name for torch.where(condition)is identical to totorch.nonzero(condition,as_tup",alsotorch.nonzero(),9114
5271,What is the equivalent of input.torch.empty_like(input)?,totorch.empty,5498
5272,What is the desired layout of the returned tensor?,layout,5498
5273,What type of tensor is returned with the same size as input?,uninitialized tensor,5498
5274,"dtype(torch.dtype, optional) – what type of returned Tensor?",the desired data type,5498
5275,What must inputandmat2 be?,3-D tensors,9680
5276,What function is used for broadcasting matrix products?,seetorch.matmul(),9680
5277,What is seetorch.matmul() used for?,broadcasting matrix products,4787
5278,What does seetorch.matmul() do?,notbroadcast,4787
5279,What is used for broadcasting matrix products?,seetorch.matmul(),7781
5280,What is the flag to choose between?,"a faster non-deterministic calculation, or a slower deterministic calculation",7781
5281,This argument is only available for what?,sparse-dense CUDA bmm,7781
5282,"What is false out(Tensor,optional)?",the output tensor,8214
5283,What is returned from a normal distribution with mean0and variance1?,a tensor filled with random numbers,5420
5284,Can be a collection like a list or tuple?,variable number of arguments,10697
5285,ifNone uses a global default what?,Default,10697
5286,What is the input tensor containing the rates of the Poisson distribution generator?,a pseudorandom number generator,5455
5287,What is returned for each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininput?,a tensor of the same size,5455
5288,What is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped?,Stack tensors,6067
5289,How are all 1-D tensors reshaped?,bytorch.atleast_2d(),6067
5290,"What is an example of a sequence of tensors to concatenate out(Tensor,optional) –",Example,6067
5291,"When this flag is False, some PyTorch warnings may only appear how many times per process?",once per process,8382
5292,Why do PyTorch warnings only appear once per process?,excessive warning information,8382
5293,What causes PyTorch warnings to always appear?,Setting it to True,8382
5294,What flag is used to force warnings to always be emitted?,b(bool),8382
5295,What is the default value for the highest integer to be drawn from the distribution?,0. high(int),5476
5296,"Low(int,optional) – what integer to be drawn from the distribution?",Lowest integer,5476
5297,What is high(int)?,One above the highest integer,9515
5298,What is returned with the same data and number of elements as input?,a tensor,5471
5299,What will the returned tensor be if it is not a view of input?,a copy,5471
5300,What does Seetorch.Tensor.view() return when it is possible to return?,a view,5471
5301,What is the tensor to be reshaped shape(tuple of python,input(Tensor),5471
5302,What is the name of the function that determines when it is possible to return a view?,Seetorch.Tensor.view(),5812
5303,What is the tensor to be reshaped shape?,input(Tensor),5812
5304,When does Seetorch.Tensor.view() occur?,when it is possible to return a view,5812
5305,"If the left boundary is closed, what is the default?",Ifrightis False,5543
5306,Returns what with the same size as input?,a new tensor,5543
5307,What is returned when the boundaries of the buckets are set byboundaries?,the indices of the buckets to which each value in theinputbelongs,5543
5308,What does the right returned index satisfie?,False boundaries,10611
5309,What satisfies False boundaries?,right returned index,10611
5310,What must the boundaries(Tensor) contain?,monotonically increasing sequence,9676
5311,What is input(TensororScalar)?,N-D tensor or a Scalar,9676
5312,What is the input(TensororScalar) containing the search value(s)?,N-D tensor or a Scalar,9676
5313,What tensor must contain a monotonically increasing sequence?,1-D,9676
5314,"What does out_int32(bool,optional) indicate?",output data type,9676
5315,Return index satisfies what?,False boundaries,10605
5316,What is input(TensororScalar) containing the search value(s)?,N-D tensor or a Scalar,8982
5317,What type of boundaries[i-1]input[m][n]?,False,2402
5318,What must a 1-D tensor contain?,monotonically increasing sequence,8983
5319,What type of tensor must contain a monotonically increasing sequence?,1-D,8981
5320,What type of sequence must a 1-D tensor contain?,monotonically increasing,8981
5321,What is the size ofboundaries?,one pass the last index,10612
5322,"If False, gets what for each value ininputfromboundaries?",lower bound index,10612
5323,"If False, gets what instead of the lower bound index for each value ininputfromboundaries?",upper bound index,10612
5324,PyTorch provides several features for working with what language?,C++,4973
5325,At what level is the following support available?,high level,4973
5326,At what level of support is PyTorch available?,high level,4973
5327,What allows PyTorch models defined in Python to be serialized and then loaded and run in C++?,TorchScript,4973
5328,In what language can you define your PyTorch models?,Python,4973
5329,What API does PyTorch use to do preprocessing?,C++ Tensor API,4973
5330,What language does TorchScript allow PyTorch models to be serialized and then loaded and run in?,C++,4973
5331,What are you looking for?,PyTorch C++ API docs,3552
5332,What does PyTorch provide for working with C++?,several features,3552
5333,What features does PyTorch provide for working with C++?,several features,3552
5334,What language can you define your models in?,Python,3552
5335,What API is used to construct the input and do preprocessing?,C++ Tensor API,3552
5336,What language does TorchScript allow PyTorch models defined in Python to be serialized and then loaded and run in?,C++,3552
5337,What can you do with models exported via TorchScript?,no-Python execution,3552
5338,What is one of the ways you can interact with TorchScript?,Constructing the input and doing preprocessing using C++ Tensor API,3552
5339,What company provides several features for working with C++?,PyTorch,4376
5340,Where are TorchScript models saved from?,Python,4129
5341,What is used for preprocessing TorchScript input?,C++ Tensor API,4129
5342,Where are TorchScript models saved?,Python,4129
5343,What is one way to do simple model modifications if needed?,Loading serialized TorchScript models saved from Python,4129
5344,What is used for preprocessing TorchScript models?,C++ Tensor API,4129
5345,Doing what if needed?,simple model modifications,2192
5346,What is used for preprocessing input?,C++ Tensor API,2192
5347,What is the C++ Tensor API used for?,preprocessing,1835
5348,What is used for preprocessing?,C++ Tensor API,1835
5349,What is the name of the API used for input and preprocessing?,C++ Tensor API,1835
5350,Doing what if needed (e.g. pulling out submodules)?,simple model modifications,2193
5351,Where can you find the full list of tensor methods?,https,2193
5352,What looks and behaves the same as the Python API?,C++ tensor indexing API,4271
5353,Where can you find details on the use of the C++ tensor indexing API?,pytorch.org/cppdocs/notes/tensor_indexing.html,4271
5354,What is the name of the tensor and autograd operations in PyTorch?,torch,4271
5355,C++ tensor indexing API looks and behaves the same as what API?,Python,4271
5356,The tensor autograd APIs and thetorch,dynamic neural networks,4271
5357,Most of the tensor and autograd operations in PyTorch Python API are also available in what language?,C++,4271
5358,Where can you find more details on the tensor autograd APIs and thetorch,https,4271
5359,What does the C++ tensor indexing API look and behave like?,Python API,4271
5360,Most of the tensor and autograd operations in PyTorch Python API are also available in what API?,C++,1836
5361,C++ classes and structs can be bound into TorchScript through what type of interface?,pybind11,1836
5362,What is an example of a tensor and autograd operation in PyTorch?,torch,1836
5363,What type of interface allows C++ classes and structs to be bound into TorchScript?,pybind11,1836
5364,Where are most of the tensor and autograd operations available in the C++ API?,PyTorch Python API,1836
5365,What is one of the tensor methods available in the C++ API?,torch,1836
5366,Where can you find the full list of methods available?,https,1836
5367,What is the name of the C++ tensor indexing API?,https,10936
5368,What are some examples of tensormethods?,add/reshape/clone,10936
5369,What language has a tensor indexing API?,C++,10936
5370,What are some examples of Torch,add/reshape/clone,10936
5371,What script does authoring a neural net model need to be done in?,TorchScript,10936
5372,What API does the astorch,Python,10936
5373,What is the name of the Torch method?,torch,10936
5374,What is undesirable in a workflow where the model has to be authored in C++?,a Python component,10936
5375,What API looks and behaves the same as the Python API?,C++ tensor indexing API,1497
5376,Where can you find more details on the tensor autograd APIs?,https,1497
5377,C++ tensor indexing API looks and behaves the same as what?,Python API,1497
5378,In what language does TorchScript author and train a neural net model?,C++,1497
5379,What is an example of a case where a model has to be authored in C++?,in workflows where a Python component is undesirable,1497
5380,Where can you find more information about the tensor autograd APIs?,https,7321
5381,What APIs are crucial for building dynamic neural networks in C++ frontend?,tensor autograd,7321
5382,Where can you find more information about the tensor autograd APIs and thetorch,https,7321
5383,In what language can a model be authored?,C++,7321
5384,What is an example of a Python component that needs to be authored in C++?,undesirable,7321
5385,"What does the ""author in"" workflow require model authoring to be done in?",TorchScript,7321
5386,What is an overview of?,PyTorch C++ model authoring and training API,7321
5387,What might be undesirable in workflows where the model has to be authored in C++?,Python component,7366
5388,In what script is authoring a neural net model done?,TorchScript,7366
5389,In what language can a neural net model be authored?,C++,7366
5390,What API does astorch,Python,7366
5391,What is the name of the C++ model authoring and training API?,PyTorch,7366
5392,What script does the author infer in C++ use?,TorchScript,7366
5393,"What language does the author in TorchScript, infer in C++ workflow require the model to be authored in?",C++,7366
5394,What is the name of the API that you can find at http,PyTorch C++ model authoring and training API,7366
5395,What is the model authoring and training API?,PyTorch C++,2500
5396,On what system are two types of libtorch binaries provided?,Linux,2500
5397,What can be found at http,a detailed tutorial,2489
5398,What is available at http,a detailed tutorial,2489
5399,What does pytorch.org provide for components such as,Docs,2181
5400,Where can you find docs for components such as,api/library_root.html,2181
5401,What library contains all of the above C++ APIs?,libtorch,2542
5402,How many types of libtorch binaries are provided on Linux?,two,2542
5403,Supportsbroadcasting to what shape?,common shape,1941
5404,What are the magnitudes of a floating-point tensor?,input(Tensor),1941
5405,What contains value(s) whose signbit(s) are applied to the magnitudes ininput?,other(TensororNumber),1941
5406,What does value represent in the given dimensiondim?,thekth smallest element of each row of theinputtensor,5326
5407,Andindicesis what of each element found?,index location,5326
5408,What returns the smallest element of each row of the inputtensor in the given dimensiondim?,a namedtuple,5326
5409,What is the index location of each element found?,Andindices,5326
5410,What happens ifdimis not given?,the last dimension of theinputis chosen,5326
5411,"If inputis a CUDA tensor and there are what, this function may nondeterministically returnindices for any of them",multiple validkth values,5326
5412,"Out(tuple,optional) – the output tuple of (Tensor, LongTensor) can",output buffers,5326
5413,"If thevaluesandindicestensors are the same size as input, what is the default?",IfkeepdimisTrue,5326
5414,What does values represent in the given dimensiondim?,thekth smallest element of each row of theinputtensor,5326
5415,What does indexes return of each element found?,index location,5326
5416,When are both thevaluesandindicestensors the same size as input?,IfkeepdimisTrue,5326
5417,"Wheninputis a what, this function may nondeterministically returnindices for any of them?",CUDA tensor,5326
5418,"Ifdimis not given, the last dimension of theinputis chosen?",IfkeepdimisTrue,3611
5419,What type of tensor is input?,CUDA,3611
5420,What is another name for dimis squeezed?,seetorch.squeeze(),3621
5421,"If both thevaluesandindicestensors are the same size as input, what is the default?",IfkeepdimisTrue,3621
5422,What is the smallest element in a CUDA tensor?,k,3621
5423,"Inputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically do what for",returnindices,4452
5424,What is the return matrixUis upper-triangular?,IfupperisTrue,3648
5425,"IfupperisTrue, the returned matrixLis is what?",lower-triangular,3648
5426,What is the return matrixLis lower-triangular?,IfupperisFalse,3648
5427,"What should be replaced with U=torch.cholesky(A,upper=True)?",input(Tensor),3648
5428,What is the name of the matrixUis upper-triangular?,IfupperisTrue,3648
5429,"WhenupperisFalse, the returned tensor will be composed of what of each of the individual matrices?",lower-triangular Cholesky factors,3648
5430,Which flag indicates whether to return a upper or lower triangular matrix?,Default,3648
5431,What returns the matrixUis upper-triangular and the decomposition has the form IfupperisFalse?,IfupperisTrue,3648
5432,What is the default value for the return matrix?,Default,3648
5433,What is the name of the warning that the matrix is lower-triangular?,Warning,3646
5434,"IfupperisTrue, andAAAis a batch of symmetric positive-definite matrices?",Warning,3646
5435,"What is the input tensorAAAof size(,n,n)(*, n, n)(,",input(Tensor),3646
5436,What is the default value for the output matrix?,"False out(Tensor,optional)",3646
5437,"IfupperisTrue, the returned tensor will be composed of what of each of the individual matrices?",upper-triangular Cholesky factors,3646
5438,"IfupperisTrue, the returned tensor will be composed of what?",lower-triangular Cholesky factors,3646
5439,"What is the input tensorAAAof size(,n,n)?",input(Tensor),3646
5440,"IfupperisTrue, andAAAis a batch of symmetric positive-definite matrices, then the returned tensor",upper-triangular Cholesky factors,3647
5441,What is a batch of symmetric positive-definite matrices?,IfupperisTrue,3647
5442,When will the returned tensor be composed of lower-triangular Cholesky factors of each of the individual matrices?,whenupperisFalse,3647
5443,What should be replaced with a replacement for torch.linalg.cholesky()?,L=torch.cholesky(A),3647
5444,"WhenupperisFalse, the returned tensor will be composed of what?",lower-triangular Cholesky factors,3647
5445,What should be replaced with torch.linalg.cholesky()?,L=torch.cholesky(A),3647
5446,What flag indicates whether to return a lower or upper triangular matrix?,upper,4053
5447,What is the default value of the flag that indicates whether to return a lower or upper triangular matrix?,False,11023
5448,What does the histogram of a tensor do?,Computes the histogram of a tensor,1766
5449,"Ifminandmaxare both zero, what are the minimum and maximum values of the data used?",Ifminandmaxare both zero,1766
5450,What is the histogram represented as?,a tensor Tensor,1766
5451,Where are these features sometimes found?,run-time flags,4911
5452,What does Thetorchtextpackage contain?,Package Reference PyTorch Libraries,4911
5453,What is the name of the search page?,Index Module Index Search Page,3842
5454,What is the tensor to compare other(Tensororfloat)?,input(Tensor),1663
5455,What is a boolean tensor that is?,True,1663
5456,Returns what with a dimension of size one inserted at the specified position?,a new tensor,5357
5457,What does the returned tensor share with this tensor?,underlying data,5357
5458,"What value within the range [-input.dim()-1,input.dim()+1]can be used?",Adimvalue,5357
5459,What will correspond tounsqueeze()applied atdim=dim+input.dim()+1?,Negativedim,5357
5460,What is the numerical rank of?,2-D tensor,5648
5461,The method to compute the matrix rank is done using what by default?,SVD,5648
5462,The computation of the rank is done by obtaining what?,eigenvalues,5648
5463,What is the threshold below which the singular values (or the eigenvalues whensymmetricisTrue) are considered to be 0.,tolis,5648
5464,What is deprecated in favor oftorch.linalg.matrix_rank()?,torch.matrix_rank(),5648
5465,What was the parametersymmetric renamed to?,intorch.linalg.matrix_rank()tohermitian,5648
5466,"What is the input 2-D tensor tol(float,optional) – the tolerance value?",input(Tensor),5648
5467,What is not specified?,Iftolis,5648
5468,"If what is not specified, tolis the threshold below which the singular values (or the eigenvalues whensymmetricisTrue) are",Iftolis,10934
5469,What does the documentation of this method provide?,exact semantics,3526
5470,What are returned if there are multiple maximal values?,the indices of the first maximal value,3526
5471,What method returns the indices of the maximum values of a tensor across a dimension?,bytorch.max(),3526
5472,What is the name of the method that returns the indices of the maximum values of a tensor across a dimension?,bytorch.max(),3526
5473,"If there are multiple maximal values, the indices of the first maximal value are returned.",multiple maximal values,7696
5474,What does the documentation of bytorch.max() provide?,exact semantics,7696
5475,What do you need to know about the second value returned bytorch.max()?,semantics,7696
5476,Returns the indices of the maximum value of all elements in the inputtensor. This is the second value returned?,bytorch.max(),5555
5477,What is returned if there are multiple maximal values?,the indices of the first maximal value,4372
5478,What method returns the second value of the indices of the maximum values of a tensor across a dimension?,bytorch.max(),4372
5479,Fills the tensor with numbers drawn from what distribution?,Cauchy,2428
5480,What does angle return pi for?,negative real numbers,1741
5481,The function would return zero for all real numbers and not propagate what?,floating-point NaNs,1741
5482,What is an example of a function that returns zero for all real numbers and not propagate floating-point NaNs?,Example,1741
5483,Writes all values from the tensorsrcintoselfat the indices specified in what?,theindextensor,8448
5484,What is the output index for each value insrc specified by?,its index insrcfordimension!=dim,8448
5485,Selfis updated as,ingather(),8448
5486,What is the reverse operation of the manner described ingather()?,Writes all values from the tensorsrcintoselfat the indices specified in theindextensor,8448
5487,What is the reverse operation of the manner described?,ingather(),8448
5488,What must the values of index be as forgather()?,between0andself.size(dim)-1inclusive,4264
5489,What must be the values of index as forgather()?,between0andself.size(dim)-1inclusive,4264
5490,What operation is used to reduce a 3-D tensor?,multiplication,4264
5491,What allows specification of an optional reduction operation?,optionalreduceargument,4264
5492,What is the index inself specified by?,index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim,4264
5493,What is the tensor?,3-D,4264
5494,"When indices are not unique, the behavior is what?",non-deterministic,8364
5495,"For each value insrc, the reduction operation is applied to what?",index inself,8364
5496,What operation is applied to all values in the tensorsrcintoselfat the indicies specified in the index?,optional reduction operation,8364
5497,Reducing with the addition operation is the same as what?,usingscatter_add_(),8364
5498,What does the optionalreduceargument allow specification of?,optional reduction operation,6969
5499,What is the multiplication operation used to update the index inself?,3-D tensor and reduction,958
5500,What is the default value of start(float)?,Default,5260
5501,How many tensors does the function return?,1,5260
5502,What is step?,the gap between two values in the tensor,5260
5503,What is the behavior of this function inconsistent with?,Python’s range builtin,8195
5504,What language has a range builtin?,Python,8195
5505,What is the default for the layout of returned Tensor?,Default,8195
5506,What is step(float)?,the gap between each pair of adjacent points,10753
5507,Usetorch.arange() is inconsistent with what programming language's range builtin?,Python,7540
5508,Why is usetorch.arange() deprecated?,its behavior is inconsistent with Python’s range builtin,7540
5509,What is the behavior of usetorch.arange() inconsistent with?,Python’s range builtin,7540
5510,What is the default value of the return tensor?,ifNone,7540
5511,What does the function infer the data type from the other input arguments?,Ifdtypeis not given,7540
5512,What language's range builtin is inconsistent with this function?,Python,7540
5513,What is the ending value for the set of points step(float)?,end(float),9268
5514,What is the default value for the end of a set of points?,Default,9268
5515,What is equivalent to callinginput.expand(shape)?,Broadcastsinput,1467
5516,What is the equivalent to Broadcastsinput to the shapeshape?,callinginput.expand(shape),1467
5517,What is another name for details?,Seeexpand(),1467
5518,What is the new shape?,shape,1467
5519,"What does shape(list, tuple, ortorch.Size) do?",Example,1467
5520,Returns True what if the data type ofinputis a complex data type?,if the data type ofinputis a complex data type,5233
5521,What is an example of a tensor with all the dimensions ofinputof size1removed?,shape,5467
5522,What is done only in the given dimension whendimis given?,squeeze operation,5467
5523,What returns a tensor with all the dimensions ofinputof size1removed?,Returns a tensor with all the dimensions ofinputof size1removed,5467
5524,"What does squeeze(input,0) leave unchanged?",tensor,2519
5525,What is theouttensor of shape?,ifinputis of shape,2519
5526,"Whendimis given, what is done only in the given dimension?",squeeze operation,2519
5527,What does a squeeze operation do when a tensor is of shape?,Note,2519
5528,What is an example of a squeeze operation?,ifinputis of shape,2519
5529,What is a squeeze operation done only in the given dimension?,Note,2519
5530,"Whendimis given, what operation is done only in the given dimension?",squeeze operation,8398
5531,Whendimis is a squeeze operation done only in the given dimension?,given,8398
5532,What does a squeeze operation only in the given dimension do?,Ifinputis of shape,8398
5533,The returned tensor shares storage with what?,the input tensor,8398
5534,What is the name of the warning that the input tensor shares the storage with the output tensor?,Warning,8398
5535,What is the input of a tensor?,shape,8398
5536,What changes the contents of the other?,changing the contents of one,8398
5537,What is the name of the warning that is given when a tensor shares the storage with the input tensor?,Warning,8398
5538,The returned tensor shares the storage with what?,the input tensor,4412
5539,What does the tensor have that can lead to unexpected errors?,a batch dimension of size 1,4412
5540,What will fork_rng() emit if your machine has a lot of devices?,a warning,2592
5541,What is iterable of CUDA IDs?,devices,2592
5542,What returns the initial seed for generating random numbers as a Pythonlong?,atorch.Generatorobject,2592
5543,What function operates on all devices?,fork_rng(),2592
5544,Returns the initial seed for generating random numbers as a Pythonlong. Returns atorch.Generatorobject.,Sets the seed for generating random numbers,2592
5545,"If you explicitly specify devices, this warning will be suppressed what?",enabled(bool),2592
5546,What is the reason for disabling the context manager without having to delete it and unindent your Python code under it?,convenience argument,2592
5547,What does fork_rng() do?,Sets the seed for generating random numbers,2592
5548,What does fork_rng() return?,atorch.Generatorobject,2592
5549,"If you explicitly specify devices, this warning will be what?",suppressed,9173
5550,What does devices(iterable of CUDA IDs) represent?,CUDA devices,9173
5551,What function will run very slowly if your machine has a lot of devices?,fork_rng(),9173
5552,What are CUDA devices for which to fork the RNG?,devices,9173
5553,"If you explicitly specify devices, this warning will be suppressed?",enabled(bool),9173
5554,What is a convenience argument for?,disabling the context manager,9173
5555,Returns the initial seed for generating random numbers as a Pythonlong. Returns atorch.Generatorobject. what?,Sets the seed for generating random numbers,9173
5556,What are remapped to positive values with the formula0xfff_fff_fff_ffff + seed,Negative inputs,9173
5557,"If enabled(bool) is true, the RNG is not forked.",ifFalse,9262
5558,What is enabled(bool) used for?,convenience argument,9262
5559,What is the state of the random number generator?,atorch.ByteTensor,9262
5560,What is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it?,enabled,9262
5561,What is enabled(bool) a convenience argument for?,disabling the context manager,9262
5562,What is used to seed the RNG?,a 64 bit number,9262
5563,What does new_state(torch.ByteTensor) do?,Sets the random number generator state,9262
5564,The value must be within what range?,inclusive range,5665
5565,What is the multiplication of a product of Householder matrices with a general matrix?,matrix-matrix,1785
5566,What does the multiplication of amnm times nmnmatrixC(given byother) with?,a matrixQ,1785
5567,What is represented using Householder reflectors?,Orthogonal or Unitary Matrices,1785
5568,What does the matrix-matrix multiplication of a product of Householder matrices with a general matrix do?,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix,1785
5569,What is used to represent Q?,Householder reflectors,1785
5570,"Supports inputs of float, double, cfloat and what other dtype?",cdouble,6131
5571,What type of inputs does it support?,batched inputs,6131
5572,What is tau(Tensor)?,tensor of shape,6131
5573,"What can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition?",torch.geqrf(),11067
5574,What is the tensor of shape where*is zero or more batch dimensions andmnequals tomorndepending on theleft,input(Tensor),11067
5575,"What is the tensor of shape(*, m, n) where*is zero or more batch dimensions?",other(Tensor),11067
5576,What can be used to form the Householder representation of matrixQfrom the QR decomposition?,torch.geqrf(),11067
5577,What can be used to form the Householder representation of matrixQ?,the QR decomposition,5767
5578,What controls the order of multiplication?,left(bool),9649
5579,What is a tensor of shape where*is zero or more batch dimensions?,tau,9649
5580,What controls whether the matrixQis conjugate transposed or not?,transpose,9649
5581,What is the output Tensor?,Tensor,9649
5582,What is the output Tensor ignored?,ifNone,9649
5583,What is the default value of the output Tensor?,Default,9649
5584,What is the name of Alias fortorch.linalg.pinv()?,Alias fortorch.linalg.pinv(),1054
5585,What is not guaranteed to produce 100% reproducible results across?,"PyTorch releases, individual commits, or different platforms",1628
5586,Results may not be reproducible between CPU and GPU executions even when using what?,identical seeds,1628
5587,"Completely reproducible results are not guaranteed across what releases, individual commits, or different platforms?",PyTorch releases,1628
5588,What can happen between CPU and GPU executions?,results may not be reproducible,1628
5589,What happens when using identical seeds?,results may not be reproducible between CPU and GPU executions,1628
5590,What operations are often slower than nondeterministic operations?,Warning Deterministic operations,1628
5591,Results may not be reproducible between what two executions?,CPU and GPU executions,1628
5592,"What can you do to ensure reproducible results across platforms, devices, and PyTorch releases?",limit the number of sources of nondeterministic behavior,1628
5593,"What is one of the steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyT",Warning,2973
5594,"What can you do to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release?",limit the number of sources of nondeterministic behavior,2973
5595,What will result in the same result?,multiple calls to those operations,2973
5596,What is another word for nondeterministic behavior?,Warning,2973
5597,What does python use to seed the global NumPy RNG?,NumPy,2973
5598,How can you avoid using nondeterministic algorithms for some operations?,"multiple calls to those operations, given the same inputs, will produce the same result",2973
5599,Are Deterministic operations faster or slower than nondeterministic operations?,slower,2973
5600,What are often slower than nondeterministic operations?,Warning Deterministic operations,2973
5601,What can you configure to avoid using nondeterministic algorithms for some operations?,PyTorch,2973
5602,Are deterministic operations faster or slower than nondeterministic operations?,slower,8145
5603,Deterministic operations are often what?,slower,8145
5604,Which operations are often slower than nondeterministic operations?,Deterministic operations,2122
5605,"What may some applications and libraries use, but not the global RNG?",NumPy Random Generator objects,8607
5606,For what devices can you usetorch.manual_seed() to seed the RNG?,all devices,8607
5607,"If you are using other libraries that use random number generators, refer to the documentation for those libraries to see how to do what for them?",set consistent seeds,8607
5608,"What may some applications and libraries use, but not the global NumPy RNG?",NumPy Random Generator objects,2509
5609,"For custom operators, you might need to set what kind of seed as well?",python,2509
5610,What do some applications and libraries need to be seeded consistently?,NumPy Random Generator objects,2509
5611,Where can you find information about other libraries that use random number generators?,the documentation,2509
5612,Where can you find information about libraries that use random number generators?,the documentation,2968
5613,What causes a benchmark to select different algorithms on subsequent runs?,benchmarking noise and different hardware,2968
5614,What can you do if you are using other libraries that use random number generators?,set consistent seeds,3557
5615,What do other libraries use?,random number generators,3557
5616,What is the name of the benchmarking feature that cuDNN disables?,withtorch.backends.cudnn.benchmark=Falsecauses,3557
5617,What happens when a benchmark is used on the same machine?,benchmark may select different algorithms on subsequent runs,3557
5618,What is the cost of disabling the benchmarking feature?,reduced performance,2155
5619,"If you do not need what feature of your application, performance might improve if the benchmarking feature is enabled withtorch.backends.",reproducibility across multiple executions,2155
5620,What setting does the benchmarking feature differ from?,thetorch.backends.cudnn.deterministicsetting,2155
5621,What setting is different from the benchmarking feature discussed below?,thetorch.backends.cudnn.deterministicsetting,2155
5622,What setting is different from thetorch.backends.cudnn.deterministicsetting discussed below?,withtorch.backends.cudnn.benchmark=True,2967
5623,What is the benchmark setting different from?,thetorch.backends.cudnn.deterministicsetting,2967
5624,"If you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled?",withtorch.backends.cudnn.benchmark=True,2967
5625,What setting is different from withtorch.backends.cudnn.benchmark=True?,thetorch.backends.cudnn.deterministicsetting,2967
5626,What lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones?,torch.use_deterministic_algorithms(),2967
5627,"If you do not need reproducibility across multiple executions of your application, performance might improve if what feature is enabled?",benchmarking,2967
5628,What type of implementation does PyTorch not have?,determinism,2967
5629,What might happen if the benchmarking feature is enabled withtorch.backends.cudnn.benchmark=True,performance might improve,2967
5630,What is the difference between the benchmarking feature and the other setting discussed below?,different from thetorch.backends.cudnn.deterministicsetting,2967
5631,What does torch.use_deterministic_algorithms allow PyTorch to use instead of nondeterministic ones?,deterministic algorithms,2967
5632,"If an operation does not act correctly according to the documentation, or if you need what, please submit an issue",a deterministic implementation of an operation that does not have one,2967
5633,What setting is different from the one discussed below?,thetorch.backends.cudnn.deterministicsetting,4496
5634,What does torch.use_deterministic_algorithms() let you configure PyTorch to use instead of nondeterministic ones?,deterministic algorithms,4496
5635,What implementation of oftorch.Tensor.index_add_() will throw an error?,CUDA implementation,4496
5636,"If an operation does not act correctly according to the documentation, or if you need what, please submit an issue?",a deterministic implementation of an operation that does not have one,4496
5637,What type of algorithm does torch.use_algorithms?,deterministic,11238
5638,What type of algorithms does torch.use?,deterministic,11238
5639,What documentation does PyTorch use for a full list of affected operations?,fortorch.use_deterministic_algorithms(),11238
5640,What type of algorithms does torch.use_algorithms() let you configure PyTorch to use instead of nondeterministic ones?,deterministic,11238
5641,"If an operation does not act correctly according to the documentation, or if you need a what implementation of an operation that does not have one, please",deterministic,11238
5642,When will the alternative deterministic implementation of bmm() be used?,when the deterministic flag is turned on,11238
5643,What do you need if an operation does not act correctly according to the documentation?,a deterministic implementation of an operation that does not have one,4848
5644,What documentation contains a full list of affected operations?,fortorch.use_deterministic_algorithms(),4848
5645,What implementation oftorch.Tensor.index_add_() is nondeterministic?,CUDA,4848
5646,What is the current version of CUDA?,10.2 or greater,4848
5647,When is the alternate deterministic implementation oftorch.bmm() used?,when the deterministic flag is turned on,2524
5648,"Whentorch.bmm() is called with sparse-dense CUDA tensors, what does it typically use",a nondeterministic algorithm,8402
5649,What is called with sparse-dense CUDA tensors?,Whentorch.bmm(),8402
5650,Whentorch.bmm() is called with what type of CUDA tensors?,sparse-dense,8402
5651,How will other PyTorch operations behave?,deterministically,8402
5652,"Whentorch.bmm() is called with sparse-dense CUDA tensors, it typically uses a",deterministic,8402
5653,If you are using CUDA tensors and your CUDA version is what?,10.2 or greater,2662
5654,When should you set the environment variableCUBLAS_WORKSPACE_CONFIG?,if you are using CUDA tensors,2662
5655,What is the CUDA version of CUDA?,10.2 or greater,2662
5656,What does disabling ensures that CUDA selects the same algorithm each time an application is run?,CUDA convolution benchmarking,8405
5657,What does disabling CUDA convolution benchmarking ensure?,CUDA selects the same algorithm each time an application is run,8405
5658,"What operations will behave deterministically, unless eithertorch.use_deterministic_algorithms() is set?",other PyTorch operations,8405
5659,What is the equivalent to calling.tensor_split?,torch,6047
5660,What is used to split a tensor with three or more dimensions into multiple tensors depthwise?,indices_or_sections,6047
5661,What is the difference between a split and a view ofinput?,ifindices_or_sectionsis an integer it must evenly divide the split dimension,6047
5662,What is the argument for indices_or_sections?,intorch.tensor_split(),6047
5663,What casting rules are described in the type promotiondocumentation?,PyTorch,2119
5664,From(dpython,originaltorch.dtype,2119
5665,To what does dpython,targettorch.dtype,2119
5666,What is an example of a PyTorch casting rule?,Example,2119
5667,What input types does complexA support?,"float, double, cfloat and cdouble dtypes",2508
5668,What type of matrices does complexA support?,batches of matrices,2508
5669,What is the name of the function that computes the sign and natural logarithm of the absolute value of the determinant of a square,Note,1802
5670,What does complexA return?,the angle and the natural logarithm of the modulus of the determinant,1802
5671,What does complexA support?,batches of matrices,1802
5672,What is the name of the feature that supports batches of matrices?,Note,1802
5673,What does torch.linalg.det() compute?,the sign and natural logarithm of the absolute value of the determinant of a square matrix,1802
5674,What types of inputs does this function support?,"float, double, cfloat and cdouble dtypes",1802
5675,What is ignored?,ifNone,1802
5676,What is the default value of the function that computes the sign and logarithm of the absolute value of the determinant of a square,Default,1802
5677,What is the name of the function that computes the sign and logarithm of the absolute value of the determinant of a square matrix,tuple,1802
5678,What does logabsdet always be?,real-valued,1802
5679,What types of inputs does torch.linalg.det() support?,"float, double, cfloat and cdouble dtypes",6122
5680,What does torch.lu() do when inputs are on a CUDA device?,synchronizes that device with the CPU,6122
5681,What is ignored by torch.linalg.det?,ifNone,4419
5682,What is the default value of torch.lu()?,Default,7539
5683,What is ignored by torch.lu()?,ifNone,7539
5684,"What is the tensor of shape(*, n, n)where*is zero or more batch dimensions?",A(Tensor),4398
5685,"Out(tuple,optional) – what?",output tuple of two tensors,4398
5686,Ignored what if a tuple of two tensors is output tuple of two tensor,ifNone,4398
5687,What is the default value for a tuple of two tensors?,Default,4398
5688,"What will always be real-valued, even whenAis complex?",logabsdet,4398
5689,What will have the same dtype asA?,sign,4398
5690,What is an example of a real-valued tuple?,Examples,4398
5691,What is the tensor of sizeendstartstepleftlceil fractextend -,1-D,1951
5692,What type of tensor does anumpy.ndarray return?,a tensor filled with the scalar value0,1951
5693,What does the view of an existingtorch.Tensorinput have?,"specifiedsize,strideandstorage_offset",1951
5694,What is the name of the tensor created from?,anumpy.ndarray,1951
5695,What returns a tensor filled with the scalar value1?,"a tensor filled with the scalar value1, with the same size asinput",1951
5696,What is the same size asinput?,a tensor filled with the scalar value0,1951
5697,What is the shape of a tensor filled with the scalar value1?,the shape defined by the variable argumentsize,1951
5698,In what language are PyTorch models defined?,Python,7966
5699,What language can TorchScript models be defined in?,Python,7966
5700,What is used to construct the input and do preprocessing?,C++ Tensor API,7966
5701,What is the TorchScript C++ API used for?,Loading serialized TorchScript models saved from Python,7966
5702,What does TorchScript allow you to do in production or embedded environments?,no-Python execution,7966
5703,What is one of the ways you can interact with the TorchScript execution engine?,Constructing the input and doing preprocessing using C++ Tensor API,7966
5704,What language are PyTorch models defined in?,Python,7966
5705,What walks through interfacing TorchScript with OpenCV?,TheExtending TorchScript with Custom C++ Operatorstutorial,7952
5706,What is the name of the tensor and autograd operations in PyTorch Python API?,torch,7952
5707,What is the full list of methods available in PyTorch?,https,7952
5708,What can be bound into TorchScript through a pybind11-like interface?,C++ classes and structs,7952
5709,Most of the tensor and autograd operations in PyTorch Python API are also available in what?,C++ API,7952
5710,What can TorchScript be augmented with user-supplied code?,custom operators and custom classes,7952
5711,From what languages can custom operators and classes be invoked in TorchScript code?,Python or from C++,7952
5712,What can TorchScript be augmented with user-supplied code through?,custom operators and custom classes,7952
5713,What does TheExtending TorchScript with Custom C++ Operatorstutorial walk through interfacing TorchScript with?,OpenCV,7952
5714,What is an example of a tensormethod?,torch,7952
5715,Where can you find the full list of methods available in TorchScript?,https,7952
5716,From what languages can custom operators and classes be invoked in TorchScript code run?,Python or from C++,7952
5717,Computes what decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices,Cholesky,1669
5718,"IfupperisTrue, the returned matrixUis what?",upper-triangular,1669
5719,What classification will these features be maintained long-term and there should generally be no major performance limitations or gaps in documentation?,Stable,6052
5720,What is the name of the library that contains Thetorchtextpackage?,Package Reference PyTorch Libraries,6052
5721,What is tagged as Beta because the API may change based on user feedback?,Beta,6052
5722,What is the classification of features that will be maintained long-term?,Stable,6052
5723,What is a package reference?,PyTorch Libraries,6052
5724,What library can be a source of nondeterminism across multiple executions of an application?,cuDNN library,7018
5725,What can an optional feature run when a cuDNN convolution is called with a new set of size parameters?,multiple convolution algorithms,7018
5726,"Due to benchmarking noise and different hardware, the benchmark may select what on subsequent runs?",different algorithms,7018
5727,"If you do not need reproducibility across multiple executions of your application, what might improve if the benchmarking feature is enabled?",performance,7018
5728,What is the cuDNN library used for?,CUDA convolution operations,7018
5729,"When a cuDNN convolution is called with a new set of size parameters, what can run multiple convolution algorithms?",an optional feature,7018
5730,Why is benchmarking noise and different hardware important?,"benchmark may select different algorithms on subsequent runs, even on the same machine",7018
5731,What are the parameters for a view of an existingtorch.Tensorinput?,"specifiedsize,strideandstorage_offset",5090
5732,Where is aTensor created from?,anumpy.ndarray,5090
5733,What is the return value of a tensor filled with the scalar value0?,"a tensor filled with the scalar value0, with the same size asinput",5090
5734,What is the size of the scalar value in the tensor?,the same size asinput,5090
5735,What size is the tensor of sizeendstartstep?,1-D,5441
5736,What is the tensor with ones on the diagonal and zeros elsewhere?,2-D tensor,5441
5737,What is returned when a tensor is filled with uninitialized data?,an uninitialized tensor with the same size asinput,5441
5738,What type of tensor is created when values are evenly spaced fromstarttoend?,one-dimensional,5259
5739,What type of tensor returns ones on the diagonal and zeros elsewhere?,2-D tensor,5253
5740,What is a tensor of sizeendstartstepleftlceil fractextend?,1-D,5253
5741,What is a tensor of sizeendstartstep+1leftlfloor fractextend?,1-D,5253
5742,What type of tensor is returned with ones on the diagonal and zeros elsewhere?,2-D,2018
5743,What returns an uninitialized tensor?,an uninitialized tensor with the same size asinput,2018
5744,What is the tensor of?,sizesizefilled withfill_value,2018
5745,What returns a tensor of sizesizefilled withfill_value?,a tensor with the same size asinputfilled withfill_value,2018
5746,What is returned by dequantizing a quantized Tensor?,fp32 Tensor,2018
5747,What type of tensor is converted to a quantized tensor with given scale and zero point?,float,2018
5748,What type of tensor is created on a logarithmic scale with basebase?,one-dimensional,2018
5749,Where are the values of the one-dimensional tensor of sizesteps evenly spaced?,fromstarttoend,2019
5750,What is the name of the logarithmic scale used to create a one-dimensional tensor of sizesteps?,basebase,2019
5751,What does it do to a given sequence ofseqtensors in a given dimension?,Concatenates the given sequence ofseqtensors in the given dimension,1821
5752,What is the difference between a tensor and a specific number of chunks?,Splits a tensor into a specific number of chunks,1821
5753,What splits a tensor into multiple tensors depthwise according toindices_or_sections?,Splitsinput,6025
5754,What is the new tensor that returns a new tensor that is a narrowed version ofinputtensor,narrowed version,6025
5755,What does it do when a tensor is split into chunks?,Gathers values along an axis specified bydim,6025
5756,What is a splitsinput?,"a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",6025
5757,What is Splitsinput?,"a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",6025
5758,How does a tensor create a new tensor?,horizontally stacking,6025
5759,What does a tensor have?,Splits a tensor into a specific number of chunks,6025
5760,How is a new tensor created?,horizontally stacking the tensors intensors,2006
5761,What is the index of a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex,aLongTensor,2006
5762,"What is a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_section",Splitsinput,6048
5763,How does Alias oftorch create a new tensor?,horizontally stacking,6048
5764,What type of tensor is returned by Alias fortorch.movedim()?,narrowed version,6048
5765,What is a tensor with three or more dimensions?,Splitsinput,6048
5766,How does Splitsinput create a new tensor?,horizontally stacking,6048
5767,What happens to the dimension(s) ofinput?,Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination,2675
5768,What is the name of a tensor that splits a tensor into chunks?,Stack,2675
5769,What is the Out-of-place version oftorch.Tensor.scatter_()?,Out-of-place version oftorch.Tensor.scatter_(),5473
5770,What are all of the sub-tensors of a tensor?,views ofinput,5473
5771,What is the name of the function that converts a tensor into a new dimension?,Alias fortorch.transpose(),5473
5772,What are the dimensions of the tensor?,0 and 1,5473
5773,What does return a tensor with the elements ofinputat the given indices?,a new tensor with the elements ofinputat the given indices,5473
5774,Returns a tensor that is what version of input?,transposed,5473
5775,What happens when a tensor dimension is removed?,Removes a tensor dimension,5473
5776,What is a tensor with all the dimensions ofinput?,size1removed,5473
5777,What is the name of the tensor that returns a tensor with the same data and number of elements asinput?,Alias oftorch.vstack(),5473
5778,Where do tensors stack depthwise?,third axis,6062
5779,Splits the tensor into what?,chunks,6062
5780,What does Stack tensors in sequence depthwise?,Gathers values along an axis specified bydim,6062
5781,What does Stack tensors in sequence depthwise mean?,third axis,6062
5782,How do Stack tensors in sequence?,horizontally,6062
5783,What is another name for tensors in sequence horizontally?,Stack,6046
5784,What does the Alias fortorch.transpose() do?,Concatenates a sequence of tensors along a new dimension,6046
5785,What is a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?,aLongTensor,6046
5786,What is the index of the new tensor?,aLongTensor,5356
5787,What is the function that returns a sequence of tensors along a new dimension?,Concatenates a sequence of tensors along a new dimension,5356
5788,What does the aBoolTensor do?,Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination,5333
5789,What dimensions does the Alias fortorch.transpose() transpose?,0 and 1,5333
5790,What moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?,Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination,5333
5791,What is a new tensor that indexes theinputtensor according to the boolean maskmask?,1-D,5333
5792,What is the name of the tensor that indexes theinputtensor according to the boolean maskmask?,Alias oftorch.vstack(),5333
5793,What does return a new tensor with the elements ofinputat the given indices?,a new tensor with the elements ofinputat the given indices,1056
5794,What type of indices are used to select values from?,1-dimensional indices,1056
5795,What type of version ofinputtensor is Alias fortorch.movedim()?,narrowed,1056
5796,What does Alias fortorch.movedim() have?,the specified shape,1056
5797,What type of indices are selected?,1-dimensional indices,4275
5798,What does a tensor have with the same data and number of elements asinput?,the specified shape,4275
5799,What is returned whose mean and standard deviation are given?,a tensor of random numbers drawn from separate normal distributions,5576
5800,Returns a tensor filled with random integers generated how?,uniformly,5857
5801,What does the random number generator do?,Draws binary random numbers (0 or 1) from a Bernoulli distribution,5857
5802,What is the name of the state that draws binary random numbers from a Bernoulli distribution?,Sets the random number generator state,5857
5803,What is the function that returns a tensor where each row containsnum_samplesindices sampled from the multinomi,Draws binary random numbers (0 or 1) from a Bernoulli distribution,2212
5804,What type of tensor is generated uniformly betweenlow(inclusive) andhigh(exclusive)?,tensor filled with random integers,2212
5805,What is a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located,Draws binary random numbers (0 or 1) from a Bernoulli distribution,2212
5806,What is a tensor with the same size asinput?,a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive),5466
5807,What is returned when a tensor is filled with random integers?,a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive),5457
5808,Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with what?,mean 0 and variance 1.,5457
5809,What returns a tensor with the same size asinput?,random permutation of integers from0ton-1,5457
5810,What is a random permutation of integers from0ton-1?,mean 0 and variance 1.,5457
5811,What type of distribution is a tensor filled with random numbers from?,uniform distribution,5494
5812,What is the normal distribution of a tensor with the same size as input?,mean 0 and variance 1.,5494
5813,What is returned when a tensor is filled with random numbers from a normal distribution?,random permutation of integers from0ton-1,5494
5814,What are a few more in-place random sampling functions defined on?,Tensors,5478
5815,What is the name of the function that returns a tensor with the same size as input?,in-place version oftorch.normal(),5478
5816,What are some in-place random sampling functions defined on?,Tensors,5418
5817,What type of element is drawn from the geometric distribution?,geometric,5418
5818,What is returned by a tensor with the same size as input?,random permutation of integers from0ton-1,5423
5819,What does Alias fortorch.acosh() multiply the result by?,scalarvalue,5392
5820,What type of tensor does Alias fortorch.acosh() return?,inverse hyperbolic cosine,5392
5821,What is the inverse hyperbolic cosine of the elements of input?,arcsine,5392
5822,What does a new tensor have?,arcsine,5392
5823,Who returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?,Alias fortorch.acosh(),5392
5824,What does Alias fortorch.asin() return?,inverse hyperbolic sine,4832
5825,What is the bitwise of the given input tensor?,XOR ofinputandother,4832
5826,In what unit is the element-wise angle of the given inputtensor?,radians,4832
5827,What does it do to the bitwise NOT of the given input tensor?,Computes the bitwise AND ofinputandother,4832
5828,What Computes the inverse cosine of each element ininput?,Alias fortorch.abs(),1041
5829,What does the element-wise multiplication oftensor1bytensor2 multiply the result by?,scalarvalue,1041
5830,What does Alias fortorch.acosh do?,Adds the scalarotherto each element of the inputinputand returns a new resulting tensor,1774
5831,What does Alias fortorch.acos() compute?,inverse cosine,1774
5832,What does Alias fortorch.acos() multiply the result by?,scalarvalue,1043
5833,Computes the element-wise angle of the given inputtensor in what units?,radians,4823
5834,What does the element-wise division oftensor1bytensor2 multiply the result by?,scalarvalue,4823
5835,What does Alias fortorch.asin() have?,inverse hyperbolic sine,4823
5836,What does Alias fortorch.atan() do?,Computes the bitwise AND ofinputandother,5400
5837,What does Alias fortorch.atan() compute?,bitwise OR ofinputandother,5400
5838,What is the inverse hyperbolic sine of elements ofinput?,arctangent,5400
5839,What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements of,Alias fortorch.atanh(),5400
5840,What is the element-wise angle of inputi/otheri?,arctangent,1740
5841,What type of tensor does Alias fortorch.clamp() compute?,bitwise NOT,1740
5842,What is the bitwise of inputandother?,XOR,1740
5843,What is the element-wise angle of the giveninputtensor?,radians,1740
5844,What is the element-wise angle of the elements ofinput?,arcsine,1740
5845,What does Alias fortorch.atan() have?,arctangent,1740
5846,What type of tangent does Alias fortorch.atan() return?,hyperbolic,5376
5847,What does Alias fortorch.asin() do?,Computes the bitwise AND ofinputandother,5376
5848,What does Alias fortorch.asin() compute?,bitwise OR ofinputandother,5376
5849,What is the name of the element ofinput that returns a new tensor?,arcsine,5376
5850,What is the bitwise ofinputandother?,XOR,1048
5851,What is the element-wise ofinputi/otheri?,arctangent,1048
5852,What does Alias fortorch.asinh() compute?,bitwise OR ofinputandother,1048
5853,What is the name of the new tensor with the inverse hyperbolic tangent of the elements ofinput?,Alias fortorch.atanh,1048
5854,What is the tangent of elements ofinput?,hyperbolic,1048
5855,What is returned when the input tensor is tested toTrue?,the minimum value of each slice of theinputtensor in the given dimension(s)dim,5553
5856,What does the log of summed exponentials of each row of theinputtensor return?,the mean value of all elements in theinputtensor,5553
5857,What does return the mean value of all elements in theinputtensor?,the median of the values ininput,5553
5858,What is the minimum value of each slice of theinputtensor in the given dimension(s)dim?,the flattened tensor or along a dimension,5553
5859,What does return the maximum value of all elements in theinputtensor?,the minimum value of all elements in theinputtensor,5616
5860,Returns what of all elements in the inputtensor?,product,5616
5861,What is the return of the maximum value of each slice of theinputtensor in the given dimension(s)dim?,the minimum value of each slice of theinputtensor in the given dimension(s)dim,5616
5862,Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. Returns what?,p-norm,6767
5863,What does return ignoringNaNvalues?,the median of the values ininput,6767
5864,What quantiles of each row of theinputtensor along the dimensiondim is computed?,q-th,6767
5865,What is returned when calculating the p-norm of (input-other)?,the mean value of all elements in theinputtensor,6767
5866,What is returned when the log of summed exponentials of each row of theinputtensor is returned?,the mean value of all elements in theinputtensor,5590
5867,What is the return of the log of summed exponentials of each row of theinputtensor in the given dimensiondim?,the mean value of all elements in theinputtensor,5590
5868,Returns the mode value of each row of theinputtensor in the given dimensiondim?,matrix norm or vector norm,5646
5869,What is returned when the log of summed exponentials of each row of theinputtensor in the given dimensiondim is returned?,the mean value of all elements in theinputtensor,5646
5870,What is returned when the sum of all elements is returned?,the product of all elements in theinputtensor,5634
5871,Whose correction will be used to calculate the standard deviation if unbiasedisTrue?,Bessel,5634
5872,What does not returnNaNvalues?,the median of the values ininput,5634
5873,What is the name of a value in a given dimensiondim?,a namedtuple,5634
5874,What is the return of the values ininput?,median,5634
5875,What is returned when the input tensor is not used?,the median of the values ininput,10871
5876,"What does this return, ignoringNaNvalues?",the median of the values ininput,10871
5877,What is returned when calculating the q-th quantiles of each row of theinputtensor along the dimensiondim?,the product of all elements in theinputtensor,5620
5878,What does the function ignoreNaNvalues?,the median of the values ininput,5620
5879,What is the mean value of all elements in theinputtensor?,median,5620
5880,What does the variant of oftorch.quantile() do?,ignores,5632
5881,What will be used to calculate the standard deviation if unbiasedisTrue?,Bessel’s correction,5632
5882,What does Bessel's correction do?,Eliminates all but the first element from every consecutive group of equivalent elements,5632
5883,When is Bessel's correction applied,If unbiased is True,5632
5884,What does ignoringNaNvalues mean?,the median of the values ininput,5632
5885,What does theinputtensor return?,the product of all elements in theinputtensor,5632
5886,What does a given tensor return?,matrix norm or vector norm,5632
5887,Not a Numbers (NaNs) is treated as what?,zero,5632
5888,What is the quantile of each row of theinputtensor?,q-th,5632
5889,Whose correction will be used if unbiasedisTrue?,Bessel,5637
5890,What does not a Numbers return?,the product of all elements in theinputtensor,5637
5891,What is a variant of oftorch.quantile()?,Computes the q-th quantiles of each row of theinputtensor along the dimensiondim,5656
5892,What is the return of the median of the values ininput?,the mean value of all elements in theinputtensor,5656
5893,What sort a tensor along a given dimension in ascending order by value?,indices,7525
5894,What condition does this function check if each element ofinputis?,infinite,7525
5895,What does this function do if each element ofinputis negative infinity or not?,Tests,7525
5896,What element represents if each element ofinputis “close” to the corresponding element ofother?,boolean elements,1051
5897,What function returns a new tensor with boolean elements if each element ofinputis “close” to the corresponding,Alias fortorch.gt(),1051
5898,What function computes input>othertextinput > textotherinput>otherelement-wise?,Alias fortorch.ge(),1051
5899,What does the new tensor return if each element isfiniteor not?,boolean elements,1051
5900,What does Alias fortorch.le() test if each element ofinputis?,positive infinity,1051
5901,What does Alias fortorch.le() do if each element ofinputis negative infinity or not?,Tests,1051
5902,What elements represent if each element ofinputis real-valued or not?,boolean elements,1051
5903,What is returned whenvaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim?,"a namedtuple(values,indices)",1051
5904,What does Alias fortorch.le() do?,Alias fortorch.le(),1051
5905,What represents if each element ofinputis close to the corresponding element ofother?,boolean elements,1051
5906,What is the name of a new tensor with boolean elements representing if each element isfiniteor not?,Alias fortorch.le(),1051
5907,What is a test if each element ofinputis positive or negative infinity?,infinite,1051
5908,What is the test if each element ofinputis infinite or not?,positive infinity,1051
5909,What does a new tensor with boolean elements represent if each element ofinputis negative infinity or not?,Tests,1051
5910,What represents if each element ofinputis NaN or not?,boolean elements,1051
5911,What represents if each element ofinputis real-valued or not?,boolean elements,1051
5912,What is thekth smallest element of each row of theinputtensor in the given dimensiondim?,"a namedtuple(values,indices)",1051
5913,Returns a new tensor with what representation if each element ofinputis NaN or not?,boolean elements,5570
5914,What is the name of the function that computes the indices that sort a tensor along a given dimension?,Alias fortorch.ge(),5570
5915,Is each element ofinput infinite or negative infinity?,infinite,5570
5916,In what order does the indices sort a tensor along a given dimension?,ascending order by value,5570
5917,What does valuesis thekth smallest element of each row of theinputtensor in the given dimensiondim?,"a namedtuple(values,indices)",8010
5918,What is the name of the tensor that has the same size and elements?,Alias fortorch.ge(),8010
5919,What is the name of the tensor that returns a new tensor with boolean elements representing if each element of,Alias fortorch.gt(),8010
5920,Returns what of vectors in dimensiondimofinputandother?,cross product,5521
5921,What is the cross product of elements ofinputin the dimensiondim?,cumulative product,5521
5922,What is computed along the given dimension?,n-th forward difference,5521
5923,What reduces the number of steps required to perform a batch matrix-matrix product of matrices stored inbatch1andb,add step,4781
5924,What does the matrix-vector product of vectorsvec1andvec2 perform?,batch matrix-matrix product of matrices inbatch1andbatch2,4781
5925,What decomposition is computed for batches of symmetric positive-definite matrices?,Cholesky,4781
5926,What does the batch matrix-matrix product of?,matrices stored ininputandmat2,4840
5927,What performs a batch matrix-matrix product of matrices inbatch1 andbatch2?,batch matrix-matrix product of matrices inbatch1andbatch2,4840
5928,What does the function do when it computes the inverse of a symmetric positive-definite matrix?,returns matrixinv,4840
5929,What product is added to the matrixinput?,outer-product of vectorsvec1andvec2,4805
5930,What performs a matrix-vector product of?,matrixmatand the vectorvec,4805
5931,What does it perform?,batch matrix-matrix product of matrices inbatch1andbatch2,4805
5932,What decomposition of a symmetric positive-definite matrix is computed for batches of symmetric positive-definite matrices?,Cholesky,4783
5933,What returns the Cholesky factoruuu?,returns matrixinv,4783
5934,What does the function compute of a real square matrix?,eigenvalues and eigenvectors,1780
5935,What does it compute of a matrix or batches of matricesA?,LU factorization,1780
5936,What returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted?,LU factorization of A fromtorch.lu(),1722
5937,What type of tensor does Alias fortorch.linalg.matrix_power() return?,2-D,1722
5938,What is the dot product of two tensors?,Matrix product of two tensors,1711
5939,What is the name of the function that calculates the dot product of two tensors?,Alias fortorch.linalg.matrix_power(),1711
5940,What classification does PyTorch commit to seeing Beta features through?,Stable classification,7707
5941,What type of feature is tagged as because the API may change based on user feedback?,Beta,2409
5942,What classification do we commit to seeing a feature through for Beta features?,the Stable classification,2409
5943,Prototype features are typically not available as part of binary distributions like what?,PyPI or Conda,2409
5944,Computes the entropy oninput?,elementwise,1759
5945,Is this module in BETA?,New functions are still being added,7756
5946,What is the name of the documentation of each function that may change in future PyTorch releases?,the documentation of each function for details,7756
5947,What is the name of the function that computes the error function of input?,Computes the error function ofinput,7756
5948,What is the exponential function of input?,base two,1701
5949,What is the complementary error function?,complementary error function ofinput,1701
5950,Computes the natural of what of the absolute value of the gamma function oninput?,logarithm,9670
5951,What is an example of a function that computes the base two exponential function of the gamma function oninput?,Computesinput*log1p(other)with the following cases,9670
5952,What function provides greater precision for small values of x?,exp(x) - 1,7556
5953,What should all have the same number of dimensions?,"self,indexandsrc",7691
5954,What happens when indices are not unique?,the gradient will be incorrect,7691
5955,What is the manner described ingather()?,reverse operation,7691
5956,What does indexandsrcdo do?,not broadcast,7691
5957,Does indexandsrcdo broadcast?,not broadcast,2488
5958,What does the PyTorch Timer do when necessary?,synchronize asynchronous CUDA functions,2771
5959,What does Timer do when necessary?,synchronize asynchronous CUDA functions,7854
5960,What does Timer perform?,"warmups, set threadpool, and synchronize asynchronous CUDA functions when
necessary.",7854
5961,What can one do when defining a Timer?,"optionally specify label,sub_label,description, andenv",8356
5962,What are the fields included in the representation of result object and by theCompareclass to group and display results for comparison?,These fields are included in the representation of result object and by theCompareclass to group and display results for comparison,8356
5963,Optional setup code. Used to define variables used instmt what?,global_setup,8356
5964,String which summarizes stmt.,label,8356
5965,What is sub_label?,Provide supplemental information,8356
5966,What is a String to distinguish measurements with identical label and sub_label?,description,8356
5967,What would one set description based on to create a column of data?,input size,8356
5968,Timer can run a statement under what?,Callgrind,2772
5969,What is the name of the optional setup code used to define variables?,in stmt global_setup,2772
5970,What is the PyTorch Timer based on?,timeit.Timer,2772
5971,What is median computation?,more robust than mean,2772
5972,What is replicated for cases where an adaptive strategy is not desired?,The timeit method,2772
5973,Helper class for measuring what of PyTorch statements.,execution time,2772
5974,What fields are included in the representation of result object and by theCompareclass?,These fields are included in the representation of result object and by theCompareclass,2772
5975,What are some examples of PyTorch Timer constructor arguments?,"stmt,setup,timer,globals",2772
5976,Setup– what is used to define variables used instmt global_setup?,Optional setup code,2772
5977,What fields are included in the representation of result object and by theCompareclass to group and display results for comparison?,"optionally specifylabel,sub_label,description, and env",8355
5978,What is used to group and display results for comparison?,theCompareclass,8355
5979,What are the fields included in the representation of result object and by theCompareclass for comparison?,"specify label, sub_label, description, and env.",8354
5980,What is a Timer specific constructor?,PyTorch,8354
5981,What is setup?,Optional setup code,2492
5982,What are the PyTorch Timer specific constructor arguments?,"label,sub_label,description,env,num_threads",2492
5983,What language is global_setup only used in?,C++,2492
5984,What type of timer returns the current time?,Callable,2492
5985,What is a significant confounding factor when measuring code?,run-to-run variation,2492
5986,What does the PyTorch Timer deviate from?,thetimeitAPI,2492
5987,Where are exact algorithms discussed?,method docstrings,2492
5988,The timeit method is replicated for cases where what is not desired?,adaptive strategy,2492
5989,In what language is global_setup only used?,C++,2142
5990,What is the name of the timer constructor arguments?,stmt,2142
5991,What is used to define variables?,in stmt global_setup,2142
5992,"In addition to wall times, Timer can run a statement under what?",Callgrind,3697
5993,What is the optional setup code used to define variables?,instmt global_setup,3697
5994,What is the name of the PyTorch Timer specific constructor arguments?,stmt,10765
5995,"What specific constructor arguments do labels,sub_label,description,env,num_threads, and num_thread",PyTorch Timer,4948
5996,What is the name of the specific constructor arguments?,PyTorch Timer,4948
5997,What is placed at the top level of the file for things like#includestatements?,global_setup,9474
5998,"If PyTorch was built without GPU present, what would it do?",synchronize CUDA,9474
5999,Timer– Callable which returns what?,current time,9474
6000,What is the name of the document that describes PyTorch?,description,9474
6001,What type of setup code is used to define variables used instmt global_setup?,Optional setup code,4950
6002,What is the code snippet to be run in a loop and timed?,stmt,4950
6003,What does timer return?,current time,4950
6004,If PyTorch was built without what?,CUDA,4950
6005,What is globals the other method for?,providing variables which stmt needs,4950
6006,What does sub_label provide to disambiguate measurements with identical stmt or label?,supplemental information,4950
6007,What is an example of a sub_label?,float,4950
6008,What tag indicates that otherwise identical tasks were run in different environments?,env,4950
6009,What string summarizes stmt?,label,4950
6010,"Why would one set label to ""ReLU(x + 1)""?",improve readability,4950
6011,What is a way to create a table of the form?,usingCompare,4950
6012,What is the other method for providing variables whichstmtneeds?,globals,4950
6013,When is description included in PyTorch?,when printing a Measurement,4950
6014,What is the optional setup code used to define variables used in PyTorch?,instmt global_setup,4950
6015,What was built without CUDA or there was no GPU present?,PyTorch,9765
6016,What is the name of the code used to define variables in PyTorch?,in stmt global_setup,10677
6017,What was built without CUDA or there is no GPU present?,PyTorch,10677
6018,What returns the current time?,Callable,10677
6019,"If PyTorch was built without GPU present, what will it do?",synchronize CUDA,10677
6020,What happens if PyTorch was built without GPU?,synchronize CUDA,10920
6021,What is a code snippet to be run in a loop and timed?,stmt,10768
6022,What is the code that is placed at the top level of the file for things like#includestatements?,global_setup,10768
6023,What is used to define variables used instmt global_setup?,Optional setup code,10678
6024,Optional setup code is used to define variables used in PyTorch?,instmt global_setup,10678
6025,Code which is placed at the top level of the file for things like#includestatements. timer– Callable which returns the current time.,global_setup,9473
6026,What defines the global variables whenstmtis being executed?,globals,9476
6027,What is a string that summarizes stmt?,label,10767
6028,What are PyTorch Timer specific constructor arguments?,"stmt,setup,timer,globals",10767
6029,What language does global_setup come from?,C++,10767
6030,What might a sub_label be?,float,10767
6031,Why would one set label to “ReLU(x + 1)”?,improve readability,2143
6032,The principal use of descriptionis to signal what?,to Compare the columns of data,2143
6033,When is description also included?,when printing a Measurement,2143
6034,Timer– Callable which returns the what?,current time,2143
6035,Setup– What is used to define variables used instmt global_setup?,Optional setup code,2143
6036,A dict which defines the global variables whenstmtis being executed. This is the other method for providing variables whichstmtn,globals,2143
6037,"If PyTorch was built without what, this defaults totimeit.default_timer; otherwise it will synchronize CUDA",CUDA,2143
6038,"String which summarizes stmt. For instance, ifstmtis “torch.nn.functional.rel",label,2143
6039,What – Provide supplemental information to disambiguate measurements with identical stmt or label?,sub_label,2143
6040,String to distinguish measurements with identical label and sub_label. The principal use of descriptionis to signal toComparethe columns of data.,description,2143
6041,How would one set it based on the input size to create a table of the form?,usingCompare,2143
6042,What is the name of the string that summarizes stmt?,sub_label,10766
6043,What type of label summarizes stmt?,String,9766
6044,What would one set label to to improve readability?,ReLU(x + 1),9766
6045,What is a description of a stmt?,description,9766
6046,What is another name for a sub_label?,int,4914
6047,What is one way to disambiguate measurements with identical stmt or label?,Provide supplemental information,4914
6048,What is used to disambiguate measurements when printing Measurements or summarizing usingCompare?,description,4914
6049,What is a supplemental information used to disambiguate measurements?,description,4914
6050,What is a String that summarizes stmt?,label,9767
6051,What provides supplemental information to disambiguate measurements with identical stmt or label?,sub_label,9767
6052,How would one set description based on the input size to create a table of the form?,usingCompare,9767
6053,What type of description is used to disambiguate measurements?,description,10773
6054,What is provided to disambiguate measurements with identical stmt or label?,supplemental information,4915
6055,What is the name of the sub_label in our example?,float,4915
6056,What is used when printing Measurements or summarizing usingCompare?,ReLU(x + 1): (int),11492
6057,What is used to distinguish measurements with identical label and sub_label?,String,6083
6058,What is the principal use of description?,to signal to Compare the columns of data,6083
6059,What is description also included when printing?,Measurement,6083
6060,What would one set description based on to create a table of the form?,input size,9161
6061,Description is included when printing a what?,Measurement,9161
6062,What might one set it based on to create a table of the form?,the input size,6081
6063,What would one set the description based on to create a table of the form?,input size,6081
6064,"When printing a document, what is the use ofdescription included in?",Measurement,6081
6065,When is description included?,when printing a Measurement,6082
6066,What does the default PyTorch threadpool size try to utilize?,all cores,6082
6067,What is the default size of the PyTorch threadpool?,one,6082
6068,What is the principal use of description to signal toComparethe columns of data?,usingCompare,6082
6069,How does PyTorch measure many replicates?,keeping timer overhead to a minimum,6082
6070,What is included when printing a Measurement?,Compare,11323
6071,What is also included when printing a?,Measurement,11325
6072,When is usingCompare included?,when printing a Measurement,11325
6073,What does compare treat Measurements with when merging replicate runs?,different env specification,9273
6074,What is the size of the PyTorch thread pool when executing stmt?,num_threads,9273
6075,What is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency?,Single threaded performace,9273
6076,What tries to utilize all cores?,PyTorch threadpool size,9273
6077,How many competing objectives must the choice of block size balance?,two,9273
6078,What percentage of the computation is timer overhead?,0.1%,9273
6079,What is the value of block_size used for?,main measurement loop,9273
6080,What is used to collect instruction counts?,Callgrind,9273
6081,What contains measured runtimes and repetition counts?,A Measurement object,9273
6082,env- This tag indicates that otherwise identical tasks were run in what environment?,different,9272
6083,What does blocked_autorange do?,Measure many replicates while keeping timer overhead to a minimum,10276
6084,What executes the following pseudo-code at a high level?,blocked_autorange,10276
6085,What is the default for the PyTorch threadpool size?,one,10275
6086,Measure how many replicates while keeping timer overhead to a minimum.,many replicates,10275
6087,What does blocked_autorange execute at a high level?,pseudo-code,10275
6088,What is the goal of blocked_autorange?,Measure many replicates while keeping timer overhead to a minimum,4232
6089,What variable does blocked_autorange execute the following pseudo-code?,the variableblock_sizein the inner loop,4232
6090,How many competing objectives must a block size balance?,two,4232
6091,How do you measure many replicates?,keeping timer overhead to a minimum,4232
6092,What variable is in the inner loop of blocked_autorange?,variableblock_size,4232
6093,What results in more replicates and generally better statistics?,A small block size,844
6094,What is in the inner loop of blocked_autorange?,variableblock_size,1340
6095,What is in the inner loop?,variableblock_size,4515
6096,What does a small block size result in?,better statistics,4515
6097,What does a large block size amortize better?,cost of timer invocation,4513
6098,What is non-trivial?,CUDA syncronization time,4514
6099,What variable is in the inner loop?,variableblock_size,4514
6100,What does blocked_autorange set by running a warmup period?,block_size,4514
6101,What does a large block size do to the cost oftimerinvocation?,amortizes,843
6102,What is an example of a measurement object that can be used to compute statistics?,median,843
6103,A large block size better amortizes what?,cost of timer invocation,808
6104,What is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement?,CUDA syncronization time,808
6105,What does blocked_autorange set?,block_size,8975
6106,Globals are restricted to what?,"builtins,nn.Modules’s, and Torch Scripted functions/modules",8975
6107,What provides more detail on the subject of globals?,TheGlobalsBridgeclass,8975
6108,What are some statistics that can be used by Callgrind?,"mean, median, etc.",8975
6109,"Why are globals restricted to builtins,nn.Modules, and Torch Scripted functions/modules?",globalscannot contain arbitrary in-memory data structures,8975
6110,What is the result of a Timer measurement?,timeit.Timer.timeit,8975
6111,What is blocked_autorange used for?,main measurement loop,8974
6112,What is a measurement object that can be used to compute statistics?,median,8974
6113,What is the value of blocked_autorange used for?,main measurement loop,8973
6114,A Measurement object can be used to do what?,compute statistics,8973
6115,What can the A Measurement object be used for?,compute statistics,764
6116,What is another name for an A Measurement object?,median,764
6117,What tool can be used to collect instruction counts?,Callgrind,764
6118,What object contains measured runtimes and repetition counts?,A Measurement object,764
6119,What tool is used to collect instruction counts?,Callgrind,1616
6120,What does Callgrind do?,Collect instruction counts,1616
6121,What does Callgrind collect?,instruction counts,1616
6122,What is deterministic?,instruction counts,8059
6123,Instruction counts are ideal for what?,detailed performance analysis,8059
6124,What is a separate process so that Valgrind can instrument the program?,runs stmt in,8059
6125,Performance is severely degraded due to what?,instrumentation,8060
6126,"Unlike wall times, instruction counts are what?",deterministic,8060
6127,What must be installed in order to use this methodvalgrind?,"callgrind_control, andcallgrind_annotate",8060
6128,What class provides more detail on the subject of globals?,TheGlobalsBridgeclass,8060
6129,Who can instrument the program?,Valgrind,8060
6130,What is instruction counts ideal for?,detailed performance analysis,8060
6131,Merge will extrapolate times tonumber_per_run=1 and will not transfer any metadata.,Approximate significant figure estimate,8060
6132,What must be installed in order to use?,method valgrind,3748
6133,What must be installed in order to use methodvalgrind?,"callgrind_control, and callgrind_annotate",3750
6134,What provides instruction counts and some basic facilities for analyzing and manipulating results?,A Callgrind Stats object,3750
6135,What can globalsnot contain?,arbitrary in-memory data structures,3750
6136,What are globals restricted to?,"builtins,nn.Modules’s, and Torch Scripted functions/modules",3750
6137,What method is used for merging replicates?,Convenience method,3750
6138,What does timeit.Timer.timeit() execute?,main statement (stmt)numbertimes,3750
6139,What class provides more detail on this subject?,TheGlobalsBridgeclass,3750
6140,What depend on pickle and you may need to add an import tosetup for them to transfer properly?,nn.Modules,3750
6141,"By default, what will be collected and cached to indicate how many instructions are from the Python loop which drivesstmt?",a profile,1486
6142,What does the Timer.timeit class provide for downstream consumers?,several convenience methods,1486
6143,What is executed by timeit.Timer.timeit()?,main statement (stmt)numbertimes,1486
6144,What provides basic facilities for analyzing and manipulating results?,A Callgrind Stats object,1484
6145,What does ACallgrindStatsobject do?,Mirrors the semantics of timeit.Timer.timeit(),1484
6146,What is the name of the main statement?,stmt,1484
6147,What is the result of?,Timer measurement,2346
6148,What does this class store?,one or more measurements of a given statement,2346
6149,What is used for merging replicates?,Convenience method,2346
6150,What is the main statement called?,stmt,2346
6151,What convenience method does this class provide for downstream consumers?,__repr__,2346
6152,What property is intended to give a convenient way to estimate the precision of a measurement?,Approximate significant figure estimate,2346
6153,A Callgrind Stats object mirrors the semantics of what?,timeit.Timer.timeit(),766
6154,What does this class provide downstream consumers?,several convenience methods,766
6155,"Instruction counts are deterministic, which makes them ideal for what?",detailed performance analysis,766
6156,What are some statistics that can be computed with A Measurement object?,"mean, median, etc.",766
6157,What class provides more detail on globals?,TheGlobalsBridgeclass,766
6158,What does A Callgrind Stats object mirror?,timeit.Timer.timeit(),894
6159,What provides instruction counts and basic facilities for analyzing and manipulating results?,ACallgrindStatsobject,894
6160,What property is used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data summary?,significant figure estimation,894
6161,What can be removed from function strings?,Strip library names and some prefixes,894
6162,What does the timeit.Timer.timeit class provide?,several convenience methods,894
6163,"If a key component such as Python or PyTorch was built in separate locations in the two profiles, what can this cause?",issues,894
6164,"If a key component such as Python was built in separate locations in the two profiles, what can result in something resembling?",PyTorch,894
6165,What does stmt stand for?,main statement,2347
6166,What will extrapolate times tonumber_per_run=1 and will not transfer any metadata?,Merge,2347
6167,Approximate significant figure estimate is intended to give a convenient way what?,to estimate the precision of a measurement,2347
6168,Why does __repr__ not use this method?,it simply displays raw values,2347
6169,What is provided for merging replicates?,Convenience method,2347
6170,What is intended forCompare?,Significant figure estimation,2347
6171,What is used for Callgrind results collected by Timer?,Top level container,2347
6172,Manipulation is generally done using what class?,FunctionCounts,2347
6173,What is the most significant convenience method?,isCallgrindStats.as_standardized(),2347
6174,What can be a stumbling block when comparing two different sets of instruction counts?,path prefixes,2347
6175,What does Callgrind include when reporting a function?,full filepath,2347
6176,What is used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data summary?,significant figure estimation,2347
6177,What do you strip from function strings?,library names and some prefixes,2347
6178,What can Callgrind include the full filepath when reporting a function?,issues when diffing profiles,2347
6179,What can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing?,Stripping prefixes,2347
6180,What convenience method is provided for downstream consumers?,__repr__,2347
6181,What is the semantics of timeit.Timer.timeit()?,Mirrors,4248
6182,What is the semantics of?,timeit.Timer.timeit(),4248
6183,What does timeit.Timer.timeit() do?,Execute the main statement (stmt)numbertimes,4247
6184,Who does this class provide convenience methods for?,downstream consumers,2345
6185,What does this class store of a given statement?,one or more measurements,2345
6186,Why does Merge not transfer any metadata?,differ between replicates,2345
6187,Why will Merge not transfer any metadata?,it might differ between replicates,2345
6188,What type of object does obj have to implement write and flush?,a file-like object,5774
6189,What does a PathLike object contain?,a file name,5774
6190,See Saving and loading tensors preserves what for more details?,views,5774
6191,What is another name for saving and loading tensors?,Note,5774
6192,What does PyTorch preserve across serialization?,storage sharing,5747
6193,What is a common PyTorch convention to save tensors using?,.pt file extension,5747
6194,What release of PyTorch switchedtorch.saveto use a new zipfile-based file format?,1.6,5747
6195,What does PyTorch.saveto pass if you want to use the old format?,kwarg_use_new_zipfile_serialization=False,5747
6196,What does saving and loading tensors do?,Saves an object to a disk file,5747
6197,What does saving and loading tensors preserve?,views,5747
6198,What is an example of a file-like object that can be saved to a disk file?,Example,5747
6199,What can be specified to override the default protocol?,pickle_protocol,10411
6200,What preserves viewsfor more details?,See Saving and loading tensors,10411
6201,What does PyTorch preserves storage sharing across serialization?,Note,10411
6202,What is a common PyTorch convention to save tensors using.pt file extension?,Note,9439
6203,What type of object has to implement write and flush?,a file-like object,9439
6204,What type of object contains a file name pickle_module?,a string or os.PathLike object,9439
6205,What module can be specified to override the default protocol?,pickle_module,10410
6206,What preserves views for more details?,See Saving and loading tensors,4353
6207,What is an example of a file format that PyTorch.saveto use a new zipfile-based file format?,Example,4353
6208,Set_grad_enabled will enable or disable grads based on what?,argumentmode,10669
6209,What can set_grad_enabled be used as?,context-manager or as a function,10669
6210,What does mode(bool) do to enable grad (True) or disable (False)?,Flag,10669
6211,What can mode(bool) be used for?,conditionally enable gradients,10669
6212,What does set_grad_enabled not affect?,computation in other threads,10669
6213,What type of context manager is set_grad_enabled?,thread local,1886
6214,What will enable or disable grads based on its argumentmode?,set_grad_enabled,1886
6215,What is used to flag whether to enable grad (True) or disable (False)?,mode(bool),1886
6216,What can mode(bool) be used to do?,conditionally enable gradients,1886
6217,Set_grad_enabled can be used as what?,context-manager or as a function,1886
6218,What is an example of a mechanism that can enable or disable gradients locally?,set_grad_enabled.,1886
6219,What do you need to do with the setting of grad_enabled?,Note,1885
6220,Does this context manager affect computation in other threads?,it will not affect computation in other threads,7483
6221,What is the behavior similar to python'sitertools.combinationswhenwith_replacementis set toFals,Compute combinations of lengthrrrof the given tensor,1639
6222,What is the name of the 1D vector?,input(Tensor),1639
6223,What is the number of elements to combine?,"r(int,optional)",1639
6224,"What is the number of elements to combine with_replacement(boolean,optional)?","r(int,optional)",1639
6225,What is the behavior similar to python’sitertools.combinationswhenwith_replacementis set toFals,Compute combinations of lengthrrrof the given tensor,1639
6226,Compute combinations of lengthrrrof the given tensor. The behavior is similar to what?,python,1639
6227,What does input(Tensor) mean?,input(Tensor) – 1D vector,1639
6228,What performs a product of the matrixinputand the vectorvec?,matrix-vector,4800
6229,Does this function broadcast?,does not broadcast,4800
6230,What does return in the given dimension(s)dim?,the maximum value of each slice of the input tensor,5609
6231,What distributes gradient between equal values?,amax/aminevenly,5609
6232,What function is deprecated and will be removed in a future PyTorch release?,Alias oftorch.outer(),1064
6233,What is the replacement for oftorch.outer()?,Usetorch.outer(),1064
6234,What is the name of the function oftorch.outer()?,Alias,1064
6235,In what release will oftorch.outer() be removed?,PyTorch,1064
6236,What function will be removed in a future PyTorch release?,Usetorch.outer(),1064
6237,What is created whose values are evenly spaced frombasestarttextbasetextstartbasestarttobaseendtextbase,a one-dimensional tensor of sizesteps,2015
6238,What is created when sizesteps are evenly spaced frombasestarttextbasetextstartbasestarttobaseendtext,a one-dimensional tensor,2015
6239,Is not providing a value forsteps deprecated?,Warning Not providing a value for steps is deprecated,8172
6240,"For backwards compatibility, not providing a value forstepswill what?",create a tensor with 100 elements,8172
6241,"For backwards compatibility, not providing a value forstepswill create a tensor with how many elements?",100 elements,4347
6242,"In a future PyTorch release, failing to provide a value forstepswill throw what?",a runtime error,4347
6243,Why is not providing a value forsteps deprecated?,not reflected in the documented function signature,4347
6244,What is the starting value for the set of points end(float)?,start(float),4347
6245,Is providing a value forsteps deprecated?,Not providing a value for steps is deprecated,4347
6246,"dtype(torch.dpython:type,optional) – what to perform the computation in?",the data type,4347
6247,What default uses the global default dtype?,if None,4347
6248,What does torch.get_default_dtype() use?,global default dtype,4347
6249,"For backwards compatibility, not providing a value forstepswill create what?",a tensor with 100 elements,2016
6250,What default uses the global default dtype when bothstartandendare real?,if None,10355
6251,What is start(float)?,starting value for the set of points end(float),10752
6252,What does torch.get_default_dtype() use when bothstartandendare real?,global default dtype,10752
6253,What is the ending value for the set of points steps(int)?,end(float),9269
6254,What does torch.get_default_dtype() use when both startandendare real?,global default dtype,10764
6255,What is the size of the constructed tensor base?,steps,10764
6256,What is the base of the logarithm function?,"base(float,optional)",8944
6257,"Out(Tensor,optional) - what is the output of the logarithm function?",output tensor,8944
6258,"What is base(float,optional)?",base of the logarithm function,8944
6259,What is the default dtype used when both startandendare real?,Default: if None,8944
6260,What type of dtype is used when both startandendare real?,Default,9212
6261,"What does dtype(torch.dpython:type,optional) represent?",the data type to perform the computation in,9212
6262,What is the default value for the global default dtype?,if None,9212
6263,The output is mapped to what for which the output is mapped to+ INF?,+ INF,5393
6264,What does torch.package add for creating hermetic packages containing arbitrary PyTorch code?,torch.packageadds support,11199
6265,What can hermetic packages be used for?,to load and execute models at a later date or on a different machine,11199
6266,What does this document contain that will help you learn more about torch.package?,"tutorials, how-to guides, explanations, and an API reference",11199
6267,What does this module depend on?,the pickle module,11199
6268,What should you never unpackage data that could have come from an untrusted source or that could have been tampered with?,"Never unpackage data that could have come from an untrusted source, or that could have been tampered with",11199
6269,What type of packages does torch.package add support for?,hermetic,11198
6270,What can hermetic packages be used for at a later date or on a different machine?,to load and execute models,11198
6271,What does this document contain?,"tutorials, how-to guides, explanations, and an API reference",7493
6272,What does Torch.package depend on?,the pickle module,7493
6273,What should you only do when using the picklemodule?,unpackage data you trust,7493
6274,Do you unpack data that could have come from an untrusted source or that could have been tampered with?,Never,7493
6275,What does the module depend on?,unpackage data you trust,8200
6276,Where can you find more information about the pickle module?,the documentation for the pickle module,7745
6277,How do you see what is inside a package?,Packaging your first model,7745
6278,What is the only thing that is not secure?,unpackage data you trust,7745
6279,What does a first model need to see?,what is inside a package,8028
6280,What do I do to a package?,Patch code into a package,8028
6281,What is the difference between packaged and non-packaged code?,non-packaged code,8028
6282,What is the name of the package that contains a Torch Script module?,torch.packageFormat,8028
6283,What do I include with my package and access them later?,arbitrary resources,2960
6284,How do I package a Torch Script module?,Package a Torch Script module,2960
6285,What do I do?,See what is inside a package,2960
6286,How do I know if a package is executing?,Test in my source code,2960
6287,How do I access package contents from packaged code?,Patch code into a package,2960
6288,What do I do from a package?,Access package contents,2960
6289,How is a class packaged?,Customize,2062
6290,What can I do from a package?,Access package contents,2062
6291,What does Torch Script do?,Re-export an imported object,2062
6292,What does Torch Script do to a package?,Patch code,2062
6293,Where can you access package contents from?,packaged code,2062
6294,What is a way to test if a class is executing inside a package?,Patch code into a package,2062
6295,What is the purpose of testing a package?,Test in my source code whether or not it is executing inside a package,2062
6296,What do you need to distinguish between?,packaged code and non-packaged code,2062
6297,What can I test in my source code?,whether or not it is executing inside a package,3832
6298,What should I include with my package and access them later?,arbitrary resources,3832
6299,What can I access from packaged code?,package contents,3832
6300,What can I do to a package?,Patch code into a package,3832
6301,"Package Exporter exposes three methods that allow you to save Python objects, text, and binary data to a package.","save_pickle,save_textandsave_binary",3832
6302,What can I do with a package?,Customize how a class is packaged,3832
6303,What is one way to package a package?,Package a Torch Script module,3832
6304,What is the name of the package that Torch.packagefinds your code's dependencies?,torch.packageFormat,3832
6305,What do I do from packaged code?,Access package contents,2959
6306,What is the name of the package that contains your code’s dependencies?,torch.package,2959
6307,What is a package for?,Torch Script module,4753
6308,What does Torch Script use to access package contents?,packaged code,4753
6309,What does torch.packageFormat Overview find your code's dependencies?,Dependency Management,4753
6310,What type of code is included in a package?,Patch code,6758
6311,What do you do from packaged code?,Access package contents,6758
6312,Where can I access package contents from?,packaged code,6758
6313,Where is my source code tested to see if it is executing?,inside a package,6758
6314,What is the difference between packaged code and?,non-packaged code,6758
6315,What is the name of the package that finds your code’s dependencies?,torch.packageFormat Overview,6758
6316,What type of code does a package differ from?,non-packaged code,3830
6317,What is the name of the document that describes how a package is packaged?,torch.packageFormat Overview,3830
6318,What do you see in a package?,what is inside a package,5791
6319,What should I include with my package?,arbitrary resources,5791
6320,Distinguish between packaged code and what?,non-packaged code,920
6321,What is one way to package a Torch Script module?,Re-export an imported object,920
6322,What can you access from packaged code?,package contents,920
6323,What is the name of the module that can be used to package code?,Package a Torch Script module,920
6324,What is the name of the package format used by Torch Script?,torch.packageFormat,920
6325,"Package Exporterexposes three methods that allow you to save Python objects, text, and binary data to a package?","save_pickle,save_textandsave_binary",920
6326,What is included in a package?,Patch code,5793
6327,What is the purpose of a Torch package?,See what is inside a package,5793
6328,What can I include with my package and access them later?,arbitrary resources,5793
6329,What package does Torch package?,Torch Script module,5793
6330,Howtorch.packagefinds your code’s dependencies?,Steps,2976
6331,What does torch.package allow for?,customization,2976
6332,Howtorch.package allows for the customization of how classes are packaged?,defining__reduce__for Python’s normal pickling process,2976
6333,What do you distinguish between?,packaged code and non-packaged code,2061
6334,What type of code is different from packaged code?,non-packaged code,2061
6335,What does torch.package find your code's dependencies?,Dependency Management,11196
6336,What is the difference between packaged code and non-packaged code?,non-packaged code,918
6337,What does package a?,Torch Script module,918
6338,What does torch.package distinguish between?,packaged code and non-packaged code,2162
6339,What does torch.package package?,Torch Script module,2162
6340,What two types of code does Torch distinguish?,packaged code and non-packaged code,2162
6341,What does Torch Script do to differentiate between packaged code and non-packaged code?,Re-export an imported object,2162
6342,What does a packager do to distinguish between packages and non-packaged code?,Package a Torch Script module,2162
6343,What can you package?,Torch Script module,5097
6344,What do you need to know to create and use Torch packages?,the basic API,874
6345,Where is a tutorial that guides you through packaging and unpackaging a simple model available?,Colab,874
6346,What is the basic API for creating and using packages?,Torch,874
6347,What can you do to re-export an imported object?,Package a Torch Script module,5098
6348,What is the basic API for?,creating and using Torch packages,4712
6349,How do you package a Torch Script module?,Package a Torch Script module,4712
6350,What does a Torch Script module do?,Package a Torch Script module,4712
6351,What is the ZIP format?,The container format for a torch.package,4712
6352,What is the name of the package that Torch Script modules use?,torch.packageFormat,4712
6353,What is the basic API for creating and using?,Torch packages,877
6354,What is directly printable and will print out a file tree representation?,The Folder object,877
6355,What allows for the customization of how classes are packaged?,torch.package,877
6356,How is the customization of how classes are packaged accessed?,defining the method__reduce_package__on a class and by defining a corresponding de-packaging function,877
6357,What can you query Folder objects with?,the has_file()method,877
6358,"What exposes complementary methods that allow you to load Python objects, text and binary data from a package?",Package Importer,877
6359,What steps are included in the Torch.package tutorial?,Steps,877
6360,What is similar to defining the method__reduce_package__on a class and by defining a corresponding de-,defining__reduce__for Python’s normal pickling process,877
6361,"After completing this tutorial, you will be familiar with the basic API for what?",creating and using Torch packages,11195
6362,What does Howtorch.package find your code's dependencies?,Dependency Management torch,2974
6363,What is the basic API for creating and using Torch packages?,basic,2974
6364,Howtorch.packagefinds your code's dependencies?,Dependency Management torch.packagesharp edges,2975
6365,What is torch.packagesharp edges?,Dependency Management,2110
6366,What will you be familiar with the basic API for creating and using?,Torch packages,897
6367,What is the container format for a torch.package?,a torch.package is ZIP,897
6368,What are some common ways to interact with ZIP files?,Some common ways to interact with ZIP files,3831
6369,What can I do with a Torch package?,Customize how a class is packaged,3831
6370,What is the name of the module that is included in a package?,Package a Torch Script module,3831
6371,What keeps packages isolated from each other?,Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference,2111
6372,"After completing this exercise, you will be familiar with the basic API for creating and using what?",Torch packages,2111
6373,What is the container format for Torch packages?,a torch.packageis ZIP,875
6374,What is a common way to interact with a ZIP file?,unzipmy_package.ptwill unzip thetorch.packagearchive to disk,876
6375,What does Package Importer and Package Exporter provide?,a file_structure() method,876
6376,What is a tutorial that guides you through packaging and unpackaging a simple model available on Colab?,API Reference,896
6377,What will unzip thetorch.packagearchive to disk?,unzipmy_package.pt,7003
6378,What is the container format for?,a torch.package is ZIP,7003
6379,What is a common way to interact with ZIP files?,unzipmy_package.ptwill unzip thetorch.packagearchive to disk,7003
6380,What provides a standard way to read and write ZIP archive contents?,Python zip file module,7003
6381,What does vim have the ability to do?,natively read ZIP archives,7003
6382,What can you do with files?,edit files and :write them back into the archive,7003
6383,Who has the ability to natively read ZIP archives?,vim,7003
6384,What can you do with ZIP files?,edit files and :write them back into the archive,7003
6385,What format does a torch.package use?,The container format,2977
6386,What unzips thetorch.packagearchive to disk?,unzipmy_package.ptwill,11288
6387,What can vim do with files?,edit files and :writethem back into the archive,11288
6388,What can vim do?,edit files and :writethem back into the archive,6941
6389,What is a simple directory structure that you can use to explore the current contents of a torch.package?,The Folder object,4708
6390,Which two packages provide afile_structure()method?,Package Importer and Package Exporter,4708
6391,What does The Folder object do?,The Folder object itself is directly printable and will print out a file tree representation,4708
6392,What arguments are used to filter what is returned?,glob-styleincludeandexcludefiltering arguments,4708
6393,What is another way to query Folder objects?,the has_file()method,4708
6394,What method can you use to query Folder objects?,the has_file()method,7004
6395,"What exposes three methods that allow you to save Python objects, text, and binary data to a package?",Package Exporter,4703
6396,"What exposes complementary methods called load_pickle,load_textandload_binary?",Package Importer,4703
6397,"What exposes complementary methods calledload_pickle,load_textandload_binary?",Package Importer,4710
6398,"What is the name of the step that allows you to load Python objects, text and binary data from a package?",Steps,4710
6399,What is similar to torch.package?,defining__reduce__for Python’s normal pickling process,11202
6400,What is used for the customization of how classes are packaged?,torch.package allows,11193
6401,What is similar to defining__reduce__for?,Python’s normal pickling process,11193
6402,What does torch.package allow for the customization of how classes are packaged?,Steps,11193
6403,What does torch.packageallow for the customization of how classes are packaged?,torch.package allows,4704
6404,Who calls this method when it encounters an instance of the target class?,the Package Exporter,4704
6405,What is accessed through defining the method__reduce_package__ on a class and by defining?,a corresponding de-packaging function,11194
6406,What language's normal pickling process is similar to defining__reduce__for?,Python,11194
6407,What is the method__reduce_package__(?,"self,exporter:Package Exporter",11194
6408,Who calls the method when it encounters an instance of the target class?,the Package Exporter,11194
6409,What should the function signature's first parameter be?,a Package Importer instance,11194
6410,What function should do the work to reconstruct and return an instance of the class?,de-packaging,11194
6411,What should the method return a tuple of?,de-packaging function,2092
6412,What should be a Package Importer instance?,first parameter,2092
6413,"Define the method__reduce_package__(self,exporter:Package Exporter) on what?",the target class,2092
6414,What should do the work to reconstruct and return an instance of the class?,de-packaging function,2092
6415,What does the code behave differently depending on?,whether it’s imported normally through your Python environment or imported from a torch.package,2092
6416,Who calls the de-packaging function when it encounters an instance of the target class?,the Package Exporter,2092
6417,What should return a tuple of with the arguments needed to invoke the de-packaging function?,de-packaging function,2091
6418,"Define the method__reduce_package__(self,exporter:Package Exporter)on what?",the target class,2091
6419,What is the name of the corresponding de-packaging function returned by the method__reduce_package__?,tuple,2091
6420,What do you define for the class?,de-packaging function,2090
6421,What should a de-packaging function do?,reconstruct and return an instance of the class,2090
6422,Where is the attribute__torch_package__imported from?,a torch.package,2090
6423,What does a Package Importer add to every module that it initializes?,attribute__torch_package,2089
6424,What can your code check for the presence of the attribute__torch_package__to every module that it initializes?,whether it is executing in a packaged context or not,2089
6425,What attribute does a Package Importer add to every module that it initializes?,attribute__torch_package__,8757
6426,What can your code check for the presence of this attribute to determine?,whether it is executing in a packaged context or not,8757
6427,What is it called to have code that behaves differently depending on whether it's packaged or not?,it’s bad practice,8757
6428,What is the name of the package that a Package Importer imports code from?,a torch.package,8757
6429,Where is the code imported from?,a torch.package,4545
6430,What can code that behaves differently depending on whether it's packaged or not lead to?,hard-to-debug issues,4545
6431,What should you do if your package is intended to be heavily used?,restructuring your code,4545
6432,What offers asave_source_string() method?,Package Exporter,4545
6433,"In general, it's bad practice to have code that behaves differently depending on what?",whether it’s packaged or not,4545
6434,Is it good practice to have code that behaves differently depending on whether it's packaged or not?,it’s bad practice to have code that behaves differently depending on whether it’s packaged or not,4544
6435,What is the name of the package that your code is imported from?,a torch.package,4544
6436,What does Package Exporter offer that allows one to save arbitrary Python source code to a module of your choosing?,asave_source_string()method,4707
6437,What does Package Importerimplement for accessing resources from inside a package?,The import lib.resources API,4707
6438,What allows access to resources from within packaged code?,The import lib.resources API,4707
6439,What is it possible to access from within packaged code?,parentPackage Importer instance,4707
6440,Is it bad practice to have code that does what depending on whether it's packaged or not?,behaves differently,8228
6441,What program allows you to save arbitrary Python source code to a module of your choosing?,Package Exporter,8228
6442,What allows access to resources from within a package?,The import lib.resources API,4706
6443,Why is theimportlib.resources the recommended way to access package contents from within packaged code?,complies with the Python standard,4711
6444,What does importlib.resources comply with?,Python standard,8123
6445,What is another way to access package contents from within packaged code?,parent Package Importer instance,8123
6446,What does the Importlib.resourcesAPI allow access to from within packaged code?,parent Package Importer instance,7118
6447,Why is usingimportlib.resources the recommended way to access package contents from within packaged code?,complies with the Python standard,7118
6448,What is used to tell if an object's code is from a torch.package?,the torch.package.is_from_package()function,7924
6449,"If an object is from a package but its definition is from a module markedexternor fromstdlib, what will this check return",returns False,7924
6450,What must the newPackage Exporter aware of the originalPackage Importers find for your object's dependencies?,source code,7910
6451,Who previously imported an object?,a Package Importer,7910
6452,What must the newPackage Exporteraware of the originalPackage Importers find for your object’s dependencies?,source code,7910
6453,What do you use to package a Torch Script model?,same save_pickle and load_pickle APIs,7908
6454,What is the benefit of saving Torch Script objects that are attributes or submodules?,no extra work,7908
6455,How is saving Torch Script objects that are attributes or submodules supported?,no extra work,7911
6456,What type of function should be defined for the class?,de-packaging,2088
6457,What should the de-packaging function do?,reconstruct and return an instance of the class,2088
6458,What should be the first parameter of a de-packaging function?,a Package Importer instance,2088
6459,What package offers asave_source_string() method?,Package Exporter,4705
6460,What does Package Exporter offer?,asave_source_string()method,4705
6461,What is a ZIP archive that uses the.ptextension?,a torch.packagefile,8778
6462,What kind of files are placed in the.data/. User files?,Framework files,8778
6463,What is an example of a fully packaged ResNet model?,fully packaged ResNet model fromtorchvision,8778
6464,What type of files are placed in the.data/. User files?,Framework files,8778
6465,What is the name of everything else inside the a torch.packagefile?,User files,8778
6466,What does a fully packaged fromtorchvision look like?,ResNet model,8778
6467,What is the name of everything else in the a torch.packagefile?,User files,8779
6468,What is placed in the.data/?,Framework files,2602
6469,What is the name of everything else in a ResNet model?,User files,2602
6470,Who makes no guarantees about the contents of.data/?,Thetorch.packageformat,2602
6471,What is stored in *.storage?,serialized tensor data,2602
6472,What files are placed in the.data/?,Framework files,2602
6473,Who owns the.data/directory of a fully packaged ResNet model?,torch.package,2602
6474,What is the name of everything else in Python?,User files,2602
6475,The layout is identical to what?,Pythonregular package,2602
6476,Who owns the data/directory of a fully packaged ResNet model?,torch.package,8780
6477,What should you do if you want to get a deeper understanding of how Python packaging works?,double-check implementation details with thePython reference documentation,8780
6478,What is stored in the *.storage?,serialized tensor data,8780
6479,The layout of a torch.packagefile is identical to what?,Pythonregular package,8780
6480,What are placed in the.data/?,Framework files,2600
6481,What is the name of everything else in PyTorch?,User files,2600
6482,What makes no guarantees about the contents of.data/?,Thetorch.packageformat,7369
6483,What is the version number for the serialized format?,version,7369
6484,What is serialized tensor data?,*.storage,7369
6485,What does thetorch.packageimport use to import externalmodules?,the loading environment’s system importer,7369
6486,What is everything else in a ResNet model?,User files,8096
6487,Who owns the.data/directory?,torch.package,7370
6488,What does *.storage contain?,serialized tensor data,7370
6489,What is the layout of a Pythonregular package?,identical,7370
6490,What currently contains the following items?,the.data/directory,7368
6491,What contains the following items?,the.data/directory,7368
6492,Who owns The.data/directory?,torch.package,1294
6493,What is used to import externalmodules?,the loading environment’s system importer,1294
6494,Who knows how to load a package?,thetorch.packageimport infrastructures,2601
6495,What is everything else in the.data/ directory?,User files,2601
6496,What is a version number for?,serialized format,8781
6497,What does Package Exporter use to pickle the object?,a save_pickle,8781
6498,"For more information about pickling and the pickle format, please consult what?",Python docs,8781
6499,a torch.packagefile is a what?,ZIP archive,8781
6500,What are the two types of files inside the ZIP archive?,Framework files,8781
6501,What is everything else in the ZIP archive?,User files,8781
6502,Who put all other files in the archive in a torch.packagefile?,a user,8781
6503,What will be used to import externmodules?,loading environment’s system importer,9305
6504,What type of data is stored in extern_modules?,serialized tensor data,9305
6505,How does the layout compare to a Pythonregular package?,identical,9305
6506,Where can you find out more about how Python packaging works?,Python reference documentation,9305
6507,What is a version number for the serialized format?,version number,11347
6508,Is this essay up to date or out of date?,slightly out of date,11347
6509,What automatically finds the Python modules that your code and objects depend on?,torch.package,11347
6510,What is the purpose of this essay?,a deeper dive in how Python packaging works,11347
6511,What describes where to find the implementation of the object’s type?,a GLOBAL opcode,11347
6512,"Which package will pickle the object normally when you issue a save_pickle(obj,...)call?",Package Exporter,11347
6513,Which package automatically finds the Python modules that your code and objects depend on?,torch.package,11347
6514,What must you do for each module that the dependency resolver finds?,specify an action to take,11347
6515,Who put all other files in the archive?,a user,1089
6516,How does the layout of the archive compare to a Pythonregular package?,identical,1089
6517,What should you do if you want to get a deeper dive into how Python packaging works?,double-check implementation details with thePython reference documentation,1089
6518,What is identical to a Pythonregular package?,The layout,1089
6519,Why should you consult this essay?,it’s slightly out of date,1089
6520,What does Package Exporter use to parse the pickle bytecode?,thepickletoolsstandard library module,8395
6521,"When you issue what call,Package Exporter will pickle the object normally?",a save_pickle,8395
6522,What describes where to find the implementation of the object's type?,a GLOBAL opcode,8395
6523,Where can you find more information about pickling and the pickle format?,Python docs,8395
6524,"Who will pickle the object when you issue a save_pickle(obj,...)call?",Package Exporter,8395
6525,Why is this essay slightly out of date?,double-check implementation details with thePython reference documentation,11346
6526,"Who will pickle the object normally when you issue a save_pickle(obj,...)call?",Package Exporter,9307
6527,What is a list of modules that are consideredextern:class:Package Importer.externmodules?,module,9307
6528,Where can you check the implementation details of a Python package?,Python reference documentation,9307
6529,What is the name of the module that the dependency resolver finds?,intern,9307
6530,What will gather up allGLOBALops and mark them as dependencies of your pickled object?,dependency resolver,3691
6531,The dependency resolver will mark allGLOBALops as what?,dependencies,7024
6532,What does the dependency resolver mark allGLOBALops as?,dependencies of your pickled object,7026
6533,What must you specify for each module that the dependency resolver finds?,an action to take,7026
6534,What action is allowed to put a module into the package?,intern,7026
6535,"Patterns can be either module names (""foo.bar"") or what?",globs,7026
6536,What has limited support for the__import__(...)syntax and does not supportimportlib.import_module,AST parsing,7026
6537,What is the process that torch.packageautomatically finds the Python modules that your code and objects depend on?,dependency resolution,7026
6538,What should not be detected bytorch.package?,dynamic imports,7026
6539,"When a Python module is identified as a dependency,torch.packagewalks the module’s what representation?",python AST,7026
6540,What is the action that removes or changes dependencies in your code that is not technically part oftorch.package?,Refactoring,7026
6541,There is no way to package “just” a function or class from what?,module,7026
6542,What action is allowed to declare a module as an external dependency of the package?,extern,7026
6543,What are only defined on entire Python modules?,actions,7026
6544,What does torch.packagewalk when a module is identified as a dependency?,python AST representation,8340
6545,What walking way is used to parse dependencies?,AST,8340
6546,What has limited support for the__import__(...)syntax?,AST parsing,8340
6547,Where should you not expect dynamic imports to be detected?,bytorch.package,8340
6548,AST parsing has limited support for what?,the__import__(...)syntax,8340
6549,What are some examples of standard import statements?,"fromximporty,importz,fromwimportvasu",8340
6550,What representation does torch.packagewalk when a module is identified as a dependency?,python AST,8339
6551,In what way are the imported modules parsed?,AST walking,8339
6552,What are some of the standard forms that are supported by Torch.package?,"fromximporty,importz,fromwimportvasu",8339
6553,What is the process called that finds the Python modules that your code and objects depend on?,dependency resolution,11201
6554,What is an action that the dependency resolver can take?,declare this module as an external dependency of the package,11201
6555,What are actions defined on?,actions are only defined on entire Python modules,11201
6556,Is there a way to package just a function or class from module and leave the rest out?,There is no way to package “just” a function or class from module and leave the rest out,11201
6557,What are the allowed actions?,intern: put this module into the package,4520
6558,What finds the Python modules that your code and objects depend on?,torch.package,11200
6559,What will depending on this module do during package export?,raise an error,11200
6560,What is the name of the module that is put into the package?,intern,9687
6561,What is the name of the module that stubs out?,mock,9687
6562,What does extern do?,declare this module as an external dependency of the package,9304
6563,What is another name for stub out a module?,mock,9304
6564,What will depend on this module raise during package export?,an error,9304
6565,What is the only defined unit of dependency organization in Python?,module,9304
6566,What will dependencies on a module do during package export?,raise an error,9304
6567,Python does not offer clean boundaries between objects defined in what?,module,9304
6568,What action removes or changes dependencies in your code?,Refactoring,9304
6569,What is the name of the action that puts a module into the package?,intern,6960
6570,Why is there no way to package just a function or class from module and leave the rest out?,by design,6960
6571,What action stubs out a module?,mock,6960
6572,What does extern declare this module as?,external dependency,9303
6573,What is the action that removes or changes dependencies in your code?,Refactoring,9303
6574,What will dependencies on this module do during package export?,raise an error,9303
6575,What do you do to declare this module as an external dependency of the package?,declare this module as an external dependency of the package,9303
6576,What action stubs out the module?,mock,6959
6577,What action declares this module as an external dependency of the package?,extern,6959
6578,What action is allowed to stub out a module?,mock,6959
6579,What is the action that removes or changes dependencies in your code that is not technically part of oftorch?,Refactoring,9918
6580,What will raise an error during depending on this module?,package export,9918
6581,What does intern do?,put this module into the package,9686
6582,What is the name of the module that is declared as an external dependency of the package?,extern,3693
6583,"When a Python module is identified as a dependency,torch.packagewalks the module's what representation?",python AST,3693
6584,What is the name of the action you can take to put a module into the package?,intern,3693
6585,What is another action that is not technically part of of torch?,Refactoring,9156
6586,What will dependencies do during package export?,raise an error,9156
6587,What is the action that removes or changes dependencies in your code that is not technically part of oftorch.package?,Refactoring,9919
6588,Is there a way to package a function or class from a module and leave the rest out?,There is no way to package “just” a function or class from module and leave the rest out,9919
6589,What is the only defined unit of dependency organization?,a module,9919
6590,Actions are only defined on what?,entire Python modules,9919
6591,How do you associate a pattern with an action?,methods onPackage Importer,9919
6592,Actions are applied to modules using what?,patterns,9919
6593,Actions are defined on what?,entire Python modules,5115
6594,Why is there no way to package just a function or class from a module and leave out the rest?,Python does not offer clean boundaries between objects defined in a module,5115
6595,What is the term for removing or changing dependencies in your code?,Refactoring,5115
6596,What is refactoring?,remove or change the dependencies in your code,2437
6597,Why is there no way to package “just” a function or class from module and leave the rest out?,by design,2438
6598,Python does not offer what between objects defined in a module?,clean boundaries,2438
6599,Actions are only defined on entire what?,Python modules,2438
6600,Is there a way to package “just” a function or class from module and leave the rest out?,no way to package “just” a function or class from module and leave the rest out,2438
6601,What does this ensure?,eachPackage Importeris isolated from the loading environment,2438
6602,Refactoring is what action that is not technically part of oftorch.package?,remove or change the dependencies in your code,2438
6603,"When patterns are checked in the order that they were defined, what action is taken?",first action,2438
6604,Who will look inside your package for an intern-ed module when your packaged code tries to import an intern-ed module?,Package Importer,2438
6605,What is important to note about actions in Python?,actions are only defined on entire Python modules,4459
6606,What is the reason that Python does not offer clean boundaries between objects defined in a module?,by design,4459
6607,What does Python not offer clean boundaries between objects defined in a module?,Python does not offer clean boundaries between objects defined in a module,4459
6608,What are actions applied to modules using?,patterns,925
6609,What action is taken if a module matches a pattern?,first action,925
6610,What is the name of the module you want to package from Torchvision?,torchvision.models.resnet,925
6611,What happens when a module is placed into the package?,a module is intern-ed,925
6612,What is included in a package if a module is interned?,model code,925
6613,"If a module matches a pattern, what is applied to it?",corresponding action,926
6614,What is an example of a module name?,foo.bar,926
6615,"When patterns are checked in the order that they were defined, what action will be taken?",first,926
6616,"If a module is intern-ed, it will be placed into the package. This action is what?","your model code, or any related code you want to package",926
6617,"If a module is extern-ed, it will not be packaged. Instead, it will be added to what?",list of external dependencies,926
6618,"If a module is intern-ed, what happens?",it will be placed into the package,926
6619,This ensures that what does eachPackage Importeris isolated from?,eachPackage Importeris isolated from the loading environment,926
6620,"When time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find",package import,926
6621,What happens if a package import fails to find a module?,an error will be raised,926
6622,What can you depend on?,third-party libraries,926
6623,"If a module is intern-ed, it will be what?",placed into the package,924
6624,"If a module matches a pattern, the corresponding action is applied to it?",a module matches a pattern,3389
6625,What will be checked in the order that they were defined?,patterns,3389
6626,What happens if a module is extern-ed?,it will not be packaged,3389
6627,When does Package Importer use the default Python importer to find a module?,when time packaged code tries to import anextern-ed module,3389
6628,What can be interned?,Python source modules,3389
6629,What happens if a module is intern-ed?,it will be placed into the package,3389
6630,What happens if a module is interned?,it will be placed into the package,3382
6631,What happens when a module is placed into a package?,module is intern-ed,3382
6632,What will be placed into the package if a module is interned?,your model code,3382
6633,What would you need to do if you were trying to package a ResNet from Torchvision?,to intern the module torchvision.models.resnet,3382
6634,What is placed into the package when a module is interned?,model code,3382
6635,What is the action that is placed into the package?,your model code,3388
6636,What is applied if a module matches a pattern?,corresponding action,3388
6637,"If a module is interned, it will be what?",placed into the package,3388
6638,What is included in the package if a module is interned?,model code,3388
6639,What would you need to package a ResNet from Torchvision?,to intern the module torchvision.models.resnet,3383
6640,What is the only source module that can be interned?,Python,3383
6641,What is the action that you want to package?,model code,7459
6642,What will you need if you are trying to package a ResNet fromtorchvision?,to intern the module torchvision.models.resnet,7459
6643,What is the name of the module you need to package a ResNet fromtorchvision?,torchvision.models.resnet,7459
6644,Who will look inside your package for an intern-ed module?,Package Importer,4629
6645,What can be intern-ed?,Python source modules,4629
6646,Where can you find a list of external dependencies for this package?,on package_exporter.extern_modules,4629
6647,Does this ensure that eachPackage Importer is isolated from the loading environment?,eachPackage Importeris isolated from the loading environment,4629
6648,"If a module is mock-ed, it will not be packaged. Instead a what will be packaged in its place?",stub module,4629
6649,What module will allow you to retrieve objects from it?,stub module,4629
6650,"If it can't find that module, what will happen?",an error will be raised,4629
6651,What should be used for code that you “know” will not be needed in the loaded package?,mockshould be used for code that you “know” will not be needed in the loaded package,4629
6652,What is an example of code that you “know” will not be needed in the loaded package?,initialization/configuration code,4629
6653,What will happen if Package Importer can't find that module?,an error will be raised,7460
6654,What do C extension modules and bytecode modules need to do?,be mock-ed or extern-ed,7460
6655,What module will you need to package a ResNet fromtorchvision?,torchvision.models.resnet,7460
6656,Only what can be intern-ed?,Python source modules,7460
6657,Package Importer will use what to find anextern-ed module?,default Python importer,7460
6658,What is the action called?,"your model code, or any related code you want to package",7460
6659,What will Package Importer only use if you have my_interned_moduleavailable in both your package and the loading environment,version in your package,4628
6660,What is the only source module that can be intern-ed?,Python,4526
6661,Where can you find a list of external dependencies for a module?,on package_exporter.extern_modules,4526
6662,What other types of modules will raise an error if you attempt to intern them?,C extension modules and bytecode modules,4527
6663,What are two examples of modules that will raise an error if you attempt tointern?,C extension modules and bytecode modules,4527
6664,What do C extension modules and bytecode modules need to be?,be mock-ed or extern-ed,4525
6665,What are two examples of modules that will raise an error if you try tointern them?,C extension modules and bytecode modules,4525
6666,What do C extension modules and bytecode modules need to do to be interned?,be mock-ed or extern-ed,4525
6667,"If a module is extern-ed, it will be added to what list for this package?",external dependencies,3380
6668,Where can you find the list of external dependencies for a package?,on package_exporter.extern_modules,3380
6669,What will use the default Python importer to find anextern-ed module?,Package Importer,3380
6670,What can you depend on within your package without having to package them too?,third-party libraries,3380
6671,What happens if an external library changes in a backwards-incompatible way?,your package may fail to load,3380
6672,What should you do if you need long-term reproducibility for your package?,limit your use of extern,3380
6673,What will be raised if Package Importer can't find a module?,an error,3380
6674,"If you need long-term reproducibility for your package, try to what?",limit your use of extern,3380
6675,What does Package Importer use to find anextern-ed module?,Python importer,3380
6676,"If a module is extern-ed, it will be added to a list of what?",external dependencies,3379
6677,What will be raised if Package Importer can't find that module?,an error,3381
6678,What language does Package Importer use to find anextern-ed module?,Python,3381
6679,Where can you find a list of external dependencies for a package?,package_exporter.extern_modules,3381
6680,What is not smart enough to tell that unused imports are indeed unused?,The dependency resolver,3381
6681,"If a module is not packaged, it will not be packaged. Instead a stub module will be packaged in its place.",is mock-ed,3381
6682,What can you depend on without having to package them?,third-party libraries,4625
6683,What can you do without having to package them?,depend on third-party libraries likenumpyandscipy,4626
6684,What kind of way does an external library change?,backwards-incompatible,3807
6685,What should you do if you want long-term reproducibility for your package?,limit your use of extern,3807
6686,What will not be packaged if a stub module is used?,module is mock-ed,8225
6687,What will be packaged in its place if a module is mock-ed?,stub module,3386
6688,What will any use of a stub module raise?,a NotImplementedError,3386
6689,What happens if a module is mock-ed?,it will not be packaged,3386
6690,"What should be used for code that you ""know"" won't be needed in the loaded package?",mock,3386
6691,What should mock be used for?,last resort,3386
6692,What should you refactor your code to do?,remove unwanted dependencies,3386
6693,"What should be used for code that you ""know"" will not be needed in the loaded package?",mock,3386
6694,What does mock introduce between packaged code and non-packaged code?,behavioral differences,3386
6695,What should you do to remove unwanted dependencies?,refactor,3386
6696,"What is code that you ""know"" will not be needed in the loaded package but still want to available for use in non-packaged contents?",initialization/configuration code,3386
6697,What should mock be used as?,last resort,3386
6698,"If a module is not packaged, a stub module will be packaged in its place.",module is mock-ed,3385
6699,What is code only used for?,debugging/training,3808
6700,What does mock introduce between packaged and non-packaged code?,behavioral differences,9921
6701,What does the writefromfoo.barimportbaz specify?,real dependency,9921
6702,What allows you to specify groups of modules with a convenient syntax?,Patterns,9921
6703,What is a module that we are trying to match against a pattern called?,a candidate,9921
6704,What is a candidate composed of?,a list of segments separated by a separator string,9921
6705,What does a pattern contain?,one or more segments,9921
6706,Segments can be what?,literal string,9921
6707,What is a good way to break up large files with unrelated functionality into smaller ones?,Split up large files with unrelated functionality into smaller ones,9921
6708,What does yourutilsmodule contain?,a hodge-podge of unrelated functionality,9921
6709,What type of modules can be packaged independently of one another?,single-purpose modules,9921
6710,What is a code that you know will not be needed in the loaded package?,initialization/configuration code,9921
6711,What should we not do with unused imports in our code?,Do not leave unused imports in our code,9921
6712,What should you do with your imports?,Qualify,9921
6713,"What is an example of code that you ""know"" will not be needed in the loaded package?",initialization/configuration code,9920
6714,What is the best way to manage dependencies?,not have dependencies at all,6974
6715,What can code be refactored to do?,remove unnecessary dependencies,6974
6716,What are some guidelines for writing code with clean dependencies?,Include only what you use,6974
6717,Who will try to process unused imports?,The dependency resolver,6974
6718,What are generally good practices for writing code with?,clean dependencies,6974
6719,"Instead of writing import foo and later usingfoo.bar.baz, what do you prefer?",writefromfoo.barimportbaz,6974
6720,What is your real dependency?,foo.bar,6974
6721,What type of unrelated functionality does yourutilsmodule contain?,hodge-podge,6974
6722,What type of modules can be packaged independently of each other?,single-purpose modules,6974
6723,What should you do instead of splitting up large files with unrelated dependencies?,define single-purpose modules,6974
6724,What should we not leave in our code?,Do not leave unused imports,6974
6725,What is a good practice for writing code with clean dependencies?,Do not leave unused imports in our code,6974
6726,What are clean dependencies generally considered?,good practices,6973
6727,"Instead of writing import foo and later usingfoo.bar.baz, prefer to write import foo and later using what?",writefromfoo.barimportbaz,6973
6728,Do not leave what in our code?,unused imports,8227
6729,The syntax and behavior of patterns follows what?,Bazel/Buckglob(),8227
6730,What matches against zero or more complete segments?,double wildcard,8227
6731,A pattern contains what?,one or more segments,8227
6732,What does the dependency resolver need to do to process unused imports?,Qualify your imports,8227
6733,What is a better way to write import foo and later usefoo.bar.baz?,writefromfoo.barimportbaz,8227
6734,What is one way to break up large files with unrelated functionality into smaller ones?,Split up large files,8227
6735,What contains a hodge-podge of unrelated functionality?,yourutilsmodule,8227
6736,Prefer to define what type of modules that can be packaged independently of one another?,single-purpose modules,8227
6737,Patterns allow you to specify groups of modules with what?,convenient syntax,8227
6738,Segments can be: A what?,literal string,8227
6739,"What character matches any string, including the empty string?",wildcard,8227
6740,"The wildcard matches any string, including what?",empty string,8227
6741,A double wildcard (**) matches against what?,zero or more complete segments,8227
6742,What is an example of a double wildcard that matches against zero or more complete segments?,torch,8227
6743,What does a ** match?,matchestorchand all its submodules,8227
6744,Who is not smart enough to tell that unused imports are indeed unused?,The dependency resolver,8226
6745,What should be used as a last resort?,mock,8226
6746,What does the dependency resolver do?,Include only what you use,3833
6747,Do you leave unused imports in our code?,Do not leave unused imports in our code,3834
6748,What does the dependency resolver do to your imports?,Qualify,3834
6749,What do you only include in your code?,what you use,3834
6750,What does the dependency resolver not know about unused imports?,Do not leave unused imports,3834
6751,How do you split up large files with unrelated functionality into smaller ones?,Split up large files with unrelated functionality into smaller ones,3834
6752,"Instead of writing import foo and later usingfoo.bar.baz, what would you prefer to do?",writefromfoo.barimportbaz,3809
6753,What can you depend on from within your package without having to package them too?,third-party libraries,3809
6754,What does the dependency resolver do to make sure you don't leave unused imports in your code?,Qualify your imports,3809
6755,Qualify what?,imports,5047
6756,What is another name for imports?,Qualify your imports,5047
6757,What can be: a list of segments separated by a separator string?,Segments,5047
6758,What is a better way to write import foo?,writefromfoo.barimportbaz,5046
6759,Who knows you don't need all offoo?,the dependency resolver,5046
6760,What does writefromfoo.barimportbaz do?,lets the dependency resolver know you don’t need all offoo,5046
6761,"Instead of writing import foo and later usingfoo.bar.baz, prefer what?",writefromfoo.barimportbaz,4627
6762,"If any external library changes in a what way, your package may fail to load?",backwards-incompatible,4627
6763,What can you depend on without having to package them too?,third-party libraries,4627
6764,What does Package Importer do when a package imports anextern-ed module?,Package Importerwill use the default Python importer to find that module,4627
6765,What happens if the package importer can't find that module?,an error will be raised,4627
6766,What should be used for code that you “know” won't be needed in the loaded package?,mock,4627
6767,"Code that you “know” will not be needed in the loaded package, but you still want to available for use in non-packaged contents. For",initialization/configuration code,4627
6768,What are some guidelines for writing code with?,clean dependencies,4627
6769,"Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try",Include only what you use,4627
6770,"What is not smart enough to tell that unused imports are indeed unused, and will try to process them?",dependency resolver,4627
6771,What should you do to avoid unused imports?,Qualify your imports,4627
6772,What should you not expect dynamic imports to be detected?,bytorch.package,4519
6773,What parsing has limited support for the__import__(...)syntax?,AST,4519
6774,"In general, you should not expect dynamic imports to be detected what?",bytorch.package,8397
6775,"When a Python module is identified as a dependency,torch.packagewalks what module's AST representation?",python,8397
6776,What is one way to split up large files with unrelated functionality?,Split up large files with unrelated functionality into smaller ones,6018
6777,"If yourutilsmodule contains a hodge-podge of unrelated functionality, what will any module that depends onutils need to",pull in lots of unrelated dependencies,6018
6778,What is one way to split up large files with unrelated functionality into smaller ones?,Split up large files with unrelated functionality into smaller ones,6019
6779,What is the syntax and behavior of patterns?,follows the Bazel/Buckglob(),6019
6780,"What matches any string, including the empty string?",wildcard,6019
6781,What does the wildcard match against?,"any string, including the empty string",6019
6782,What should you do instead of pulling in lots of unrelated dependencies?,define single-purpose modules that can be packaged independently of one another,6019
6783,What does a double wildcard match against?,matchestorchand all its submodules,813
6784,What does the wildcard match?,"any string, including the empty string",813
6785,What is an example of a double wildcard?,torch,813
6786,What is another name for ortorch torch?,nottorch.nn.functional,813
6787,"What type of string matches any string, including the empty string?",wildcard,813
6788,"When specifying actions, you can pass what patterns?",multiple,813
6789,What is the name of the module that matchestorchand all its submodules?,torch,813
6790,How will a module match against an action?,if it matches any of the patterns,813
6791,What can you specify patterns to?,exlcude,813
6792,What type of string matches exactly?,literal string,832
6793,What does a string contain?,wildcard,832
6794,A double wildcard matches against what number of complete segments?,zero,832
6795,What matches all of torch's submodules?,matchestorch.nnortorch.functional,832
6796,What is another name for ortorch?,nottorch.nn.functional,832
6797,What is the name of the submodule that matchestorchand all its submodules?,torch,4754
6798,"When specifying actions, you can pass multiple patterns, e.g. a module will match against what action?",if it matches any of the patterns,4754
6799,What is a string containing?,wildcard,853
6800,What does a double wildcard do?,matches against zero or more complete segments,795
6801,What is a double wildcard?,matchestorchand all its submodules,795
6802,"When specifying actions, you can pass what?",multiple patterns,795
6803,What type of wildcard matches against zero or more complete segments?,double wildcard,795
6804,What submodule is matchestorchand all its submodules?,torch,795
6805,How does a module match against an action?,if it matches any of the patterns,795
6806,What can you specify to exlcude?,patterns,795
6807,What will any module that depends onutils need to do if yourutilsmodule contains a hodge-podge of unrelated,pull in lots of unrelated dependencies,6017
6808,What can be packaged independently of one another?,define single-purpose modules,6017
6809,What makes it really easy to bind objects and run code at module-level scope?,Python,5013
6810,What are bound to names in Python?,functions and classes,5013
6811,What is introduced when you define an object at module scope with the intention of mutating it?,mutable global state,5013
6812,Mutable global state can cause complications when used what?,withtorch.package,5013
6813,What creates an independent environment for its contents?,EveryPackage Importer,5013
6814,What can happen when modules are written in a way that assumes shared mutable global state?,hard-to-debug errors,5013
6815,What language makes it really easy to bind objects and run code at module-level scope?,Python,5013
6816,What can mutable global state cause when used withtorch.package?,it can cause complications,5013
6817,What are some benefits of mutable global state?,"it can reduce boilerplate, allow for open registration into tables",5013
6818,Python makes it really easy to bind objects and run code at what?,module-level scope,5012
6819,"What can reduce boilerplate, allow for open registration into tables, etc.?",Mutable global state,4283
6820,Mutable global state can cause what when used withtorch.package?,complications,4282
6821,"When modules are written in a way that assumes shared mutable global state, what can this behavior create?",hard-to-debug errors,2267
6822,What is an example of a class that you import from a Package Importer?,MyClassandimport_MyClassarenot the same type,2267
6823,What happens if Import_MyClassis coming from an older package with an entirely different implementation of MyClass?,it’s unsafe to consider them the same class,2267
6824,What does each importer have that allows it to uniquely identify classes?,a prefix,2267
6825,Any class that you import from a Package Importerwill be what?,a version of the class specific to that importer,5014
6826,What is quite useful when you define an object at module scope with the intention of mutating it?,Mutable global state,5014
6827,What are MyClassandimport_MyClassnot?,same type,3804
6828,What might you think it's okay to consider MyClassandimport_MyClass?,the same class,3804
6829,What does each importer have under the hood?,each importer has a prefix that allows it to uniquely identify classes,3804
6830,What is it called when you need to use a class instead of checking that it is of a given type?,duck typing,6891
6831,What should not work when one of the arguments if from a package and the other is not?,expectisinstancechecks,6891
6832,What is another option if you need the functionality of isinstancechecks?,duck typing,6891
6833,What is a way to use class instead of explicitly checking that it is of a given type?,duck typing,8048
6834,What is done by using the class instead of explicitly checking that it is of a given type?,duck typing,2190
6835,What can you do to make the typing relationship an explicit part of the class contract?,Make the typing relationship an explicit part of the class contract,2190
6836,What attribute can be added to a class contract?,tagself.handler,2190
6837,"What creates an independent, isolated environment for its modules and objects?",EachPackage Importerinstance,2240
6838,What does Package Importer implement?,theimport_moduleand__import__methods,2240
6839,What does Package Importer provide the same core API as?,the import libimporter,2240
6840,How does thatPackage Importerinstance fulfill future import requests?,by looking in the package rather than searching the user’s Python environment,2240
6841,What does eachPackage Importerinstance do?,"Modules in a package can only import other packaged modules, or modules markedextern",2239
6842,How many Package Importerinstances can be used to load a single package?,multiple,2239
6843,What can only import other packaged modules?,Modules in a package,2239
6844,"If you use what to load a single package, you will get multiple independent environments that do not interact?",multiple Package Importer instances,2241
6845,What does eachPackage Importerinstance create?,"Modules in a package can only import other packaged modules, or modules markedextern",2241
6846,What happens when you invokePackage Importer.import_module()?,Package Importerwill construct and return a new module,2241
6847,What does thatPackage Importerinstancepatches the returned module to?,use self,2241
6848,What is used to extend Python's import infrastructure?,custom importer,2241
6849,How does the module use self(i.e. thatPackage Importerinstance) to fulfill future import requests?,looking in the package,2241
6850,What is a name that becomes torch_package_0>.torchvision.models.resnet18,liketorchvision.models.resnet18,2241
6851,What is the name of a file that becomestorch_package_0>.torchvision/modules/re,liketorchvision/models/resnet18.py,2241
6852,Where can developer-facing details about mangling be found?,consultmangling.mdintorch/package/,2241
6853,What does.import_module() invoke?,Package Importer,8393
6854,What does Package Importerpatches the returned module to?,use self,8393
6855,What becomestorch_package_0>.torchvision/modules/resnet18.py?,liketorchvision/models/resnet18.py,8393
6856,What does the module use self(i.e. thatPackage Importerinstance) do instead of searching the user’s Python environment?,looking in the package,8393
6857,What is a name for a module?,liketorchvision.models.resnet18,8393
6858,How does the returned module fulfill future import requests?,by looking in the package,8392
6859,What prefix does Package Importermangles add to the__name__and__file__ of all imported modules?,a mangle,7863
6860,What is the name of a package that becomestorch_package_0>.torchvision.models.res,liketorchvision.models.resnet18,7864
6861,Where can you find developer-facing details about mangling?,consultmangling.mdintorch/package/,7864
6862,What prefix does Package Importermangles add to all imported modules?,a mangle,7864
6863,What becomes torch_package_0>.torchvision/modules/resnet18.py?,liketorchvision/models/resnet18.py,7864
6864,What helps avoid inadvertent punning of module names between different packages?,Name mangling,7864
6865,What is a name that becomestorch_package_0>.torchvision.models.resnet18?,liketorchvision.models.resnet18,2587
6866,What does name mangling help avoid?,inadvertent punning,2586
6867,What is a name for__file__?,liketorchvision/models/resnet18.py,2586
6868,What helps you debug by making it easier to see if a package is referring to a package or not?,stack traces and print statements,2586
6869,When is this exception raised?,when there is an issue with exporting a package,7503
6870,"Exporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into what?",self-contained package,7503
6871,What is the file format of a PyTorch model code?,specially organized zip file,7503
6872,When a mock or extern is marked as what?,allow_empty=False,7502
6873,What type of data can be picked up by exporters?,Python,7502
6874,What is marked when a mock or extern is not matched with any module during packaging?,as allow_empty=False,7624
6875,"Exporters allow you to write packages of code, pickled Python data, and what else into a self-contained package?",arbitrary binary and text resources,7624
6876,When is this exception thrown?,when a mock or extern is marked as allow_empty=False,7625
6877,Imports can load code in what way?,hermetic,3685
6878,What can PyTorch model code be used for in the future?,transfer learning,3685
6879,What does this allow for the packaging of?,PyTorch model code and data,3686
6880,What can load code in a hermetic way?,Imports,3686
6881,What is the file format of PyTorch code?,specially organized zip file,3686
6882,When does a package run locally because it is importing a locally-installed package fail?,when the package is copied to another machine,3686
6883,The code contained in packages is copied file-by-file from what when it is created?,the original source,6985
6884,What can future users of a package do to modify the code?,unzip,6985
6885,What can future users of a package do with the code contained in a package?,unzip,6985
6886,What is the file format of a package?,specially organized zip file,6987
6887,"If a single Importer is passed, use that to search for modules.",importer,6987
6888,Who can optionally scan source code when source code is added to the package?,the exporter,6987
6889,What can future users of a package do to edit the code in order to perform custom modifications to it?,unzip,6987
6890,The importer for packages ensures that code in what module can only be loaded from within the package?,module can only be loaded from within the package,6987
6891,What is stdout useful for tracking down?,why certain files get included,6987
6892,What does the exporter do to a package?,Write the package to the filesystem,6987
6893,What are modules explicitly listed as?,external usingextern(),7120
6894,What lists all the modules that a package externally depends on?,fileextern_modulesin the zip archive,7120
6895,Who ensures that code in the module can only be loaded from within the package?,The importer,7120
6896,What is the name of the location to export to?,f– The location to export to,7120
6897,What does the fileextern_modulesin the zip archive list?,all the modules that a package externally depends on,7120
6898,Why does the fileextern_modulesin the zip archive prevent “implicit” dependencies where the package runs locally?,it is importing a locally-installed package,7120
6899,What happens when a package is copied to another machine?,fails,7119
6900,What in the zip archive lists all the modules that a package externally depends on?,fileextern_modules,7119
6901,What does the exporter do when source code is added to a package?,dependencies=True,8376
6902,"When an exporter adds what to a package, it can optionally scan it for further code dependencies?",source code,8376
6903,What does the exporter look for?,import statements,8377
6904,What does f stand for?,The location to export to,8377
6905,What does the exporter do when source code is added to the package?,dependencies=True,8377
6906,What can the location to export to be?,a string/Path object,8377
6907,What will be constructed out of a sequence of importers if a sequence of importers are passedsed?,an Ordered Importer,8377
6908,"If an importer is passed, use that to search for modules. If a sequence of importers are passedsed, anOrderedIm",a single Importer,8377
6909,What can be the location to export to?,a string/Path object,1952
6910,What is the first step in creating an exporter?,Create an exporter,1952
6911,What can an importer search for if a single Importer is passed?,modules,1952
6912,What does an exporter do?,Create an exporter,1952
6913,What type of object can contain a filename or a binary I/O object?,a string/Path object,9437
6914,Where can verbose print information about dependency resolution?,stdout,9437
6915,Print information about dependency resolution to stdout. Useful for tracking down why certain files get included?,verbose,9437
6916,"If a single Importer is passed, use that to search for what?",modules,9437
6917,What can be a filename or a binary I/O object?,a string/Path object,9437
6918,What is verbose useful for?,tracking down why certain files get included,11343
6919,Where does verbose print information about dependency resolution?,stdout,11343
6920,Any calls to what function are now invalid?,afterclose(),11343
6921,What does verbose do?,Write the package to the filesystem,11343
6922,What does the package do?,Write the package to the filesystem,11343
6923,"What can include(Union[List[str],str]] be?",glob-style pattern,11343
6924,What happens to calls afterclose()?,Any calls afterclose()are now invalid,11343
6925,"If an importer is passed, use that to search for modules.",a single Importer,7121
6926,What is verbose useful for tracking down?,why certain files get included,7121
6927,Print information about dependency resolution to stdout. Useful for tracking down why certain files get included. Write the package to the filesystem.,verbose,7121
6928,What is preferable to blocklist modules who names match the given glob patterns from the list of modules the package can import?,resource guard syntax,7121
6929,"What can include(Union[List[str],str]] - A string for the names of the modules to be externed",glob-style pattern,7121
6930,"What is added to a package, the exporter can optionally scan it for further code dependencies?",source code,7121
6931,"If a sequence of importers are passed, what will be constructed out of them?",an Ordered Importer,7121
6932,Why does the fileextern_modules prevent “implicit” dependencies where the package runs locally?,it is importing a locally-installed package,7121
6933,What does the exporter look for when source code is added to the package?,import statements,7121
6934,What ensures that code in the module can only be loaded from within the package?,The importer,7121
6935,What does the importer do to a package?,Write the package to the filesystem,7121
6936,"If a sequence of importers are passedsed, what will be constructed out of them?",an Ordered Importer,9438
6937,"If a single Importer is passed, use that to search for what s?",module,9598
6938,What is a good way to track down why certain files get included?,Write the package to the filesystem,9598
6939,What does importer search for if a single Importer is passed?,module,9598
6940,What is a better way to write a package to the filesystem?,resource guard syntax,8445
6941,What does the package do to the filesystem?,Write the package,8445
6942,What do you do to the filesystem?,Write the package,11342
6943,What does verbose print information about dependency resolution to?,stdout,11342
6944,What is the best way to blocklist modules?,resource guard syntax,11342
6945,Print information about dependency resolution to stdout. Useful for tracking down why certain files get included.,verbose,11342
6946,What happens when a package is written to the filesystem?,Any calls afterclose()are now invalid,8447
6947,What does the package have to do?,Write the package to the filesystem,8447
6948,"If a dependency on any matching packages is found, what is raised?",aPackagingErroris,8447
6949,What is preferable to use instead of write the package to the filesystem?,resource guard syntax,8447
6950,What does Includemodulein do?,This will prevent dependency discovery from saving it in the package,8447
6951,What is preferable to use instead of afterclose()?,resource guard syntax,8446
6952,What is raised if a dependency on any matching packages is found?,aPackagingErroris,8446
6953,What happens if the package is written to the filesystem?,Any calls afterclose()are now invalid,8446
6954,What does Includemodulein prevent from saving in the package?,dependency discovery,8446
6955,What does the package need to do?,Write the package to the filesystem,8446
6956,What names match the given glob patterns from the list of modules the package can import?,Blocklist modules,1449
6957,What do the names of blocklist modules match from the list of modules the package can import?,glob patterns,1449
6958,"What does include(Union[List[str],str]) contain?",string,9604
6959,"What can include(Union[List[str],str]) be?",glob-style pattern,9604
6960,What is an optional pattern that excludes some patterns that match the include string?,exclude,9604
6961,"What is a string e.g. ""my_package.my_subpackage""?","include(Union[List[str],str])",9604
6962,When is an extern module glob pattern added?,with allow_empty=False,9604
6963,What is guaranteed to only be handed out once for this package?,Get an id,9604
6964,What type of pattern can be used to include a list of strings for the names of the modules to be externed?,glob-style pattern,9604
6965,"What is include(Union[List[str],str]) a string e.g. ""my_package.",list of strings for the names of the modules to be externed,9604
6966,"What can include(Union[List[str],str]] – A string e.g. ""my_package",glob-style pattern,9604
6967,What flag specifies whether the extern modules specified by this call to theexternmethod must be matched to some module during packaging?,allow_empty(bool),9604
6968,"If an extern module glob pattern is added, andclose() is called (either explicitly or via__exit__) before any modules",with allow_empty=False,9604
6969,"If an internmodule glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via",If allow_empty=True,9604
6970,What is another name for the names of the modules to be externed?,list of strings,6012
6971,What must a module match in order to be included in the package?,someinternpattern,6012
6972,What must be specified in order to be included in a package?,modules that should be packaged,6012
6973,What happens if an internmodule glob pattern is added with allow_empty=False?,an exception is thrown,6012
6974,What must be specified in order to be included in the package?,modules that should be packaged,6012
6975,"What is include(Union[List[str],str])?",list of strings for the names of the modules to be externed,8851
6976,What is a string e.g. “my_package.my_subpackage”?,"include(Union[List[str],str])",8851
6977,What flag specifies whether the intern modules specified by this call to theinternmethod must be matched to some module during packaging?,allow_empty(bool),8851
6978,Mocked modules will return what for any attribute accessed from it?,fake object,8851
6979,What are examples of files that are imported by model files but whose functionality is never used?,custom serialization code or training helpers,8851
6980,"If an extern module glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via_",If allow_empty=True,8851
6981,Use this function to do what without having to modify the original code?,mock this functionality out,8851
6982,"If an intern module glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via_",If allow_empty=True,8851
6983,"If an extern module glob pattern is added, andclose()is called (either explicitly or via__exit__) before any modules",with allow_empty=False,8851
6984,"Because we copy what, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used?",file-by-file,8851
6985,What function is used to mock modules that are imported by model files but whose functionality is never used?,"include(Union[List[str],str])",8851
6986,"What type of pattern can include(Union[List[str],str]) be?",glob-style pattern,9603
6987,"What is include(Union[List[str],str]) a string for the names of the modules to be externed?",list of strings,9601
6988,"What does exclude(Union[List[str],str],str]) match?",include string,9286
6989,What is the name of the optional pattern that excludes some patterns that match the include string?,'torch,9286
6990,"Include='torch.', exclude='torch.foo' will mock all torch packages except'torch",**,9286
6991,What is an example of an optional pattern that excludes some patterns that match the include string?,torch,9286
6992,What will mock all torch packages except'torch.foo'?,torch.foo,9286
6993,What is the name of the list of external modules the package can import?,Includemodulein,3836
6994,What will includemodulein prevent from saving it in the package?,dependency discovery,3836
6995,Who will load an external module directly from the standard import system?,The importer,3836
6996,What must also exist in the process loading the package?,Code,3836
6997,What will includemodulein prevent from saving in the package?,dependency discovery,3836
6998,"What does include(Union[List[str],str],str], or for the names of the modules to be externed",list of strings,3835
6999,"What can include(Union[List[str],str],str]) be?",glob-style pattern,3835
7000,What is an example of a glob-style pattern?,"include(Union[List[str],str])",3835
7001,Includemodulein will prevent what from saving it in the package?,dependency discovery,9287
7002,What does exclude match?,include string,9287
7003,"What can include(Union[List[str],str]] - A string e.g. ""my_package",glob-style pattern,9287
7004,What is an optional flag that specifies whether the extern modules specified by this call to theexternmethod must be matched to some module during packaging?,allow_empty,8850
7005,"If an extern module glob pattern is added with allow_empty=False, andclose()is called before any modules match that pattern,",If allow_empty=True,8850
7006,How many times is this id guaranteed to be handed out for this package?,once,8850
7007,What is thrown if an extern module glob pattern is added with allow_empty=False?,an exception,8850
7008,What does this id do?,Specify modules that should be packaged,8850
7009,How many times is an id guaranteed to be handed out for this package?,once,8849
7010,How many times is an id guaranteed to be handed out for a package?,once,2690
7011,What type of pattern is described inmock()?,glob-style pattern,2690
7012,How many times is an id guaranteed to be given out for a package?,once,2690
7013,What should be packaged?,modules,2690
7014,"What can include(Union[List[str],str]] – A string for the names of the modules to be externed",glob-style pattern,2690
7015,What must match someinternpattern in order to be included in the package?,modules that should be packaged,6011
7016,"What does include(Union[List[str],str]) stand for?",list of strings,9602
7017,"What is a string e.g. ""my_package.my_subpackage"" or list of strings for the names of the",include,9602
7018,"What does exclude(Union[List[str],str]] exclude some patterns that match?",include string,9289
7019,What is an optional flag that specifies whether the intern modules specified by this call to theinternmethod must be matched to some module during packaging?,allow_empty,8852
7020,"If an internmodule glob pattern is added with allow_empty=False, andclose()is called before any modules match that",If allow_empty=True,8852
7021,What is thrown if an internmodule glob pattern is added with allow_empty=False?,an exception,8852
7022,What is added with allow_empty=False?,an internmodule glob pattern,8852
7023,What will sometimes find files that are imported by model files but whose functionality is never used?,dependency resolution,8852
7024,Replace some required modules with what?,mock implementation,8852
7025,How does dependency resolution find files that are imported by model files but whose functionality is never used?,file-by-file,8852
7026,What is the name of the function that allows you to mock a module without having to modify the original code?,"include(Union[List[str],str])",8852
7027,What will replace some required modules with?,mock implementation,5156
7028,What is an example of a file that is imported by model files but never used?,training helpers,5156
7029,What is the name of the function that includes a mock implementation of a module?,Union,5156
7030,What do you replace some required modules with?,mock implementation,5156
7031,What will a mock implementation return for any attribute accessed from it?,fake object,5156
7032,What does a mock implementation of a dependency resolution function not have to do?,modify the original code,5156
7033,What is the name of the function used to mock functionality out without having to modify the original code?,"include(Union[List[str],str])",5156
7034,What is the function used to do?,mock this functionality out without having to modify the original code,9290
7035,How do dependency resolution find files that are imported by model files but whose functionality is never used?,file-by-file,9290
7036,"What is another name for union[List[str],str])?",include,9290
7037,What is an optional flag that specifies whether the mock implementation(s) specified by this call to themock()method must be matched to some module,allow_empty(bool),8854
7038,"If a mock has not been matched to a module used by the package being exported, an exception is thrown?",If allow_empty=True,8854
7039,"If a mock is added with allow_empty=False, andclose()is called and the mock has not been matched to",If allow_empty=True,8854
7040,What does it do when a module matches against anextern()pattern?,Registers an extern hook on the exporter,8854
7041,When is a mock added?,with allow_empty=False,8853
7042,What does it do when a mock is not matched to a module used by the package being exported?,Registers an extern hook on the exporter,8853
7043,What is thrown if the mock has not been matched to a module used by the package being exported?,an exception,8853
7044,"If a mock is added with allow_empty=False, andclose()is called and no such exception is thrown?",If allow_empty=True,8853
7045,What does register on the exporter?,an extern hook,8853
7046,What does torch.utils.hooks.RemovableHandle do?,Save the code formoduleinto the package,2954
7047,What will be called in order of registration?,Hooks,2954
7048,What registers an intern hook on the exporter?,torch.utils.hooks.RemovableHandle,2954
7049,When will the intern hook be called?,each time a module matches against an intern()pattern,2954
7050,How are hooks called?,in order of registration,2954
7051,What is the handle that can be used to remove the added hook?,callinghandle.remove(),2954
7052,What does torch.utils.hooks.RemovableHandle register on the exporter?,mock hook,2954
7053,What registers a mock hook on the exporter?,torch.utils.hooks.RemovableHandle,2954
7054,What does torch.utils.hooks.RemovableHandle save to the package?,raw bytes,2954
7055,What is the name of the module package this resource should go it?,package(str),2954
7056,"What is a unique name for the resource, used to identify it to load?",resource(str),2954
7057,What is the name of the data to save?,binary(str),2954
7058,What is the name of the module that will be saved to provide code for this package?,module_name(str),2954
7059,"If True, we scan the source for what?",dependencies,2954
7060,What saves a python object to the archive?,pickle,2954
7061,What can be saved to the archive using pickle?,python object,2954
7062,What is the equivalent of totorch.save()?,totorch.save(),2954
7063,"What does not save the code, only the objects?",Stanard pickle,2954
7064,Code will be saved to provide code for this package?,module_name(str),2954
7065,When will the hook be called?,each time a module matches against anextern()pattern,7107
7066,How can a handle be used to remove the added hook?,callinghandle.remove(),2951
7067,When will the mock hook be called?,each time a module matches against amock()pattern,2951
7068,What should an intern hook have?,signature,5125
7069,What should the hook have when a module matches against an intern()pattern?,signature,5125
7070,When will the hook be called on the exporter?,each time a module matches against an intern()pattern,5129
7071,How are hooks called on the exporter?,in order of registration,5129
7072,What is a mock hook called each time a module matches against?,a mock()pattern,5129
7073,The hook will be called each time a module matches against what?,an intern()pattern,7105
7074,How can a handle be used to remove an added hook?,handle.remove(),5128
7075,What should a mock hook have?,signature,5128
7076,The mock hook will be called each time a module matches against what?,amock()pattern,5128
7077,What should the mock hook have?,signature,5128
7078,What does torch.utils.hooks.RemovableHandle register?,an intern hook on the exporter,799
7079,What is the name of the handle that can be used to remove the added hook?,torch.utils.hooks.RemovableHandle,2953
7080,A handle that can be used to remove the added hook by calling what?,handle.remove(),11241
7081,The mock hook is called each time a module matches against what?,amock()pattern,11241
7082,When is the mock hook called on the exporter?,each time a module matches against amock()pattern,798
7083,How many bytes does torch.utils.hooks.RemovableHandle save to the package?,bytes,798
7084,What can be used to remove the added hook by callinghandle.remove()?,handle,798
7085,What is the data to save?,binary(str),798
7086,When will the mock hook on the exporter be called?,each time a module matches against amock()pattern,800
7087,What does torch.utils.hooks.RemovableHandle Register?,an intern hook on the exporter,800
7088,Which tool saves raw bytes to the package?,torch.utils.hooks.RemovableHandle,800
7089,What is the code formoduleinto the package?,Save the code formoduleinto the package,800
7090,What is a handle that can be used to remove the added hook by?,callinghandle.remove(),800
7091,What does input return?,the cumulative product of elements ofinputin the dimensiondim,5523
7092,What is the dimension to do the operation over dtype?,dim(int),5523
7093,"If inputis a what, the result will also be a vector of size N, with elements?",vector of size N,5523
7094,What is the default value for the input tensor?,None,5523
7095,What does inputin the dimensiondim return?,the cumulative product of elements,5523
7096,What is the dimension to do the operation over?,dim(int),5523
7097,What returns the cumulative sum of elements of inputin the dimensiondim?,If input is,5524
7098,What is dim(int)?,"the dimension to do the operation over dtype(torch.dtype, optional)",5524
7099,If a matrix on the left is multiplyed by a negative power as what?,ifn> 0,4364
7100,"What is the tensor of shape(*, m, m)where*is zero or more batch dimensions?",A(Tensor),4364
7101,What does n(int) represent?,exponent,4364
7102,What is ignored when using torch.linalg.solve()?,if None,4364
7103,What is the default value of torch.linalg.solve()?,Default:None,4364
7104,What computesA.inv() @Bwith a numerically stable algorithm?,torch.linalg.solve(),3930
7105,What is the default value of if None?,Default:None,3930
7106,ifn 0 and any matrix in the batch of matricesAis what?,not invertible,3930
7107,What is an example of a matrix that is not invertible?,Examples,3930
7108,Why is it always preferable to usesolve() when possible?,faster and more numerically stable,3929
7109,What is A(Tensor)?,tensor of shape,3929
7110,What is faster and more numerically stable than computing the inverse explicitly?,usesolve(),3929
7111,"What is the default value of out(Tensor,optional)?",if None,3929
7112,What does torch.linalg.solve()compute?,A.inv(),3929
7113,What is the default setting for output tensor?,Default:None,3929
7114,What type of tensor is created when the values are evenly spaced from starttoend?,one-dimensional,1947
7115,What is the index of the new tensor that indexes the input tensor along dimensiondimusing the entries inindex?,a Long Tensor,6044
7116,What type of tensor indexes the input tensor according to the boolean maskmask?,1-D,6044
7117,Which tensor indexes the input tensor according to the boolean maskmask?,1-D,6059
7118,What is the name of a tensor that indexes the input tensor along dimensiondimusing the entries inindex?,Stack,2673
7119,What is the name of the tensor that indexes the input tensor along dimensiondimusing the entries inindex?,a Long Tensor,2673
7120,What is the name of the new tensor that indexes the input tensor along dimensiondimusing the entries inindex?,a Long Tensor,6043
7121,Returns a new tensor which indexes the input tensor according to the boolean maskmask which is,1-D,5354
7122,What is the name of the index that returns a new tensor that indexes the input tensor along dimensiondimusing,a Long Tensor,5353
7123,Returns a new 1-D tensor which indexes the input tensor according to what boolean maskmask,a Bool Tensor,5353
7124,Returns a new tensor that indexes the input tensor according to what boolean maskmask?,1-D,5331
7125,What does a torch.ByteTensor set?,random number generator state,5575
7126,What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?,a tensor,5575
7127,What does a torch.ByteTensor do?,Sets the random number generator state,5664
7128,Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?,exclusive,5448
7129,Returns what with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive,a tensor,5448
7130,Random integers are generated uniformly betweenlow(inclusive) and high(exclusive)?,exclusive,5491
7131,What is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?,tensor,5416
7132,What does return the maximum value of each slice of the input tensor in the given dimensiondim?,the minimum value of each slice of the input tensor in the given dimension(s)dim,5613
7133,Returns what value of each slice of the input tensor in the given dimension(s)dim?,minimum value,5548
7134,Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim. Returns the mean,p-norm of (input-other) Returns the log of summed exponentials,5548
7135,Computes what quantiles of each row of the input tensor along the dimensiondim?,q-th,5548
7136,What is the result of Computes the q-th quantiles of each row of the input tensor along the dimensiondim,variant oftorch.quantile()that “ignores”NaNvalues,5548
7137,"Ifunbiasedis True, what will be used?",Bessel’s correction,5548
7138,"Returns the input tensor. Eliminates all but the first element from every consecutive group of equivalent elements. Ifunbiasedis True,",unique elements,5548
7139,"Ifunbiasedis True, Bessel’s correction will be used.",Ifunbiasedis True,5548
7140,Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.,p-norm,5642
7141,What does ignoring NaN values return?,the median of the values ininput,5606
7142,What value of all elements in the input tensor is returned?,maximum value,5606
7143,Returns what value ignoring NaN values?,the median of the values ininput,6764
7144,What is returned ignoring NaN values?,the median of the values ininput,10869
7145,What does the input tensor return ignoring NaN values?,the median of the values ininput,10869
7146,What is the log of summed exponentials of each row of the input tensor in the given dimensiondim?,p-norm,5654
7147,What returns the log of summed exponentials of each row of the input tensor in the given dimensiondim?,the p-norm of (input-other) Returns the log of summed exponentials,5654
7148,What returns the mode value of each row of the input tensor in the given dimensiondim?,"a named tuple(values,indices)",5654
7149,"What is returned, ignoring NaN values?",the median of the values ininput,5631
7150,"Ifunbiasedis True, Bessel's correction will be used to calculate what?",standard deviation,5631
7151,What is the mode value of each row of the input tensor in the given dimensiondim?,a named tuple,5588
7152,Returns what of each row of the input tensor in the given dimensiondim?,the log of summed exponentials,5588
7153,What are the quantiles of each row of the input tensor along the dimensiondim?,q-th,5588
7154,"Returns what value, ignoring NaN values?",the median of the values ininput,5588
7155,"Returns what of the values ininput, ignoring NaN values?",the median,5559
7156,Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim?,p-norm,5653
7157,What does return the log of summed exponentials of each row of the input tensor in the given dimensiondim?,the p-norm,5653
7158,"What does it return, ignoring NaN values?",the median of the values ininput,5633
7159,"What is returned when a named tuple(values,indices) returns the cumulative minimum of elements ofinputin the dimensiondim",cumulative product of elements ofinputin the dimensiondim,5305
7160,"What is returned when a named tuple(values,indices) returns the cumulative product of elements ofinputin the dimensiondim",cumulative sum of elements ofinputin the dimensiondim,5519
7161,"If input is a vector, then returns a 2-D square tensor Creates a tensor whose diagonals",1-D tensor,5519
7162,What is a tool that allows the collecton of performance metrics during training and inference?,PyTorch Profiler,4944
7163,What can be used to better understand what model operators are the most expensive?,context manager API,4944
7164,An earlier version of the API intorch.autogradmodule is considered what?,legacy,4944
7165,What is a tool that allows the collecton of the performance metrics during the training and inference?,PyTorch Profiler,4944
7166,What API can be used to better understand what model operators are the most expensive?,context manager,4944
7167,What is the name of the API used by PyTorch Profiler?,context manager,4945
7168,What API is considered legacy and will be deprecated?,API intorch.autogradmodule,1121
7169,What is the name of the API that will be deprecated?,Profiler context manager,1121
7170,What is the default value of Profiler context manager?,ProfilerActivity.CPU,1121
7171,What is an older version of intorch.autogradmodule considered legacy and will be deprecated?,Profiler context manager,1121
7172,What is the name of the callable that takes step (int) as a single parameter and returns ProfilerActionvalue?,schedule(callable),1121
7173,Activities(iterable) – what to use in profiling?,list of activity groups,1121
7174,What are the default values for ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA?,ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA,1121
7175,What does activities(iterable) list of to use in profiling?,activity groups,4908
7176,What is the name of the profiler that is used for profiling?,Profiler context manager,4908
7177,An earlier version of the API is considered legacy and will be deprecated.,intorch.autogradmodule,4355
7178,Activities(iterable) – list of what to use in profiling?,activity groups,4355
7179,What is the default value for ProfilerActivity.CUDA?,ProfilerActivity.CPU,4355
7180,What is the name of the module that is used for profiling?,Profiler context manager,4354
7181,Activities(iterable) – what does activities(iterable) stand for?,list of activity groups,1120
7182,What is considered legacy and will be deprecated?,intorch.autogradmodule,4947
7183,What is deprecated since version 1.8.1?,use_cuda(bool),4947
7184,What does Usetensorboard_trace_handler() generate result files for?,TensorBoard,4947
7185,What are the default values for PyTorch Profiler?,ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA,4947
7186,Use formula to estimate the FLOPS of specific operators (matrix multiplication and 2D convolution). use_cuda(bool,with_flops,4947
7187,What is a callable that takes step (int) as a single parameter and returnsProfilerActionvalue?,schedule,8793
7188,What is the default value of ProfilerActivity?,ProfilerActivity.CPU,8793
7189,What is the default value for profiling?,ProfilerActivity.CPU,8793
7190,What is the name of the profiler context manager?,Profiler context manager,4356
7191,An earlier version of what is considered legacy and will be deprecated?,API intorch.autogradmodule,4356
7192,What are the default values for Profiler context manager?,ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA,4356
7193,What is bool deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the call,use_cuda,4356
7194,"For more information, see what?",PyTorch Profiler TensorBoard,4356
7195,What is the name of the list of activity groups to use in profiling?,activities(iterable),4946
7196,What is the name of the callable that is called at each step whenschedulereturnsProfilerAction.RECORD_,on_trace_ready(callable),4946
7197,What is the default value for PyTorch Profiler?,ProfilerActivity.CPU,4946
7198,What is the default value of profiler context manager?,ProfilerActivity.CPU,4907
7199,What is a callable that takes step as a single parameter and returnsProfilerActionvalue?,schedule,10648
7200,When is on_trace_ready(callable) called?,whenschedulereturnsProfilerAction,10648
7201,What saves information about operator's input shapes?,record_shapes,10564
7202,What does record_shapes do?,save information about operator’s input shapes,10564
7203,What bool tracks tensor memory allocation/deallocation?,profile_memory,10564
7204,What is the name of the file and line number in a bool?,with_stack,10564
7205,What is deprecated since version 1.8.1:useactivitiesinstead?,use_cuda,10564
7206,What command is used to generate result files for TensorBoard?,tensorboard--logdirdir_name,10564
7207,What is the name of the callable that takes step (int) as a single parameter and returnsProfilerActionvalue?,schedule,10647
7208,What is the name of the callable that is called at each step whenschedulereturnsProfilerAction?,on_trace_ready,10647
7209,When is on_trace_ready(callable) called at each step?,whenschedulereturnsProfilerAction,10293
7210,What does profile_memory(bool) do?,track tensor memory allocation/deallocation,10293
7211,What is used to track tensor memory allocation/deallocation?,profile_memory,10293
7212,What is used to estimate the FLOPS of specific operators?,with_flops,10293
7213,What is used to generate the callable schedule?,Useschedule(),10293
7214,Usetensorboard_trace_handler() to generate result files for what?,TensorBoard,10293
7215,What is on_trace_ready?,callable,10293
7216,What bool is used to record source information for the ops?,with_stack,10293
7217,What saves information about operator input shapes?,record_shapes,10292
7218,What does with_stack(bool) do?,record source information,11418
7219,What is a formula to estimate the FLOPS of specific operators?,with_flops,11413
7220,When was use_cuda(bool) deprecated?,1.8.1,11413
7221,What function is deprecated since version 1.8.1?,use_cuda,11413
7222,Usetensorboard_trace_handler() to generate result files for TensorBoard?,on_trace_ready,11413
7223,What are examples of schedules?,"profiler’sschedule,on_trace_readyandstepfunctions",11413
7224,What is the callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_S,on_trace_ready,10294
7225,Save information about operator’s input shapes. profile_memory(bool) – track tensor memory allocation/deallocation.,record_shapes,10294
7226,Profile_memory(bool) – what?,track tensor memory allocation/deallocation,10294
7227,Save stack traces in a file in what format?,format suitable for visualization,10294
7228,What path(str) – save stacks file to this?,path(str) – save stacks file to this,10294
7229,When was use_cuda deprecated?,1.8.1,11313
7230,What is the name of the function that is deprecated since version 1.8.1?,use_cuda,11313
7231,What generates the callable schedule?,Useschedule(),11313
7232,What does Usetensorboard_trace_handler() do to generate result files for TensorBoard?,on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name),11313
7233,"What does the profiler'sschedule,on_trace_readyandstepfunctions add into the trace file?",a user defined metadata with a string key and a string value,11313
7234,"Using the profiler’sschedule,on_trace_readyandstepfunctions adds a user defined metadata with what",a string key and a string value,11313
7235,What is a bool that tracks tensor memory allocation/deallocation?,profile_memory,10448
7236,What is the name of the function that generates the callable schedule?,Useschedule(),4438
7237,What are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process?,Non-default schedules,4909
7238,"What is the name of the list of activity groups (CPU, CUDA) to use in profiling?",activities(iterable),4909
7239,What is a callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profile,schedule(callable),4909
7240,With_flops(bool) – use formula to estimate what of specific operators (matrix multiplication and 2D convolution)?,FLOPS,4909
7241,Profiler is what?,context manager,4909
7242,"Profiler context manager. Activities(iterable) – list of activity groups (CPU, CUDA) to use in profiling.",Examples: Using the profiler,4909
7243,What simply records all the events continuously for the duration of the context manager?,The default schedule,8099
7244,When are non-default schedules useful?,when profiling long training jobs,8099
7245,What does path(str) do to save stacks file to?,path(str) – save stacks file to this location,8099
7246,What version of TensorBoard was deprecated since version 1.8.1?,1.8.1:useactivities,2117
7247,What is the name of the result file generated by Usetensorboard_trace_handler()?,on_trace_ready,8098
7248,What command is used to see the results in TensorBoard?,tensorboard--logdirdir_name,983
7249,What can be found in the specified directory after profiling?,result files,983
7250,What is the metric to use for self_cpu_time_total or self_cuda_time_total?,metric,983
7251,Enabling shape and stack tracing results in what?,additional overhead,2256
7252,What results in additional overhead?,Enabling shape and stack tracing,2256
7253,What functions can be used to generate results for TensorBoard?,"profiler’sschedule,on_trace_readyandstepfunctions",4441
7254,What are some examples of how to enable shape and stack tracing?,"profiler’sschedule,on_trace_readyandstepfunctions",4441
7255,Where can result files be found after profiling?,specified directory,10297
7256,What is used to generate result files for TensorBoard?,on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name),8100
7257,What functions are used to generate result files for TensorBoard?,"profiler’sschedule,on_trace_readyandstepfunctions",8100
7258,What are some examples of tracing functions?,"profiler’sschedule,on_trace_readyandstepfunctions",8100
7259,What does on_trace_ready mean?,on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name),10296
7260,What is an example of using?,FlameGraph tool,10296
7261,What does the profiler'sschedule add into the trace file?,a user defined metadata with a string key and a string value,10295
7262,"What does the profiler'sschedule,on_trace_readyandstepfunctions add to the trace file?",a string key and a string value,982
7263,"Using the profiler'sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with",trace file,982
7264,What is the name of the command to see the results in TensorBoard?,tensorboard,10814
7265,What is the name of the program that displays the results in TensorBoard?,tensorboard,10814
7266,"What does the profiler'sschedule,on_trace_readyandstepfunctions add?",a user defined metadata with a string key and a valid json value into the trace file,10923
7267,To see the results in what?,TensorBoard,10924
7268,Exports the collected trace in what format?,Chrome JSON,5582
7269,What are members of the profiler?,CPU CUDA,5582
7270,What are the members of Profiler actions that can be taken at the specified intervals?,CPU CUDA,5582
7271,When is the list of unaggregated profiler events used?,trace callback,8117
7272,What is the profiler'sschedule used on?,trace,8116
7273,In what format can stack traces be saved?,a format suitable for visualization,959
7274,Save stack traces in a file in a format suitable for what?,visualization,10816
7275,What does path(str) do to save stacks file to this location?,path(str) – save stacks file to this location,10816
7276,Signals the profiler that what has happened?,next profiling step has started,10816
7277,What member returns a callable that can be used as profiler schedule argument?,CPU CUDA,10816
7278,What should you set when creating profiler context manager?,record_shapes/with_stack,10816
7279,What will the profiler skip?,firstskip_firststeps,10816
7280,What is specified with the repeat parameter?,optional number of cycles,10816
7281,What does --logdirdir_name to see the results in TensorBoard?,tensorboard,10816
7282,What saves stacks file to this location?,path(str),10399
7283,path(str) – save what file to this location?,stacks file,10399
7284,What is the name of the file to save stacks file to?,path(str),960
7285,What signals the profiler that the next profiling step has started?,Signals the profiler that the next profiling step has started,960
7286,What format can stack traces be saved in?,a format suitable for visualization,2360
7287,What format does FlameGraph export the collected trace in?,Chrome JSON,2360
7288,What is the metric to use?,self_cpu_time_total,9913
7289,What are the members of the profiler that can be taken at the specified intervals?,CPU CUDA,9913
7290,What is an example of using FlameGraph tool?,git clone,2296
7291,When setting record_shapes/with_stack make sure to set record_shapes/with_stack when,profiler context manager,2296
7292,What does self_cpu_time_total stand for?,metric,5581
7293,What format should stack traces be saved in?,a format suitable for visualization,5742
7294,To use shape/stack functionality make sure to set what when creating profiler context manager?,record_shapes/with_stack,1390
7295,What events are grouped by operator name and (optionally) input shapes and stack?,Averages,1390
7296,Set record_shapes/with_stack when creating what?,profiler context manager,4437
7297,What does the profiler do when the next profiling step has started?,Signals the profiler that the next profiling step has started,4437
7298,What Signals the profiler that the next profiling step has started?,Signals the profiler that the next profiling step has started,4437
7299,What returns a callable that can be used as profiler schedule argument?,CPU CUDA,4437
7300,What does the profiler do after the firstskip_firststeps?,wait for waitsteps,4437
7301,What specifies the optional number of cycles?,the repeat parameter,4437
7302,What value means that the cycles will continue until the profiling is finished?,the zero value,4437
7303,What can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profilerschedulear,Profiler actions,4437
7304,What does the profiler skip?,firstskip_firststeps,4437
7305,What is the directory delivered to tensorboard as?,logdir,4437
7306,What does it do when a profiler context manager is created?,Signals the profiler that the next profiling step has started,4370
7307,How are events grouped?,by operator name and (optionally) input shapes and stack,9048
7308,What is the name of the file that records CPU time?,cd FlameGraph,9048
7309,What does the profiler do when setting record_shapes/with_stack?,Signals the profiler that the next profiling step has started,2295
7310,What does Brendangregg do?,git clone,9466
7311,What is a member of a profiler?,CPU CUDA,9047
7312,How are events grouped in profiler.stacks?,by operator name,9047
7313,What are the members of the profiler?,CPU CUDA,85
7314,What does perf_viz.svg group events by operator name and (optionally) input shapes and stack?,profiler.stacks,85
7315,What is the countname of the CPU time?,us,86
7316,What grouping of events does perf_viz.svg Averages use?,profiler.stacks,86
7317,What is a member of the profiler that can be taken at the specified intervals?,CPU CUDA,1389
7318,What does the profiler group by operator name and input shapes and stack?,Averages events,1389
7319,What do you need to set to use shape/stack functionality?,record_shapes/with_stack,4436
7320,What does this do to the profiler?,Signals the profiler that the next profiling step has started,7925
7321,What do profiler context managers need to set to use shape/stack functionality?,record_shapes/with_stack,7925
7322,When creating what do you need to set record_shapes/with_stack?,profiler context manager,7925
7323,What is a member of Profiler actions that can be taken at the specified intervals?,CPU CUDA,7925
7324,What is a member of the profiler?,CPU CUDA,5901
7325,What can be taken at the specified intervals?,Profiler actions,5901
7326,What are actions that can be taken at the specified intervals?,Profiler,4906
7327,What are the members of the Profiler that can be taken at the specified intervals?,CPU CUDA,4906
7328,What is the name of the person who can take actions at specified intervals?,Profiler,4906
7329,What are Profiler actions that can be taken at specified intervals Members?,CPU CUDA,4906
7330,Members: CPU CUDA Returns a callable that can be used as what?,profiler schedule argument,4239
7331,"Who will skip the firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps",The profiler,4239
7332,What does this function also eliminate?,non-consecutive duplicate values,5683
7333,"When what is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument?",dim,5683
7334,What type of duplicate values does this function eliminate?,non-consecutive,5683
7335,"When dim is specified, what are the two implementations of torch.unique?",the CUDA implementation and the CPU implementation,5683
7336,Is sorting slow or fast?,slow,2047
7337,When is it recommended to usetorch.unique_consecutive()?,if your input tensor is already sorted,2047
7338,What is the name of the unique of the flattened input?,If None,2047
7339,Whether to return the indices for where elements in the original input ended up in the returned unique list?,return_inverse,2047
7340,What is output(Tensor) a tuple of tensors containing?,output list of unique scalar elements,2047
7341,"Whether to return the counts for each unique element. dim(int) – the dimension to apply unique. if None, the unique of",return_counts,2047
7342,"When dim is specified, the tensor is always sort at the beginning regardless of thesortargument?",CUDA implementation and the CPU implementation,2047
7343,What does this function eliminate?,non-consecutive duplicate values,7542
7344,Why is this function different fromtorch.unique_consecutive()?,eliminates non-consecutive duplicate values,7542
7345,What is the name of the function that returns the indices for where elements in the original input ended up in the returned unique list?,return_inverse,7542
7346,"When dim is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument.",CUDA implementation and the CPU implementation,7542
7347,Where does this function currently sort the tensor when dim is specified?,CUDA implementation and the CPU implementation,4420
7348,"When dim is specified, what two implementations of Torch.unique sort the tensor at the beginning regardless of thesortargument",CUDA implementation and the CPU implementation,4420
7349,What is the input tensor called?,input(Tensor),7541
7350,What doestorch.unique always sort the tensor regardless of?,thesortargument,7541
7351,"When dim is specified, what two implementations dotorch.unique always sort the tensor at the beginning regardless of thesortar",the CUDA implementation and the CPU implementation,7541
7352,In what implementation does Torch.unique always sort the tensor at the beginning regardless of thesortargument?,CUDA,2046
7353,In what implementation is torch.unique always sort the tensor at the beginning regardless of thesortargument?,CUDA,2046
7354,Input(Tensor) – the input tensor sorted(bool) – Whether to sort what in ascend,unique elements,2046
7355,What is the name of the input tensor sorted?,input(Tensor),9655
7356,What returns the indices for where elements in the original input ended up in the returned unique list?,return_inverse(bool),9655
7357,What is the name of the input tensor sorted(bool)?,input(Tensor),9655
7358,What is the name of the function that returns a single tensor?,inverse_indices,9655
7359,In which implementation does torch.unique always sort the tensor at the beginning regardless of thesortargument?,CUDA implementation,4365
7360,Whether to sort the input tensor in ascending order before returning as output?,unique elements,4365
7361,What is the name of the function that returns the counts for each unique element?,return_counts,10721
7362,What is the dimension to apply unique?,dim(int),10721
7363,What returns the unique of the flattened input?,If None,10721
7364,Whether to sort the unique elements in ascending order before returning as output?,sorted,10721
7365,What is sorted(bool)?,Whether to sort the unique elements in ascending order before returning as output,10721
7366,What is output(Tensor)?,output list of unique scalar elements,10721
7367,What is the output list of unique scalar elements?,output(Tensor),10721
7368,What will return an additional returned tensor representing the indices for where elements in the original input map to in the output?,ifreturn_inverseis True,10721
7369,What function returns an additional returned tensor for where elements in the original input map to in the output?,ifreturn_inverseis True,10721
7370,What function returns the indices for where elements in the original input ended up in the returned unique list?,return_inverse,10721
7371,What is another term for a tensor?,a tuple of tensors,9654
7372,What is the default value for a tensor or a tuple of tensors containing unique elements?,default:None,9654
7373,What containing a unique element is default:None?,tensor or a tuple of tensors,10720
7374,A tensor or a tuple of tensors containing tensors containing tensor,default:None,10720
7375,"if None, what is returned?",the unique of the flattened input,10602
7376,"Whether to also return the counts for each unique element. dim(int) – the dimension to apply unique. if None, the unique",return_counts,10602
7377,What is the name of the function that returns the output list of unique scalar elements?,inverse_indices,9184
7378,What function returns an additional returned tensor representing the indices for where elements in the original input map to in the output?,ifreturn_inverseis True,867
7379,What does output(Tensor) contain?,unique scalar elements,867
7380,What is the default return value for inverse_indices(Tensor)?,ifreturn_inverseis True,9692
7381,What will this function return if it is not true?,a single tensor,9692
7382,What is the name of the function that returns the indices for where elements in the original input map to in the output?,inverse_indices,9692
7383,What is the default return value for counts(Tensor)?,ifreturn_countsis True,9692
7384,What is the name of the indices returned by ifreturn_inverseis True?,inverse_indices,9692
7385,What is a package implementing various optimization algorithms?,torch.optimis,11192
7386,What can be easily integrated in the future?,more sophisticated ones,11192
7387,What is an iterable containing the parameters to optimize?,an Optimizer,11192
7388,How many ways can a step() method be used?,two ways,11192
7389,What does the optimizer update the parameters based on?,computed gradients,11192
7390,What is a keyword argument useful when you only want to do?,vary a single option,11192
7391,What model's parameters will use a learning rate of1e-3?,model.classifier,11192
7392,What does torch.optim implement that updates the parameters?,a step()method,11192
7393,What is the name of the package that implements various optimization algorithms?,usetorch.optim,11190
7394,What do you need to construct to use torch.optim?,an optimizer object,11190
7395,What is the name of the object that will hold the current state and update the parameters based on the computed gradients?,usetorch.optimyou,7929
7396,What do you have to give it an iterable containing the parameters (all should beVariables) to optimize?,an Optimizer,7929
7397,What must you construct to usetorch.optim?,an optimizer object,7929
7398,What are some options that you can specify in an Optimizer?,"learning rate, weight decay",7929
7399,What are some options you can specify for an Optimizer?,"the learning rate, weight decay, etc",7877
7400,What do you have to give it an iterable containing the parameters to optimize?,an Optimizer,7877
7401,How do you move a model to the GPU?,via.cuda(),7877
7402,What is this useful when one wants to specify?,per-layer learning rates,7877
7403,What do the parameters update based on?,computed gradients,11191
7404,What will be different objects with those before the call?,Parameters of a model after.cuda(),7874
7405,What do you need to do before constructing optimizers for a model?,move a model to GPU via.cuda(),7874
7406,What must you give to construct an Optimizer?,an iterable,7875
7407,Parameters of a model will be different objects with those before the call.,after.cuda(),7875
7408,What is an example of a model where optimized parameters live in consistent locations when optimizers are constructed and used?,Example,7875
7409,What are some options that can be specified by an Optimizer?,"learning rate, weight decay",7875
7410,When do you need to move a model to GPU via.cuda()?,before constructing optimizers,7875
7411,"If you need to move a model to GPU via.cuda(), please do so before constructing what?",before constructing optimizers,3570
7412,Options can still be passed as what?,keyword arguments,3570
7413,Options as keyword arguments will be used as what?,defaults,3570
7414,Optimizers also support specifying what?,per-parameter options,3570
7415,Parameters of a model after.cuda() will be different objects with what?,before the call,4377
7416,Where should optimized parameters live when optimizers are constructed and used?,consistent locations,3571
7417,What should each of the iterable ofdicts contain?,aparamskey,3571
7418,What should other keys match?,keyword arguments,3571
7419,Option arguments will be used as what in the groups that didn't override them?,defaults,3571
7420,How can the function be called once the gradients are computed?,backward(),3571
7421,"What should clear the gradients, compute the loss, and return it?",The closure,3571
7422,Which optimization algorithms need to reevaluate the function multiple times?,Conjugate Gradient and LBFGS,3571
7423,What need to be specified as collections that have a deterministic ordering that is consistent between runs?,Warning Parameters,3571
7424,Parameters of a model will be different objects with those before the call?,after.cuda(),3571
7425,What can you still pass options as?,keyword arguments,3571
7426,What class is used for all optimizers?,Base class,3571
7427,What is an example of a consistent location when optimizers are constructed and used?,Example,7930
7428,What is an iterable containing the parameters (all should beVariables) to optimize?,an Optimizer,7930
7429,What will the parameters update based on?,computed gradients,7930
7430,What are some options you can specify in an Optimizer?,"learning rate, weight decay",7930
7431,Optimizers support specifying what?,per-parameter options,4378
7432,"Instead of passing an iterable ofVariables, pass in what?",iterable ofdicts,4689
7433,What model.classifier's parameters will use a learning rate of1e-3?,model.classifier,4689
7434,When are keyword arguments used as defaults?,"when you only want to vary a single option, while keeping all others consistent between parameter groups",4689
7435,Options as keyword arguments will be used as what in the groups that didn't override them?,defaults,4689
7436,Who supports specifying per-parameter options?,Optimizers,4689
7437,What is the name of the key that should match the keyword arguments accepted by the optimizers?,Note,2317
7438,What is a keyword argument useful for when you want to only pass options as keyword arguments?,vary a single option,8596
7439,What is an example of a way to specify?,per-layer learning rates,8596
7440,What is a good example of what you can pass options as keyword arguments?,per-layer learning rates,8596
7441,Options will be used as what in the groups that didn’t override them?,defaults,8596
7442,What is an example of a useful feature that can be passed as keyword arguments?,per-layer learning rates,4455
7443,What model's parameters will use the default learning rate of1e-2?,model.base,2318
7444,What does all optimizers implement that updates the parameters?,a step() method,4379
7445,What is an example of a closure for all optimizers?,Base class,4379
7446,"Instead of passing an iterable ofVariables, what do optimizers use to specify per-parameter options?",pass in an iterable ofdicts,4379
7447,What is a step() method useful when one wants to specify?,per-layer learning rates,2534
7448,What method updates the parameters?,a step()method,1085
7449,What are two examples of algorithms that need to reevaluate the function multiple times?,Conjugate Gradient and LBFGS,1085
7450,When can a step()method be called?,once the gradients are computed using e.g.backward(),2537
7451,What is useful when one wants to specify?,per-layer learning rates,2537
7452,What algorithm was heavily inspired?,L-BFGS algorithm,2537
7453,How is the a step()method used by most optimizers?,simplified version,1086
7454,What are examples of objects that don't satisfy Warning Parameters?,sets and iterators over values of dictionaries,1086
7455,What are two examples of optimization algorithms that need to reevaluate the function multiple times?,Conjugate Gradient and LBFGS,2328
7456,The function can be called once the gradients are computed using e.g. what?,backward(),7611
7457,What kind of version is supported by most optimizers?,simplified,7611
7458,What should the closure do?,compute the loss,5971
7459,What algorithms need to reevaluate the function multiple times?,Conjugate Gradient and LBFGS,5971
7460,What version of the function is supported by most optimizers?,simplified version,7614
7461,What needs to be specified as collections that have a deterministic ordering that is consistent between runs?,Warning Parameters,7614
7462,What is params(iterable)?,iterable oftorch.Tensors ordicts,7614
7463,What is a dict that contains default values of optimization options?,defaults,7614
7464,What adds a param group to theOptimizersparam_groups?,Optimizer.add_param_group,7614
7465,What version of the function can be called once the gradients are computed using e.g.backward()?,simplified,7614
7466,"The closure that allows them to reevaluate the function multiple times should clear the gradients, what should the closure do?",compute the loss,7614
7467,What is the closure for all optimizers?,Base class,1087
7468,All optimizers implement what method that updates the parameters?,a step()method,1087
7469,What does Adamax algorithm implement?,Averaged Stochastic Gradient Descent,1087
7470,What is a closure that allows algorithms to reevaluate the function multiple times?,recompute your model,1087
7471,What algorithm does the optimizer implement?,resilient backpropagation algorithm,1087
7472,What type of version is supported by most optimizers?,simplified,7613
7473,What is an example for all optimizers?,Base class,2300
7474,What are examples of objects that don't satisfy Warning Parameters' properties?,sets and iterators over values of dictionaries,7725
7475,What class's parameters will use a learning rate of 1e-3?,model.classifier,7725
7476,What does params(iterable) specify to be optimized?,Tensors,4734
7477,What specifies what Tensors should be optimized?,params(iterable),4734
7478,What is the name of the dict that contains default values of optimization options?,Optimizer.add_param_group,4734
7479,Parameters need to be specified as what?,collections that have a deterministic ordering that is consistent between runs,4734
7480,What are examples of objects that don't satisfy deterministic ordering?,sets and iterators over values of dictionaries,4734
7481,What are examples of objects that don’t satisfy the properties of parameters?,sets and iterators over values of dictionaries,4734
7482,What is the name of all optimizers?,Base class,1404
7483,What is an iterable oftorch.Tensors ordict?,params,10395
7484,What does params(iterable) do?,Specifies what Tensors should be optimized,10395
7485,What algorithm implements Adadelta?,Adagrad algorithm,10395
7486,What should be applied after optimizer’s update?,Learning rate scheduling,10395
7487,What algorithm does Optimizer.zero_grad set the gradients of all optimizedtorch.Tensors to zero?,Adadelta algorithm,10395
7488,What is a dict containing default values of optimization options?,defaults,9154
7489,What are examples of objects that don’t satisfy Warning Parameters?,sets and iterators over values of dictionaries,5972
7490,What Loads the optimizer state?,Optimizer.load_state_dict,4681
7491,How did 1.1.0 change the behavior of the learning rate scheduler?,BC-breaking way,4681
7492,What does lr_scheduler.Lamb do?,lr_scheduler.Lamb,4681
7493,What performs a single optimization step (parameter update)?,Optimizer.step,4684
7494,What is lr_scheduler?,Multiplicative,4684
7495,What is the name of the optimization step?,Optimizer.zero_grad,10393
7496,What algorithm does Optimizer.zero_grad implement?,Adagrad,4687
7497,What does Optimizer.load_state_dict load?,optimizer state,4678
7498,What is the gradient of all optimizedtorch.Tensors?,zero,4688
7499,Which algorithm implements Adadelta algorithm?,Adagrad algorithm,4688
7500,Which algorithm implements the resilient backpropagation algorithm?,RMSprop algorithm,4688
7501,When should learning rate scheduling be applied?,after optimizer’s update,4688
7502,"In many places in the documentation, we will use the following template to refer to what?",schedulers,4688
7503,What Sets the gradients of all optimizedtorch.Tensors to zero?,Optimizer.zero_grad,4688
7504,What happens when a learning rate scheduler is called back-to-back?,each scheduler is applied one after the other on the learning rate obtained by the one preceding it,4688
7505,Which algorithm implements?,Adam,4688
7506,What Sets the learning rate of each parameter group to the initial lr times a given function?,lr_scheduler.LambdaLR,4688
7507,What algorithm implements Optimizer.zero_grad?,Adadelta algorithm,4688
7508,What algorithm does Adadelta implement?,Adagrad algorithm,3674
7509,What algorithm implements?,resilient backpropagation algorithm,3674
7510,What algorithm does Adam implement?,AdamW algorithm,3674
7511,What is the lazy version of Adam algorithm suitable for?,sparse tensors,3674
7512,What is a variant of Adam based on infinity norm?,Adamax algorithm,3674
7513,What algorithm does Adamax implement?,Averaged Stochastic Gradient Descent,3674
7514,Which algorithm implements lazy version of Adam algorithm suitable for sparse tensors?,AdamW,3674
7515,What algorithm is suitable for sparse tensors?,lazy version of Adam algorithm,3674
7516,What algorithm implements a variant of Adam based on infinity norm?,Adamax,3674
7517,What algorithm is heavily inspired byminFunc?,L-BFGS,3674
7518,What is the L-BFGS algorithm heavily inspired by?,minFunc,3674
7519,What algorithm implements the resilient backpropagation algorithm?,RMSprop,3674
7520,What does the resilient backpropagation algorithm implement?,stochastic gradient descent,3674
7521,What algorithm is implemented optionally with momentum?,stochastic gradient descent,3674
7522,Which algorithm implements Adadelta?,Adagrad,3676
7523,What do we use the following template to refer to in many places in the documentation?,schedulers,3676
7524,When was the learning rate scheduler expected to be called before the optimizer's update?,1.1.0,3676
7525,What should you check if you are unable to reproduce results after upgrading to PyTorch 1.1.0?,if you are callingscheduler.step()at the wrong time,3676
7526,"If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check what?",if you are callingscheduler.step()at the wrong time,3676
7527,What Multiply the learning rate of each parameter group by the factor given in the specified function?,MultiplicativeLR,3676
7528,What provides several methods to adjust the learning rate based on the number of epochs?,torch.optim.lr_scheduler,3676
7529,What should be applied after the optimizer's update?,Learning rate scheduling,3676
7530,"Prior to PyTorch 1.1.0, what was the learning rate scheduler expected to be called?",before the optimizer’s update,3676
7531,What is the name of the learning rate scheduler?,lr_scheduler.StepLR,3676
7532,What algorithm does PyTorch implement?,resilient backpropagation algorithm,3676
7533,What will the learning rate scheduler skip if you use the learning rate scheduler before the optimizer's update?,first value of the learning rate schedule,3676
7534,Learning rate scheduling should be applied one after the other on the learning rate obtained by the one preceding it.,schedulers algorithms,3676
7535,What does lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the,lr_scheduler.StepLR,3676
7536,What does theOptimizersparam_groups do?,Add a param group,942
7537,What does Optimizer.state_dict do?,Loads the optimizer state,4135
7538,What do we use the following template to refer to?,schedulers algorithms,4135
7539,"Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer's update?",BC,4135
7540,Returns the state of the optimizer as what?,adict,5666
7541,Implements what algorithm?,Adadelta algorithm,5666
7542,What Sets the learning rate of each parameter group to the initial lr times a given lr times?,lr_scheduler.LambdaLR,5666
7543,What algorithm implements Adam?,AdamW,3680
7544,What version of Adam algorithm is suitable for sparse tensors?,lazy,3680
7545,What algorithm is heavily inspired by minFunc?,L-BFGS algorithm,3680
7546,Which algorithm implements Adam algorithm?,AdamW algorithm,3680
7547,L-BFGS algorithm is heavily inspired by what?,minFunc,4686
7548,What algorithm does L-BFGS implement?,RMSprop algorithm,10394
7549,What is L-BFGS algorithm heavily inspired by?,minFunc,10394
7550,Sets the gradients of all optimizedtorch.Tensors to what?,zero,5849
7551,What algorithm implements Adadelta algorithm?,Adagrad algorithm,5849
7552,What algorithm is implemented?,resilient backpropagation algorithm,5849
7553,Which algorithm implements the lazy version of Adam algorithm suitable for sparse tensors?,AdamW algorithm,5849
7554,What is implemented (optionally with momentum)?,stochastic gradient descent,5849
7555,Learning rate scheduling should be applied when?,after optimizer’s update,5849
7556,When was the learning rate scheduler expected to be called?,before the optimizer’s update,5849
7557,What happens if you use the learning rate scheduler before the optimizer’s update?,this will skip the first value of the learning rate schedule,5849
7558,What is the result of upgrading to PyTorch 1.1.0?,unable to reproduce results,5849
7559,What version of Adam algorithm suitable for sparse tensors?,lazy version,5849
7560,What does lr_scheduler.LambdaLR stand for?,MultiplicativeLR,5849
7561,What algorithm implements the gradients of all optimizedtorch.Tensors to zero?,Adadelta algorithm,5849
7562,What does MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function?,lr,5849
7563,Which algorithm implements the gradients of all optimizedtorch.Tensors to zero?,Adadelta algorithm,5848
7564,What algorithm does RMSprop implement?,resilient backpropagation algorithm,3677
7565,What algorithm does Adagrad implement?,Adam algorithm,3677
7566,Which algorithm implements Adam?,AdamW algorithm,3677
7567,Which algorithm implements Adagrad?,Adam,3678
7568,What implements the lazy version of Adam algorithm suitable for sparse tensors?,Adam algorithm,3679
7569,What algorithm implements the lazy version of Adam algorithm?,AdamW,3679
7570,What algorithm implements Adam algorithm?,AdamW algorithm,3679
7571,What version of the function can be called once the gradients are computed?,simplified,7612
7572,What does a closure allow an optimization algorithm to do?,recompute your model,7612
7573,What will be used for all parameters?,momentum of 0.9,2533
7574,What does torch.optim.lr_scheduler provide methods to adjust the learning rate based on?,epochs.torch.optim.lr_scheduler,11186
7575,What are most learning rate schedulers called?,back-to-back,11186
7576,How is each scheduler applied?,one after the other on the learning rate obtained by the one preceding it,11186
7577,What does reduceLROnPlateauallow learning rate reducing based on?,validation measurements,11185
7578,How is each learning rate scheduler applied?,one after the other on the learning rate obtained by the one preceding it,4087
7579,What does the learning rate scheduler call before the optimizer's update?,callingoptimizer.step(),4087
7580,What should be called if you are callingscheduler.step() at the wrong time?,lr_scheduler.LambdaLR,4087
7581,What should be applied after an optimizer's update?,Learning rate scheduling,4086
7582,"In many places in the documentation, we will use what to refer to schedulers algorithms?",template,4269
7583,What are learning rate schedulers called back-to-back?,chaining schedulers,4269
7584,How is each scheduler applied on the learning rate obtained by the one preceding it?,one after the other,4269
7585,What type of algorithms are often referenced in documentation?,schedulers,2313
7586,What type of algorithms are often referenced in the documentation?,schedulers,3741
7587,In what year did 1.1.0 change the behavior of the learning rate scheduler?,BC,8174
7588,What is the learning rate scheduler called before the optimizer's update?,lr_scheduler.ExponentialLR,4893
7589,"Prior to PyTorch 1.1.0, when was the learning rate scheduler expected to be called?",before the optimizer’s update,4893
7590,"Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called when?",before the optimizer’s update,4892
7591,How did 1.1.0 change the learning rate scheduler behavior?,BC,4892
7592,What does lr_scheduler.LambdaLR call?,lr_scheduler.LambdaLR,4892
7593,lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every what,step,9850
7594,What LR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of,MultiStep,9850
7595,What does MultiStepLR Decay the learning rate of each parameter group by?,gamma,9843
7596,What is the learning rate of each parameter group according to cyclical learning rate policy?,OneCycleLR,9843
7597,What LR Decays the learning rate of each parameter group by gamma every epoch?,Exponential,9843
7598,What type of LR does lr_scheduler?,Exponential,9836
7599,Sets the learning rate of each parameter group to what?,the initial lr times a given function,5851
7600,What does lr_scheduler stand for?,ExponentialLR,5851
7601,What does lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by?,the factor given in the specified function,9842
7602,How does lr_scheduler.StepLR determine the learning rate of each parameter group?,gamma,9842
7603,What does lr_scheduler.CosineAnnealingLR do?,lr_scheduler.CosineAnnealingLR,9842
7604,What is the learning rate of each parameter group determined by?,gamma,9835
7605,What is the learning rate of each parameter group according to?,1cycle,9835
7606,lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the,one of the milestones,4280
7607,How does lr_scheduler determine the learning rate of each parameter group?,gamma,4280
7608,What Decays the learning rate of each parameter group by gamma every epoch?,ExponentialLR,9840
7609,When does ReduceLROnPlateau Reduce learning rate?,when a metric has stopped improving,9840
7610,What does lr_scheduler.CosineAnnealingWarmRestart?,lr_scheduler.CosineAnnealingWarmRestarts,9840
7611,OneCycleLR Sets the learning rate of each parameter group according to what policy?,1cycle learning rate policy,9840
7612,What does CosineAnnealingWarmRestarts do?,CosineAnnealingWarmRestarts,9840
7613,MultiStepLR Decays the learning rate of each parameter group by what?,gamma,9840
7614,What is another name for CosineAnnealingLR?,CosineAnnealingLR,2079
7615,Decays the learning rate of each parameter group by gamma what?,every step_size epochs,2079
7616,What does MultiStepLR use to determine the learning rate of each parameter group?,gamma,2079
7617,What is the learning rate of each parameter group by the factor given in the specified function?,Multiply,4281
7618,StepLR Decays the learning rate of each parameter group by gamma every what?,step_size epochs,4281
7619,How does MultiStepLR determine the learning rate of each parameter group?,gamma,9849
7620,What Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestone,MultiStepLR,2080
7621,Decays the learning rate of each parameter group by what?,gamma,2082
7622,What is the learning rate of each parameter group determined by every epoch?,gamma,9839
7623,What sets the learning rate of each parameter group by gamma every epoch?,lr_scheduler,2081
7624,What reduces learning rate when a metric has stopped improving?,lr_scheduler.CyclicLR,2078
7625,What type of LR reduces learning rate when a metric has stopped improving?,Cyclic,2078
7626,What does it do to the learning rate of each parameter group by gamma every epoch?,Decays the learning rate of each parameter group by gamma every epoch,2078
7627,What is the learning rate of each parameter group by gamma every epoch?,Decays,2078
7628,When does ReduceLROnPlateau reduce learning rate?,when a metric has stopped improving,2078
7629,What does ExponentialLR Decay the learning rate of each parameter group by every epoch?,gamma,9834
7630,What set the learning rate of each parameter group according to cyclical learning rate policy?,OneCycleLR,9829
7631,What is used to set the learning rate of each parameter group?,a cosine annealing schedule,5825
7632,What does lr_scheduler.CyclicLR stand for?,OneCycleLR,5825
7633,What is the name of the cosine annealing schedule?,lr_scheduler,5825
7634,What does lr_scheduler.ReduceLROnPlateau do when a metric has stopped improving?,Reduce learning rate,9847
7635,What happens when a metric has stopped improving?,Reduce learning rate,5112
7636,What is the learning rate of each parameter group according to the OneCycleLR learning rate policy?,1cycle,9832
7637,lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what?,1cycle learning rate policy,9832
7638,What does lr_scheduler.CosineAnnealingWarmRestart do?,lr_scheduler.CosineAnnealingWarmRestarts,5850
7639,lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what learning rate policy?,1cycle,5850
7640,What is the cyclical learning rate policy?,CLR,5850
7641,What does lr_scheduler.OneCycleLR set the learning rate of each parameter group according to?,1cycle learning rate policy,5850
7642,How long is the learning rate of each parameter group?,1cycle,9845
7643,What is the learning rate of each parameter group set according to?,1cycle learning rate policy,9845
7644,What does SWA stand for?,Stochastic Weight Averaging,11188
7645,What is the utility function used to do at the end of training?,update SWA batch normalization statistics,11188
7646,What function is used to update the averages of the parameters of the model?,theupdate_parameters()function,11188
7647,What has SWA been proposed in?,Wider Optima and Better Generalization,5727
7648,What does SWA lead to?,Wider Optima and Better Generalization,11187
7649,What class computes the weights of the SWA model?,AveragedModelclass,1383
7650,What will keep track of the running averages of the parameters of the model?,arbitrarytorch.nn.Moduleobject.swa_model,1383
7651,What function can be used to update the averages of the parameters of the model?,theupdate_parameters()function,1383
7652,What will keep track of the running averages of the parameters of the modelmodel?,arbitrarytorch.nn.Moduleobject.swa_model,2791
7653,What is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloaderloader?,update_bn(),2791
7654,What is used to update the running averages of the parameters of the model?,theupdate_parameters()function,2791
7655,What function can be used to update the averages of the parameters of a model?,theupdate_parameters()function,2790
7656,What is the learning rate set to in SWA?,a high constant value,8040
7657,What is the learning rate annealed to in each parameter group?,0.05 in 5 epochs,8040
7658,What does SWALR do?,"anneals the learning rate to a fixed value, and then keeps it constant",8040
7659,What annealing to a fixed value can be used instead of linear annealing?,cosine,8040
7660,How can you use cosine annealing to a fixed value instead of linear annealing?,"settinganneal_strategy=""cos""",8564
7661,What type of annealing can you use instead of linear annealing?,cosine,8564
7662,"If your dataloader has a different structure, you can update the batch normalization statistics of what?",theswa_model,8564
7663,What is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloaderloaderat,update_bn(),1385
7664,What is swa_model?,SWA model,1385
7665,What does the exampleema_modelcompute?,an exponential moving average,1385
7666,What does update_bn() assume that each batch in the dataloaderloaderis either a tensors or a list,each batch in the dataloaderloaderis either a tensors or a list of tensors,1385
7667,What serves to compute the weights of the SWA model?,AveragedModelclass,1385
7668,The following code creates a learning rate scheduler that linearly anneals the learning rate from its initial value to what?,0.05,1385
7669,What can you do if your dataloader has a different structure?,update the batch normalization statistics,1385
7670,"By default,torch.optim.swa_utils.AveragedModelcomputes what of the parameters that you provide",running equal average,1385
7671,What is the SWA model that accumulates the averages of the weights?,swa_model,1385
7672,How many weights does the model train for?,300,1385
7673,What does update_bn() apply to every element in the dataloader?,theswa_model,11292
7674,What applies theswa_modelto every element in the dataloader?,update_bn(),11290
7675,what does update_bn() apply to every element in the dataloader?,theswa_model,11290
7676,What does update_bn() assume each batch in the dataloaderloader is a?,tensor,8222
7677,What does update_bn() assume that each batch in the dataloaderloader is either a list of tensors or,tensors,8222
7678,"If your dataloader has what, you can update the batch normalization statistics of theswa_model by doing a forward pass with the",a different structure,11293
7679,What does update_bn() assume that each batch in the dataloaderloader is a tensors or?,a list of tensors,11291
7680,What does the SWA model accumulate?,the averages of the weights,3775
7681,How many epochs do we train the model for?,300,3775
7682,How many epochs does the SWA model train?,300,2315
7683,"What is filled with numbers sampled from the discrete uniform distribution over[from,to-1]?",Fillsselftensor,2432
7684,"If not specified, the values are usually only what?",bounded byselftensor’s data type,2432
7685,"What will be [0,2mantissa] for floating point types if unspecified?",range,2432
7686,"What will be uniform in[0,253]?",random_(),2432
7687,What does the variant oftorch.quantile() do with NaNvalues?,ignores,7616
7688,What happens if all values in a reduced row areNaN?,If all values in a reduced row areNaNthen the quantiles for that reduction will beNaN,7616
7689,"What is the name of the variant oftorch.quantile() that ""ignores""NaNvalues?",documentation fortorch.quantile(),7616
7690,"What is a variant oftorch.quantile() that ""ignores""NaNvalues?",fortorch.quantile(),7616
7691,What does fortorch.quantile() do?,ignores,7617
7692,"If all values in a reduced row areNaN, then the quantiles for that reduction will beNaN?",If all values in a reduced row areNaN,7617
7693,"What is a scalar or 1D tensor of quantile values in the range [0, 1] dim(int",q(floatorTensor),7617
7694,What is a variant oftorch.quantile() that “ignores”NaNvalues?,fortorch.quantile(),7617
7695,What represents the eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrixinputor,a named tuple,7560
7696,What is represented by eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrixinput,a named tuple,7560
7697,The boolean argumenteigenvectorsdefines computation of both eigenvectors and what else?,eigenvalues,7560
7698,What does this function calculate?,all eigenvalues (and vectors),7561
7699,What portion of the input matrix is used by default?,upper triangular portion,7561
7700,Is torch.symeig() deprecated?,Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh(),7561
7701,What is used if upperis False?,lower triangular portion,7561
7702,What should be replaced with torch.symeig()?,"L,_=torch.symeig(A,upper=upper)",7561
7703,What defines computation of both eigenvectors and eigenvalues or eigenvalues only?,boolean argumenteigenvectors,6979
7704,The default behavior has changed from using which portion of the matrix by default to using the lower triangular portion?,upper triangular portion,6979
7705,"What should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper","L,_=torch.symeig",6979
7706,What is computed if it is False?,eigenvalues,3431
7707,What are computed if it is False?,eigenvalues,3431
7708,What are computed if it is True?,both eigenvalues and eigenvectors,3431
7709,What does.stride() do?,"stridesV.contiguous().transpose(-1, -2)",3431
7710,When are both eigenvalues and eigenvectors computed?,If it is True,7510
7711,"Ifupperis False, what is used?",lower triangular portion,7510
7712,The default behavior has changed from using the upper triangular portion of the matrix by default to using what?,lower triangular portion,7510
7713,"Irrespective of the original strides, what happens to the returned matrixV?",the returned matrixVwill be transposed,7510
7714,How is the matrixVtransposed?,"stridesV.contiguous().transpose(-1, -2).stride()",7510
7715,When is such operation only stable?,when all eigenvalues are distinct,7510
7716,What are computed if the argumenteigenvectors is False?,eigenvalues,7510
7717,"If inputis what, then the eigenvalues of each matrix in the batch are returned in ascending order?",a batch of matrices,7510
7718,What defines computation of both eigenvectors and eigenvalues?,eigenvectors,6978
7719,What are computed if argumenteigenvectors is False?,eigenvalues,6978
7720,What is used if upperis True?,lower triangular portion,6978
7721,What portion of the input matrixinput is used by default?,upper triangular portion,3430
7722,What is the default value for the lower triangular portion of the matrix?,Ifupperis False,5929
7723,What is deprecated in favor oftorch.linalg.eigh()?,Warning torch.symeig()is,5929
7724,What is used to transpose the matrixV?,stridesV.contiguous().transpose,5929
7725,When are the eigenvalues of each matrix in a batch returned in ascending order?,If input is a batch of matrices,5929
7726,"If inputis what, then the eigenvalues of each matrix in the batch is returned in ascending order?",a batch of matrices,5929
7727,What should be replaced with the lower triangular portion?,"L,_=torch.symeig(A,upper=upper)",5928
7728,What is the default behavior of the lower triangular portion of the matrix?,Ifupperis False,3645
7729,What is used ifupperis False?,lower triangular portion,7509
7730,What function is deprecated in favor oftorch.linalg.eigh()?,torch.symeig(),7509
7731,What will be removed in a future PyTorch release?,Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh(),8218
7732,"What should be replaced with L,V=torch.symeig?","L,_=torch.symeig",4051
7733,When are the eigenvalues of each matrix in a batch of matrices returned in ascending order?,If input is,4050
7734,Which matrix will be transposed regardless of the original strides?,matrixV,4050
7735,"If inputis a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order",If input is a batch of matrices,4050
7736,In what order are eigenvalues returned?,ascending,4050
7737,In what order are the eigenvalues returned?,ascending order,7041
7738,"Irrespective of the original strides, what happens to the returned matrix?",matrixVwill be transposed,4052
7739,What matrix will be transposed regardless of the original strides?,matrixV,4399
7740,Warning Extra care needs to be taken when what?,backward through outputs,8147
7741,When is a backward through output operation stable?,when all eigenvalues are distinct,3868
7742,What needs to be taken when backward through outputs?,Extra care,3868
7743,"What is the input tensor of size(*,n,n)(*,n,n)(*,n,n)",symmetric or Hermitian matrices,2364
7744,What controls whether eigenvectors have to be computed?,eigenvectors,2364
7745,When is backward through outputs stable?,when all eigenvalues are distinct,8148
7746,"What is the input tensor of size(,n,n)(*, n, n)(,n,",zero,8148
7747,"eigenvectors(boolean,optional) – controls whether eigenvectors have to be computed what?",upper,8148
7748,What controls whether eigenvectors have to be computed upper?,eigenvectors,8148
7749,What is an empty tensor?,If eigenvectors=False,8148
7750,"If eigenvectors=False, it's an empty tensor. Otherwise, this tensor contains what?",orthonormal eigenvectors of theinput,8148
7751,What order are the eigenvalues in?,ascending,8148
7752,"If eigenvectors=False, it’s an empty tensor. Otherwise, this tensor contains what?",orthonormal eigenvectors,8148
7753,In what order are the eigenvalues in a named tuple?,ascending,8148
7754,What are the eigenvalues in ascending order?,eigenvectors,8148
7755,"If eigenvectors=False, what is it?",empty tensor,8148
7756,"What is an example of a (Tensor, Tensor)?",Examples,8148
7757,What are the batch dimensions consisting of?,symmetric or Hermitian matrices,9653
7758,What is a named tuple?,"eigenvalues, eigenvectors",9653
7759,In what order are the eigenvalues in?,ascending,11297
7760,What controls whether to consider upper-triangular or lower-triangular region out?,upper,11297
7761,What is the eigenvectors(Tensor)?,Shape,11297
7762,What is the name of the tuple that contains the orthonormal eigenvectors of the input?,Tensor,11297
7763,"What controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) – the output","upper(boolean,optional)",11297
7764,"What is the output tuple of (Tensor, Tensor) containing eigenvalues in ascending order?",eigenvectors,11297
7765,"If eigenvectors=False, it’s an empty tensor. Otherwise, this tensor contains the ortho",If eigenvectors=False,11297
7766,"eigenvectors(boolean,optional) – controls whether to consider what regions?",upper-triangular or lower-triangular region,9238
7767,What is the name of the function used by Alias fortorch.special.expit?,Alias fortorch.special.expit(),1059
7768,What does Alias fortorch.special.expit() do?,Alias fortorch.special.expit(),1059
7769,What is the element-wise greatest common divisor of inputandother?,GCD,1743
7770,Both inputandothermust have what?,integer types,1743
7771,"What does gcd(0,0)=0gcd(0,0) =?","0gcd(0,0)",1743
7772,Other(Tensor) – what is the output tensor?,second input tensor out,1743
7773,What returns true if the data type ofinputis a floating point data type?,if the data type ofinputis a floating point data type,5234
7774,Returns True what if the data type ofinputis a floating point data type?,if the data type ofinputis a floating point data type,5234
7775,What are the basic building blocks for graphs?,torch,7431
7776,Holds submodules in a dictionary. Holds parameters in a dictionary. Holds submodules in a dictionary. Hold,Holds submodules in a list,7431
7777,Holds parameters in a list. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary,Holds parameters in a list,7431
7778,nn.Conv2d Applies a what convolution over an input signal composed of several input planes?,2D,7431
7779,nn.Conv3d Applies a what type of convolution over an input signal composed of several input planes?,3D,7431
7780,nn.ConvTranspose1d Applies a what transposed convolution operator over an input image composed of several input planes,1D,7431
7781,What is to be considered a module parameter?,Tensor,805
7782,What is a kind of Tensor that is to be considered?,module parameter,805
7783,What does nn.Conv1d and nn.Conv3d use?,nn.ConvTranspose2d,805
7784,"Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) What are Pooling layers?",Containers Convolution Layers,1859
7785,"What is a weighted sum, nonlinearity?",Padding Layers Non-linear Activations,4719
7786,What is a parameter that is to be considered a module parameter?,not initialized,4333
7787,What is another term for Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Function,Non-linear Activations,4333
7788,What type of container is a base class for all neural network modules?,sequential container,4333
7789,"Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other",Pooling layers,4874
7790,nn.Conv1d Applies what convolution over an input signal composed of several input planes?,1D,4874
7791,What transposed convolution operator does nn.ConvTranspose1d Applies?,1D,4874
7792,What are Padding Layers?,Pooling layers,4872
7793,"What are Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normal",Padding Layers,4718
7794,What is not initialized?,buffer,780
7795,What applies a 3D convolution over an input signal composed of several input planes?,nn.ConvTranspose3d,780
7796,What hook is registered for all modules?,global forward hook,780
7797,What does nn.ConvTranspose1d and nn.Conv2d use?,nn.ConvTranspose3d,780
7798,Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shu,Normalization Layers,4337
7799,What class holds submodules in a list?,Base class,4337
7800,What does a sequential container hold in a list?,parameters,835
7801,What type of container holds submodules in a list?,sequential container,835
7802,What is registered for all the modules?,global forward hook,835
7803,What does a sequential container hold in a dictionary?,parameters,835
7804,A sequential container. Holds submodules in what?,dictionary,835
7805,What is another name for a sequential container?,nn.LazyConv1d,835
7806,What does a sequential container do?,Holds submodules in a list,1403
7807,Holds submodules in what?,a dictionary,1403
7808,What is a base class for all neural network modules?,sequential container,1403
7809,What does Holds parameters in a list?,Holds parameters in a list,1403
7810,What is a forward pre-hook common to all modules?,Global Hooks For Module Registers,1403
7811,What is common to all the modules?,a backward hook,1403
7812,What is held in a dictionary?,parameters,1403
7813,Where do parameters in a neural network belong?,a dictionary,1403
7814,A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds what class for all neural network,Base class,1403
7815,What are Sparse Layers?,Dropout Layers,2215
7816,A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds what for all neural network modules,Base class,779
7817,What is the name of the class that registers a global forward hook for all the modules?,nn.Conv1d,779
7818,What is a global forward hook common to all modules?,nn.Conv1d,779
7819,What is a module parameter a kind of?,Tensor,2160
7820,Where are parameters stored?,a dictionary,2160
7821,What are Shuffle Layers?,Loss Functions Vision Layers,4159
7822,What does Holds parameters in a dictionary?,Holds parameters in a list,2070
7823,"What is multi-GPU, distributed?",DataParallel Layers,2070
7824,What does Holds parameters in a dictionary do?,Holds parameters in a dictionary,2070
7825,"What is multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization",DataParallel Layers,2070
7826,Where can parameters be held?,a dictionary,804
7827,What does the Global Hooks For Module register for all the modules?,global forward hook,804
7828,What is a module parameter?,A parameter that is not initialized,804
7829,What are Vision Layers Shuffle Layers?,Loss Functions,4160
7830,"What are multi-GPU, distributed?",Shuffle Layers DataParallel Layers,5897
7831,What is a global hook for module?,Global Hooks For Module,5897
7832,What is a container that holds submodules in a list?,sequential container,829
7833,What does nn.Conv1d do?,Holds parameters in a dictionary,829
7834,Where are parameters held?,a dictionary,5068
7835,What does a global hook for module do?,Holds submodules in a list,2069
7836,What is the name of the hook that is to be considered a module parameter?,Global Hooks For Module,2069
7837,What type of initialization is to be considered a module parameter?,Tensor,8124
7838,What does a module hold in a list?,Holds parameters in a dictionary,4066
7839,What is registered for all modules?,global forward hook,4067
7840,What type of convolution does nn.Conv1d apply over an input signal composed of several input planes?,1D,10017
7841,What applies a 1D transposed convolution operator over an input image composed of several input planes?,nn.ConvTranspose1d,10017
7842,What type of convolution does nn.Conv2d Applies over an input signal composed of several input planes?,2D,10017
7843,What applies a 1D convolution over an input signal composed of several input planes?,nn.Conv2d,4335
7844,Where are submodules held?,a dictionary,834
7845,What type of hook is registered for all the modules?,a global forward hook,834
7846,What Registers a forward pre-hook common to all modules?,Global Hooks For Module,2949
7847,What is another name for nn.LazyConv1d?,nn.LazyConv1d,2949
7848,What hook does Global Hooks For Module register?,global forward hook,2949
7849,What type of convolution does nn.Conv2d apply over an input signal composed of several input planes?,2D,10021
7850,nn.ConvTranspose2d Applies what type of transposed convolution operator over an input image composed of several input planes,2D,10021
7851,What applies a 2D convolution over an input signal composed of several input planes?,nn.Conv3d,7997
7852,What is a global forward pre-hook common to all modules?,Global Hooks For Module Registers a forward pre-hook common to all modules,7997
7853,What is another term for holding submodules in a list?,Holds submodules in a dictionary,2948
7854,What does a dictionary hold?,parameters,2948
7855,nn.Conv1d Applies a 1D convolution over an input signal composed of what?,several input planes,10016
7856,nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of what?,several input planes,10016
7857,What type of convolution does nn.Conv3d apply over an input signal composed of several input planes?,3D convolution,10024
7858,What is nn.LazyConv3d a torch.nn.Conv2dmodule with lazy initialization of the,nn.LazyConvTranspose1d,10024
7859,nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of what?,several input planes,1196
7860,Which a torch.nn.Conv2dmodule has lazy initialization of thein_channelsargument of theConv,nn.LazyConv3d,1196
7861,Applies a 1D convolution over an input signal composed of what?,several input planes,1195
7862,What does nn.ConvTranspose1d apply over an input image composed of several input planes?,1D transposed convolution operator,1224
7863,What does nn.ConvTranspose2d apply over an input image composed of several input planes?,2D transposed convolution operator,10027
7864,nn.ConvTranspose1d Applies what transposed convolution operator over an input image composed of several input planes?,1D,10027
7865,What does nn.ConvTranspose1d Apply over an input image composed of several input planes?,1D transposed convolution operator,10027
7866,What is another name for nn.Conv2d?,nn.ConvTranspose3d,10020
7867,nn.Conv3d Applies what type of convolution over an input signal composed of several input planes?,3D,1209
7868,What is the name of the program that applies a 2D transposed convolution over an input signal?,nn.ConvTranspose3d,1209
7869,nn.ConvTranspose1d Applies what type of transposed convolution operator over an input image composed of several input planes,1D,1223
7870,nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of what?,several input planes,1223
7871,What type of transposed convolution operator does nn.ConvTranspose2d apply over an input image composed of several input planes,2D,1223
7872,What is the name of the program that applies a 3D transposed convolution over an input signal?,nn.LazyConv1d,1223
7873,nn.Conv3d Applies a 3D convolution over an input signal composed of what?,several input planes,10023
7874,What does nn.ConvTranspose3d apply over an input image composed of several input planes?,3D transposed convolution operator,10032
7875,a torch.nn.Conv1dmodule with lazy initialization of what of theConv1d?,thein_channelsargument,10032
7876,What is a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theCon,nn.LazyConv2d,10032
7877,What is a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument,nn.LazyConvTranspose1d,10032
7878,a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of the,nn.LazyConvTranspose2d,10032
7879,Which a torch.nn.Conv1dmodule has lazy initialization of thein_channelsargument of theConv,nn.LazyConv2d,10026
7880,nn.ConvTranspose3d Applies what transposed convolution operator over an input image composed of several input planes?,3D,10026
7881,nn.ConvTranspose2d Applies what transposed convolution operator over an input image composed of several input planes?,2D,1201
7882,What type of transposed convolution operator does nn.ConvTranspose3d Applies over an input image composed of several input plane,3D,1201
7883,What is nn.LazyConv1d a torch.nn.Conv2dmodule with lazy initialization of the,nn.LazyConvTranspose2d,1201
7884,What transposed convolution operator is applied over an input image composed of several input planes?,1D,1201
7885,What is inferred from theinput.size(1)?,thein_channelsargument,10117
7886,What Combines an array of sliding local blocks into a large containing tensor?,nn.Fold,10117
7887,What is the name of the module with lazy initialization of thein_channelsargument of theConv1d?,nn.LazyConv2d,10029
7888,What type of transposed convolution operator does nn.ConvTranspose3d apply?,3D,10029
7889,What is lazy initialization of of theConv1d that is inferred from theinput.size(1)?,thein_channelsargument,10029
7890,Which a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv,nn.LazyConv2d,1200
7891,What transposed convolution operator does nn.ConvTranspose2d Applies over an input image composed of several input planes?,1D,1200
7892,What type of transposed convolution operator does nn.ConvTranspose2d apply?,2D,10030
7893,What is the name of the 3D transposed convolution operator?,nn.LazyConvTranspose3d,10030
7894,What applies over an input image composed of several input planes?,2D transposed convolution operator,1218
7895,What type of transposed convolution operator does nn.ConvTranspose3d Applies?,3D,836
7896,Holds what?,parameters in a dictionary,836
7897,Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary,Holds parameters in a list,836
7898,Holds submodules in a list <sep>,Holds submodules in a list,836
7899,What module with lazy initialization of thein_channelsargument of theConv3d is inferred from theinput.size,nn.LazyConv3d a torch.nn.Conv3d,836
7900,What module has lazy initialization of thein_channelsargument of theConvTranspose1d?,nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d,836
7901,Holds submodules in a list. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in,sequential container,836
7902,What is the name of the nn.LazyConvTranspose2d?,Ator,836
7903,What does a torch.nn.Conv3dmodule use to initialize thein_channelsargument of theConv,nn.LazyConvTranspose1d,10113
7904,What is the lazy initialization of thein_channelsargument of theConv3d that is inferred from theinput.,nn.LazyConv3d a torch.nn.Conv3dmodule,10114
7905,What is the lazy initialization of thein_channelsargument of theConvTranspose3d that is inferred from the,nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule,10114
7906,What extracts sliding local blocks from a batched input tensor?,nn.Fold,10114
7907,a torch.nn.Conv1dmodule with lazy initialization of what?,thein_channelsargument of theConv1d,8774
7908,What is the lazy initialization of thein_channelsargument of theConvTranspose1d that is inferred from the,nn.LazyConvTranspose1d,8774
7909,What is the lazy initialization of thein_channelsargument of theConvTranspose2d that is inferred from the,nn.LazyConvTranspose2d,8774
7910,a torch.nn.Conv2dmodule with what initialization of thein_channelsargument of theConv2,lazy,8773
7911,What is a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theCon,nn.LazyConv3d,1219
7912,What does nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule,thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1),1219
7913,What type of transposed convolution operator does nn.ConvTranspose3d Applies a 3D transposed convolution operator,2D,1219
7914,What is the name of a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channels,nn.LazyConvTranspose1d,1219
7915,Which a torch.nn.ConvTranspose2dmodule has lazy initialization of thein_channelsargument of,nn.LazyConvTranspose2d,10116
7916,Which a torch.nn.ConvTranspose1dmodule has lazy initialization of thein_channelsargument of,nn.LazyConvTranspose1d,10116
7917,What is the name of the module that has lazy initialization of thein_channelsargument of theConv2d?,nn.LazyConvTranspose2d,10116
7918,a torch.nn.Conv2dmodule with lazy initialization of what?,thein_channelsargument of theConv2d,8775
7919,a torch.nn.Conv3dmodule with what initialization of thein_channelsargument of theConv3,lazy,8775
7920,Which a torch.nn.Conv3dmodule has lazy initialization of thein_channelsargument of theConv,nn.LazyConvTranspose3d,10119
7921,What is the name of the module that has lazy initialization of thein_channelsargument of theConv3d?,nn.LazyConvTranspose3d,10119
7922,What type of initialization of thein_channelsargument of theConvTranspose1d that is inferred from thein,lazy,8776
7923,What is nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule,nn.Unfold,10121
7924,What type of initialization of thein_channelsargument of theConvTranspose2d that is inferred from thein,lazy,8777
7925,What is another name for lazy initialization of thein_channelsargument of theConvTranspose3d?,nn.Unfold,8777
7926,What does nn.Unfold extract sliding local blocks from?,batched input tensor,10123
7927,nn.MaxPool1d Applies what type of max pooling over an input signal composed of several input planes?,1D,10139
7928,What does nn.MaxPool1d apply over an input signal composed of several input planes?,1D max pooling,10139
7929,What type of max pooling does nn.MaxPool2d apply over an input signal composed of several input planes?,2D,10140
7930,What does nn.LPPool1d stand for?,nn.LPPool1d,10140
7931,What type of max pooling does nn.MaxPool3d apply over an input signal composed of several input planes?,3D,10143
7932,nn.MaxPool2d Applies what type of max pooling over an input signal composed of several input planes?,2D,10143
7933,What does nn.MaxPool2d apply over an input signal composed of several input planes?,2D max pooling,10143
7934,What does nn.MaxPool2d Applies over an input signal composed of several input planes?,2D max pooling,1198
7935,MaxPool2d Applies what type of max pooling over an input signal composed of several input planes?,2D,1198
7936,What type of pooling does nn.MaxPool2d apply over an input signal composed of several input planes?,1D max pooling,1198
7937,What is the name of the pooling algorithm that computes a partial inverse of MaxPool1d?,nn.AvgPool1d,1198
7938,What does nn.MaxPool3d apply over an input signal composed of several input planes?,3D max pooling,10145
7939,What Computes a partial inverse of MaxPool1d?,nn.MaxUnpool1d,10149
7940,What Computes a partial inverse of MaxPool2d?,nn.MaxUnpool2d,10141
7941,What is the fractional max pooling over an input signal composed of several input planes?,2D,10141
7942,What type of pooling does nn.LPPool1d apply?,1D power-average,10141
7943,What does nn.MaxPool1d Apply?,1D max pooling over an input signal,10141
7944,nn.MaxPool2d Applies a 2D max pooling over an input signal composed of what?,several input planes,10141
7945,nn.MaxPool3d Applies a 3D max pooling over an input signal composed of what?,several input planes,10141
7946,MaxUnpool1d Computes a partial what of MaxPool1d?,inverse,10141
7947,nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of what?,several input planes,10141
7948,What type of pooling does AdaptiveMaxPool3d apply?,3D adaptive max,10141
7949,What Computes a partial inverse of MaxPool3d?,nn.MaxUnpool3d,1646
7950,What computes a partial inverse ofMaxPool1d?,nn.AdaptiveMaxPool2d,1646
7951,What does nn.MaxUnpool2d do?,Computes a partial inverse ofMaxPool1d,1646
7952,What does nn.MaxPool3d Applies over an input signal composed of several input planes?,3D max pooling,1216
7953,What Computes a partial inverse ofMaxPool2d?,nn.MaxUnpool2d,1645
7954,What is a partial inverse of MaxPool1d?,nn.FractionalMaxPool2d,1645
7955,What Computes a partial inverse ofMaxPool3d?,nn.MaxUnpool3d,1649
7956,What does nn.AvgPool1d apply over an input signal composed of several input planes?,1D average pooling,1651
7957,What computes a partial inverse ofMaxPool3d?,nn.AdaptiveMaxPool3d,1651
7958,What applies a 1D average pooling over an input signal composed of several input planes?,nn.AvgPool1d,1199
7959,MaxPool3d Applies what type of max pooling over an input signal composed of several input planes?,3D,10146
7960,What is another name for AdaptiveMaxPool1d?,AdaptiveMaxPool1d,10146
7961,What type of pooling does nn.MaxPool3d apply over an input signal composed of several input planes?,2D max pooling,1215
7962,MaxPool3d Applies what over an input signal composed of several input planes?,3D max pooling,1215
7963,What does nn.AvgPool1d Applies over an input signal composed of several input planes?,1D average pooling,1232
7964,What does nn.FractionalMaxPool3d Applies over an input signal composed of several input planes?,3D fractional max pooling,1232
7965,What type of pooling does nn.AvgPool1d apply over an input signal composed of several input planes?,1D average pooling,1231
7966,What applies a 3D pooling over an input signal composed of several input planes?,nn.AvgPool3d,1231
7967,What type of pooling does nn.AvgPool2d apply over an input signal composed of several input planes?,2D average pooling,9993
7968,What applies a 2D average pooling over an input signal composed of several input planes?,nn.AvgPool2d,1192
7969,What does nn.AvgPool2d apply over an input signal composed of several input planes?,2D average pooling,1192
7970,What applies a 3D average pooling over an input signal composed of several input planes?,nn.AdaptiveMaxPool3d,1192
7971,nn.AvgPool1d Applies what type of average pooling over an input signal composed of several input planes,1D,9990
7972,What type of pooling does nn.AvgPool3d apply over an input signal composed of several input planes?,2D average pooling,1206
7973,What does nn.FractionalMaxPool2d do?,nn.FractionalMaxPool2d,10148
7974,What is the name of the pooling algorithm that computes a partial inverse of MaxPool2d?,nn.FractionalMaxPool2d,10151
7975,What does nn.AvgPool3d apply over an input signal composed of several input planes?,3D average pooling,9995
7976,What does nn.FractionalMaxPool2d apply over an input signal composed of several input planes?,2D fractional max pooling,10057
7977,nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of what?,several input planes,10057
7978,What type of fractional max pooling does nn.FractionalMaxPool2d apply?,2D,10057
7979,nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of,several input planes,10057
7980,Computes a partial what of MaxPool2d?,inverse,1650
7981,What does nn.MaxUnpool3d compute?,partial inverse,10153
7982,What does nn.AvgPool2d Apply over an input signal composed of several input planes?,2D average pooling,9992
7983,What does nn.FractionalMaxPool3d apply over an input signal composed of several input planes?,3D fractional max pooling,10059
7984,What does nn.LPPool1d apply over an input signal composed of several input planes?,1D power-average pooling,10059
7985,AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of what?,several input planes,10059
7986,AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of what?,several input planes,10059
7987,AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of what?,several input planes,10059
7988,AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of what?,several input planes,10059
7989,What does nn.AvgPool3d Applies over an input signal composed of several input planes?,2D average pooling,1205
7990,What does nn.FractionalMaxPool2d Applies over an input signal composed of several input planes?,2D fractional max pooling,1220
7991,nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed,several input planes,1220
7992,What applies over an input signal composed of several input planes?,3D average pooling,1220
7993,What does Applies over an input signal composed of several input planes?,3D average pooling,1220
7994,What does nn.LPPool1d Apply over an input signal composed of several input planes?,1D power-average pooling,1225
7995,AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes?,nn,1225
7996,What does nn.LPPool2d Apply over an input signal composed of several input planes?,2D power-average pooling,10100
7997,What is another name for AdaptiveMaxPool3d?,AdaptiveAvgPool1d,10100
7998,AdaptiveAvgPool1d Applies a 1D adaptive max pooling over an input signal composed of several input plane,nn,10100
7999,nn.FractionalMaxPool3d Applies a fractional max pooling over an input signal composed of several,3D,1210
8000,What type of fractional max pooling does nn.FractionalMaxPool3d Applies?,3D,1210
8001,nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of what?,several input planes,10098
8002,nn.LPPool1d Applies what power-average pooling over an input signal composed of several input planes?,1D,10098
8003,What type of adaptive max pooling does AdaptiveMaxPool1d apply?,1D,1217
8004,What type of adaptive max pooling does AdaptiveMaxPool3d apply?,3D,1217
8005,How many input planes does AdaptiveAvgPool1d have?,nn,1217
8006,What type of power-average pooling is applied over an input signal composed of several input planes?,2D,1217
8007,AdaptiveMaxPool3d Applies what type of adaptive max pooling over an input signal composed of several input planes?,3D,1204
8008,What type of adaptive average pooling does AdaptiveAvgPool1d apply?,1D,1204
8009,How many input planes does AdaptiveAvgPool2d have?,nn,1204
8010,What does AdaptiveAvgPool1d Applies over an input signal composed of several input planes?,1D adaptive average pooling,1204
8011,AdaptiveMaxPool2d Applies what type of adaptive max pooling over an input signal composed of several input planes?,2D,1187
8012,AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input plane,nn,1187
8013,AdaptiveAvgPool2d Applies a 2D adaptive max pooling over an input signal composed of several input plane,nn,9984
8014,What type of pooling does AdaptiveAvgPool1d apply?,1D,9988
8015,What type of pooling does AdaptiveAvgPool2d apply?,2D,9988
8016,What does AdaptiveMaxPool3d apply over an input signal composed of several input planes?,3D adaptive max pooling,9988
8017,What does AdaptiveAvgPool1d apply over an input signal composed of several input planes?,1D adaptive average pooling,9988
8018,AdaptiveAvgPool2d Applies what type of adaptive average pooling over an input signal composed of several input planes,2D,9988
8019,AdaptiveAvgPool3d Applies a 3D adaptive max pooling over an input signal composed of several input plane,nn,9986
8020,What does AdaptiveAvgPool1d Applies?,1D adaptive average pooling over an input signal,9986
8021,What does nn.ReplicationPad1d Pads the input tensor using?,replication of the input boundary,10185
8022,nn.ZeroPad2d Pads the input tensor boundaries with what value?,zero,10185
8023,nn.ConstantPad1d Pads the input tensor boundaries with what value?,constant value,10185
8024,What does nn.ReplicationPad3d do?,replication of the input boundary,10186
8025,ReplicationPad2d Pads the input tensor using what?,replication of the input boundary,10186
8026,Which Pads the input tensor boundaries with a constant value?,nn.ConstantPad3d,10186
8027,What Pads the input tensor boundaries with zero?,nn.ConstantPad1d,4723
8028,ReplicationPad3d Pads the input tensor using what?,replication of the input boundary,4723
8029,What does ZeroPad2d pad the input tensor boundaries with?,zero,4723
8030,What does nn.ReflectionPad1d Pads the input tensor using?,reflection of the input boundary,10183
8031,ReflectionPad2d Pads the input tensor using what?,reflection of the input boundary,10183
8032,What Pads the input tensor using replication of the input boundary?,nn.ReplicationPad3d Pads,10183
8033,What does nn.ReplicationPad2d Pads the input tensor use?,replication of the input boundary,10183
8034,What does nn.ZeroPad2d pad the input tensor boundaries with?,zero,10183
8035,What value does ZeroPad2d pad the input tensor boundaries with?,zero,10183
8036,What do constantPad1d Pads the input tensor boundaries with?,constant value,10183
8037,What do constantPad2d Pads the input tensor boundaries with?,constant value,10183
8038,What does replicationPad1d Pads the input tensor using?,replication,10183
8039,nn.ConstantPad3d Pads the input tensor boundaries with what value?,constant value,10183
8040,What is used to pad the input tensor?,reflection of the input boundary,4724
8041,What does Pads the input tensor use?,reflection of the input boundary,4724
8042,ReplicationPad1d Pads the input tensor using what?,replication of the input boundary,4724
8043,What value does constantPad1d Pads the input tensor boundaries with?,constant,4722
8044,Pads the input tensor using what?,replication of the input boundary,4722
8045,What does nn.ReplicationPad2d use?,replication of the input boundary,4722
8046,What is the input tensor using replication of the input boundary?,Pads,4722
8047,What do constantPad3d Pads the input tensor boundaries with?,constant value,4722
8048,What Applies the hard shrinkage function element-wise?,nn.Hardshrink,10070
8049,What function does Hardsigmoid apply element-wise?,hard shrinkage,1254
8050,What is the hard shrinkage function element-wise?,nn,1254
8051,What Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend information from different representation subspace,nn.LogSigmoid,10073
8052,What does ReLU Apply element-wise?,rectified linear unit function,10073
8053,ReLU Applies what element-wise function?,rectified linear unit function,10073
8054,What element-wise function does nn.LeakyReLU Applies: nn.LogSigmoid Applies: n,hardswish function,1258
8055,What applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation sub,nn.LogSigmoid,10127
8056,What is the name of the randomized leaky rectified liner unit function?,nn.SELU,1252
8057,What Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation sub,nn.LogSigmoid,1251
8058,"What is the randomized leaky rectified liner unit function, element-wise, as described in the paper?",nn.CELU,1251
8059,What function does GELU apply?,Gaussian Error Linear Units,10061
8060,What Applies the Gaussian Error Linear Units function?,nn.GELU,10061
8061,What does GELU apply?,Gaussian Error Linear Units function,10161
8062,How does the model attend to information from different representation subspaces?,jointly,1098
8063,Allows the model to jointly attend to what from different representation subspaces?,information,1098
8064,"Who applies the randomized leaky rectified liner unit function, element-wise?",nn.Mish,1260
8065,"Who applies the Sigmoid Linear Unit (SiLU) function, element-wise?",nn.Mish,1260
8066,"What Applies the randomized leaky rectified liner unit function, element-wise?",nn.RReLU,10179
8067,"nn.RReLU Applies the randomized leaky rectified liner unit function, what?",element-wise,10179
8068,"What Applied element-wise, as: nn.CELU Applies the element-wise function: nn.Sigmoid App",nn.SELU,10188
8069,Who Applies the element-wise function?,nn.Sigmoid,1250
8070,"Applied element-wise, as: what?",nn,1165
8071,What is the name of the Gaussian Error Linear Units function?,nn.Sigmoid,1165
8072,"What Applies the Mish function, element-wise?",Mish,10191
8073,What function does nn.Sigmoid Applies?,Gaussian Error Linear Units,1248
8074,Which Gaussian Error Linear Units function Applies the element-wise function?,nn.Sigmoid,1248
8075,What Applies Softmax over features to each spatial location?,nn.Softmax2d,10197
8076,What Applies SoftMax over features to each spatial location?,Softmax2d,10202
8077,Who wrote Efficient softmax approximation for GPUs?,"Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou",10202
8078,"Along with Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and what other person, what",Hervé Jégou,10202
8079,What applies Softmax over features to each spatial location?,nn.LogSoftmax,10201
8080,What function is applied to an n-dimensional input Tensor?,Softmin,1249
8081,What is another name for Softmax2d?,nn.LogSoftmax,1249
8082,nn.Softmax2d Applies SoftMax to each spatial location?,over features,10199
8083,What is the input of LogSoftmax?,Tensor,10199
8084,What dimension is the input Tensor?,n-dimensional,10199
8085,What applies Batch Normalization over a 4D input?,nn.BatchNorm3d,10005
8086,What is a mini-batch of?,1D inputs with optional additional channel dimension,1175
8087,What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm1d?,nn.LazyBatchNorm2d,1175
8088,What inputs does Batch Normalization apply over?,2D or 3D,1175
8089,nn.BatchNorm2d Applies Batch Normalization over a what input?,4D,1175
8090,What is a 2D or 3D input?,a mini-batch of 1D inputs with optional additional channel dimension,1174
8091,What applies Batch Normalization over a 2D or 3D input?,nn.BatchNorm3d,1174
8092,What input does BatchNorm2d apply Batch Normalization over?,4D,10006
8093,What applies Batch Normalization over a 5D input?,nn.GroupNorm,10006
8094,nn.BatchNorm3d Applies Batch Normalization over what input?,5D,10006
8095,nn.BatchNorm2d Applies Batch Normalization over what input?,4D input,10006
8096,What is the name of the group that applies Batch Normalization over a 5D input?,nn.GroupNorm,10006
8097,What input does nn.BatchNorm3d apply Batch Normalization over?,5D input,10007
8098,What is the name of the module that uses lazy initialization of thenum_featuresargument of theBatchNorm1d?,nn.LazyBatchNorm2d,10007
8099,What is another name for nn.LazyBatchNorm2d?,nn.LazyBatchNorm2d,10007
8100,How many inputs does Batch Normalization apply over?,4D,1178
8101,What input does Batch Normalization apply over?,5D,1178
8102,What is the name of the group that applies Batch Normalization over a 4D input?,nn.GroupNorm,1178
8103,How many inputs does nn.BatchNorm3d apply Batch Normalization over?,5D,1177
8104,What is the mini-batch of 2D inputs with additional channel dimension?,4D input,1177
8105,nn.BatchNorm3d Applies Batch Normalization over a what input?,5D,1177
8106,What Applies Batch Normalization over a 5D input as described in the paperBatch Normalization: Accelerating Deep Network Training by Reduc,nn.LazyBatchNorm1d,1177
8107,What is the lazy initialization of thenum_featuresargument of theBatchNorm1d inferred from?,theinput.size(1),1181
8108,What Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization?,nn.GroupNorm,1181
8109,What does GroupNorm Applies over a mini-batch of inputs as described in the paperGroup Normalization?,Group Normalization,1181
8110,a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatch,nn.LazyBatchNorm2d,1181
8111,What input is used for Batch Normalization?,5D,1180
8112,What is inferred from the number_featuresargument of theBatchNorm1d?,theinput.size,1180
8113,What is the lazy initialization of theBatchNorm1d inferred from?,theinput.size(1),10105
8114,Which module has lazy initialization of thenum_featuresargument of theBatchNorm3d that is inferred from theinput,nn.GroupNorm,10105
8115,What is the lazy initialization of theBatchNorm2d inferred from?,theinput.size(1),10008
8116,Where is the number_featuresargument of theBatchNorm3d inferred from?,theinput.size,10008
8117,Over what input is Batch Normalization applied?,5D input,10008
8118,What is the name of the module that has lazy initialization of thenum_featuresargument of theBatchNorm3d?,nn.GroupNorm,8768
8119,What is the name of a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargu,nn.LazyBatchNorm3d,8768
8120,a torch.nn.BatchNorm1dmodule with lazy initialization of what?,thenum_featuresargument of theBatchNorm1d,8769
8121,a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatch,nn.LazyBatchNorm3d,8769
8122,a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatch,nn.LazyBatchNorm2d,8769
8123,What is the name of the module that allows Batch Normalization over a N-Dimensional input?,nn.InstanceNorm1d,8769
8124,What is the lazy initialization of thenum_featuresargument of theBatchNorm2d that is inferred from theinput,nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule,10009
8125,What input does nn.InstanceNorm1d apply Instance Normalization over?,3D input,10009
8126,What type of input does nn.InstanceNorm2d apply Instance Normalization over?,4D,10009
8127,What is a mini-batch of 3D inputs with additional channel dimension?,5D input,10009
8128,What is a mini-batch of [N-2]D inputs with additional channel dimension?,N-Dimensional input,10009
8129,What is a mini-batch of 2D inputs with additional channel dimension?,4D input,10009
8130,What is nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule?,nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule,10009
8131,What is the lazy initialization of thenum_featuresargument of theBatchNorm3d that is inferred from theinput,nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule,10009
8132,What is the lazy initialization of thenum_featuresargument of theBatchNorm2d inferred from?,theinput.size(1),10108
8133,What is the number of featuresargument of theBatchNorm3d inferred from?,theinput.size,10108
8134,What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm2d?,nn.InstanceNorm2d,10108
8135,a torch.nn.BatchNorm2dmodule with lazy initialization of what?,thenum_featuresargument of theBatchNorm2d,8770
8136,a torch.nn.BatchNorm3dmodule with lazy initialization of what?,thenum_featuresargument of theBatchNorm3d,8772
8137,What is the mini-batch of 2D inputs with additional channel dimension described in the paperInstance Normalization: The Missing Ingredient for Fast,4D input,8772
8138,What is the name of the module that applies Batch Normalization over a 4D input?,nn.InstanceNorm3d,8772
8139,What is inferred from theBatchNorm3d?,theinput.size,10110
8140,What does GroupNorm apply Group Normalization over?,a mini-batch of inputs,8771
8141,What is the paperBatch Normalization?,Accelerating Deep Network Training,10111
8142,What Applies Instance Normalization over a 3D input?,nn.InstanceNorm1d,10111
8143,What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm3d?,nn.InstanceNorm3d,10111
8144,What is the name of the class that applies Batch Normalization over a N-Dimensional input?,nn.InstanceNorm1d,1184
8145,What is the paperInstance Normalization?,The Missing Ingredient for Fast Stylization,1182
8146,What applies Instance Normalization over a 3D input as described in the paperInstance Normalization: The Missing Ingredient for Fast Sty,nn.InstanceNorm2d,1182
8147,InstanceNorm1d Applies Instance Normalization over what input?,3D,1185
8148,What Applies Instance Normalization over a 4D input?,nn.InstanceNorm3d,1185
8149,What is a mini-batch of 2D inputs with additional channel dimension described in the paperInstance Normalization: The Missing Ingredient for,4D input,1185
8150,What applies Group Normalization over a mini-batch of inputs as described in the paperInstance Normalization: The Missing Ingredient for,nn.InstanceNorm3d,1185
8151,What applies Instance Normalization over a 3D input?,nn.InstanceNorm2d,10206
8152,What does nn.InstanceNorm1d apply Instance Normalization over?,a 3D input,10085
8153,InstanceNorm2d Applies Instance Normalization over what input?,4D,10086
8154,What is an example of a mini-batch of 1D inputs with optional additional channel dimension?,3D input,10086
8155,InstanceNorm3d Applies Instance Normalization over what input?,5D,10086
8156,What does nn.LayerNorm Apply over a mini-batch of inputs?,Layer Normalization,10086
8157,What does LayerNorm Applies over a mini-batch of inputs?,Layer Normalization,10086
8158,What applies Instance Normalization over a 4D input?,nn.InstanceNorm3d,10068
8159,What Applies Instance Normalization over a 5D input?,nn.LayerNorm,1186
8160,How many inputs does nn.LayerNorm apply Instance Normalization over?,4D,1186
8161,What does nn.LayerNorm Applies Instance Normalization over a 5D input?,nn.LayerNorm,10088
8162,InstanceNorm3d Applies Instance Normalization over a 5D input as described in what paper?,Instance Normalization: The Missing Ingredient for Fast Stylization,10090
8163,What does LayerNorm apply over a mini-batch of inputs?,Layer Normalization,10090
8164,What Applies a multi-layer Elman RNN withtanhtanhtanhorReLUtextReLU,nn.RNNBase,10176
8165,What applies a multi-layer gated recurrent unit to an input sequence?,nn.GRU,10176
8166,What does an Elman RNN cell have?,tanh or ReLU non-linearity,10176
8167,What does GRU stand for?,gated recurrent unit,10176
8168,What is a long short-term memory cell?,nn.LSTMCell,10176
8169,What type of model is nn.Transformer?,transformer,10212
8170,Is the placeholder identity operator argument sensitive or argument sensitive?,argument-insensitive,10083
8171,What type of transformation does nn.Linear apply to the incoming data?,linear,10083
8172,Identity A placeholder identity operator that is what?,argument-insensitive,10083
8173,What is the value of xAT + by=xAT+b nn.Linear Applies a linear transformation to the,xAT+by,10083
8174,When does nn.Dropout randomly zeroes some of the elements of the input tensor?,During training,10045
8175,What randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution?,nn.Dropout,10045
8176,What Randomly zero out entire channels?,nn.Dropout3d,10045
8177,What does AlphaDropout apply over the input?,Alpha Dropout,10045
8178,What does nn.Dropout2d do?,nn.Dropout3d,2229
8179,What program randomly zeros out entire channels?,nn.Dropout2d,2229
8180,What is the name of the program that randomly zeros out entire channels?,nn.Dropout3d,2229
8181,What is another name for nn.Dropout2d?,nn.Dropout3d,10044
8182,What is a channel in Dropout3d?,3D feature map,10046
8183,nn.Dropout3d Randomly zero out entire channels (a channel is a what?,3D feature map,10046
8184,What does AlphaDropout Applies over the input?,Alpha Dropout,10046
8185,What is the name of the simple lookup table that stores embeddings of a fixed dictionary and size?,nn,10051
8186,"nn.EmbeddingBag Computes sums or means of ‘bags’ of embeddings, without what?",instantiating the intermediate embeddings,10051
8187,What does nn.EmbeddingBag compute?,means of ‘bags’,10051
8188,nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1 andx2x,dim,10037
8189,"What is used to calculate the pairwise distance between vectorsv1v_1v1,v2v_2v2?",p-norm,10037
8190,What is the cosine similarity betweenx1x_1x1 andx2x_2x2?,nn,10037
8191,What is the mean squared error?,squared L2 norm,1976
8192,What is the mean absolute error?,MAE,1976
8193,What creates a criterion that uses a criterion that uses a criterion that uses a cri,nn.HuberLoss,1976
8194,What is NLLLoss?,negative log likelihood loss,1979
8195,What does NLLLoss stand for?,negative log likelihood loss,1979
8196,What is measured between each element in inputxxxand targetyyy?,mean squared error,1979
8197,What creates a criterion that measures the mean squared error between each element in the inputxxxand targetyyy?,nn.SmoothL1Loss,1979
8198,What is the criterion that measures the mean absolute error between each element in the inputxxxand targetyyy?,MAE,1974
8199,What criterion measures the mean absolute error between each element in the inputxxxand targetyyy?,nn.PoissonNLLLoss,1974
8200,What is NLLLoss Negative log likelihood loss with Poisson distribution of target?,Poisson,6920
8201,What type of NLLLoss Gaussian negative log likelihood loss?,Gaussian,1977
8202,What is a criterion that measures the mean squared error?,squared L2 norm,1977
8203,What criterion measures the mean squared error between each element in the inputxxxand targetyyy?,nn.KLDivLoss,1977
8204,What is the name of the criterion that measures the mean squared error between each element in the inputxxxand targetyyy?,nn.KLDivLoss,10133
8205,What is NLLLoss a criterion that measures the Binary Cross Entropy between the target and the output?,negative log likelihood loss,10038
8206,What does BCELoss measure between the target and the output?,Binary Cross Entropy,10093
8207,What is the name of the criterion that measures the Binary Cross Entropy between the target and the output?,nn.BCEWithLogitsLoss,10093
8208,What type of loss is nn.NLLLoss?,negative log likelihood loss,10012
8209,What does PoissonNLLLoss Negative log likelihood loss have?,Poisson distribution of target,7194
8210,What is the name of the loss that occurs with Poisson distribution of target?,negative log likelihood loss,7194
8211,nn.PoissonNLLLoss Negative log likelihood loss with what?,Poisson distribution of target,7194
8212,What is the criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tens,nn.MultiLabelSoftMarginLoss,7194
8213,What is the Kullback-Leibler divergence loss measure?,nn.KLDivLoss,2681
8214,What is Gaussian negative log likelihood loss?,Gaussian negative log likelihood loss,2681
8215,What creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1,nn.SmoothL1Loss,2681
8216,What is the name of the loss?,Connectionist Temporal Classification loss,6919
8217,What is the name of the loss that combines aSigmoidlayer and theBCELossin?,nn.MarginRankingLoss,6919
8218,What loss measures the Binary Cross Entropy between the target and the output?,MarginRankingLoss,4300
8219,What is the name of the loss measure that measures the Binary Cross Entropy between the target and the output?,nn.MarginRankingLoss,4300
8220,What loss is associated with Poisson distribution of target?,Negative log likelihood loss,4300
8221,What measure measures the Binary Cross Entropy between the target and the output?,nn.KLDivLoss,4300
8222,What is the name of the loss that measures the Binary Cross Entropy between the target and the output?,nn.MarginRankingLoss,7193
8223,What is the name of the loss with Poisson distribution of target?,negative log likelihood loss,7193
8224,What is the negative log likelihood loss with Poisson distribution of target?,Poisson,10162
8225,What type of loss is NLLLoss?,Gaussian,10096
8226,What is another name for mean absolute error?,MAE,10096
8227,What is the Negative log likelihood loss with?,Poisson distribution of target,10172
8228,What label contains 1 or - 1?,1D mini-batch tensoryy,10136
8229,nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels,tensoryyy,10136
8230,What measure the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1),nn.HingeEmbeddingLoss,10136
8231,What is the name of the criterion that measures the loss given inputs?,nn.HingeEmbeddingLoss,10092
8232,"What is the name of the criterion that measures the loss given inputsx1x1x1,x2x2x2?",nn.HingeEmbeddingLoss,10092
8233,"What creates a criterion that measures the loss given inputsx1x1x1,x2x2x2?",nn.MarginRankingLoss,1978
8234,What is the loss given an input tensorxxand a labels tensoryyy?,MultiLabelMarginLoss,1978
8235,What two classes does nn.CrossEntropyLoss combine?,LogSoftmaxandNLLLossin,1978
8236,Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss,Poisson,1978
8237,What type of negative log likelihood loss is used?,Gaussian,2679
8238,What two classes does BCEWithLogitsLoss combine?,aSigmoidlayer and theBCELossin,10002
8239,What is a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term,nn.SoftMarginLoss,10002
8240,What is the criterion that measures the Binary Cross Entropy between the target and the output?,nn.BCEWithLogitsLoss,6925
8241,What is the criterion that measures the loss given inputs?,nn.HingeEmbeddingLoss,6925
8242,"What class measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensor",MultiLabelMarginLoss,7716
8243,"What is a label that measures the loss given inputsx1x1x1,x2x2x2, and two 1D mini-b",1D mini-batch tensoryy,7716
8244,What is a label?,1D mini-batch tensoryy,10137
8245,What does MultiLabelSoftMarginLoss optimize?,multi-label one-versus-all loss based on max-entropy,10137
8246,What class measures the loss given an input tensorxxxand a labels tensoryyy?,MultiLabelMarginLoss,9997
8247,What does a label 1D mini-batch tensoryyy contain?,1 or -1),1973
8248,What type of hinge loss does MultiLabelMarginLoss optimize?,multi-class multi-classification hinge loss,1973
8249,What Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -,nn.HingeEmbeddingLoss,10078
8250,What creates a criterion that optimizes a multi-class multi-classification hinge loss?,nn.MultiLabelMarginLoss,10078
8251,What does SmoothL1Loss use if the absolute element-wise error falls below beta?,a squared term,10078
8252,What does SoftMarginLoss optimize?,a two-class classification logistic loss,10078
8253,What type of loss does MultiLabelSoftMarginLoss optimize?,multi-label one-versus-all loss based on max-entropy,10078
8254,When does a criterion use a squared term?,if the absolute element-wise error falls below beta,10078
8255,"What creates a criterion that measures the loss given input tensorsx1x_1x1,x2",nn.CosineEmbeddingLoss,10078
8256,"If the absolute element-wise error falls below delta, what is a squared term?",if the absolute element-wise error falls below delta,10078
8257,What type of hinge loss does Multi LabelMarginLoss optimize?,multi-class multi-classification hinge loss,10078
8258,What is the loss given an input tensorxxxand a labels tensoryyy?,MultiLabelMarginLoss,1968
8259,"What is the name of the class that measures the loss given inputsx1x1x1,x2x2x2, and a labels",MultiLabelMarginLoss,10001
8260,What two classes does this loss combine?,aSigmoidlayer and theBCELossin,7717
8261,What does MultiLabelMarginLoss optimize?,multi-class multi-classification hinge loss,2680
8262,What is inputxxx?,a 2D mini-batchTensor,10160
8263,What does MultiMarginLoss optimize?,multi-class classification hinge loss,10160
8264,What is the name of the hinge loss that uses a squared term if the absolute element-wise error falls below delta?,SmoothL1Loss,10077
8265,What is a criterion that uses a squared term if the absolute element-wise error falls below delta?,SmoothL1Loss,4237
8266,What criterion uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1,SmoothL1Loss,10156
8267,What is the name of the criterion that optimizes a multi-class multi-classification hinge loss?,nn.SoftMarginLoss,1982
8268,What does nn.HuberLoss create a criterion that uses a squared term?,if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise,1982
8269,What optimizes a multi-class multi-classification hinge loss?,criterion,1983
8270,What is a criterion that optimizes a multi-class multi-classification hinge loss?,MultiMarginLoss,1983
8271,What does nn.SoftMarginLoss optimize between input tensorxxxand target tensoryy,a two-class classification logistic loss,10081
8272,When does SmoothL1Loss use a squared term?,if the absolute element-wise error falls below beta and an L1 term otherwise,10081
8273,What criterion measures the loss given input tensorsx1x_1x1 and target tensoryyy,MultiMarginLoss,10081
8274,What is the name of the criterion that optimizes a two-class classification logistic loss between input tensorxxxand target,nn,10195
8275,"If the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise, what is a squared term?",if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise,10080
8276,What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target,nn.MultiLabelSoftMarginLoss,10173
8277,NLLLoss Gaussian negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss,Gaussian,10173
8278,What is a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target ten,MultiLabelSoftMarginLoss,1988
8279,When does SmoothL1Loss create a criterion that uses a squared term?,if the absolute element-wise error falls below beta and an L1 term otherwise,7486
8280,MultiLabelMarginLoss creates a criterion that optimizes what?,multi-class multi-classification hinge loss,7486
8281,NLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Le,Gaussian,7486
8282,This criterion combines what two single classes?,LogSoftmaxandNLLLossin,7486
8283,What is the name of the criterion that optimizes a multi-label one-versus-all loss?,nn.CosineEmbeddingLoss,9999
8284,What term does SmoothL1Loss use if the absolute element-wise error falls below beta?,squared,10193
8285,What criterion optimizes a multi-class classification hinge loss?,TripletMarginLoss,10193
8286,What is the name of the criterion that optimizes a multi-label one-versus-all loss based on max-en,nn.CosineEmbeddingLoss,1987
8287,What term is used if the absolute element-wise error falls below beta?,squared term,1987
8288,What is the name of the criterion that uses a squared term if the absolute element-wise error falls below beta?,nn,1987
8289,Which criterion optimizes a multi-label one-versus-all loss based on max-entropy?,nn.CosineEmbeddingLoss,4238
8290,What is a matrix-based loss between inputxxx(a 2D mini-batchTensor) and outputyyy?,multi-class multi-classification hinge loss,4238
8291,What is the value of a labels tensoryyy?,1 or -1,4238
8292,What does a criterion use if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise,if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise,4238
8293,"What criterion measures the loss given input tensorsx1x_1x1,x2x_2x2",MultiMarginLoss,1989
8294,What is the criterion that optimizes between input tensorxxxand target tensoryyy?,a two-class classification logistic loss,1985
8295,What optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy?,criterion,1986
8296,"What criterion measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x",TripletMarginWithDistanceLoss,1986
8297,What creates a criterion that optimizes a multi-label one-versus-all loss based on max-entrop,nn.MultiLabelSoftMarginLoss,7718
8298,This loss combines what two single classes?,aSigmoidlayer and theBCELossin,7718
8299,When does a SmoothL1Loss create a criterion that uses a squared term?,if the absolute element-wise error falls below beta,7718
8300,What is the criterion that optimizes a multi-label one-versus-all loss based on max-entropy?,nn,10158
8301,"What is a criterion that measures the loss given input tensorsx1x_1x1,x2x",aTensorlabelyyy,10035
8302,What does TripletMarginWithDistanceLoss measure?,triplet loss,10035
8303,What is the name of the criterion that optimizes a multi-class classification hinge loss?,TripletMarginLoss,10034
8304,What creates a criterion that optimizes a multi-class classification hinge loss?,MultiMarginLoss,1971
8305,What is the criterion that measures the triplet loss given an input tensor with a value greater than000?,TripletMarginWithDistanceLoss,1984
8306,What type of loss does TripletMarginLoss measure?,triplet,1984
8307,What is a criterion that optimizes between inputxxx and outputyyy?,a multi-class classification hinge loss,1981
8308,What does nn.TripletMarginLoss create?,a criterion,1981
8309,What does the criterion measure?,triplet loss,1980
8310,What is the criterion that measures the triplet loss?,TripletMarginWithDistanceLoss,1980
8311,What is the criterion that measures the triplet loss given an input tensor?,nn.TripletMarginWithDistanceLoss,1980
8312,What is a downscale factor?,Upsample,10167
8313,What does nn.PixelUnshuffle do to reverse thePixelShuffleoperation?,rearranging elements in a tensor of shape,10170
8314,What does UpsamplingNearest2d apply to an input signal composed of several input channels?,2D nearest neighbor,10170
8315,nn.PixelUnshuffle Reve what ses thePixelShuffleoperation?,r,10170
8316,UpsamplingNearest2d Applies a 2D what to an input signal composed of several input channels?,nearest neighbor,10170
8317,What type of upsampling does nn.UpsamplingNearest2d do?,Bilinear2d,10170
8318,What applies a 2D nearest neighbor upsampling to an input signal composed of several input channels?,nn.UpsamplingBilinear2d,5702
8319,nn.UpsamplingNearest2d Applies what type of upsampling to an input signal composed of several input channels,2D nearest neighbor,5702
8320,"What is the tensor of shape(,C,Hr,Wr)(*, C, H time",r,5702
8321,UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input channels?,nearest neighbor,5702
8322,What type of upsampling does UpsamplingNearest2d do?,Bilinear2d,5702
8323,"What upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric",nn.Upsample,10168
8324,UpsamplingNearest2d Applies a 2D what upsampling to an input signal composed of several input channels?,nearest neighbor,10168
8325,UpsamplingBilinear2d Applies what upsampling to an input signal composed of several input channels?,2D bilinear,10168
8326,What does nn.UpsamplingNearest2d apply to an input signal composed of several input channels?,2D nearest neighbor,10219
8327,What type of upsampling does nn.UpsamplingBilinear2d apply to an input signal composed of several input,bilinear,10219
8328,UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input channels.,nearest neighbor,10219
8329,UpsamplingBilinear2d Applies a 2D what upsampling to an input signal composed of several input channels?,bilinear,10219
8330,nn.DataParallel Implements data parallelism at what level?,module level,10042
8331,nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based on what?,ontorch.distributedpackage,10042
8332,What implements data parallelism at the module level?,nn.DataParallel,10042
8333,What is the norm of an iterable of parameters?,Clips gradient,1588
8334,What is the name of an iterable of parameters?,Clips gradient norm,1588
8335,What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) channels along the specified,prune.ln_structured Prunes tensor,1588
8336,Clips gradient of an iterable of parameters at what?,specified value,1588
8337,What is prune.global_unstructured Prune?,prune.global_unstructured,1588
8338,Clips gradient of an iterable of parameters at what value?,specified value,1587
8339,What is the name of the pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?,CustomFromMask prune.identity,1587
8340,Where are the unpruned units in prune.RandomUnstructured Prune located?,a tensor at random,1587
8341,What is prune.identity?,CustomFromMask,1587
8342,What type of Structured Prune entire (currently unpruned) channels in a tensor based on their Ln,Ln,1587
8343,How many vectors are prune.BasePruningMethod Abstract base class for creation of new pruning techniques?,one vector,2615
8344,What is prune.PruningContainer Container holding a sequence of pruning methods for?,iterative pruning,10485
8345,What prune.global_unstructured is globally prunes tensors corresponding to all parameters inparameters?,prune.global_unstructured,10485
8346,What is the name of the container holding a sequence of pruning methods for iterative pruning?,prune.remove,1858
8347,What is the pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?,Utility,1858
8348,What is the lowest L1Unstructured Prune units in a tensor by zeroing out the ones with the lowest?,L1-norm,1858
8349,What is removed from current unpruned channels along the specifieddimselected at random?,specifiedamountof,1858
8350,What is the lowest L1-norm?,L1-norm,1858
8351,What is the pruning method used in container holding a sequence of pruning methods for?,iterative pruning,1858
8352,What is the function that converts parameters to one vector?,Convert parameters to one vector,1589
8353,What type of pruning is prune.PruningContainer Container holding a sequence of pruning methods for?,iterative pruning,1589
8354,What does prune.Identity generate the pruning parametrization with?,mask of ones,10466
8355,What is prune.BasePruningMethod Abstract?,base class,10466
8356,What type of pruning is prune.PruningContainer Container used for?,iterative pruning,10466
8357,Where are the channels currently unpruned in a prune.RandomStructured Prune?,tensor at random,10466
8358,What type of pruning method does prune?,random,10466
8359,What type of pruning method does prune.identity apply pruning reparametrization to the tensor corresponding to the parameter callednamein,random_unstructured,10466
8360,How many vectors can parameters be converted to?,one,1906
8361,What is the name of the structure that prunes channels in a tensor based on their Ln-norm?,Ln,1906
8362,What utility pruning method does not prune any units but generates the pruning parametrization with a mask of ones?,prune.Identity,10473
8363,What is prune.LnStructured Prune channels in a tensor based on?,Ln-norm,10473
8364,What is the name of the entire (currently unpruned) channels in a tensor at random?,RandomStructured Prune,10473
8365,What does customFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodule without,pruning reparametrization,10473
8366,What is the gradient of an iterable of parameters at specified value?,Clips gradient,2616
8367,What is currently unpruned units in a tensor at random?,RandomUnstructured Prune,8127
8368,What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) units with the lowest,prune.l1_unstructured Prunes tensor,8127
8369,Prunes tensor corresponding to parameter callednameinmoduleby removing what (currently unpruned) units selected at,specifiedamountof,8127
8370,LnStructured Prune entire (currently unpruned) channels in a tensor based on what,Ln-norm,8127
8371,What Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask?,prune.custom_from_mask,8127
8372,What removes the pruning reparameterization from a module and the pruning method?,prune.remove,8127
8373,What type of pruning method prunes units in a tensor at random?,L1Unstructured,1905
8374,How many vectors does prune.BasePruningMethod convert to the parameters?,one,1904
8375,What structured prune entire channels in a tensor based on their Ln-norm?,Ln,1904
8376,What does the Utility pruning method do?,does not prune any units,915
8377,Abstract base class for creation of new what?,pruning techniques,915
8378,What is the name of the pruning method that prunes units in a tensor at random?,RandomStructured,915
8379,What is the pruning method that prunes units in a tensor at random?,L1Unstructured,1903
8380,What does the prune.Identity Utility pruning method generate the pruning parametrization with?,mask of ones,10472
8381,What is the term for channels in a tensor at random?,RandomStructured Prune,10472
8382,What is prune.RandomStructured Prune?,entire (currently unpruned) channels,10472
8383,What does the identification utility pruning method generate with a mask of ones?,pruning parametrization,10472
8384,What is the term for units in a tensor that is currently unpruned?,L1Unstructured Prune,10472
8385,What type of pruning method is used to prune entire channels in a tensor at random?,LnStructured,10472
8386,In what direction are randomUnstructured Prune units located?,tensor,10465
8387,What is a base class for creation of?,pruning techniques,10465
8388,What does prune.Identity Utility pruning method do?,does not prune any units,10465
8389,How do prune.L1Unstructured Prune (currently unpruned) units in a tensor at random?,by zeroing out the ones with the lowest L1-norm,10465
8390,How do prune.L1Unstructured Prune (currently unpruned) units in a tensor?,zeroing out the ones with the lowest L1-norm,10478
8391,What is prune.LnStructured Prune entire (currently unpruned) channels in a tensor,Ln-norm,10478
8392,What is prune.L1Unstructured Prune units in a tensor by zeroing out the ones with the lowest?,L1-norm,10478
8393,At what rate do prune.RandomStructured Prune entire (currently unpruned) channels in a,random,10478
8394,prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying what?,pre-computed mask inmask,10478
8395,Applies what to a parameter based on their Ln-norm?,weight normalization to a parameter,10478
8396,What is the lowest L1-norm in a tensor?,L1,10483
8397,Where are the unpruned units in the prune.RandomUnstructured Prune located?,a tensor at random,10483
8398,What does the identification utility pruning method generate the pruning parametrization with?,mask of ones,1856
8399,Where are RandomUnstructured Prune currently unpruned units located?,a tensor at random,1856
8400,What is a random structure in a tensor?,entire (currently unpruned) channels,1856
8401,What type of pruning is a container holding a sequence of pruning methods for?,iterative pruning,1856
8402,What is a Utility pruning method that generates the pruning parametrization with a mask of ones?,does not prune any units,1856
8403,What is the term for RandomStructured Prune entire (currently unpruned) channels in a tensor,LnStructured,1856
8404,How do you prune L1Unstructured Prune units in a tensor?,zeroing out the ones with the lowest L1-norm,8125
8405,What type of unit is RandomUnstructured Prune currently?,unpruned,8125
8406,In what tensor does RandomStructured Prune channels?,tensor at random,8125
8407,What is the LnStructured Prune based on?,Ln-norm,8125
8408,What is the term for unpruned units in a tensor?,L1Unstructured Prune,8125
8409,What is the pruning parametrization with a mask of ones?,RandomStructured Prune,8125
8410,What is currently unpruned units in a tensor by zeroing out the ones with the lowest L1-norm,prune.L1Unstructured Prune,10492
8411,Where are the channels in a prune.RandomStructured Prune located?,tensor at random,10492
8412,What is the term for channels in a tensor that are currently unpruned?,RandomStructured Prune,10492
8413,What corresponding to parameter callednameinmodule?,prune.l1_unstructured Prunes tensor,10492
8414,What method does not prune any units but generates the pruning parametrization with a mask of ones?,pruning,10474
8415,What is the name of the globally prunes tensors corresponding to all parameters inparameters?,global_unstructured,10474
8416,prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof,L1-norm,10474
8417,What does not prune any units but generates the pruning parametrization with a mask of ones?,prune.Identity Utility pruning method,10474
8418,What is the current status of RandomUnstructured Prune units in a tensor?,unpruned,10491
8419,How can prune.L1Unstructured Prune units in a tensor be unpruned?,zeroing out the ones with the lowest L1-norm,10491
8420,What is the name of the program that prune uses?,CustomFromMask prune.identity,10491
8421,What are the channels in a tensor based on?,Ln-norm,8126
8422,What is prune.L1Unstructured Prune?,L1-norm,8126
8423,What is the name of the entire channels in a tensor at random?,RandomStructured Prune,8126
8424,What are currently unpruned units in a tensor at random?,Prune,4916
8425,Where are unpruned channels in a tensor?,a tensor at random,4916
8426,What do LnStructured Prune channels in a tensor based on?,Ln-norm,4916
8427,What is the name of the command that prune uses?,CustomFromMask prune.identity,4916
8428,How do prune.L1Unstructured Prune units in a tensor?,by zeroing out the ones with the lowest L1-norm,10476
8429,At what rate do prune.RandomStructured Prune channels in a tensor?,random,10476
8430,At what rate are channels in a tensor prune.RandomStructured Prune?,random,10476
8431,What type of Structured Prune channels in a tensor based on their Ln-norm?,Ln,10476
8432,What type of pruning reparametrization does prune.identity Applies pruning reparametrization to the tensor corresponding to,random_unstructured,10476
8433,What applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units?,prune.CustomFromMask prune.identity,4926
8434,Prune entire channels in a tensor based on what?,Ln-norm,4926
8435,What is _unstructured?,l1,10480
8436,What is the tensor based on?,Ln-norm,10480
8437,What Prune entire channels in a tensor based on their Ln-norm?,prune.LnStructured,10480
8438,CustomFromMask prune.identity Applies what to the tensor corresponding to the parameter callednameinmodule?,pruning reparametrization,4920
8439,How do you prune unpruned units in a tensor?,zeroing out the ones with the lowest L1-norm,4920
8440,What is the name of the tensor with the lowest L1-norm?,prune.ln_structured,4920
8441,What is done to current unpruned units in a tensor?,Prune,4920
8442,At what rate do prune.RandomStructured prune entire channels in a tensor?,random,4919
8443,What is the LnStructured Prune channels in a tensor based on?,Ln-norm,4919
8444,Prune units in a tensor by zeroing out the ones with the lowest what?,L1-norm,4919
8445,Prune entire channels in a tensor at random. prune.LnStructured Prune entire (currently unpru,RandomStructured,4919
8446,What type of structure is used to prune units in a tensor?,random_unstructured,4919
8447,What applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?,CustomFromMask prune.identity,1590
8448,Convert parameters to what?,one vector,1590
8449,LnStructured Prune entire (currently unpruned) channels in a tensor based on their,Ln-norm,1590
8450,What gradient of an iterable of parameters at specified value. Convert parameters to one vector Convert one vector to the parameters prune.BaseP,Clips,1590
8451,Which Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unprune,prune.ln_structured,1590
8452,What is the name of the tree that prunes tensors corresponding to tensors?,global_unstructured,1590
8453,What is Structured Prune entire (currently unpruned) channels in a tensor based on their Ln-,Ln,1857
8454,What is a sequence of pruning methods for?,iterative pruning,1857
8455,What does prune.RandomStructured Prune entire (currently unpruned) channels in?,a tensor at random,10488
8456,What prune.random_unstructured Prunes tensor corresponding to parameter callednameinmodule?,prune.ln_structured,10488
8457,What do prune.LnStructured Prune channels in a tensor based on?,Ln-norm,4917
8458,What type of structure does prune use?,random,4917
8459,At what rate do prune.RandomStructured Prune entire channels in a tensor?,random,10487
8460,What type of structure does prune.identity apply pruning reparametrization to the tensor corresponding to the parameter callednameinmodul,random_unstructured,10487
8461,What does prune.LnStructured do?,Prune entire (currently unpruned) channels in a tensor at random,4923
8462,What is prune.LnStructured?,Prune entire (currently unpruned) channels in a tensor at random,4923
8463,What does prune.customFromMask apply to the tensor corresponding to the parameter callednameinmodule?,pruning reparametrization,4923
8464,_unstructured Prunes tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently,random,4923
8465,What is prune.random_unstructured Prunes tensor corresponding to parameter callednameinmodule?,prune.ln_structured,4923
8466,What Prune entire (currently unpruned) channels in a tensor based on their Ln-norm,prune.LnStructured,4923
8467,What does prune.LnStructured Prune channels in a tensor based on?,Ln-norm,4922
8468,What type of pruning does _unstructured Prunes tensor corresponding to parameter callednameinmodule?,random,4922
8469,What do prune.LnStructured Prune entire (currently unpruned) channels in a tensor,Ln-norm,10489
8470,How does prune.is_pruned check whethermoduleis pruned?,by looking forforward_pre_hooks,10489
8471,What removes the spectral normalization?,Removes the spectral,10489
8472,What type of unstructured Prunes tensor corresponding to parameter callednameinmodule?,random,4925
8473,What does prune.random_unstructured do?,prune.l1_unstructured,4925
8474,What is prune.random_unstructured?,prune.l1_unstructured,4925
8475,What type of _unstructured Prunes tensor corresponding to parameter callednameinmodule?,random,10500
8476,What does prune.identity apply to the tensor corresponding to the parameter callednameinmodule?,reparametrization,10500
8477,What tensor corresponding to parameter callednameinmodule removing the specifiedamountof (currently unpruned) units,prune.l1_unstructured Prunes tensor,10500
8478,What is the name of the unstructured Prunes tensor?,random,10468
8479,What tensor corresponding to parameter callednameinmodule?,prune.l1_unstructured Prunes tensor,10468
8480,What is the lowest value of the prune.l1_unstructured Prunes tensor?,L1-norm,4931
8481,The prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing what (currently un,specifiedamountof,4931
8482,What is the value of the prune.l1_unstructured Prunes tensor?,lowest L1-norm,4932
8483,What are currently unpruned units selected at random?,specifiedamountof,4932
8484,What does prune.custom_from_mask do?,prune.remove,4932
8485,What type of _structured Prunes tensor is prune?,random,10499
8486,What is the name of the Prunes tensor that removes units with the lowest L1-norm?,prune.random_structured,10499
8487,What Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unprune,prune.l1_unstructured,1245
8488,Applies what to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?,pruning reparametrization,1245
8489,Which Prunes tensor corresponding to parameter callednameinmodule removes the specifiedamountof (currently unpruned,prune.random_structured,1245
8490,What is the name of prune's unstructured Prunes tensor?,random,10511
8491,What is the name of the Prunes tensor?,prune.ln_structured,4934
8492,What type of _structured Prunes tensor corresponding to parameter callednameinmodule?,random,4934
8493,What is the lowest unit in the prune tensor?,L1-norm,4934
8494,Which Prunes tensor corresponding to parameter callednameinmodule removing the specifiedamountof (currently unpruned,prune.global_unstructured,10508
8495,What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) units selected at random,Prunes tensor,4933
8496,What is applied to the prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmodule?,pre-computed mask inmask,4933
8497,What global prunes tensors corresponding to all parameters inparameters by applying the specifiedpruning_method?,global_unstructured,4933
8498,What check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMeth,prune.is_pruned,4933
8499,What is applied to a parameter in the given module?,spectral normalization,4933
8500,What does intorch.nn.utils.parameterize.register_parametrization() implement?,parametrizations,4933
8501,What does prune.l1_unstructured Prunes tensor do?,removing the specifiedamountof (currently unpruned) units with the lowest L1-norm,4927
8502,What does prune.custom_from_mask apply?,pre-computed mask inmask,4927
8503,What is removed from a module?,spectral normalization reparameterization,4927
8504,Applies what to a parameter in the given module?,weight normalization,4927
8505,Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.,prune.ln_structured,4927
8506,What does prune.is_pruned check whethermoduleis pruned by?,looking forforward_pre_hooks,4927
8507,What does prune.CustomFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodul,pruning reparametrization,4927
8508,What type of _unstructured Prunes tensor is pruned?,random,4927
8509,Prune channels based on their Ln-norm. prune.global_unstructured Globally prunes tensors,specifiedpruning_method,4927
8510,What is implemented using the new parametrization functionality intorch.nn.utils.parameterize.register_paramete?,Parametrizations,4927
8511,What is the prune.random_structured Prunes tensor?,prune.ln_structured,10503
8512,What is used to apply the custom_from_mask Prunes tensor?,pre-computed mask inmask,10509
8513,What does prune.is_pruned do to a parameter in a given module?,Applies weight normalization,10509
8514,What is the name of the pre-computed mask inmask?,prune.remove,10512
8515,What is applied to prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmodule?,pre-computed mask inmask,10512
8516,Globally prunes tensors corresponding to what?,all parameters inparameters,10505
8517,What is used to apply the custom_from_mask?,pre-computed mask inmask,4928
8518,What prunes tensors corresponding to all parameters inparameters by applying the specifiedpruning_method?,global_unstructured,10497
8519,What does prune.remove remove from a module?,forward hook,10497
8520,What globally prunes tensors corresponding to all parameters inparameters?,prune.global_unstructured,10497
8521,What does prune.global_unstructured apply to a parameter in the given module?,spectral normalization,10497
8522,Parametrizations implemented using what new parametrization functionality?,intorch.nn.utils.parameterize.register_parametrization(),10497
8523,What applies spectral normalization to a parameter in the given module?,parametrizations,10497
8524,How does prune.is_pruned check whethermodule is pruned?,by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod,10497
8525,What does prune.custom_from_mask use to prune tensor corresponding to parameter callednameinmodule?,pre-computed mask inmask,10496
8526,How does prune.is_pruned check if a module is pruned?,by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod,10496
8527,What global prunes tensors corresponding to all parameters inparameters?,global_unstructured,10496
8528,What module does prune.is_pruned inherit from?,theBasePruningMethod,10496
8529,What removes the pruning reparameterization from a module?,Removes,4921
8530,What does prune.is_pruned apply to a parameter in the given module?,weight normalization,4921
8531,What does prune.remove do?,prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook,4921
8532,What refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision?,Quantization,5059
8533,What types of quantization does PyTorch support?,per tensor and per channel asymmetric linear quantization,5059
8534,Where can you find information on how to use quantized functions in PyTorch?,theQuantizationdocumentation,5059
8535,Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than what?,floating point precision,5059
8536,What document explains how to use quantized functions in PyTorch?,theQuantizationdocumentation,5059
8537,What is Pytorch Hub designed to facilitate?,research reproducibility,5018
8538,What repository does Pytorch Hub support publishing pre-trained models to?,github repository,5023
8539,What is each entrypoint defined as?,python,9551
8540,What can have multiple entrypoints?,hubconf.pycan,9551
8541,What specifies an entrypoint forresnet18model if we expand the implementation inpytorch/vision/hubconf.p,snippet,9551
8542,In most cases importing the right function is what?,inhubconf.pyis sufficient,9551
8543,What can hubconf.py have?,multiple entrypoints,9551
8544,What is a pre-trained model repository designed to facilitate research reproducibility?,Pytorch Hub,5019
8545,Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to what repository?,github,5019
8546,Where can you see the full script for Pytorch Hub?,inpytorch/vision repo,5019
8547,Pytorch/vision repo dependencies variable is alistof what required for training a model?,dependencies,5019
8548,What supports publishing pre-trained models to a github repository?,Pytorch Hub,5022
8549,Pytorch Hub supports publishing pre-trained models to what repository?,github,5024
8550,What do we want to use the expanded version as to show how it works?,an example,5024
8551,What is required for training a model?,dependencies,5024
8552,What are the allowed arguments for a model?,positional/keyword,5024
8553,What can't be published models?,a random commit,5024
8554,What is the minimum size of a pre-trained model?,2GB,5024
8555,Each entrypoint is defined as what type of function?,python,9550
8556,What is used as an example to show how hubconf.py works?,expanded version,9550
8557,Where can you see the full script inpytorch/vision repo?,full script inpytorch/vision repo,9550
8558,What does the code snippet specify if we expand the implementation inpytorch/vision/hubconf.py?,an entrypoint forresnet18model,2778
8559,In most cases importing the right function is sufficient.,inhubconf.py,2778
8560,What is a listof package names required toload the model?,inpytorch/vision repo dependencies variable,2778
8561,What might be slightly different from inpytorch/vision repo dependencies variable?,dependencies required for training a model,2778
8562,What are passed along to the real callable function?,argsandkwargs,2778
8563,Here we just want to use the expanded version as what to show how it works?,an example,2778
8564,What is highly recommended to add to the help message?,a few examples,2778
8565,What are callables prefixed with?,underscore,2778
8566,What are auxiliary tools to make the user workflow smoother?,tokenizers,2778
8567,What can you put in the entrypoint definition?,the following logic,2778
8568,Docstring of the function works as a help message. It explains what are the allowed arguments?,positional/keyword,2778
8569,"If less than 2GB, what is it recommended to do?",attach it to aproject release,2778
8570,Where can you see the full script?,inpytorch/vision repo,9552
8571,How does the code snippet specify an entrypoint forresnet18model?,if we expand the implementation inpytorch/vision/hubconf.py,9552
8572,Why did we use the expanded version as an example?,to show how it works,9552
8573,The published models should be at least in a what?,branch/tag,9552
8574,hubconf.pycan have what?,multiple entrypoints,9552
8575,Can't be a branch/tag?,random commit,9552
8576,What provides convenient APIs to explore all available models in hub?,Pytorch Hub,9552
8577,List all entrypoints available which in github hubconf?,in github hubconf,9552
8578,What is github(string) – a string?,github(string) – a string,9552
8579,What is alistof package names required toloadthemodel?,dependencies variable,9157
8580,How different might dependencies variable be from dependencies required for training a model?,slightly different,9157
8581,What is highly recommended to do with the Docstring?,add a few examples,9157
8582,What is highly recommended to add here?,a few examples,9157
8583,What is passed along to the real callable function?,argsandkwargsare,8897
8584,What is an example of a github branch?,‘pytorch/vision[:hub]’,8897
8585,List all entrypoints available what?,in github hubconf,8897
8586,What is a string with format repo_owner/repo_name[:tag_name]?,github(string),8897
8587,What is the name of the function that determines whether to discard the existing cache and force a fresh download?,"force_reload(bool,optional)",8897
8588,What part of the function works as a help message?,Docstring,2183
8589,What can an entrypoint function return to make the user workflow smoother?,auxiliary tools,2183
8590,What is an example of an entrypoint function that can make the user workflow smoother?,tokenizers,2183
8591,Where can you put the following logic?,entrypoint definition,2183
8592,What is an example of an auxiliary tool to make the user workflow smoother?,tokenizers,2183
8593,The published models should be at least in a branch/tag. It can’t be a what?,random commit,2183
8594,Docstring of the function explains what are the allowed?,positional/keyword arguments,2183
8595,What is highly recommended to add to the Docstring?,a few examples,2183
8596,List what available in github hubconf?,all entrypoints,2183
8597,What is a list of available entrypoint names entrypoints?,Default is False,2183
8598,What is a string of entry?,model(string),2183
8599,What is it recommended to do if the weight is less than 2GB?,attach it to aproject release,2183
8600,What does the Docstring of the function explain?,what does the model do and what are the allowed positional/keyword arguments,9159
8601,What is a list of package names required to load the model?,dependencies variable,9159
8602,What is the name of the command to force a fresh download?,force_reload,9159
8603,What is the name of entrypointmodel?,docstring,9159
8604,What is the default branch?,ismasterif not specified,9159
8605,Docstring of the function works as a help message. It explains what are the allowed?,positional/keyword arguments,9159
8606,What might be slightly different from dependencies variable?,dependencies required for training a model,9159
8607,"Entrypoint function can return a model(nn.module), or what to make the user workflow smoother?",auxiliary tools,9159
8608,What is a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.,Default is False,9159
8609,What is dependencies variable slightly different from?,dependencies,9158
8610,What is alistof package names required toload the model?,dependencies variable,9158
8611,What is the minimum size of a pretrained weight?,2GB,9158
8612,What are the allowed arguments for the model?,positional/keyword,2777
8613,What is highly recommended to add to the Docstring of the function?,a few examples,8896
8614,What should you do if you have less than 2GB of pretrained weights?,attach it to aproject release,8896
8615,What do callables prefixed with are considered as helper functions?,underscore,8895
8616,Where can pretrained weights be stored locally?,github repo,4888
8617,What should you do if your weight is less than 2GB?,attach it to aproject release,4888
8618,What can't be included in the published models?,a random commit,4888
8619,What is the name of all entrypoints available in Pytorch Hub?,in github hubconf,4888
8620,How big is the size of the pretrained weights?,less than 2GB,4887
8621,What should you do if the weight is less than 2GB?,attach it to aproject release,4887
8622,The published models should be at least in what?,branch/tag,7240
8623,What type of commit can’t a published model be?,random commit,7240
8624,What can’t be published models?,a random commit,7240
8625,What can't be a published model?,random commit,1515
8626,How can pretrained weights be loaded?,"stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url()",1515
8627,Where can you put the following logic in the example abovetorchvision.models.resnet.resnet18handlespret,entrypoint definition,1515
8628,Callables prefixed with underscore are considered as what?,helper functions,1515
8629,"If less than 2GB, it’s recommended to what?",attach it to aproject release,1515
8630,What can't be a branch/tag?,random commit,1515
8631,What should the published models be at least in?,branch/tag,7242
8632,What does Pytorch Hub list?,all entrypoints available in github hubconf,7242
8633,What is a string with format “repo_owner/repo_name[:tag_name]” with an optional tag/,github(string),7242
8634,"What provides convenient APIs to explore all available models in hub throughtorch.hub.list(), show docstring and examples throughtor",Pytorch Hub,7242
8635,"What is the default value for force_reload(bool,optional)?",Default is False,7242
8636,What can't be published models in a branch/tag?,a random commit,2258
8637,What is an example of auxiliary tools to make the user workflow smoother?,tokenizers,2258
8638,What should the published models be in?,branch/tag,7241
8639,What provides convenient APIs to explore all available models in hub throughtorch.hub.list()?,Pytorch Hub,7241
8640,What is the default setting for force_reload?,Default is False,7241
8641,What is a string of entrypoint name defined in repo’s hubconf.py?,model(string),7241
8642,What type of entrypointmodel does Pytorch Hub show?,docstring,7241
8643,What is a string with format “repo_owner/repo_name[:tag_name]?,github(string),5021
8644,What is the name of all entrypoints available in hub?,in github hubconf,5021
8645,What is the default branch of github?,local,9472
8646,What is a string with format repo_owner/repo_name[:tag_name]> with an optional tag,github(string),9472
8647,What is the typical use case?,Loading a model,9472
8648,"if sourceis'github',repo_or_diris expected to be of what?",formrepo_owner/repo_name[:tag_name]with an optional tag/branch,9472
8649,Where can a model be loaded from?,github repo or a local directory,9472
8650,What does github do?,List all entrypoints available in github hubconf,4119
8651,"What is a string with format ""repo_owner/repo_name[:tag_name] with an optional tag/bra",github,4119
8652,What is available in github hubconf?,all entrypoints,4119
8653,What is the default setting for a list of available entrypoint names?,Default is False,4119
8654,What is the default value of force_reload?,True,9387
8655,What is the default value for force_reload?,is False,9387
8656,"Does force_reload(bool,optional) – whether to force a fresh download of the github repo unconditionally?",Does not have any effect if source='local',9387
8657,Does not have any effect what?,if source='local',9387
8658,"What is the default value of check_hash(bool,optional)?",True,9387
8659,What default is used to force a fresh download of the github repo unconditionally?,is False,9387
8660,What is the name of the option to force a fresh download of the github repo unconditionally?,force_reload,9387
8661,What does not have any effect if source='local'?,message about first download cannot be muted,9387
8662,"Does not have any effect if source='local'. Default is False. verbose(bool,optional)",local,9387
8663,What is the name of the entrypointmodel?,docstring,9471
8664,What is a string with format repo_owner/repo_name[:tag_name] with an optional tag/,github(string),9471
8665,What is the name of the function that can be used to force a fresh download?,force_reload,9471
8666,Where can you load a model from?,a local directory,9471
8667,"What does force_reload(bool,optional) do?",Default is False,9471
8668,What branch ismaster if not specified?,The default branch,9471
8669,What is model(string)?,"a string of entrypoint name defined in repo’s hubconf.py force_reload(bool,optional)",9471
8670,"What is a string with format ""repo_owner/repo_name[:tag_name]?",github,9470
8671,What is the docstring of?,entrypointmodel,5895
8672,What is the default value for the default value?,False,5895
8673,"What is the name of the entrypoint model defined in repo's hubconf.py force_reload(bool,optional)",pytorch/vision[:hub],5895
8674,How do you load a model from a github repo?,a local directory,5895
8675,What is the name of the command to discard the cache and force a fresh download?,force_reload,9382
8676,Where is the entrypoint name defined in the hubconf.py?,repo,9384
8677,What is the default is False. a list of available entrypoint names entrypoints?,force_reload,9384
8678,What is the default if not specified?,branch,9270
8679,What is the name of a model defined in a repo's hubconf.py?,pytorch/vision[:hub],9467
8680,Load a model from a github repo or what?,a local directory,9927
8681,"What is the default setting for force_reload(bool,optional)?",False,9927
8682,"what is a string of entrypoint name defined in repo's hubconf.py force_reload(bool,optional)",model(string),9927
8683,"What default value does force_reload(bool,optional) return?",False,2291
8684,"What is the name of the entrypoint model defined in the repo's hubconf.py force_reload(bool,optional",pytorch/vision[:hub],5894
8685,"What is a string of entrypoint name defined in repo's hubconf.py force_reload(bool,optional)",model(string),9926
8686,What is the default setting for model(string)?,False,9926
8687,What repo can a model be loaded from?,github,2288
8688,"Loading a model is the typical use case, but can also be used to load other objects such as what?","tokenizers, loss functions, etc",4126
8689,What is the default location for how repo_or_dir is to be interpreted?,github,4126
8690,What is 'github'?,Default,4126
8691,What is the typical use case of a github repo or a local directory?,Load a model,4126
8692,What is expected to be of the formrepo_owner/repo_name[:tag_name]with an optional tag/bra,if sourceis'github',8762
8693,"if sourceis'local',repo_or_diris expected to be what?",path to a local directory,9576
8694,What does repo_or_dir(string) mean?,repo name,9576
8695,What is specified to be interpreted?,how repo_or_dir is,9576
8696,What does repo_or_diris expect to be of the formrepo_owner/repo_name[:tag_name,if sourceis'github,9576
8697,"What is the default value of progress(bool,optional)?","None progress(bool,optional) – whether or not to display a progress bar to stderr",9576
8698,"if sourceis'local',repo_or_diris expected to be what to a local directory?",path,4125
8699,Loading a model can also be used to load other objects such as what?,"tokenizers, loss functions",4125
8700,Loading a model is the typical use case but can also be used to load other objects such as what?,"tokenizers, loss functions, etc",4523
8701,What is repo_or_dir(string)?,repo name,4523
8702,"if sourceis'github',repo_or_diris expected to be of the formrepo_owner/repo",tag/branch,4523
8703,What is expected to be a path to a local directory?,"if sourceis'local',repo_or_diris",9578
8704,What is repo_or_diris expected to be?,a path to a local directory,9578
8705,What is the default for repo_or_diris?,github,9578
8706,What is the name of the repo?,repo_or_dir,10586
8707,What does repo_or_dir(string) contain?,repo name,10586
8708,What is the url(string) of the object to download?,URL,10586
8709,What is the URL of the object to download?,a local path,10586
8710,What is the default setting to force a fresh download of the github repo unconditionally?,False,10586
8711,Where is the Example Download object located?,given URL to a local path,10586
8712,What is the name of a callable defined in the repo/dir'shubconf.py?,model(string),9930
8713,Where is the name of a callable defined?,repo/dir’shubconf.py,9930
8714,What is the name of the command to force a fresh download of the github repo?,force_reload,9930
8715,"Does force_reload(bool,optional) have any effect if source='local'?",Does not have any effect,9930
8716,What is the default location to be interpreted?,github,9930
8717,What does *args(optional) provide for callablemodel?,corresponding args,9574
8718,"if sourceis',repo_or_diris expected to be of the formrepo_owner/repo_name[",github,9574
8719,What is the corresponding args for callablemodel?,"source(string,optional)",4121
8720,What does repo_or_diris expect to be a path to a local directory?,if sourceis'local',9577
8721,What is the default name for repo_or_diris?,github,9577
8722,*args(optional) – the corresponding args for what?,callablemodel,84
8723,What specifies how repo_or_dir is to be interpreted?,"source(string,optional)",83
8724,What is the default name of the git repo?,Default is'github',83
8725,Does the message about first download have any effect if source='local'?,Does not have any effect,83
8726,What is the default setting for the SHA256 downloaded file?,None,83
8727,What is the default name for how repo_or_dir is to be interpreted?,github,9383
8728,Specifies how repo_or_dir is to be interpreted. Default is'github'.,source,9383
8729,Specifies how repo_or_dir is to be interpreted. Default is' what?,github,9383
8730,What is the name of the command that can be used to force a fresh download?,force_reload,9383
8731,"Force_reload(bool,optional) – whether to discard the existing cache and force a fresh download. Default is what?",False,9383
8732,What is the name of a callable defined in the repo/dir’shubconf.py?,model(string),9931
8733,"What does source(string,optional) specify?",how repo_or_dir is to be interpreted,9931
8734,What are the corresponding args for callablemodel?,*args,9931
8735,What are the corresponding kwargs for callablemodel?,**kwargs,9931
8736,url(string) – URL of the object to download what?,model_dir,9931
8737,"What does source(string,optional) – 'github'|'local'?","source(string,optional) –'github'|'local'",9931
8738,"Default is True. verbose(bool,optional) – If False, mute messages about hitting local",local,9931
8739,What object at the given URL to a local path?,Example Download object,9931
8740,"What is the default value for progress(bool,optional) – whether or not to display a progress bar to stderr",None,9931
8741,What does if source='local' mean?,a path to a local directory,10585
8742,What does github specify to be interpreted?,how repo_or_dir is,10728
8743,"What does source(string,optional) specify to be interpreted?",how repo_or_dir is,10728
8744,What is the default if source='local'?,True,10728
8745,"Does not have any effect if source='local'. Default is True. verbose(bool,optional)",Does not have any effect,10728
8746,What is the output of when called with the given*argsand**kwargs?,themodelcallable,10728
8747,What is at the given URL to a local path?,Example Download object,10728
8748,"If False, what happens to messages about hitting local caches?",mute messages about hitting local caches,10728
8749,What is the default for how repo_or_dir is to be interpreted?,github,4524
8750,What is a way to force a fresh download of the github repo?,force_reload,4524
8751,What does repo_or_dir(string) refer to?,repo name,10587
8752,url(string) – URL of the object to download?,dst,10587
8753,What is the default for a github repo?,github,10587
8754,"What does source(string,optional) Specifies how repo_or_dir is to be interpreted?","source(string,optional) –'github'|'local'",10587
8755,What is the option to force a fresh download of the github repo unconditionally?,force_reload,10587
8756,"Force_reload(bool,optional) – whether to force a fresh download of the github repo unconditionally. What",Does not have any effect if source='local',10587
8757,"If downloaded file is a what, it will be automatically decompressed. If the object is already present inmodel_dir, it’s des",zip file,10587
8758,What is the name of the source of the github repo?,source,9929
8759,What is the name of the corresponding args for callablemodel?,"source(string,optional)",9929
8760,What is the default name of the github repo?,github,10729
8761,What is the name of the string that specifies how repo_or_dir is to be interpreted?,source,10729
8762,What message cannot be muted if source='local'?,message about first download cannot be muted,10729
8763,"What does verbose(bool,optional) mute messages about?",hitting local caches,10729
8764,"What does source(string,optional) Specify?",how repo_or_dir is to be interpreted,10729
8765,What cannot be muted?,first download,10729
8766,Default is what?,True,10729
8767,What is the name of the function that can be used to force a fresh download of the github repo unconditionally?,force_reload,10729
8768,If downloaded file is a what?,zip file,10729
8769,Default: None what – whether or not to display a progress bar to stderr Default: True Example Loads,"progress(bool,optional)",10729
8770,"Check_hash(bool,optional) – whether or not to display a progress bar to stderr. Default",True,10729
8771,Does force_reload have any effect?,Does not have any effect,82
8772,What is 'github'|'local'?,source,82
8773,What is the name of the option to force a fresh download of the github repo?,force_reload,82
8774,"What does verbose(bool,optional) do to mute messages about hitting local caches?",If False,11340
8775,What is the default value for muting messages about hitting local caches?,True,11340
8776,What is the default setting for muting messages about hitting local caches?,Default is True,11340
8777,What is the default to display a progress bar to stderr?,"None progress(bool,optional)",11340
8778,What is the default value for displaying a progress bar to stderr?,Default: None,11340
8779,What type of object is at the given URL to a local path?,Example Download object,11340
8780,What is the URL of the object to download dst(string) – Full path where object will be saved?,url(string) – URL of the object to download dst(string) – Full path where object will be saved,11340
8781,The message about first download cannot be what?,muted,11341
8782,What is the corresponding kwargs for?,callablemodel,11341
8783,The output of themodelcallable when called with what?,given*argsand**kwargs,11341
8784,What happens when called with the given*argsand**kwargs?,output of themodelcallable,11341
8785,What message cannot be muted?,message about first download,11341
8786,Example Download object at what?,given URL to a local path,11341
8787,"What do verbose(bool,optional) mute messages about?",hitting local caches,11341
8788,What does If False do?,mute messages about hitting local caches,9386
8789,What is the name of the command to force a fresh download of the github repo unconditionally?,force_reload,9386
8790,What does the modelcallable callable callable callable callable callable callable callable callable callable callable callable callable call,Example Download object,9386
8791,What is the corresponding kwargs for callablemodel?,kwargs,9575
8792,What is the output of themodelcallable when called with the given*argsand**kwargs?,a local path,7217
8793,What is the default value ofmodel_diris?,the directory returned byget_dir(),7217
8794,What is the download object at the given URL to?,a local path,11339
8795,Where is the Example Download object?,given URL to a local path,76
8796,What does the object at the given URL download to?,a local path,76
8797,What is the default value for the SHA256 downloaded file?,None,76
8798,What is the default setting for the SHA256 downloaded file to start withhash_prefix?,None,76
8799,What type of file should start withhash_prefix?,SHA256,9202
8800,What is the default value for the progress bar to stderr?,Default: None,2285
8801,What does the Torch serialized object load at the given URL to a local path?,Example Download object,2285
8802,Example Download object at the given URL to what?,a local path,2286
8803,What object is at the given URL to a local path?,Download object,2286
8804,Filename fromurlwill be used if not set?,Filename fromurlwill be used if not set,2286
8805,help(model.foo) to check what argumentsmodel.footakes to run?,to check what argumentsmodel.footakes to run,2286
8806,What does dst(string) mean?,Full path where object will be saved,9204
8807,"If the object is already present, it’s deserialized and returned.",inmodel_dir,9204
8808,"What is the URL of the object to download model_dir(string,optional)?","url(string) – URL of the object to download model_dir(string,optional)",9204
8809,"What is the default value of file_name(string,optional)?","False file_name(string,optional) – name for the downloaded file",9204
8810,"What is the default setting for progress(bool,optional)?",Default: None,9204
8811,"If downloaded file is a what, it will be automatically decompressed?",zip file,9204
8812,What is the filename-sha256>.extwheresha256>?,first eight or more digits,9204
8813,"Default: False file_name(string,optional) – name for the downloaded file.",Filename fromurlwill be used if not set,9204
8814,"After you have loaded a model, what should you do?",how can you find out what you can do with the model,9204
8815,What is instantiated by *argsand**kwargsintorch.hub.load()?,model,9204
8816,dst(string) – what?,Full path where object will be saved,9204
8817,What is used to check what argumentsmodel.footakes to run?,model.foo,9204
8818,"If None, the SHA256 downloaded file should start what?",withhash_prefix,9203
8819,What is the default value of the Torch serialized object?,True Example Loads the Torch serialized object at the given URL,9203
8820,Download object at the given URL to what?,local path,2204
8821,What is used to ensure unique names and to verify the contents of the file?,The hash,2204
8822,"What is the URL of the object to download model_dir(string,optional) – directory in which to save the object map_location(",url(string),2204
8823,What type of object is instantiated by *argsand**kwargsintorch.hub.load()?,model,2204
8824,"If downloaded file is what, it will be automatically decompressed?",a zip file,10461
8825,What is the default value to display a progress bar to stderr?,True,10461
8826,What default value is used to load the Torch serialized object at the given URL?,True,10461
8827,What is the name of the path where the object will be saved?,Full path,11301
8828,"If downloaded file is a zip file, it will be what?",automatically decompressed,4134
8829,What can you do after you have loaded a model?,how can you find out what you can do with the model,4134
8830,What is the name of the SHA256 downloaded file?,hash_prefix,9507
8831,What should the SHA256 downloaded file start?,withhash_prefix,9507
8832,"What is the default value of progress(bool,optional) to display a progress bar to stderr?",True,9507
8833,What is the default setting for displaying a progress bar to stderr?,True Example Loads the Torch serialized object at the given URL,9506
8834,"To what does progress(bool,optional) display a progress bar?",stderr,10460
8835,Default: True Example Loads the Torch serialized object at the given URL?,Default: True Example Loads the Torch serialized object at the given URL,10460
8836,"If the object is already present, it's deserialized and returned?",inmodel_dir,11302
8837,What is the default value of model_diris?,byget_dir(),77
8838,What is the default value ofmodel_dirishub_dir>/checkpointswherehub_diris?,byget_dir(),3498
8839,What is the SHA256 hash of the contents of a file?,first eight or more digits,3498
8840,Where is the Torch serialized object loaded?,the given URL,2289
8841,What serialized object does the example load at the given URL?,Torch,2289
8842,"If the downloaded file is what, it will be automatically decompressed?",a zip file,2289
8843,"If the object is already present, what is it deserialized and returned?",inmodel_dir,4132
8844,"If the object is already present, what is deserialized and returned?",inmodel_dir,3497
8845,What is the default value of model_dir?,True,3497
8846,What is used to specify how to remap storage locations?,a function or a dict,9932
8847,What is the first eight or more digits of the SHA256 hash of the contents of a file?,first eight or more digits,3410
8848,"If the object is already present, it's deserialized and returned.",inmodel_dir,3410
8849,What is an example of a filename that will be used if not set?,Example,3410
8850,"What is progress(bool,optional)?",whether or not to display a progress bar to stderr,10462
8851,Why is the hash used?,to ensure unique names and to verify the contents of the file,9067
8852,What is extwheresha256>?,first eight or more digits,9067
8853,What is an example of a filename fromurl?,Example,9067
8854,What is the default setting for file_name?,"False file_name(string,optional) – name for the downloaded file",9872
8855,What is the name of the downloaded file?,file_name,9337
8856,What is the purpose of argsand**kwargsintorch.hub.load()?,toinstantiatea model,9337
8857,What does $XDG_CACHE_HOME/torch/hub do?,if environment variableXDG_CACHE_HOMEis set,9337
8858,What should you do after you have loaded a model?,how can you find out what you can do with the model,4506
8859,What is a suggested workflow to see all available methods of the model?,dir(model),4506
8860,What is the purpose of *argsand**kwargsintorch.hub.load()?,toinstantiatea model,4506
8861,What is a suggested workflow to see all available methods of a model?,dir(model),4506
8862,What is used to instantiate a model?,*argsand**kwargsintorch.hub.load(),4506
8863,Who makes function help messages clear and succinct?,repo owners,4506
8864,What type of example would be helpful to include?,minimal working,4506
8865,What should repo owners make clear and succinct?,help messages,4506
8866,What should repo owners include to help users explore without referring to documentation back and forth?,a minimal working example,4506
8867,What does help(model.foo) use to check what argumentsmodel.footakes to run?,model.foo,9336
8868,What arguments do you need to run help(model.foo)?,model.footakes,9336
8869,What are the locations used in the order of?,Callinghub.set_dir,7162
8870,What is used for storing downloaded models & weights?,Torch Hub cache directory,7162
8871,How are the locations used?,in the order of Callinghub.set_dir,7162
8872,What is the Torch Hub cache directory used for storing downloaded models & weights?,$XDG_CACHE_HOME/torch/hub,7162
8873,What is the default path to the Torch Hub cache directory?,Ifset_dir()is not called,7162
8874,What is the path to a local folder to save downloaded models & weights?,d(string),7162
8875,What is optionally set to save downloaded models & weights?,Torch Hub directory,7162
8876,What is the name of the directory that is set by callinghub.set_dir(PATH_TO_HUB_DIR>),$TORCH_HOME/hub,1518
8877,What is the default path if set_dir()is not called?,$TORCH_HOME,9068
8878,What directory is used to save downloaded models & weights?,Torch Hub,9068
8879,What is the default path for the Torch Hub cache directory?,$XDG_CACHE_HOME/torch,2692
8880,What is the Torch Hub directory used for storing downloaded models & weights?,Torch Hub cache directory,2692
8881,What is the Torch Hub directory used to save downloaded models & weights?,Optionally set the Torch Hub directory,2692
8882,What is used to save downloaded models & weights?,Optionally set the Torch Hub directory,11476
8883,Why is the default path $TORCH_HOME/hub?,Ifset_dir()is not called,3636
8884,What directory can be optionally set to save downloaded models & weights?,Torch Hub,3636
8885,What is the name of the path to a local folder to save downloaded models & weights?,d(string),3636
8886,What is the default path for environment variable$TORCH_HOME?,$XDG_CACHE_HOME/torch,3636
8887,What is the default behavior of Hub?,we don’t clean up files after loading it,1493
8888,Hub uses the cache by default if it already exists in the directory returned what?,byget_dir(),1493
8889,How can users force a reload?,callinghub.load,1493
8890,What does the force a reload delete?,github folder,1493
8891,What does Hub not do after loading it?,clean up files,1493
8892,What does Hub use if it already exists in the directory returned byget_dir()?,the cache,1493
8893,What does force a reload do?,delete the existing github folder and downloaded weights,1493
8894,What works by importing the package as if it was installed?,Torch hub,7942
8895,What is cachessys.modulesandsys.path_importer_cache?,normal Python behavior,7942
8896,How does Torch hub work?,importing the package as if it was installed,7942
8897,In what language are some side effects introduced by importing?,Python,7942
8898,What might join the party and give you surprises if you try to load two different branches of the same repo in the same python,Cache,7942
8899,Is it okay to load two different branches of the same repo in separate processes?,totally fine,7942
8900,What is normal Python behavior?,cachessys.modulesandsys.path_importer_cache,7942
8901,What is a known limitation of importing in Python?,userCANNOTload two different branches of the same repo in thesame python process,7942
8902,How many packages with the same name are installed in Python?,two,7942
8903,Cache might join the party and give you what if you try that?,surprise,7942
8904,How do you load two different branches of the same repo?,in separate processes,7942
8905,What causes some side effects?,importing in Python,7941
8906,Where can you see new items in Python?,cachessys.modulesandsys.path_importer_cache,7941
8907,What is a known limitation that is worth mentioning?,userCANNOTload two different branches of the same repo in thesame python process,807
8908,In what language are two packages with the same name installed?,Python,807
8909,What is a known limitation?,userCANNOTload two different branches of the same repo in thesame python process,807
8910,How many packages with the same name are installed in the same Python process?,two,807
8911,What might join the party and give you surprises if you try it?,Cache,807
8912,Is it okay to install two different branches of the same repo in the same process?,load them in separate processes,807
8913,Returns what with the logarithm to the base 10 of the elements of input?,a new tensor,5402
8914,What does liketorch.minimum() do?,Computes the element-wise minimum ofinputandother,1750
8915,"If both elements are what is the minimum, then the non-NaN element is taken as the minimum?",,1750
8916,What is the name of the function that computes the element-wise minimum of inputandother?,liketorch.minimum(),1751
8917,What does liketorch.minimum() compute?,the element-wise minimum ofinputandother,1751
8918,What is liketorch.minimum() a wrapper around?,C++’sstd::fminand,1751
8919,What is an example of a function that computes the element-wise minimum of inputandother?,Example:,1751
8920,What language is this function a wrapper around?,C++,7659
8921,What types of inputs does fminand support?,integer and floating-point inputs,7659
8922,What is taken as the minimum if exactly one of the two elements being compared is a NaN?,the non-NaN element,7659
8923,What is this function a wrapper around?,C++’sstd::fminand,7535
8924,What function is similar to:fminand?,NumPy’sfminfunction,7535
8925,What types of inputs does the function support?,"float, double, cfloat and cdouble dtypes",6126
8926,What type of matrices does the function support?,batches of matrices,6126
8927,What is a faster and more numerically stable way to multiply a matrix on the left by the inverse?,usingtorch.linalg.solve(),6126
8928,What is the output of inputandvec2?,Outer product ofinputandvec2,4697
8929,"If input is a vector of sizennandvec2is a vector of what, thenoutmust be a matrix of",sizemmm,4697
8930,What does input(Tensor) vec2(Tensor) vec2(Tensor) ve,"1-D input vector out(Tensor,optional) – optional output matrix",4697
8931,Returns what with the tangent of the elements of input?,a new tensor,5408
8932,What is the matrix to be added vec1(Tensor)?,input(Tensor),3649
8933,Input(Tensor) – matrix to be added what?,vec1,9648
8934,What does the dot product do for higher dimensions?,sums the product of elements frominputandotheralong their last dimension,1703
8935,"If eitherinputorotheris a what, the result is equivalent totorch.mul(input, other)?",scalar,1703
8936,What must happen if bothinputandotherare non-scalars?,the size of their last dimension must match,1703
8937,What is the output shape?,input.shape,1703
8938,What is the output shape of input.shape[:-1] + other.shape[:-1]?,Example,1703
8939,"What element represents if each element ofinputis ""close"" to the corresponding element ofother?",boolean elements,5365
8940,What is considered equal to each other whenequal_nanis True?,NaNs,5365
8941,"What does the second tensor to compare atol(float,optional) represent?",absolute tolerance,5365
8942,What do boolean elements represent in a new tensor?,if each element ofinputis “close” to the corresponding element ofother,5365
8943,"Atol(float,optional) – second tensor to compare atol(float,optional) – what is",absolute tolerance,5365
8944,"What is the default value of input(Tensor) to compare atol(float,optional) – absolute tolerance?",1e-08,5365
8945,What is Closeness defined as?,whereinputandotherare finite,5366
8946,When are NaNs considered equal to each other?,whenequal_nanis True,5366
8947,"Atol(float,optional) – what is input(Tensor) a second tensor to compare?",absolute tolerance,5366
8948,"Atol(float,optional) – what does input(Tensor) compare atol(float,optional) compare",absolute tolerance,11382
8949,Returns what with the reciprocal of the square-root of each of the elements of input?,a new tensor,5405
8950,What is the size of the new index?,tensor,2440
8951,"If the left boundary ofsorted_sequenceis closed, what is the default?",Ifrightis False,2440
8952,What does the sorted_sequence right returned index satisfies?,1-D False,2440
8953,What is the default value for the left boundary ofsorted_sequence?,Ifrightis False,2440
8954,sorted_sequence right returned index satisfies what rule?,1-D False,2440
8955,Return what with the same size asvalues?,a new tensor,2442
8956,"Right(bool,optional) – if False, return the what?",first,2442
8957,Where are the indices located?,Find the indices,2442
8958,"If rightis False (default), then the left boundary ofsorted_sequenceis closed.",Ifrightis False,2442
8959,What is the name of the right returned index that satisfies 1-D False sorted_sequence?,sorted_sequence,2442
8960,What do values(TensororScalar) contain?,search value(s),2442
8961,"Return the last such index, if False, what?",return the last such index,2442
8962,"Return the last such index. If False, return the first suitable location that is found. If True, return the last such index?",If,2442
8963,sorted_sequence right returned index satisfies what?,1-D False,10722
8964,What right returned index satisfies 1-D False sorted_sequence?,sorted_sequence,10722
8965,Where does a sorted_sequence(Tensor) contain monotonically increasing sequence?,the innermost dimension,10603
8966,What satisfies sorted_sequence[i-1]values[m][n]?,1-D False,10603
8967,On what dimension is a sorted_sequence(Tensor) containing monotonically increasing sequence?,the innermost dimension,2404
8968,What does the sorted_sequence(Tensor) contain?,monotonically increasing sequence,2404
8969,What satisfies 1-D False sorted_sequence[i-1]values[m][n],right returned index,10610
8970,On what dimension does a tensor contain a monotonically increasing sequence?,the innermost dimension,2403
8971,Which sorted_sequence[i-1]=values[m][n]?,1-D True,94
8972,What is a values(TensorScalar) containing the search value(s)?,N-D tensor or a Scalar,10726
8973,What does sorted_sequence(Tensor) contain?,monotonically increasing sequence,10726
8974,On what dimension is the sequence sorted_sequence(Tensor) containing monotonically increasing sequence?,the innermost dimension,10726
8975,True sorted_sequence[i-1]=values[m][n]...[l][x]sorted,1-D,95
8976,Values(TensororScalar) – N-D tensor or a Scalar containing what?,search value(s),10727
8977,What is a sorted_sequence(Tensor) containing on the innermost dimension?,monotonically increasing sequence,10727
8978,sorted_sequence(Tensor) – N-D or what tensor?,1-D,10727
8979,On what dimension does a sorted_sequence(Tensor) contain monotonically increasing sequence?,the innermost dimension,8004
8980,What is sorted_sequence(Tensor)?,N-D or 1-D tensor,10724
8981,What are values(TensororScalar) containing the search value(s)?,N-D tensor or a Scalar,10724
8982,What are examples of non-numerical values?,"nan, inf",10724
8983,What is the default value of the lower bound index?,False,10724
8984,Values(TensororScalar) – N-D tensor or Scalar containing what?,search value,10723
8985,Where is the monotonically increasing sequence located?,the innermost dimension,10723
8986,What does values(TensororScalar) contain?,search value,10723
8987,What type of tensor is a values(TensorScalar)?,N-D tensor or a Scalar,11326
8988,What are values(TensororScalar)?,N-D tensor or a Scalar,11326
8989,"If False, return the first suitable location that is found. If True, return the last such index?",if False,10613
8990,"If no suitable index is found, return what value for non-numerical value?",0,10613
8991,What does False return for each value invalueson the correspondinginnermostdimension of thesorted_sequence?,lower bound index,10613
8992,"If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or what?",the size ofinnermostdimension withinsorted_sequence,10613
8993,"If False, gets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence",upper bound index,10613
8994,What is returned if False?,"If True, return the last such index",10725
8995,"If False, gets what for each value invalueson the correspondinginnermostdimension of thesorted_sequence?",lower bound index,10725
8996,"If False, return the first suitable location that is found.",return the last such index,10725
8997,"If no suitable index found, what is returned for non-numerical value (eg. nan, inf) or the size of",return 0,10725
8998,What is a non-numerical value?,inf,10725
8999,"If False, gets what instead of the lower bound index?",upper bound index,10725
9000,What is the default value of sorted_sequence?,False,10725
9001,sorted_sequence(Tensor) – N-D or what?,1-D tensor,10725
9002,What is the default value for a sorted_sequence?,False,10725
9003,What does True get instead of the lower bound index?,upper bound index,8005
9004,What is a sorted_sequence(Tensor) containing monotonically increasing sequence on the innermost dimension?,N-D or 1-D tensor,8005
9005,"If no suitable index found, what is returned for non-numerical value?",return 0,8005
9006,What index does False get for each value invalueson the correspondinginnermostdimension of thesorted_sequence?,lower bound index,8005
9007,What is the default value of a sorted_sequence?,False,8005
9008,If your use case is always what?,1-D sorted sequence,10346
9009,What type of matrix can be created from provided tensors?,block diagonal matrix,1940
9010,How are the input tensors arranged in order?,their upper left and lower right corners are diagonally adjacent,1940
9011,All other elements are set to what?,0.,1940
9012,What is an example of a block diagonal matrix?,Tensor Example:,1940
9013,What can be created from provided tensors?,a block diagonal matrix,1940
9014,What are diagonally adjacent in a 2 dimensional tensor?,their upper left and lower right corners,1940
9015,What does aRuntimeError throw if the matrix is not invertible?,Computes the inverse of a square matrix if it exists,1775
9016,What happens if the matrix is not invertible?,Throws aRuntimeError,1775
9017,What is the inverse of a square matrix if it exists?,unique,1775
9018,The inverse matrix exists if and only what?,if AAA is invertible,7152
9019,The inverse matrix exists if and only ifAAAisinvertible. What is the inverse matrix?,unique,7152
9020,What input types does the inverse matrix support?,"float, double, cfloat and cdouble dtypes",7152
9021,What is a possible way to multiply a matrix on the left by the inverse?,usingtorch.linalg.solve(),7152
9022,"The inverse matrix exists if and only ifAAAisinvertible. In this case, the inverse is what?",unique,7152
9023,"The inverse matrix supports input of float, double, cfloat and what other dtype?",cdouble,7152
9024,"In this case, the inverse matrix exists if and only ifAAAisinvertible.",unique,4098
9025,What does the inverse matrix support?,batches of matrices,7153
9026,Why is it always preferable to usesolve() for multiplying a matrix on the left by the inverse?,faster and more numerically stable,7153
9027,What does torch.linalg.pinv() compute?,pseudoinverse,7153
9028,What does torch.linalg.solve()computesA.inv() @Bwith?,a numerically stable algorithm,7153
9029,"When inputs are on what device, this function synchronizes that device with the CPU?",CUDA,8365
9030,"What is the tensor of shape(*, n, n)where*?",zero or more batch dimensions,5770
9031,What is the default value for output tensor?,Default:None,5770
9032,What if the matrixAor any matrix in the batch of matricesAis not invertible?,RuntimeError,5770
9033,What does RuntimeError stand for?,Examples,5770
9034,What does torch.backends control?,backends that PyTorch supports,11006
9035,What is the point of using CUDA in a PyTorch binary?,"if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",11006
9036,What is PyTorch built with support for?,CUDA,11007
9037,What does cufft_plan_cache do?,Clears the cuFFT plan cache,11007
9038,What does PyTorch support on Ampere devices?,SeeTensorFloat-32(TF32),11007
9039,Aboolthat controls whether what is enabled in PyTorch?,cuDNN,11007
9040,What controls the behavior of various backends that PyTorch supports?,torch.backends,11008
9041,What does not mean that PyTorch is built with CUDA support?,CUDA,11008
9042,What support does PyTorch have?,OpenMP,11008
9043,What does cufft_plan_cache cache?,cuFFT plans,11008
9044,What is returned when a bool indicating if CUDNN is currently available?,version of cuDNN,11008
9045,What type of algorithms does _algorithms_enabled() enable?,deterministic,11008
9046,Aboolthat controls whether what is enabled?,cuDNN,11008
9047,Aboolthat controls whether what may be used in matrix multiplications on Ampere or newer GPUs?,TensorFloat-32 tensor cores,11008
9048,What does cuDNN do if True?,benchmark multiple convolution algorithms and select the fastest,11008
9049,What causes cuDNN to benchmark multiple convolution algorithms and select the fastest?,Aboolthat,11008
9050,What type of algorithms does _algorithms_enabled() use?,deterministic,11008
9051,Which backend controls if TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere,SeeTensorFloat-32(TF32),11008
9052,Aintthat controls what of the cuFFT plan?,cache capacity,11008
9053,Does this mean CUDA support is available?,"if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",10997
9054,What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication?,Abool,10997
9055,What does this mean?,"if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",5692
9056,What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Ampere or newer GPU,Aboolthat,5692
9057,What does not mean PyTorch is built with CUDA support?,CUDA,5692
9058,What is used on Ampere devices?,SeeTensorFloat-32(TF32),5692
9059,What controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer,Abool,5692
9060,What is built with CUDA support?,PyTorch,5692
9061,What does the PyTorch binary do?,SeeTensorFloat-32(TF32) on Ampere devices,5692
9062,What is the name of the cuFFT plans?,cufft_plan_cachecaches,5692
9063,What does it do to the cuFFT plan cache?,Clears the cuFFT plan cache,5692
9064,What does Aint that controls cache capacity of cuFFT plan do?,Clears the cuFFT plan cache,5692
9065,What caches the cuFFT plans?,cufft_plan_cache,5692
9066,What does Abool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Amper,SeeTensorFloat-32(TF32) on Ampere devices,909
9067,What shows the number of plans currently in the cuFFT plan cache?,readonly int that,909
9068,What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on newer GPUs?,Aboolthat,909
9069,What does aint that controls the cache capacity of a cuFFT plan do?,Clears the cuFFT plan cache,909
9070,Aboolthat controls whether TensorFloat-32 tensor cores may be used in what?,matrix multiplications,909
9071,What is the name of the tensor cores used in matrix multiplications on Ampere devices?,SeeTensorFloat-32(TF32),909
9072,What does Aintthat controls cache capacity of cuFFT plan do?,Clears the cuFFT plan cache,909
9073,What does torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch,whether PyTorch is built with CUDA support,10995
9074,Why is CUDA not supported by PyTorch?,"if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",11002
9075,What is the name of the tensor cores used on Ampere devices?,SeeTensorFloat-32(TF32),11002
9076,What does PyTorch use on Ampere devices?,SeeTensorFloat-32(TF32),11002
9077,What does abool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Am,SeeTensorFloat-32(TF32),11000
9078,What support does torch.backends.openmp return?,CUDA,11003
9079,What does cufft_plan_cachecachecache do?,Clears the cuFFT plan cache,11003
9080,What does it do when a PyTorch binary is built with CUDA support?,Clears the cuFFT plan cache,11003
9081,What is returned by torch.backends.openmp?,version of cuDNN,11003
9082,If PyTorch was run on a machine with working what would we be able to use it?,CUDA drivers and devices,5691
9083,What does abool that controls if TensorFloat-32 tensor cores may be used in matrix multiplication on,SeeTensorFloat-32(TF32) on Ampere devices,10996
9084,What controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere devices,SeeTensorFloat-32(TF32) on Ampere devices,10996
9085,Returns what version of cuDNN?,version of cuDNN,10996
9086,What does openmp return?,whether PyTorch is built with CUDA support,10996
9087,Aintthat controls what of a cuFFT plan?,cache capacity,9142
9088,Aintthat controls what of a cuFFT plan. Clears the cuFFT plan cache.,cache capacity,9142
9089,What is returned when the cuFFT plan cache is cleared?,version of cuDNN,9142
9090,A readonly int that shows the number of plans currently in what?,cuFFT plan cache,9142
9091,What does cufft_plan_cachecache do?,Clears the cuFFT plan cache,9142
9092,What indicates if CUDNN is currently available?,bool,5687
9093,Abool that controls what?,whether cuDNN is enabled,5687
9094,What may be used in cuDNN convolutions on Ampere or newer GPUs?,TensorFloat-32 tensor cores,5687
9095,Aboolthat controls where tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs,TensorFloat-32,5687
9096,What is the name of the tensor cores that can be used in cuDNN convolutions on Ampere devices?,SeeTensorFloat-32(TF32),5687
9097,What controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions?,SeeTensorFloat-32(TF32) on Ampere devices,5687
9098,"Aboolthat, if True, causes cuDNN to only use what?",deterministic convolution algorithms,5687
9099,What does Returns a bool indicating if CUDNN is currently available?,version of cuDNN,5687
9100,What type of algorithm is enabled in _algorithms_enabled()?,deterministic,5687
9101,What does a bool that is True cause cuDNN to do?,benchmark multiple convolution algorithms and select the fastest,5687
9102,Aboolthat controls what?,whether cuDNN is enabled,911
9103,What does cufft_plan_cachecache cache?,cuFFT plans,911
9104,What is built with MKL support?,PyTorch,911
9105,Aintthat controls what of the cuFFT plan. Clears the cuFFT plan cache.,cache capacity,911
9106,What does Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions,SeeTensorFloat-32(TF32),911
9107,What type of algorithms does _algorithms_enabled() and torch.use_deterministic_algorith,deterministic,911
9108,What can TensorFloat-32 tensor cores be used in?,matrix multiplications,910
9109,Aboolthat controls whether what is enabled. Returns a bool indicating if CUDNN is currently available.,cuDNN,910
9110,What does abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolution,SeeTensorFloat-32(TF32) on Ampere devices,996
9111,Aintthat controls what of cuFFT plan?,cache capacity,996
9112,What does Aint do?,Clears the cuFFT plan cache,996
9113,Aboolthat controls where what may be used in cuDNN convolutions on Ampere or newer GPUs?,TensorFloat-32 tensor cores,996
9114,What does Aint that controls cache capacity of cuFFT plan return?,version of cuDNN,996
9115,What does aint that controls cache capacity of cuFFT plan do?,Clears the cuFFT plan cache,833
9116,What type of intthat shows the number of plans currently in the cuFFT plan cache?,readonly,833
9117,What does Aintthat controls cache capacity of cuFFT plan cache do?,Clears the cuFFT plan cache,833
9118,What is returned when a cuFFT plan cache is cleared?,version of cuDNN,833
9119,What does the bool that returns the version of cuDNN control?,whether cuDNN is enabled,1586
9120,What does cuFFT do?,Clears the cuFFT plan cache,1586
9121,What does the bool that returns the version of cuDNN do?,controls whether cuDNN is enabled,5686
9122,What is _enabled()?,deterministic_algorithms,5290
9123,What returns a indicating if CUDNN is currently available?,bool,5290
9124,What does the Abool that returns a bool that controls?,whether cuDNN is enabled,5290
9125,What does the Abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolution,SeeTensorFloat-32(TF32) on Ampere devices,5290
9126,Aboolthat causes cuDNN to only use deterministic convolution algorithms if what?,True,5290
9127,Returns what indicating if CUDNN is currently available?,a bool,5290
9128,Which bool controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Am,SeeTensorFloat-32(TF32),5290
9129,What does Abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions,SeeTensorFloat-32(TF32) on Ampere devices,912
9130,"What does Aboolthat, if True, cause cuDNN to do?",benchmark multiple convolution algorithms and select the fastest,912
9131,What type of algorithms are enabled by _algorithms_enabled()?,deterministic,912
9132,What controls where TensorFloat-32 tensor cores may be used in cuDNN convolution?,Aboolthat,908
9133,"Aboolthat, if True, causes cuDNN to only use what convolution algorithms?",deterministic,908
9134,What does Aboolthat return?,whether PyTorch is built with MKL support,908
9135,"What does aboolthat, if True, cause cuDNN to only use?",deterministic convolution algorithms,913
9136,What type of support does PyTorch have?,OpenMP,5694
9137,What is built with OpenMP support?,PyTorch,5694
9138,For what type of tensor does it compute the logical OR?,bool tensors,1699
9139,"What is the second input tensor out(Tensor,optional) called?",output tensor,1699
9140,What does the one dimensional discrete Fourier transform ofinput. Compute the one dimensional inverse discrete Fourier transform ofinput,Discrete Fourier transforms and related functions,2156
9141,What does the N dimensional discrete Fourier transform ofinput compute?,2 dimensional inverse discrete Fourier transform ofinput,2156
9142,What does the 2 dimensional inverse discrete Fourier transform ofinput compute?,N dimensional discrete Fourier transform ofinput,2156
9143,What is the 2 dimensional discrete Fourier transform ofinput?,Computes the 2 dimensional discrete Fourier transform ofinput,2156
9144,What is the one dimensional inverse discrete Fourier transform ofinput?,2 dimensional inverse discrete Fourier transform ofinput,1796
9145,Computes the one dimensional inverse discrete Fourier transform ofinput.,2 dimensional discrete Fourier transform,1796
9146,Computes what for a signal of sizen?,the discrete Fourier Transform sample frequencies,1796
9147,Computes the sample frequencies with a signal of sizen.,forrfft(),1796
9148,Computes the one dimensional inverse discrete Fourier transform of realinput. Computes the inverse ofrfft2,2-dimensional discrete Fourier transform of realinput,1796
9149,What is the one dimensional discrete Fourier transform ofinput?,N dimensional discrete Fourier transform ofinput,2158
9150,What is the one dimensional Fourier transform of real-valuedinput?,N dimensional inverse discrete Fourier transform ofinput,2158
9151,Computes the discrete Fourier Transform sample frequencies for a signal of what?,sizen,2158
9152,What provides n-dimensional FFT data to have negative frequency terms first?,byfftn(),2158
9153,What is the N dimensional discrete Fourier transform ofinput?,Computes the N dimensional discrete Fourier transform ofinput,2157
9154,What is one dimensional discrete Fourier transform ofinput?,Discrete Fourier transforms,2157
9155,Computes the inverse ofrfft2(). Computes the N-dimensional discrete Fourier transform of realinput.,2-dimensional discrete Fourier transform of realinput,2157
9156,Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes the,inverse ofrfftn(),2157
9157,Computes the one dimensional Fourier transform of real-valuedinput. Computes the inverse ofrfft(). Comp,Computes the N dimensional inverse discrete Fourier transform ofinput,2157
9158,Computes the 2-dimensional discrete Fourier transform of realinput. Computes the inverse ofrfft2()?,Computes the inverse ofrfft2(),2157
9159,Computes what ofinput?,one dimensional inverse discrete Fourier transform,1793
9160,What is the N dimensional inverse discrete Fourier transform ofinput?,Computes the N dimensional discrete Fourier transform,1793
9161,What is the name of the one dimensional inverse discrete Fourier transform ofinput?,2 dimensional inverse discrete Fourier transform ofinput,1793
9162,Computes the one dimensional Fourier transform of what?,real-valuedinput,1793
9163,What is the name of the one dimensional Fourier transform of real-valuedinput?,N dimensional inverse discrete Fourier transform ofinput,1793
9164,What computes the one dimensional Fourier transform of real-valuedinput?,inverse ofrfft(),1793
9165,What does Computes the one dimensional Fourier transform of real-valuedinput?,Computes the N dimensional inverse discrete Fourier transform ofinput,1793
9166,What does the inverse ofrfft() compute?,one dimensional Fourier transform of real-valuedinput,1793
9167,What does the inverse ofrfft2() compute?,2-dimensional discrete Fourier transform of realinput,1793
9168,What does Computes the 2 dimensional inverse discrete Fourier transform ofinput?,Computes the 2 dimensional inverse discrete Fourier transform ofinput,1793
9169,Computes the one dimensional Fourier transform of real-valuedinput. Computes what?,inverse ofrfft(),1793
9170,Computes the N-dimensional discrete Fourier transform of what?,realinput,1793
9171,What does the inverse ofrfftn() compute?,N-dimensional discrete Fourier transform of realinput,1793
9172,What is the one dimensional discrete Fourier transform of?,Hermitian symmetricinputsignal,1793
9173,Computes the 2-dimensional discrete Fourier transform of realinput. Computes what?,inverse ofrfft2(),1793
9174,Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes what?,inverse ofrfftn(),1793
9175,Computes the one dimensional discrete Fourier transform of what?,Hermitian symmetricinputsignal,1793
9176,What computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal?,inverse ofrfftn(),1793
9177,Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.,inverse ofhfft(),1793
9178,Computes the inverse ofrfft2(). Computes the inverse ofrfftn(). Computes,2-dimensional discrete Fourier transform of realinput,1793
9179,Computes the one dimensional inverse discrete Fourier transform ofinput. Computes the one dimensional inverse discrete Fourier,2 dimensional discrete Fourier transform,1793
9180,What does Computes the 2 dimensional discrete Fourier transform ofinput?,Computes the 2 dimensional inverse discrete Fourier transform ofinput,1792
9181,Computes what of real-valuedinput?,one dimensional Fourier transform,1686
9182,What does Computes the N dimensional inverse discrete Fourier transform ofinput?,Computes the N dimensional discrete Fourier transform ofinput,1686
9183,Computes the inverse ofrfft2(). Computes the inverse ofrfftn().,2-dimensional discrete Fourier transform of realinput,1686
9184,Computes the N-dimensional discrete Fourier transform of realinput.,inverse ofrfftn(),1686
9185,What does Computes the one dimensional inverse discrete Fourier transform ofinput?,Computes the 2 dimensional inverse discrete Fourier transform ofinput,1795
9186,Computes the one dimensional Fourier transform of real-valuedinput.,inverse ofrfft(),1795
9187,What is the name of the transform that is computed?,2 dimensional discrete Fourier transform ofinput,1665
9188,Computes the one dimensional Fourier transform of real-valuedinput. Computes the 2-dimensional discrete Fourier transform of realin,inverse ofrfft(),1665
9189,Computes the inverse ofrfft2().,2-dimensional discrete Fourier transform of realinput,1665
9190,Computes the 2-dimensional discrete Fourier transform of realinput.,inverse ofrfft2(),1667
9191,What is the name of the two dimensional inverse discrete Fourier transform of realinput?,N dimensional discrete Fourier transform ofinput,1667
9192,Computes the inverse ofrfft2(). Computes the inverse ofrfft2().,2-dimensional discrete Fourier transform of realinput,1667
9193,Computes the 2-dimensional discrete Fourier transform of realinput. Computes the N-dimensional discrete Fourier transform of realin,inverse ofrfft2(),1688
9194,Computes the one dimensional Fourier transform of real-valuedinput. Computes the one dimensional Fourier transform of real-value,inverse ofrfft(),1687
9195,What does the inverse ofrfft() do?,one dimensional Fourier transform of real-valuedinput,1791
9196,What dimension is the discrete Fourier transform of realinput?,N-dimensional,1689
9197,What does a Hermitian symmetricinputsignal do?,one dimensional discrete Fourier transform,1689
9198,Computes the one dimensional discrete Fourier transform of a what symmetricinputsignal?,Hermitian,1689
9199,Computes the 2-dimensional discrete Fourier transform of realinput. Computes the inverse ofrfft2().,inverse ofrfft(),1783
9200,Computes the 2-dimensional discrete Fourier transform of realinput. Computes the one dimensional discrete Fourier transform of a,inverse ofrfftn(),1783
9201,Computes the one dimensional discrete Fourier transform of what symmetricinputsignal?,Hermitian,1784
9202,Computes the discrete Fourier Transform sample frequencies for a signal of sizen. Computes the discrete Fourier Transform sample,inverse ofhfft(),1784
9203,Computes the sample frequencies forrfft()with a signal of what?,sizen,1784
9204,Computes the N-dimensional discrete Fourier transform of realinput. Computes the one dimensional discrete Fourier transform of,inverse ofrfft2(),1784
9205,What is the discrete Fourier transform of realinput?,2-dimensional,1668
9206,What transform of realinput is computed?,N-dimensional discrete Fourier transform,1668
9207,Computes the N-dimensional discrete Fourier transform of realinput. Computes what?,inverse ofrfftn(),1668
9208,What does the inverse ofhfft() compute?,one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal,1666
9209,What is the name of the function that reorders n-dimensional FFT data to have negative frequency terms first?,Inverse offftshift(),1666
9210,Computes the sample frequencies forrfft() with a signal of what?,sizen,1702
9211,Computes the discrete sample frequencies for a signal of sizen.,Fourier Transform,1702
9212,"Reorders n-dimensional FFT data, as provided byfftn(), to have what first?",negative frequency terms,1702
9213,What function reorders n-dimensional FFT data to have negative frequency terms first?,Inverse offftshift(),1702
9214,Computes the discrete sample frequencies for a signal of sizen?,Fourier Transform,1702
9215,What provides n-dimensional FFT data?,byfftn(),1702
9216,Computes the sample frequencies with a signal of sizen?,forrfft(),1702
9217,What global function returns a tensor with dtypetorch.int64?,dtype default,7279
9218,What is the default value of the function that returns a tensor filled with random integers?,0. high(int),5413
9219,What variable defines the shape of the tensor?,argumentsize,7280
9220,What is a tuple defining the shape of the output tensor?,size(tuple),7280
9221,What does torch.float32 use?,global dtype default,7280
9222,What is the default value for a global default tensor type?,if None,7280
9223,What is the default value for the lowest integer to be drawn from the distribution?,0. high(int),8425
9224,What is the global name of the function that returns a tensor with dtypetorch.int64?,dtype default,8425
9225,What does Torch.float32 return a tensor with?,global dtype default,8425
9226,What does torch.float32 return a tensor with?,global dtype default,4453
9227,What does this function return with the global dtype default?,a tensor with dtypetorch.int64,4453
9228,"With the global dtype default (torch.float32), this function returns a tensor with what?",dtypetorch.int64,8426
9229,What does if None use for the default tensor type?,current device,8426
9230,How many times above the highest integer to be drawn from the distribution is high(int)?,One,9516
9231,"The shapes of themasktensor and the input tensor don’t need to match, but what?",they must bebroadcastable,5327
9232,The returned tensor doesnotuse the same storage as the original tensor?,input(Tensor),5327
9233,What is returned that is filled with random numbers from a normal distribution with mean 0 and variance 1?,a tensor with the same size as input,5488
9234,What does if None default to?,defaults to the device ofinput,5488
9235,What is the default value for autograd to return a tensor with the same size as input?,False,5488
9236,"What does dtype(torch.dtype, optional) return?",the desired data type,5487
9237,What does if None do?,defaults to the dtype ofinput,5487
9238,Where are exact algorithms discussed in the PyTorch Timer?,method docstrings,2491
9239,What is important because some elements of PyTorch are lazily initialized?,warmups,7855
9240,What does Timer deviate from?,timeitAPI,7855
9241,What class group and display results for comparison?,theCompareclass,8366
9242,What class does this class deviate from?,timeitAPI,8366
9243,What fields can one optionally define when defining a Timer?,"specifylabel,sub_label,description, andenv",8366
9244,"Timer specific constructor arguments: stmt,setup,timer,globals, what?",PyTorch,8366
9245,What is setup code used to define variables used in PyTorch?,instmt global_setup,3698
9246,What is the name of the constructor that describes PyTorch?,sub_label,4949
9247,Code snippet to be run in a loop and timed?,stmt,10769
9248,What happens if PyTorch was built without GPU or there is no GPU present?,synchronize CUDA,10921
9249,What is a description of PyTorch?,description,10921
9250,How does block_autorange measure many replicates?,keeping timer overhead to a minimum,4234
9251,Block_autorange executes the following pseudo-code: Note what in the inner loop?,variableblock_size,4234
9252,What does blocked_autorange execute in the inner loop?,variableblock_size,1342
9253,How does blocked_autorange set block_size?,running a warmup period,1342
9254,What is a measurement object that contains measured runtimes and repetition counts?,median,1342
9255,What better amortizes the cost oftimerinvocation?,A large block size,809
9256,What are examples of statistics that can be collected using Callgrind?,"mean, median, etc.",809
9257,What is collected and cached to indicate how many instructions are from the Python loop which drivesstmt?,a profile,809
9258,"Block_size is set by running a warmup period, increasing block_size until timer overhead is less than what percentage of the overall computation?",0.1%,4233
9259,What is blocked_autorange's block_size used for?,main measurement loop,1341
9260,What is the name of the measurement object that can be used to compute statistics?,median,765
9261,How are instruction counts different from wall times?,deterministic,1617
9262,What are instruction counts ideal for?,detailed performance analysis,1617
9263,What rely on pickle and you may need to add an import tosetup for them to transfer properly?,nn.Modules,3749
9264,What can globals not contain?,arbitrary in-memory data structures,1428
9265,What do nn.Modules rely on?,pickle,1429
9266,Execute what?,main statement (stmt)numbertimes,1429
9267,What is the purpose of the Approximate significant figure estimate property?,to give a convenient way to estimate the precision of a measurement,1429
9268,Why is there a process boundary between the caller and thestmtexecution?,globalscannot contain arbitrary in-memory data structures,1429
9269,"What are restricted to builtins,nn.Modules, and Torch Scripted functions/modules?",globals,1429
9270,What provides more detail on this subject?,TheGlobalsBridgeclass,1429
9271,What does ACallgrindStats mirror?,timeit.Timer.timeit(),1429
9272,What does this class provide?,several convenience methods,1429
9273,Merge will extrapolate times tonumber_per_run=1 and will not transfer any metadata. (Since it might differ between,Convenience method for merging replicates,1429
9274,What region does the Approximate significant figure estimate only use?,interquartile region,1429
9275,What is used to provide a more human interpretable data summary?,thetrim_sigfigmethod,1429
9276,What is significant figure estimation intended for?,forCompare,1429
9277,What container is used for Callgrind results collected by Timer?,Top level container,1429
9278,What does timeit.Timer.timeit provide downstream consumers?,several convenience methods,1485
9279,What does Merge extrapolate times tonumber_per_run=1 and not transfer any metadata?,Approximate significant figure estimate,1485
9280,What is the static z value of timeit.Timer.timeit()?,1.645,4249
9281,What is the name of the function that mirrors the semantics of?,timeit.Timer.timeit(),4249
9282,What convenience method does timeit.Timer.timeit provide?,__repr__,4249
9283,What is a tutorial about packaging a Torch Script module?,first model,8029
9284,What is a Torch Script module used for?,Patch code into a package,8029
9285,What do you do from a package?,Access package contents,8029
9286,What can you do to package a Torch Script module?,Package a Torch Script module,919
9287,What does a package contain?,Package a Torch Script module,5792
9288,What is a way to test if a package is executing inside a package?,Patch code into a package,6759
9289,What can I do from packaged code?,Access package contents,6759
9290,What does Torch distinguish between?,packaged code and non-packaged code,2163
9291,What is one way to distinguish between packaged code and non-packaged code?,Re-export an imported object,2164
9292,What can you do to differentiate between packaged code and non-packaged code?,Package a Torch Script module,2164
9293,What is the name of the package that finds your code's dependencies?,torch.packageFormat,2164
9294,What defines your code's dependencies?,torch.packageFormat,4713
9295,What can you do with a Torch Script module?,edit files and :writethem back into the archive,4713
9296,What is a Torch Script module?,Package a Torch Script module,4713
9297,What is similar to defining a corresponding de-packaging function on a class and by defining a corresponding de-packaging,defining__reduce__for Python’s normal pickling process,4713
9298,What will you be familiar with after completing this tutorial?,basic API,11197
9299,What does torch.packageallow for?,customization,11197
9300,What is a similar behavior to defining the method__reduce_package__on a class and by defining a,defining__reduce__for Python’s normal pickling process,11197
9301,What steps are included in the Torch.packageFormat Overview Howtorch.packagekeeps packages isolated from each other?,Steps,11197
9302,What will return a printable and queryableFolderobject?,afile_structure()method,4709
9303,"What are the three methods that allow you to save Python objects, text, and binary data to a package?","save_pickle,save_textandsave_binary",4709
9304,Package Importerexposes complementary methods called what?,load_pickle,4709
9305,What can vim do with ZIP archives?,edit files and :writethem back into the archive,11350
9306,What are the three methods that Package Exporter exposes?,"save_pickle,save_textandsave_binary",11350
9307,How is torch.package accessed?,defining the method__reduce_package__on a class and by defining a corresponding de-packaging function,8559
9308,What is the name of the step in the Python pickling process?,Steps,8559
9309,What can you do with the has_file() method?,query Folder objects,8559
9310,What is it bad practice to have code that behaves differently depending on whether it's packaged or not?,behaves differently,8229
9311,What offers asave_source_string() method that allows one to save arbitrary Python source code to a module of your choosing?,Package Exporter,8229
9312,Why should you consult this essay for a deeper dive in how Python packaging works?,it’s slightly out of date,2055
9313,Where can you check the implementation details of Python packages?,Python reference documentation,9306
9314,What standard forms does Torch.package look for when a Python module is identified as a dependency?,"fromximporty,importz,fromwimportvasu",8396
9315,"When import statements are encountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same walking way",AST,8396
9316,What are the standard forms of import statements that Torch.package looks for when a Python module is identified as a dependency?,"fromximporty,importz,fromwimportvasu",3692
9317,What does AST parsing have limited support for?,the__import__(...)syntax,3692
9318,What are the standard forms of import statements that torch.package looks for?,"fromximporty,importz,fromwimportvasu",8341
9319,What walking way does torch.package parse dependencies in?,AST,7025
9320,What should not be detected by torch.package?,dynamic imports,7025
9321,What does depending on this module do during package export?,raise an error,4521
9322,"If any external library changes in a way, your package may fail to load.",backwards-incompatible,8224
9323,"What is code that you ""know"" will not be needed in the loaded package?",initialization/configuration code,8224
9324,What does Package Importer invoke?,Package Importer.import_module(),7618
9325,What prefix does Package Importermangles the__name__and__file__of all imported modules?,a mangle,7618
9326,What can future users of a package do?,"unzip the package, and edit the code in order to perform custom modifications to it",6986
9327,How can future users of a package edit the code in order to perform custom modifications to it?,unzip,6986
9328,"What allows you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package?",Exporters,2354
9329,What is the name of the file to export to?,Create an exporter,1953
9330,What can be used to create an exporter?,a string/Path object,1953
9331,"If what is passed, use that to search for modules?",a single Importer,1953
9332,"If a single Importer is passed, use that to search for modules?",importer,9599
9333,Blocklist modules whose names match what from the list of modules the package can import?,glob patterns,1450
9334,Includemodulein the list of external modules the package can import. This will prevent what from saving it in the package?,dependency discovery,9288
9335,"What is included(Union[List[str],str])?",list of strings for the names of the modules to be externed,9288
9336,"What does include(Union[List[str],str]] mean?",list of strings for the names of the modules to be externed,9288
9337,What happens if an extern module glob pattern is added with allow_empty=False?,an exception is thrown,9288
9338,What does get an id for a package?,Specify modules that should be packaged,9288
9339,"What can also be a glob-style pattern, as described inmock()?",include,9605
9340,How often is an id guaranteed to be handed out for a package?,once,2691
9341,What is a string e.g. “my_package.my_subpackage” or list of strings for the names of the,include,9284
9342,What is a handle that can be used to remove the added hook?,callinghandle.remove(),5127
9343,What does the exporter register on the exporter?,extern hook,5127
9344,Hooks will be called in what order?,order of registration,5127
9345,What does a module match against?,an intern()pattern,5127
9346,What is used to identify it to load?,resource(str) – A unique name for the resource,5127
9347,"Ifdependenciesis true, this method will also scan the pickled objects for which modules are required to reconstruct?",Ifdependenciesis true,5127
9348,When will hooks be called on the exporter?,in order of registration,5126
9349,What saves raw bytes to the package?,torch.utils.hooks.RemovableHandle,7106
9350,"What should go it (e.g. ""my_package.my_subpackage"")?",package(str) – The name of module package this resource,7106
9351,What does resource(str) stand for?,A unique name for the resource,7106
9352,Code will be saved to provide code for this package. what is e.g. my_package.my_subpackage?,module_name(str),7106
9353,What will need to be present in theimporterlist for this to be possible to save objects that have previously been packaged?,importer’simport_modulemethod,7106
9354,What is the code to save?,Save the code formoduleinto the package,2952
9355,Code for the module is resolved using what?,the importers path,2952
9356,In what order will hooks be called?,order of registration,2952
9357,What does binary(str) stand for?,binary(str) – The data to save,2952
9358,"Code will be saved to provide code for this package. dependencies(bool,optional) – If True, we scan the source for",module_name(str),2952
9359,What is the equivalent of pickle to save a python object to the archive?,totorch.save(),2952
9360,"What does not save the code, only the objects. Ifdependenciesis true, this method will also scan the pickled objects for which modules are required",Stanard pickle,2952
9361,"If true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.",Ifdependencies,2952
9362,resource(str) – A what?,unique name for the resource,2952
9363,"If true, we scan the source for dependencies. Save a python object to the archive using pickle. Save a",If True,2952
9364,Hooks will be called where?,in order of registration,2952
9365,What is the name of the tool that saves raw bytes to the package?,torch.utils.hooks.RemovableHandle,2952
9366,Package(str) – The name of what module package this resource should go it?,module,2952
9367,What will need to be present in theimporterlist for this to work?,importer’simport_modulemethod,2952
9368,Package(str) – The name of what package this resource should go in?,module,2952
9369,What is returned when a tensor is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?,tensor with the same shape,5492
9370,What quantiles of each row of the input tensor along the dimensiondim?,q-th,6765
9371,"What is returned when a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the",the cumulative product of elements ofinputin the dimensiondim,5545
9372,What does Usetensorboard_trace_handler() generate?,result files,4439
9373,What does Python Profiler TensorBoard plugin use?,tensorboard,10815
9374,What is used to save stacks file to this location?,path(str),2361
9375,What is the name of the file where stack traces are saved?,path(str),5743
9376,What returns the counts for each unique element?,return_counts,5684
9377,"When dim is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument?",CUDA implementation and the CPU implementation,4366
9378,"If you need to do what before constructing optimizers for a model, please do so before constructing optimizers for it.",move a model to GPU via.cuda(),7876
9379,What should each iterable ofdicts contain?,aparamskey,7876
9380,"What do you want to specify when you only want to vary a single option, while keeping all others consistent between parameter groups?",per-layer learning rates,3733
9381,What can still be passed as keyword arguments?,options,8597
9382,Options will be used as what in the groups that didn't override them?,defaults,8598
9383,"Some optimization algorithms need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute",Conjugate Gradient and LBFGS,8598
9384,What is a keyword argument useful for?,"when you only want to vary a single option, while keeping all others consistent between parameter groups",8598
9385,"What is this useful when you only want to vary a single option, while keeping all others consistent between parameter groups?",per-layer learning rates,8598
9386,The function can be called once the gradients are computed using what?,e.g.backward(),7931
9387,What do you have to do to usetorch.optim?,construct an optimizer object,7931
9388,To construct what you have to give it an iterable containing the parameters (all should beVariables) to optimize?,an Optimizer,7931
9389,"Instead of passing an iterable ofVariables, what is used to specify per-parameter options?",pass in an iterable ofdicts,7931
9390,What is this useful when you want to specify?,per-layer learning rates,7931
9391,What class's parameters will use a learning rate of1e-3?,model.classifier,2536
9392,Some optimization algorithms such as what need to reevaluate the function multiple times?,Conjugate Gradient and LBFGS,2535
9393,What does Optimizer.add_param_group Add a param group to theOptimizersparam_groups?,Optimizer.load_state_dict,4690
9394,Option arguments will be used as what in the groups that didn’t override them?,defaults,4690
9395,What are examples of objects that don't satisfy the properties of parameters?,sets and iterators over values of dictionaries,4735
9396,What is implemented optionally with momentum?,stochastic gradient descent,4679
9397,What should be applied after optimizer's update?,Learning rate scheduling,4679
9398,What algorithm does Optimizer.add_param_group implement?,Adam algorithm,4679
9399,What algorithm does Optimizer.step implement?,Adam algorithm,4683
9400,Which algorithm implements the Adadelta algorithm?,Adagrad,3675
9401,What is the learning rate scheduler called?,callingscheduler.step(),4807
9402,Learning rate schedulers can be called back-to-back to refer to what?,schedulers algorithms,4807
9403,What does the following template refer to?,schedulers algorithms,2314
9404,What does the following template refer to in PyTorch documentation?,schedulers algorithms,3742
9405,What does lr_scheduler.StepLR Decay the learning rate of each parameter group by?,gamma,3742
9406,What language did 1.1.0 change the behavior of the learning rate scheduler?,BC,8175
9407,What is the learning rate of each parameter group set to?,the initial lr times a given function,5852
9408,What LR Multiply the learning rate of each parameter group by the factor given in the specified function?,Multiplicative,9837
9409,What Decays the learning rate of each parameter group by gamma every step_size epochs?,StepLR,9837
9410,What is a learning rate scheduler?,"anneals the learning rate to a fixed value, and then keeps it constant",8041
9411,What does update_bn() assume each batch in the dataloaderloader is?,tensors,8223
9412,"By default,what is computed by torch.optim.swa_utils.AveragedModelcomputes?",running equal average of the parameters that you provide,8223
9413,What is the model that accumulates the averages of the weights?,SWA model,8223
9414,How many epochs does the model train for?,300,8223
9415,What does this function return of a real symmetric or complex Hermitian matrixinputor a batch thereof?,eigenvalues and eigenvectors,7562
9416,What are computed if the boolean argumenteigenvectors is False?,eigenvalues,7562
9417,What portion of the matrix is used by default?,upper triangular portion,7562
9418,What is used if the input matrixinputis supposed to be symmetric or Hermitian?,lower triangular portion,7562
9419,In what order are the eigenvalues of each matrix in a batch of matrices returned?,ascending order,7562
9420,What happens to the returned matrix regardless of the original strides?,matrixVwill be transposed,7562
9421,Holds submodules in a list. Holds parameters in a list. Holds parameters in a dictionary.,dictionary,7430
9422,What is the basic building block for graphs?,nn.Conv1d,7430
9423,What is a global forward hook common to all the modules?,nn.Conv1d,1860
9424,"What are Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations",Pooling layers,4873
9425,nn.Conv1d Applies a what convolution over an input signal composed of several input planes?,1D,4338
9426,What are Transformer Layers?,Normalization Layers Recurrent Layers,4338
9427,What is another name for nn.Conv3d?,nn.ConvTranspose1d,5898
9428,What type of hook is registered for all modules?,global forward hook,4115
9429,What does nn.ConvTranspose1d use?,nn.ConvTranspose1d,8133
9430,What does nn.Conv1d Applies over an input signal composed of several input planes?,1D convolution,5069
9431,Global Hooks For Module Registers is what?,a forward pre-hook common to all modules,5069
9432,A kind of Tensor that is to be considered a module parameter. A parameter that is not initialized. A buffer that is not initialized,Quantized Functions Lazy Modules Initialization,5069
9433,What is the name of the module that Quantized Functions Lazy Modules Initialization is to be considered a module parameter?,nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d,5069
9434,nn.Conv1d Applies what over an input signal composed of several input planes?,1D convolution,10018
9435,nn.Conv3d Applies what convolution over an input signal composed of several input planes?,3D,10018
9436,What type of transposed convolution operator does nn.ConvTranspose2d Applies?,2D,10018
9437,What type of transposed convolution operator over an input image composed of several input planes?,3D,10018
9438,nn.Conv2d Applies a 2D convolution over an input signal composed of what?,several input planes,10018
9439,What module with lazy initialization of thein_channelsargument of theConvTranspose1d is inferred from thein,nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d,10018
9440,What is the module with lazy initialization of the nn.LazyConvTranspose3d a torch.nn.Con,nn.LazyConvTranspose3d a torch.nn.ConvTranspose3d,10018
9441,What does nn.Conv1d and nn.Conv2d use?,nn.ConvTranspose3d,830
9442,Applies a transposed convolution operator over an input image composed of several input planes?,3D,1236
9443,What does nn.LPPool1d Applies over an input signal composed of several input planes?,1D power-average pooling,1211
9444,What is the name of the Sigmoid Linear Unit (SiLU) function?,nn.Mish,10071
9445,What input does nn.BatchNorm2d Applies Batch Normalization over?,4D,1176
9446,What input does BatchNorm3d apply Batch Normalization over?,5D,1176
9447,What is a mini-batch of 1D inputs with optional additional channel dimension?,3D input,1176
9448,What inputs does the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift cover?,2D or 3D,1176
9449,What is the paperGroup Normalization?,nn.GroupNorm Applies Group Normalization over a mini-batch of inputs,10106
9450,What is the name of the instanceNorm1d module?,nn.InstanceNorm1d,10106
9451,What does SyncBatchNorm Applies Batch Normalization over a N-Dimensional input?,nn.LayerNorm,10207
9452,What is the name of the document that applies Batch Normalization over a N-Dimensional input?,nn.LayerNorm,1183
9453,What loss is nn.NLLLoss?,negative log likelihood loss,10095
9454,"What label does MarginRankingLoss create a criterion that measures the loss given inputsx1x1x1,",1D mini-batch tensoryy,1975
9455,What criterion optimizes a multi-class multi-classification hinge loss?,nn.HuberLoss,4301
9456,What type of loss is associated with Poisson distribution of target?,Negative log likelihood loss,4301
9457,"If the absolute element-wise error falls below delta, what is the criterion that uses a squared term?",if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise,10065
9458,What is the name of the criterion that uses a squared term if the absolute element-wise error falls below delta?,nn.Smooth,10134
9459,What is a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-,nn.Smooth,10134
9460,What does nn.MSELoss measure between each element in inputxxxand targetyyy?,mean squared error,10134
9461,What measure measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -,nn.HingeEmbeddingLoss,6926
9462,Which criterion uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?,nn.SoftMarginLoss,9998
9463,What criterion uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?,nn.SoftMarginLoss,1969
9464,What type of loss is optimized between inputxxx(a 2D mini-batchTensor) and outputyyy?,multi-class multi-classification hinge loss,1972
9465,What criterion optimizes a two-class classification logistic loss between input tensorxxxand target tensoryy,MultiLabelSoftMarginLoss,1972
9466,CustomFromMask prune.identity Applies what reparametrization to the tensor corresponding to the parameter calledname,pruning,916
9467,What is the Abstract base class for creation of?,pruning techniques,916
9468,prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the,L1-norm,10493
9469,At what rate are channels in a tensor pruned?,random,10493
9470,Which Prunes tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unprune,prune.ln_structured,10493
9471,At what rate do prune.RandomUnstructured Prune units in a tensor?,random,10493
9472,prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying what?,specifiedpruning_method,10493
9473,What check whethermoduleis pruned by looking forforward?,prune.is_pruned,10493
9474,What does customFromMask prune.identity apply to the tensor corresponding to the parameter callednameinmodule?,pruning reparametrization,10484
9475,What is the term for current unpruned units in a tensor?,prune.L1Unstructured Prune,10477
9476,What does prune.CustomFromMask prune.identity apply to the tensor corresponding to the parameter callednameinmodule,pruning reparametrization,10469
9477,What is the name of the Prunes tensor corresponding to parameter callednameinmodule?,prune.ln_structured,10469
9478,What globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method?,global_unstructured,1247
9479,What is applied to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?,pruning reparametrization,1247
9480,Removes what reparameterization from a module?,spectral normalization,1247
9481,What is the name of the tensor that prunes tensors corresponding to all parameters inparameters?,prune.custom_from_mask,1246
9482,What tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned),prune.l1_unstructured Prunes tensor,1246
9483,Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned,L1-norm,4935
9484,What does prune.is_pruned do?,prune.is_pruned,4935
9485,What is the name of the global prunes tensors corresponding to all parameters inparameters?,global_unstructured,10513
9486,What does prune.is_pruned inherit from?,theBasePruningMethod,10507
9487,What is applied to a parameter in a given module?,weight normalization,4929
9488,What type of normalization is applied to a parameter in a given module?,spectral,10506
9489,What is the name of the example where you can put the following logic in the entrypoint definition?,abovetorchvision.models.resnet.resnet18handlespretrained,2182
9490,What is the default setting for a list of available entrypoint names entrypoints?,Default is False,4889
9491,"What can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from",Pretrained weights,4889
9492,Where is the default branch ismasterif not specified?,github,4889
9493,What function does Pytorch use to force a fresh download?,force_reload,4889
9494,What is the default value for a string of entrypoint name defined in repo’s hubconf.py force_reload(bool,is False,4889
9495,Can’t be a what?,random commit,4889
9496,Where can you find a string with format repo_owner/repo_name[:tag_name]>?,github,4889
9497,What does if sourceis'github' mean?,if sourceis'github',4889
9498,What does in github hubconf list?,all entrypoints available,4120
9499,What type of tag/branch does github(string) have?,optional,4120
9500,What is the default is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.,force_reload,4120
9501,What is the default value for a string of entrypoint name defined in repo's hubconf.py force_reload(bool,Default is False,4120
9502,What is the default is False?,force_reload,9385
9503,"What is force_reload(bool,optional) set to?",Default is False,9385
9504,What ismasterif not specified?,branch,9385
9505,"What does force_reload(bool,optional) default to?",Default is False,9385
9506,What is the default setting for the default value?,False,2292
9507,What is the default setting for a new download?,False,9468
9508,Load what from a github repo or a local directory?,a model,9468
9509,What is the name of a callable (entrypoint) defined in the repo/dir’shubconf.py?,model(string),2293
9510,Example Show the docstring of what?,entrypointmodel,2293
9511,What is the name of the function to force a fresh download of the github repo unconditionally?,force_reload,2293
9512,"What does source(string,optional) –'github'|'local'?","source(string,optional) –'github'|'local'",2293
9513,What is the output of the modelcallable when called with the given*argsand**kwargs?,output,9928
9514,"What is a string of entrypoint name defined in repo’s hubconf.py force_reload(bool,optional)",model(string),9928
9515,"if sourceis'local',repo_or_diris expected to be a what?",path to a local directory,9928
9516,What is the default value for model(string)?,is False,9928
9517,Example Load a model from what?,a github repo or a local directory,9928
9518,repo_or_dir(string) – what does repo_owner/repo_name[:tag_name] mean?,repo name,9928
9519,What type of object can be found at the given URL?,Example Download object,9928
9520,What serialized object does Example Load at the given URL?,Torch,2290
9521,How do you find the indices from the innermost dimension ofsorted_sequence?,Find the indices,2441
9522,"Ifrightis False (default), then the left boundary ofsorted_sequenceis closed.",Ifrightis False,2441
9523,What does sorted_sequence right return index satisfies?,1-D False sorted_sequence,2441
9524,Return index satisfies what type of False sorted_sequence[i-1]values[m][,1-D,10604
9525,What is False sorted_sequence[i-1]values[m][n]?,1-D,92
9526,What is another name for sorted_sequence(Tensor)?,1-D tensor,4296
9527,What backend controls if TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere,SeeTensorFloat-32(TF32),7432
9528,What does the backend that controls cache capacity of cuFFT plan cache do?,Clears the cuFFT plan cache,7432
9529,What backend controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Amper,SeeTensorFloat-32(TF32),7432
9530,What is the name of the tensor cores that can be used in matrix multiplications on Ampere devices?,SeeTensorFloat-32(TF32),5693
9531,What does it do when the cuFFT plan cache is cleared?,Clears the cuFFT plan cache,5693
9532,What is returned by torch.backends.cudnn torch.backends.mkl torch.backends.mk,whether PyTorch is built with CUDA support,10998
9533,What is returned when PyTorch is built with CUDA support?,version of cuDNN,11001
9534,What does torch.backends.mkl return?,whether PyTorch is built with CUDA support,10999
9535,Computes the one dimensional Fourier transform of real-valuedinput. Computes the one dimensional Fourier transform of realinput,N dimensional inverse discrete Fourier transform,1794
9536,Computes the one dimensional Fourier transform of real-valuedinput. Computes the inverse ofrfft2().,2-dimensional discrete Fourier transform of realinput,1794
9537,What does Torch.float32 use?,global dtype default,4454
9538,What is a tutorial that guides you through packaging and unpackaging a simple model?,API Reference,898
9539,What is the name of the steps that are used to create and use Torch packages?,Steps,898
9540,What is everything else?,User files,8097
9541,"Python module is identified as a dependency,torch.packagewalks the module’s what representation?",python AST,8097
9542,"When a Python module is identified as a dependency,torch.packageregisters the imported modules as dependencies that are dependencies that",import statements,8097
9543,Who owns the data/directory of the ResNet model?,torch.package,1295
9544,What representation does torch.package walk when a Python module is identified as a dependency?,python AST,1295
9545,"When a Python module is identified as a dependency,torch.packagewalks the module’s python AST",import statements,1295
9546,What does the.data/directory contain?,version: a version number for the serialized format,2056
9547,"For a deeper dive in how Python packaging works, please consult what?",this essay,2056
9548,"For each module that the dependency resolver finds, you must specify what?",an action to,2056
9549,What does mock do?,stub out this module,6961
9550,What will happen if you depend on this module during package export?,raise an error,6961
9551,Remove or change the dependencies in your code.,Refactoring,6961
9552,What are two examples of modules that will raise an error if you attempt to intern them?,C extension modules and bytecode modules,4528
9553,"If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of what?",external dependencies,4528
9554,"If a module is mock-ed, it will what?",not be packaged,4528
9555,What do these kinds of modules need to do?,be mock-ed or extern-ed,4528
9556,Where can you find the list of external dependencies for this package?,on package_exporter.extern_modules,4528
9557,What happens if a package importer can't find a module?,an error will be raised,4528
9558,"Code that you ""know"" will not be needed in the loaded package, but you still want to available for use in non-packaged contents. For",initialization/configuration code,4528
9559,What is included in the package if a module is intern-ed?,model code,3384
9560,What will happen if a module isn't found?,an error will be raised,3384
9561,What are examples of non-packaged code that mockshould be used for?,"initialization/configuration code, or code only used for debugging/training",3387
9562,Do not leave unused imports in our code?,Do not leave unused imports in our code,3387
9563,What do you want to do with your imports?,Qualify,3387
9564,Who names match the given glob patterns from the list of modules the package can import?,Blocklist modules,1451
9565,What happens if a dependency on any matching packages is found?,aPackagingErroris raised,1451
9566,What is an example of a string for the names of the modules to be externed?,my_package.my_subpackage,1451
9567,How is a glob-style pattern described?,inmock(),1451
9568,What is an example of a string that specifies the names of the modules to be externald?,my_package.my_subpackage,1451
9569,What describes a glob-style pattern?,inmock(),1451
9570,Get an id. This id is what?,guaranteed to only be handed out once for this package,1451
9571,Specify what?,modules that should be packaged,1451
9572,What is an example of a string for the names of modules that should be included in the package?,my_package.my_subpackage,1451
9573,What must be specified in order to get an id?,modules that should be packaged,9285
9574,"What can include(Union[List[str],str]] – A string e.g. “my_package",glob-style pattern,9285
9575,"If an internmodule glob pattern is added with allow_empty=False, andclose()is called (either explicitly or",an exception is thrown,9285
9576,Replace some required modules with a what?,mock implementation,9285
9577,How do we copy?,file-by-file,9285
9578,Use this function to do what?,mock this functionality out without having to modify the original code,9285
9579,"If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit",If allow_empty=True,8855
9580,Registers what?,an extern hook on the exporter,8855
9581,"A unique name for the resource, used to identify it to load. binary(str) – The data to save. Save the code for",resource,8855
9582,What is binary(str)?,binary(str) – The data to save,8855
9583,What does binary(str) – The data to save?,Save the code formoduleinto the package,8855
9584,What does module_name(str) stand for?,module_name(str),8855
9585,What is created of sizestepswhose values are evenly spaced from starttoend inclusive?,one-dimensional tensor,5091
9586,"What type of tensor of sizestepswhose values are evenly spaced from starttoend, inclusive?",one-dimensional,1852
9587,What type of tensor of sizestepswhose values are evenly spaced from starttoend inclusive?,one-dimensional tensor,1948
9588,What is the name of the index that returns a new tensor which indexes the input tensor along dimensiondimusing,a Long Tensor,2004
9589,What is the boolean maskmask that returns a new 1-D tensor which indexes the input tensor,a Bool Tensor,6060
9590,What is the name of the index which indexes the input tensor along dimensiondimusing the entries inindex?,a Long Tensor,6060
9591,What is returned with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive,a tensor,2210
9592,Sets the seed for generating random numbers. Returns the random number generator state as a torch.ByteTensor.,random number generator state,5864
9593,What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive),a tensor,5449
9594,What does Alias fortorch.special.erf() do?,Alias fortorch.special.erfc(),5377
9595,Use_cuda(bool) – Deprecated since version 1.8.1: what?,useactivitiesinstead,11414
9596,Usetensorboard_trace_handler()to generate result files for what?,TensorBoard,11414
9597,What does with_flops(bool) do?,use formula to estimate the FLOPS of specific operators,11414
9598,Schedule(callable) – callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifie,profiler action,10649
9599,What adds a user defined metadata with a string key and a string value into the trace file?,"Using the profiler’sschedule,on_trace_readyandstepfunctions",10649
9600,Track tensor memory allocation/deallocation. with_stack(bool) – record source information (file and line number),profile_memory,10565
9601,What does record_shapes(bool) do?,save information about operator’s input shapes,10565
9602,What record source information (file and line number) for the ops. with_flops(bool) – use formula to estimate the FL,with_stack,10565
9603,Use formula to estimate FLOPS of specific operators (matrix multiplication and 2D convolution). use_cuda(bool),with_flops,10449
9604,profile_memory(bool) – what is profile_memory?,track tensor memory allocation/deallocation,10449
9605,The default schedule simply records all the events continuously for what?,the duration of the context manager,2118
9606,Deprecated since version 1.8.1: what?,useactivitiesinstead,2118
9607,To use shape/stack functionality make sure to set what?,record_shapes/with_stack,2118
9608,What does Useschedule() generate?,callable schedule,4440
9609,What does Usetensorboard_trace_handler generate?,result files,4440
9610,What Signals the profiler?,Signals the profiler,4440
9611,What should you set to use shape/stack functionality?,record_shapes/with_stack,4442
9612,What is the result file generated for?,TensorBoard,4442
9613,The profiler will skip what?,firstskip_firststeps,4442
9614,"After profiling, what can be found in the specified directory?",result files,984
9615,What is this useful when you only want to do?,vary a single option,2319
9616,What does iterable oftorch.Tensors ordicts do?,Specifies what Tensors should be optimized,3734
9617,How many ways can a step()method be used?,two ways,7726
9618,Examples of objects that don’t satisfy those properties are what?,sets and iterators over values of dictionaries,7726
9619,"Model.classifier’s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",model.classifier,7726
9620,What algorithm implements the resilient back?,RMSprop algorithm,7726
9621,What is implemented in the resilient backpropagation algorithm?,resilient backpropagation,7726
9622,Some optimization algorithms such as Conjugate Gradient and LBFGS need to what?,reevaluate the function multiple times,5973
9623,What is an iterable oftorch.Tensors ordicts?,params,5973
9624,Optimizer.add_param_group <sep>,Add a param group to theOptimizersparam_groups,5973
9625,Sets the gradients of all optimizedtorch.Tensors to zero. Implements Adadelta algorithm. Implements Ad,Optimizer.zero_grad,5973
9626,What does defaults– (dict) contain?,default values of optimization options,5973
9627,Optimizer.load_state_dict Loads what?,optimizer state,5973
9628,What algorithm implements stochastic gradient descent?,resilient backpropagation algorithm,5973
9629,What class is used by all optimizers?,Base class,7615
9630,When can the function be called?,once the gradients are computed using e.g.backward(),7615
9631,What is supported by most optimizers?,a simplified version,7615
9632,"The closure should clear the gradients, what should the closure do?",compute the loss,7615
9633,ReduceLROnPlateauallows what?,dynamic learning rate,7615
9634,defaults– (dict): a dict containing what?,default values of optimization options,2329
9635,What algorithm does Adadelta algorithm implement?,Adagrad algorithm,2329
9636,Which algorithm implements a lazy version of Adam algorithm suitable for sparse tensors?,AdamW algorithm,8173
9637,Params is an iterable oftorch.Tensors ordicts. Specifies what Tensors should,iterable,8173
9638,What are examples of objects that don’t satisfy those properties?,sets and iterators over values of dictionaries,4736
9639,What does defaults- (dict) contain?,default values of optimization options,4736
9640,"In many places in the documentation, we will use what?",template,4736
9641,What is params?,iterable,4736
9642,Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer,Optimizer.state_dict,943
9643,What does the param group do?,Add a param group to theOptimizersparam_groups,943
9644,Are you unable to reproduce results after upgrading to PyTorch 1.1.0?,unable to reproduce results,943
9645,What Returns the state of the optimizer as adict?,Optimizer.state_dict,4682
9646,What is implemented?,Averaged Stochastic Gradient Descent,4682
9647,What Sets the learning rate of each parameter group to the initial tensors?,lr_scheduler.LambdaLR,4682
9648,In what book has SWA been proposed?,Averaging Weights Leads to Wider Optima and Better Generalization,11189
9649,What class serves to compute the weights of the SWA model?,AveragedModelclass,11189
9650,What is the learning rate that the following code creates a scheduler that linearly anneals the learning rate from its initial value to?,0.05 in 5 epochs,11189
9651,What can you update if your dataloader has a different structure?,If your dataloader has a different structure,11189
9652,How can you create an averaged model?,running,1384
9653,A parameter that is what?,not initialized,4068
9654,Holds submodules in a list. Holds parameters in a dictionary. Holds submodules in a dictionary. Hold,Holds submodules in a list,4068
9655,What module has lazy initialization?,nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d,4068
9656,Registers a forward pre-hook common to all modules. Registers a global forward hook for all the modules Registers a backward hook,Global Hooks For Module,781
9657,Holds parameters in a dictionary. Holds parameters in a dictionary <sep>,Holds parameters in a dictionary,781
9658,What module has lazy initialization of thein_channelsargument of theConv2d that is inferred from theinput.,nn.LazyConv2d a torch.nn.Conv2d,781
9659,Holds submodules in a dictionary. Holds parameters in a list. Holds parameters in a dictionary. Holds submodul,Holds submodules in a list,781
9660,What module has lazy initialization of thein_channelsargument of theConv3d?,nn.LazyConv3d a torch.nn.Conv3d,781
9661,Holds submodules in a dictionary. Holds parameters in a list. Holds parameters in a dictionary.,Holds submodules in a dictionary,781
9662,Global Hooks For Module Registers a forward pre-hook common to all modules. Registers a what?,global forward hook,2950
9663,nn.Conv2d Applies what convolution over an input signal composed of several input planes?,2D,2950
9664,Global Hooks For Module Registers what?,a forward pre-hook common to all modules,2950
9665,Holds submodules in a list. Holds parameters in a list. Holds submodules in a dictionary.,Holds submodules in a list,2950
9666,Holds parameters in a list. Holds submodules in a dictionary. Holds parameters in a dictionary.,Holds parameters in a list,2950
9667,Holds what in a dictionary?,parameters,2950
9668,What does nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule,lazy initialization of thein_channelsargument of theConvTranspose1d,2950
9669,What does nn.LazyConv2d a torch.nn.Conv2dmodule have?,lazy initialization of thein_channelsargument of theConv2d,11107
9670,"What are Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other)?",torch.nn Containers Convolution Layers Pooling layers Padding Layers,11107
9671,Holds submodules in a dictionary. Holds parameters in a list. Holds submodules in a dictionary. What,Holds submodules in a list,11107
9672,Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary <sep>,Holds parameters in a dictionary,11107
9673,What convolution does nn.Conv1d apply over an input signal composed of several input planes?,1D,11107
9674,What type of transposed convolution operator does nn.ConvTranspose2d use?,2D,11107
9675,Holds submodules in a dictionary <sep>,Holds submodules in a dictionary,831
9676,What does nn.ConvTranspose1d Applies over an input image composed of several input planes?,1D transposed convolution operator,831
9677,A buffer that is not initialized. Base class for all neural network modules. A sequential container. Holds submodules in a list.,parameter that is not initialized,831
9678,A sequential container. Holds submodules in a list. Holds submodules in a dictionary. Holds parameters in,Base class for all neural network modules,831
9679,What container holds submodules in a list?,sequential container,831
9680,nn.ConvTranspose2d Applies a what type of transposed convolution operator over an input image composed of several input,2D,831
9681,What type of transposed convolution operator does nn.ConvTranspose3d use?,3D,4339
9682,What module has lazy initialization of thein_channelsargument of theConv1d?,nn.LazyConv1d a torch.nn.Conv1dmodule,4339
9683,Holds parameters in a list <sep>,Holds parameters in a list,806
9684,What is nn.LazyConv1d a torch.nn.Conv1dmodule?,nn.LazyConv1d a torch.nn.Conv1dmodule,806
9685,What module has lazy initialization of thein_channelsargument of theConv2d?,nn.LazyConv2d a torch.nn.Conv2dmodule,806
9686,What is not initialized. Base class for all neural network modules. A sequential container. Holds submodules in a list. Holds,A buffer,806
9687,A kind of Tensor that is to be considered a what?,module parameter,806
9688,What is a parameter that is?,not initialized,806
9689,For all neural network modules. A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds,Base class,806
9690,What module has lazy initialization of thein_channels?,nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d,806
9691,Normalization Layers Recurrent Layers Transformer Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shu,Non-linear Activations,4334
9692,Holds parameters in a dictionary. Holds submodules in a dictionary. Holds parameters in a dictionary <sep>,Holds parameters in a dictionary,7998
9693,Global Hooks For Module Registers What is a forward pre-hook common to all modules?,a forward pre-hook common to all modules,8134
9694,What module with lazy initialization of thein_channelsargument of theConv3d is inferred from theinput?,nn.LazyConv3d a torch.nn.Conv3d,2161
9695,"What are multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization?",DataParallel Layers,2071
9696,"DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization What is the",nn.LazyConvTranspose,2071
9697,What Allows the model to jointly attend to information from different representation subspaces?,MultiheadAttention,10049
9698,"nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function:",nn,10049
9699,What input is a mini-batch of [N-2]D inputs with additional channel dimension?,N-Dimensional,1179
9700,What is the lazy initialization of thenum_featuresargument of theBatchNorm1d that is inferred from theinput,nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule,1179
9701,InstanceNorm1d Applies Instance Normalization over a what input?,3D,1179
9702,What is an example of a mini-batch of 2D inputs with additional channel dimension?,4D input,1179
9703,Who Applies Instance Normalization over a 4D input?,nn.InstanceNorm2d,1179
9704,Which two classes does nn.CrossEntropyLoss combine?,LogSoftmaxandNLLLossin,10039
9705,PoissonNLLLoss Negative log likelihood loss with what?,Poisson distribution of target,10039
9706,What is a criterion that uses a squared term if the absolute element-wise error falls below delta and an L1 term,nn.SoftMargin,10039
9707,What loss is nn.CTCLoss?,Connectionist Temporal Classification loss,10013
9708,What is the term for Gaussian negative log likelihood loss?,Gaussian,10013
9709,MultiLabelMarginLoss creates a criterion that optimizes what type of hinge loss?,multi-class multi-classification hinge loss,10013
9710,What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand input,nn.SoftMarginLoss,10013
9711,What creates a criterion?,nn.CosineEmbeddingLoss,1970
9712,What does the criterion measure between the target and the output?,Binary Cross Entropy,1970
9713,When does nn.SmoothL1Loss create a criterion that uses a squared term?,if the absolute element-wise error falls below beta and an L1 term otherwise,10003
9714,What two single classes does the BCEWithLogitsLoss combine?,aSigmoidlayer and theBCELossin,10003
9715,What is the name of the loss caused by the Connectionist Temporal Classification loss?,Connectionist Temporal Classification loss,6921
9716,NLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gau,Poisson,10163
9717,What creates a criterion that optimizes a multi-label one-versus-all loss?,nn.MultiLabelSoftMarginLoss,10066
9718,PruningContainer Container holding a sequence of pruning methods for what?,iterative pruning,1907
9719,Which prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually,prune.CustomFromMask,1907
9720,Convert parameters to how many vectors?,one vector,1907
9721,What is the name of the tree that globally prunes tensors corresponding to all parameters inparameters?,prune.global_unstructured,1907
9722,How many channels are currently unpruned?,specifiedamountof,4930
9723,What is applied to prune.global_unstructured Globally prunes tensors corresponding to all parameters inparameters?,specifiedpruning_method,4930
9724,What is applied to prune.custom_from_mask?,pre-computed mask inmask,4930
9725,Applies what normalization to a parameter in the given module?,spectral,4930
9726,What parametrize.is_parametrized ReturnsTrueif module has an active parametrization?,parametrize.is_parametrized ReturnsTrueif module has an active parametrization,4930
9727,Which tensor corresponding to parameter callednameinmodule is prune.random_structured Prunes tensor?,prune.ln_structured Prunes tensor,2617
9728,Abstract base class for what?,creation of new pruning techniques,917
9729,PruningContainer holds a sequence of pruning methods for what?,iterative pruning,917
9730,What is the name of the pruning method that globally prunes tensors corresponding to all parameters inparameters?,prune.global_unstructured,917
9731,What Prunes tensor corresponding to parameter callednamein?,prune.custom_from_mask,917
9732,What does prune.customFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodul,pruning reparametrization,10481
9733,What is prune.l1_unstructured Prunes tensor removing the specifiedamountof (currently unpruned,L1-norm,10481
9734,What do prune.is_pruned modules inherit from?,theBasePruningMethod,10481
9735,What type of _unstructured Prunes tensor is prune?,random,10481
9736,What is pruned by in modules that inherit from theBasePruningMethod?,looking forforward_pre_hooks,10481
9737,What is implemented using the new parametrization functionality intorch.nn.utils?,Parametrizations,10481
9738,What does prune.customFromMask apply to a parameter in the given module?,spectral normalization,10481
9739,What does this make it possible to supply to the C++ and CUDA?,different flags,8586
9740,What is the name of the subclass that takes care of passing the minimum required compiler flags?,Note,4501
9741,What is True (default)?,use_ninja,6176
9742,What is the name of the program that attempts to build using the Ninja backend?,use_ninja,6904
9743,Why does the Ninja backend use #CPUS + 2 workers to build the extension?,may use up too many resources on some systems,6904
9744,What does the setuptools.build_ext subclass support?,CUDA files,6904
9745,What type of compilation does setuptools.build_ext take care of?,CUDA,4502
9746,What may the Ninja backend use to build the extension?,too many resources,4502
9747,What does setuptools.build_ext subclass take care of passing the minimum required compiler flags?,custom setuptools build extension,4502
9748,What is slower the building process will be?,the more archs get included,4502
9749,What does this make it possible to supply to the C++ and CUDA compiler during mixed compilation?,different flags,4502
9750,Fallbacks to what backend if Ninja is not available?,standard distutils,4502
9751,"By default, the Ninja backend uses what to build the extension?",#CPUS + 2 workers,4502
9752,How can one control the number of workers?,setting the MAX_JOBS environment variable to a non-negative number,4502
9753,What is the library loaded into the current Python process as?,module,4502
9754,"When enabled, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw what when called?",RuntimeError,5870
9755,What are deterministic algorithms?,"algorithms which, given the same input, and when run on the same software and hardware, always produce the same output",5870
9756,"When mode=True, the following normally-nondeterministic operations will act what?",deterministically,5870
9757,What is torch.nn.ConvTranspose1d when called on?,CUDA tensor torch,11128
9758,When is index_add() called on?,CUDA tensor torch,10968
9759,When input dimension is one and called on a which CUDA tensor that requires grad torch?,CUDA tensor that requires grad torch,10969
9760,What requires grad torch.gather() when input dimension is one and called on?,CUDA tensor,11066
9761,What does a CUDA tensor require?,grad torch,11066
9762,When mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA,RuntimeError,10965
9763,"When mode=True, the following normally-nondeterministic operations will throw a what?",RuntimeError when mode=True,7086
9764,What will normally-nondeterministic operations throw when mode=True: torch?,RuntimeError,7086
9765,What is the torch.nn.AvgPool3d when attempting to differentiate?,CUDA tensor torch,11120
9766,What is the torch.nn.AdaptiveAvgPool2d when attempting to differentiate?,CUDA tensor torch,11111
9767,What is the torch.nn.AdaptiveAvgPool3d when attempting to differentiate?,CUDA tensor torch,11112
9768,What is a torch.nn.MaxPool3d when attempting to differentiate?,CUDA tensor torch,11143
9769,What is the torch.nn.AdaptiveMaxPool2d when attempting to differentiate?,CUDA tensor torch,11116
9770,What type of torch is used when trying to differentiate a CUDA tensor torch?,torch.nn.AdaptiveMaxPool2d,11116
9771,What is a torch.nn.FractionalMaxPool2d when attempting to differentiate?,CUDA tensor torch,11137
9772,What is a torch.nn.FractionalMaxPool3d when attempting to differentiate?,CUDA tensor torch,11140
9773,"For more details, see what for more details: https://docs.nvidia.com/cuda/cublas/index.",CUDA documentation,11158
9774,What is a torch.nn.ReplicationPad1d when attempting to differentiate?,CUDA tensor torch,11158
9775,What is a reflectionPad1d when attempting to differentiate?,CUDA tensor torch,8963
9776,"For more details, see what for more details?",CUDA documentation,11153
9777,What is the torch.nn.ReflectionPad1d when attempting to differentiate?,CUDA tensor torch,11153
9778,What type of torch is used?,bicubic trilinear torch,8958
9779,What is the CUDA documentation for?,See the CUDA documentation for more details,8958
9780,What type of torch does a CUDA tensor torch have?,trilinear torch,11263
9781,What is the torch.nn.ReflectionPad2d when attempting to differentiate?,CUDA tensor torch,11155
9782,What is torch.nn.ReplicationPad2d when attempting to differentiate?,CUDA tensor torch,11160
9783,What documentation provides more details about nondeterministic CUDA operations?,CUDA,11160
9784,What is the default value for the dimension along which to split a tensor?,0,6028
9785,What program is the numpy.array_split() based on?,NumPy,6028
9786,What is the tensor to split indices_or_sections?,n,6028
9787,"If i is divisible by n along dimension dim, each section will be of equal size, input.size(dim) /",n,6028
9788,What is the dimension along which to split the tensor?,dim,6028
9789,"What is a list or tuple of ints, or a one-dimensional long tensor?",indices_or_sections,6028
9790,"What would result in the tensors input[:2], input[2:3], and input[3:]?",indices_or_sections,6028
9791,"If indices_or_sections is what, it must be a zero-dimensional or one-dimensional long tensor on the",a tensor,6028
9792,What does a quantized model use instead of floating point values?,integers,8177
9793,How much faster is hardware support for INT8 compared to FP32?,2 to 4 times faster,8177
9794,What does PyTorch support to quantize a deep learning model?,multiple approaches,8177
9795,In most cases the model is trained in what?,FP32,4994
9796,What supports multiple approaches to quantizing a deep learning model?,PyTorch,4994
9797,What type of training does PyTorch support?,quantization aware training,4994
9798,What does PyTorch convert the trained model into?,lower precision,4994
9799,What is the name of the qconfig set by PyTorch?,torch.quantization.get_default_qconfig,4994
9800,What CPU is typically found in mobile/embedded devices?,ARM,4994
9801,What can quantized tensors be used to do?,directly construct models,1346
9802,What backend is set to qnnpack?,qnnpack,1346
9803,What provides a way to represent quantized tensors and perform operations with them?,PyTorch,1346
9804,What can quantized tensors be used for?,directly construct models that perform all or part of the computation in lower precision,1346
9805,What is provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss?,Higher-level APIs,1346
9806,PyTorch doesn't provide quantized operator implementations on what?,CUDA,1346
9807,What is the name of the Quantization-aware training that supports both CPU and CUDA?,FakeQuantize,1346
9808,What does PyTorch do to test the quantized functionality?,Move the model to CPU in order to test the quantized functionality,1346
9809,What is necessary to ensure when preparing a quantized model?,qconfig and the engine used for quantized computations match the backend on which the model will be executed,1346
9810,What backend can be used on the ARM QNNPACK library?,qnnpack,1346
9811,What is it recommended to set if you are interested in quantizing a model to run on ARM?,qconfig,1346
9812,What must match the backend on which a quantized model will be executed?,qconfig and the engine,1346
9813,What parameter should be set to match the backend?,torch.backends.quantized.engine,1346
9814,What should the torch.backends.quantized.engine parameter be set to?,match the backend,1346
9815,What CPUs support AVX2 support?,x86 CPUs,1347
9816,What does PyTorch not provide quantized operator implementations on?,CUDA,1347
9817,Where should a model be moved in order to test the quantized functionality?,CPU,1347
9818,What must match the backend on which the model will be executed?,qconfig and the engine,1347
9819,"For inference, the backend is set to qnnpack as follows torch.backends.quantized.engine = 'q",qnnpack,1347
9820,What can PyTorch be used to do?,directly construct models,1347
9821,Quantization currently supports two backends: what?,fbgemm,1347
9822,What should you set if you are interested in quantizing a model to run on ARM?,qconfig,1353
9823,What is qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization?,qnnpack,10531
9824,"FX Graph Mode Quantization is a new automated quantization framework in PyTorch, what is it called?",eager mode quantization,10531
9825,What is FX Graph Mode Quantization Release Status beta prototype?,Operator Fusion Manual Automatic,10531
9826,What are the two modes of quantization in PyTorch?,Eager Mode Quantization and FX Graph Mode Quantization,4977
9827,What domain library will FX Graph Mode Quantization be integrated into?,torchvision,4977
9828,What is a new automated quantization framework in PyTorch?,FX Graph Mode Quantization,11005
9829,What does Eager Mode Quantization only support?,modules and not functionals,11005
9830,People might need to refactor the model to make it compatible with what?,FX Graph Mode Quantization,2245
9831,What is not expected to work on arbitrary models?,FX Graph Mode Quantization,2378
9832,"To make FX Graph Mode Quantization work, users might need to be familiar with what?",torch.fx,2378
9833,What is the name of the new automated quantization framework in PyTorch?,FX Graph Mode Quantization,2378
9834,What does FX Graph Mode Quantization improve upon Eager Mode Quantization?,adding support for functionals and automating the quantization process,2378
9835,What is the alternative to FX Graph Mode Quantization?,eager mode quantization,2378
9836,How many types of quantization are supported in Eager Mode Quantization?,three types,2378
9837,What is qconfig for quantization aware training?,torch.quantization.get_default_qat_qconfig('qnnpack'),9377
9838,What is the torch.backends.quantized.engine parameter set to match the backend?,quantization aware training,9381
9839,What is another type of quantization supported in Eager Mode Quantization?,static quantization,4309
9840,What does the model execution time dominated by?,loading weights from memory,4309
9841,What type of models are LSTM and dynamic quantization used for?,Transformer type models with small batch size,4198
9842,What is Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quant,Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules,4198
9843,What type of models are LSTM and FX Graph Mode Quantization used for?,Transformer type models with small batch size,7091
9844,What is the name of the type of quantization that quantizes the weights and activations of a model?,dynamic quantization,4878
9845,What does static quantization fuses activations into where possible?,preceding layers,4878
9846,Post Training Quantization is typically used when what are important?,memory bandwidth and compute savings,4878
9847,What is required post training?,calibration,4878
9848,What is another name for static quantization?,Post Training Quantization,6073
9849,What is the name of the type of quantization used in Post Training Quantization?,static quantization,7407
9850,What type of quantization is supported in Eager Mode Quantization?,dynamic quantization,7407
9851,What is used when the model execution time is dominated by?,loading weights from memory,7407
9852,Static quantization fuses activations where possible?,preceding layers,4879
9853,What is the name of the FX Graph Mode Post Training Static Quantization?,User Guide on Using FX Graph Mode Quantization,5063
9854,What is the benefit of per channel quantization?,lesser error,4990
9855,Quantized Tensors allow for what?,serialization of data in a quantized format,4990
9856,What means that all values within a tensor are scaled the same way?,Per tensor,4990
9857,"In order to do quantization in PyTorch, we need to be able to represent quantized data in what?",Tensors,4990
9858,What are quantized data represented as in a Quantized Tensor?,int8/uint8/int32,7170
9859,In what program do we need to be able to represent quantized data in Tensors?,PyTorch,7170
9860,What quantization parameters are stored in a Quantized Tensor?,scale and zero_point,7170
9861,Many operations for quantized tensors are available under the same API as full float version in torch or what?,torch.nn,5071
9862,Where do we support fused versions corresponding to common fusion patterns that impact quantization?,torch.nn.intrinsic.quantized,5071
9863,What support a limited subset of data manipulation methods of the regular full-precision tensor?,Quantized Tensors,5071
9864,What is represented with no quantization error?,zero,5071
9865,How can additional data types and quantization schemes be implemented?,custom operator mechanism,5071
9866,What do operator implementations currently only support for weights of the conv and linear operators?,per channel quantization,5071
9867,Where are quantized versions of NN modules that perform re-quantization available?,torch.nn.quantized,5071
9868,What are the output quantization parameters?,scale and zero_point,5071
9869,Where do we support modules prepared for quantization aware training?,torch.nn.qat and torch.nn.intrinsic.qat,5071
9870,How does quantization currently work?,module by module basis,3944
9871,What API takes in lists of modules to be fused?,torch.quantization.fuse_modules(),3944
9872,Why is it necessary to make some modifications to the model definition prior to quantization?,currently quantization works on a module by module basis,3944
9873,What is required to the model definition prior to quantization?,make some modifications,3944
9874,What means that the model.conv layer will not be quantized?,setting model.conv1.qconfig,3944
9875,What is used to specify where activations are quantized and de-quantized?,QuantStub and DeQuantStub modules,3944
9876,What is used to wrap tensor operations that require special handling for quantization into modules?,torch.nn.quantized.FloatFunctional,3944
9877,What operations require special handling to determine output quantization parameters?,add and cat,3944
9878,What is fusion?,combine operations/modules into a single module to obtain higher accuracy and performance,3944
9879,What is used to combine operations/modules into a single module?,torch.quantization.fuse_modules() API,3944
9880,What does the torch.quantization.fuse_modules() API do?,Fuse modules,3944
9881,What should have the same log_dir. max_queue (int) – Size of the queue for pending events and summ,crashed and resumed experiments,10519
9882,"How often, in seconds, to flush the pending events and summaries to disk?",flush_secs,10519
9883,"flush_secs (int) – How often, in seconds, to flush the pending events and summaries to disk?",every two minutes,10519
9884,More details on what can be found in tensorboard.summary.writer.event_file_writer.EventFile,filename construction,10519
9885,What is an example of a suffix added to all event filenames in the log_dir directory?,Add scalar data to summary,10519
9886,What is the parent name for the tags tag_scalar_dict (dict) – Key-value pair storing the tag and corresponding,main_tag,10519
9887,What does tag (string) refer to?,Data identifier values,10519
9888,What is the name of the global step value to record bins?,global_step,9475
9889,What does Add image data to summary require?,pillow package,9475
9890,What is also suitable as long as corresponding dataformats argument is passed?,Tensor,9475
9891,What dataformats argument is passed?,"CHW, HWC, HW",9475
9892,What provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions?,torch.autograd,10990
9893,What type of Tensor types do we only support autograd for?,floating point Tensor types,10990
9894,What is another name for computes and returns the sum of gradients of outputs with respect to the inputs?,Computes and returns the sum of gradients of outputs with respect to the inputs,10990
9895,What is required to declare Tensor s for which gradients should be computed?,requires_grad=True keyword,10989
9896,What are very unlikely to change?,function signatures,7452
9897,Context-manager sets gradient calculation to what?,on or off,7452
9898,In what state is the API in?,beta,7452
9899,What does the autograd section contain?,higher level API,7452
9900,Context-manager that enables or disables what?,inference mode,7452
9901,"If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use",lambda,7455
9902,"What does f(input, constant, flag=flag) have as f(input, constant, flag=flag)?",boolean flag,7455
9903,Functional.jacobian Function that computes what of a given function. functional.hessian Function that computes the Hessian of,Jacobian,7455
9904,What are the differences between Locally disabling gradient computation?,no-grad and inference mode,7455
9905,This API works with what?,user-provided functions,7455
9906,Sets gradient calculation to what?,on or off,7455
9907,What disabled gradient calculation?,Context-manager,2631
9908,What does a context-manager enable or disable?,inference mode,7792
9909,"For more information on the differences between no-grad and inference mode, see what?",Locally disabling gradient computation,7792
9910,What does the context-manager set gradient calculation to?,on or off,7792
9911,Context-manager that enables or disables what mode when a non-sparse param receives a non-spar,inference mode,7792
9912,What does a context-manager do when a non-sparse param receives a non-sparse gradient?,enables or disables inference mode,1872
9913,"If your function takes other arguments that are not Tensors or Tensors that don't have requires_grad set, you can use",lambda,8191
9914,is thread local and is automatically propagated into the async tasks enabled,profiler,1373
9915,"if shapes recording is set, information about input dimensions will be collected",record_shapes,1373
9916,"record_shapes (bool, optional) – If shapes recording is set, information about input dimensions will be collected and further group by",prof.key_averages,1373
9917,skew your profiling data,shape recording,1373
9918,it is recommended to use what to validate the timing?,separate runs with and without shape recording,1373
9919,most likely the skew will be what for bottom most events?,negligible,1373
9920,what might be artificially increased because of the shape collection?,the total self cpu time,1373
9921,FLOPS,floating pointer operations per second,1373
9922,FLOPS (floating pointer operations per second) value using the operator’s input shape and total time,hardware performance,1373
9923,the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator’s input shape and total time.,matrix multiplication and 2D convolution operators,1373
9924,track tensor memory allocation/deallocation,profile_memory,1373
9925,record source information (file and line number) for the ops,with_stack,1373
9926,Context manager manages what?,autograd profiler state,1864
9927,Under the hood it just records events of functions being executed in what language?,C++,1864
9928,What can you do to report runtime of PyTorch functions?,wrap any code into it,1864
9929,"is thread local and is automatically propagated into the async tasks enabled (bool, optional) – Setting this to False makes",profiler,1864
9930,Most likely the skew will be what for bottom most events?,negligible,1864
9931,"If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator’",with_flops,1864
9932,FLOPS (floating pointer operations per second) value using the operator’s input shape and total time allows one to estimate what?,hardware performance,1864
9933,"profile_memory (bool, optional) – what?",track tensor memory allocation/deallocation,1864
9934,"record source information (file and line number) for the ops. use_kineto (bool, optional) – experimental, enable profil",with_stack,1864
9935,"enable profiling with Kineto profiler. use_cpu (bool, optional) – profile CPU events; setting to False",use_kineto,1864
9936,"use_cpu (bool, optional) – what does use_cpu do?",profile CPU events,1864
9937,What is the name of the profiler that is used to lower the overhead for GPU-only profiling?,Example profiler,1864
9938,"If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group",record_shapes,11310
9939,"If with_flops is set, the profiler will estimate what value?",FLOPS,11310
9940,What does the FLOPS value allow one to estimate?,hardware performance,11310
9941,What is used to enable profiling with Kineto profiler?,use_kineto,11310
9942,"Use_cpu (bool, optional) – what does use_cpu do?",profile CPU events,11310
9943,"What (bool, optional) – Enables timing of CUDA events as well using the cudaEvent API?",use_cuda,11310
9944,"Record source information (file and line number) for the ops. what (bool, optional) – record source information (file and line number",with_stack,11310
9945,What might skew your profiling data?,shape recording,10560
9946,What might be artificially increased because of the shape collection?,the total self cpu time,10560
9947,What is the likely skew for bottom most events?,negligible,10560
9948,Why might the total self cpu time be artificially increased?,the shape collection,10560
9949,What allows one to see which dimensions have been used under the hood and further group by them?,prof.key_averages,10560
9950,Context manager that makes every autograd operation emit an NVTX range is useful when running the program under what?,nvprof,10560
9951,What does setting this to False do?,makes this context manager a no-op,9257
9952,What does one have to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them,CUDA profiling,2357
9953,In what language can nvprof() be used to load results for inspection?,Python REPL.,2357
9954,What does setting enabled=False make this context manager a no-op?,record_shapes,2357
9955,Where will arguments be listed?,in the order they are received by the backend op,2357
9956,Set enabled=False to what default?,True,2357
9957,Exports an EventList as what?,Chrome tracing tools file,2357
9958,Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Average,profiler.profile.self_cpu_time_total,2357
9959,This order may not match what order?,order in which those arguments were passed on the Python side,2357
9960,Where can you view a profile created using emit_nvtx?,Nvidia Visual Profiler,2357
9961,What does emit_nvtx append to ease correlating each backward-pass op with the corresponding forward-pass o,sequence number information,2357
9962,What default value does enabled=False default to?,True,2297
9963,What correlation can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?,Forward-backward,2297
9964,What is done to help correlating each backward-pass op with the corresponding forward-pass op?,To ease this,2297
9965,What order may not match the order in which arguments are received by the backend op?,order in which those arguments were passed on the Python side,10452
9966,Averages all events. Context manager that makes every autograd operation emit an NVTX range.,total_average,10452
9967,What is an example of a forward-backward correlation?,Example Forward-backward correlation,10452
9968,Why does Nvidia Visual Profiler use record_shapes instead of record_shapes?,ease this task,10452
9969,Image/Video Train a generative adversarial network to generate new celebrities.,GAN,3665
9970,What are some examples of fine tuning a pre-trained Mask R-CNN model?,"Interpretability,Getting-Started,TensorBoard",3857
9971,Image/Video Train a convolutional neural network for image classification using transfer learning. Image/Video Train a generative adversarial network (,Image/Video,3857
9972,What is an intermediate representation of a PyTorch model?,Production Introduction to TorchScript,3857
9973,What is a pre-trained Mask R-CNN model?,Finetune,2447
9974,What does python -m torch.utils.bottleneck -h refer to?,script.py,5716
9975,"If the profiler outputs don't help, you could try looking at the result of what with nvprof?",torch.autograd.profiler.emit_nvtx(),8139
9976,What are the two types of autograd profiles?,CPU-only-mode or CUDA-mode,8139
9977,What is the reality if your script spends most of its time executing on the GPU?,much more complicated,8139
9978,What is the first profiler that bottleneck runs (cProfile) will include in its time reporting?,CUDA startup time,8139
9979,"For more complicated uses of profilers, please see https://docs.python.org/3/library/profile.html or torch.",multi-GPU,8139
9980,What are any number of arguments to script.py?,args,11368
9981,"If your script spends most of its time executing on the GPU, it makes sense to start looking for what in the output of the CUDA",responsible CUDA operators,11368
9982,"If deterministic output compared to non-checkpointed passes is not required, what can be done to omit stashing and",supply preserve_rng_state=False to checkpoint or checkpoint_sequential,1572
9983,What does the stashing logic save and restore to the run_fn?,current device and the device of all cuda Tensor arguments,1572
9984,What does checkpointing trade?,compute for memory,1572
9985,"In the backwards pass, the saved inputs and function are retrieved and what?",the forward pass is computed on function again,1572
9986,What does the stashing logic save and restore the RNG state for?,current device and the device of all cuda Tensor arguments,4360
9987,What does the checkpointed part do instead of storing all intermediate activations of the entire computation graph for computing backward?,the checkpointed part does not save intermediate activations,1575
9988,"In the backwards pass, what happens after the saved inputs and function are retrieved?",the forward pass is computed on function again,1575
9989,What does the stashing logic have no way to anticipate if the user will move Tensors to a new device within the run_f,the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself,7305
9990,What part of a model does not save intermediate activations?,the checkpointed part,1566
9991,What can the checkpointed part be applied to?,any part of a model,1566
9992,What will not be considered as part of autograd if the output consists of nested structures consisting of Tensors?,custom structures,7214
9993,What are examples of nested structures consisting of Tensors?,"custom objects, lists, dicts",7214
9994,What is not supported by Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed?,torch.autograd.grad(),6008
9995,What describes what to run in the forward pass of the model or part of the model?,function,6008
9996,What should function know how to handle?,inputs passed as the tuple,6008
9997,"If function invocation during backward does anything different than the one during forward, the checkpointed version won’t be equivalent, and unfortunately it",global variable,3411
9998,What is an example of a function that should know how to handle the inputs passed as the tuple?,LSTM,3411
9999,What is the name of the function that checks the inputs of each checkpointed segment?,checkpoint(),3411
10000,What is a tensor that is input to functions?,Number of chunks to create in the model input,3411
10001,What should a function know how to handle the inputs passed as?,tuple,8155
10002,What does a function describe?,what to run in the forward pass of the model or part of the model,8155
10003,What is a way to circumvent the issue of a tensor having no gradient in the model?,detach the tensors outside of the checkpoint function,8155
10004,In what function should the first input be used as activation and the second as hidden?,LSTM,8155
10005,What function describes how checkpointing works?,checkpoint(),8155
10006,What do segments represent in the model input?,Number of chunks,8155
10007,Output of running functions sequentially on what?,*inputs,8155
10008,What is the name of the function that checks the outputs of each checkpointed segment?,checkpoint(),1570
10009,What are the functions used to run sequentially?,A torch.nn.Sequential or the list of modules or functions,1570
10010,In what model should the first input be used as activation and the second as hidden?,LSTM,8159
10011,What does it do to a matrix or batches of matrices A?,Computes the LU factorization,1682
10012,Where is the status of the factorization present in the return tuple?,third element,1682
10013,Why is the LU factorization of batches of square matrices with size less than 32 on a CUDA device?,LU factorization is repeated for singular matrices,1682
10014,What is the default value of a tuple of tensors containing factorization (Tensor)?,None,1682
10015,"L, U, and P can be derived using what?",torch.lu_unpack(),7220
10016,The pivots returned by the function are what?,1-indexed,7220
10017,The status of the factorization is present in what element of the return tuple?,third element,7220
10018,Why is LU factorization repeated for singular matrices?,LU factorization is repeated for singular matrices,7220
10019,What is the default value for an optional output tuple?,False out,7220
10020,Why is LU factorization with pivot = False not available for CPU?,not available for CPU,4387
10021,"If get_infos is False, then the elements in the tuple are what?","Tensor, IntTensor",4387
10022,Where is the status of the factorization present?,third element of the return tuple,4387
10023,"What does if set to True, returns an info IntTensor?",True get_infos,4387
10024,What could be reconstructed by applying swap(per)?,final permutation perm,4387
10025,What default value does get_infos return an info IntTensor?,True,4409
10026,What are the builtin location tags 'cpu' and 'cuda:device_id' for?,CUDA tensors,3447
10027,"If map_location returns a storage, it will be used as what?",final deserialized object,3447
10028,"If map_location is a string containing a device tag, it indicates the location where all tensors should be loaded. Otherwise",torch.device object or a string containing a device tag,3447
10029,"If map_location is a torch.device object or a string containing a device tag, it indicates the location where all ten",dict,3447
10030,Who can register their own location tags and tagging and deserialization methods using torch.serialization.register_package()?,User extensions,3447
10031,What is the risk of using pickle module implicitly?,insecure,3447
10032,What is possible to construct?,construct,3447
10033,"How to use torch.atanh, give an example?",">>> a = torch.randn(4).uniform_(-1, 1)
>>> a
tensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])
>>> torch.atanh(a)
tensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])",326
10034,"How to use torch.logaddexp, give an example?",">>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))
tensor([-0.3069, -0.6867, -0.8731])
>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))
tensor([-1., -2., -3.])
>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))
tensor([1.1269e+00, 2.0000e+03, 3.0000e+04])",7772
10035,"How to use torch.testing.assert_close, give an example?",">>> # tensor to tensor comparison
>>> expected = torch.tensor([1e0, 1e-1, 1e-2])
>>> actual = torch.acos(torch.cos(expected))
>>> torch.testing.assert_close(actual, expected)",2553
10036,"How  For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs., give an example?",">>> expected = torch.tensor([1.0, 2.0, 3.0])
>>> actual = torch.tensor([1.0, 4.0, 5.0])
>>> # The default mismatch message can be overwritten.
>>> torch.testing.assert_close(actual, expected, msg=""Argh, the tensors are not close!"")
AssertionError: Argh, the tensors are not close!
>>> # The error message can also created at runtime by passing a callable.
>>> def custom_msg(actual, expected, diagnostic_info):
...     return (
...         f""Argh, we found {diagnostic_info.total_mismatches} mismatches! ""
...         f""That is {diagnostic_info.mismatch_ratio:.1%}!""
...     )
>>> torch.testing.assert_close(actual, expected, msg=custom_msg)
AssertionError: Argh, we found 2 mismatches! That is 66.7%!",2554
10037,"How to use torch.diag, give an example?",">>> a = torch.randn(3)
>>> a
tensor([ 0.5950,-0.0872, 2.3298])
>>> torch.diag(a)
tensor([[ 0.5950, 0.0000, 0.0000],
        [ 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 2.3298]])
>>> torch.diag(a, 1)
tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
        [ 0.0000, 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 0.0000, 2.3298],
        [ 0.0000, 0.0000, 0.0000, 0.0000]])",2699
10038,"How  Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix:, give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[-0.4264, 0.0255,-0.1064],
        [ 0.8795,-0.2429, 0.1374],
        [ 0.1029,-0.6482,-1.6300]])
>>> torch.diag(a, 0)
tensor([-0.4264,-0.2429,-1.6300])
>>> torch.diag(a, 1)
tensor([ 0.0255, 0.1374])",2700
10039,"How to use The context managers torch.no_grad(), torch.enable_grad(), and
torch.set_grad_enabled() are helpful for locally disabling and enabling
gradient computation. See Locally disabling gradient computation for more details on
their usage.  These context managers are thread local, so they won’t
work if you send work to another thread using the threading module, etc., give an example?",">>> x = torch.zeros(1, requires_grad=True)
>>> with torch.no_grad():
...     y = x * 2
>>> y.requires_grad
False

>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...     y = x * 2
>>> y.requires_grad
False

>>> torch.set_grad_enabled(True)  # this can also be used as a function
>>> y = x * 2
>>> y.requires_grad
True

>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False",468
10040,"How  , give an example?",">>> x = torch.zeros(1, requires_grad=True)
>>> with torch.no_grad():
...     y = x * 2
>>> y.requires_grad
False

>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...     y = x * 2
>>> y.requires_grad
False

>>> torch.set_grad_enabled(True)  # this can also be used as a function
>>> y = x * 2
>>> y.requires_grad
True

>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False",468
10041,"How to use torch.overrides.get_ignored_functions, give an example?",">>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()
True
>>> torch.add in torch.overrides.get_ignored_functions()
False",394
10042,"How to use torch.overrides.get_testing_overrides, give an example?",">>> import inspect
>>> my_add = torch.overrides.get_testing_overrides()[torch.add]
>>> inspect.signature(my_add)
<Signature (input, other, out=None)>",354
10043,"How to use torch.overrides.handle_torch_function, give an example?",">>> def func(a):
...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__
...         return handle_torch_function(func, (a,), a)
...     return a + 0",186
10044,"How to use torch.overrides.is_tensor_like, give an example?",">>> class SubTensor(torch.Tensor): ...
>>> is_tensor_like(SubTensor([0]))
True",854
10045,"How  A subclass of tensor is generally a Tensor-like.Built-in or user types aren’t usually Tensor-like., give an example?",">>> is_tensor_like(6)
False
>>> is_tensor_like(None)
False
>>> class NotATensor: ...
>>> is_tensor_like(NotATensor())
False",857
10046,"How  Built-in or user types aren’t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__., give an example?",">>> class TensorLike:
...     def __torch_function__(self, func, types, args, kwargs):
...         return -1
>>> is_tensor_like(TensorLike())
True",1476
10047,"How to use torch.overrides.is_tensor_method_or_property, give an example?",">>> is_tensor_method_or_property(torch.Tensor.add)
True
>>> is_tensor_method_or_property(torch.add)
False",7720
10048,"How to use torch.overrides.wrap_torch_function, give an example?",">>> def dispatcher(a): # Must have the same signature as func
...     return (a,)
>>> @torch.overrides.wrap_torch_function(dispatcher)
>>> def func(a): # This will make func dispatchable by __torch_function__
...     return a + 0",343
10049,"How to use torch.mvlgamma, give an example?",">>> a = torch.empty(2, 3).uniform_(1, 2)
>>> a
tensor([[1.6835, 1.8474, 1.1929],
        [1.0475, 1.7162, 1.4180]])
>>> torch.mvlgamma(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
        [1.0311, 0.3901, 0.5049]])",1083
10050,"How to use torch.fake_quantize_per_channel_affine, give an example?",">>> x = torch.randn(2, 2, 2)
>>> x
tensor([[[-0.2525, -0.0466],
         [ 0.3491, -0.2168]],

        [[-0.5906,  1.6258],
         [ 0.6444, -0.0542]]])
>>> scales = (torch.randn(2) + 1) * 0.05
>>> scales
tensor([0.0475, 0.0486])
>>> zero_points = torch.zeros(2).to(torch.long)
>>> zero_points
tensor([0, 0])
>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)
tensor([[[0.0000, 0.0000],
         [0.3405, 0.0000]],

        [[0.0000, 1.6134],
        [0.6323, 0.0000]]])",455
10051,"How to use torch.atleast_1d, give an example?",">>> x = torch.randn(2)
>>> x
tensor([1.4584, 0.7583])
>>> torch.atleast_1d(x)
tensor([1.4584, 0.7583])
>>> x = torch.tensor(1.)
>>> x
tensor(1.)
>>> torch.atleast_1d(x)
tensor([1.])
>>> x = torch.tensor(0.5)
>>> y = torch.tensor(1.)
>>> torch.atleast_1d((x,y))
(tensor([0.5000]), tensor([1.]))",454
10052,"How to use torch.heaviside, give an example?",">>> input = torch.tensor([-1.5, 0, 2.0])
>>> values = torch.tensor([0.5])
>>> torch.heaviside(input, values)
tensor([0.0000, 0.5000, 1.0000])
>>> values = torch.tensor([1.2, -2.0, 3.5])
>>> torch.heaviside(input, values)
tensor([0., -2., 1.])",374
10053,"How to use torch.atleast_2d, give an example?",">>> x = torch.tensor(1.)
>>> x
tensor(1.)
>>> torch.atleast_2d(x)
tensor([[1.]])
>>> x = torch.randn(2,2)
>>> x
tensor([[2.2086, 2.5165],
        [0.1757, 0.5194]])
>>> torch.atleast_2d(x)
tensor([[2.2086, 2.5165],
        [0.1757, 0.5194]])
>>> x = torch.tensor(0.5)
>>> y = torch.tensor(1.)
>>> torch.atleast_2d((x,y))
(tensor([[0.5000]]), tensor([[1.]]))",459
10054,"How to use torch.ldexp, give an example?",">>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))
tensor([2.])
>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))
tensor([ 2.,  4.,  8., 16.])",8037
10055,"How to use torch.promote_types, give an example?",">>> torch.promote_types(torch.int32, torch.float32)
torch.float32
>>> torch.promote_types(torch.uint8, torch.long)
torch.long",424
10056,"How to use torch.nextafter, give an example?",">>> eps = torch.finfo(torch.float32).eps
>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])
tensor([True, True])",7284
10057,"How to use torch.set_default_tensor_type, give an example?",">>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
torch.float32
>>> torch.set_default_tensor_type(torch.DoubleTensor)
>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
torch.float64",7023
10058,"How to use torch.diagflat, give an example?",">>> a = torch.randn(3)
>>> a
tensor([-0.2956, -0.9068,  0.1695])
>>> torch.diagflat(a)
tensor([[-0.2956,  0.0000,  0.0000],
        [ 0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.1695]])
>>> torch.diagflat(a, 1)
tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.1695],
        [ 0.0000,  0.0000,  0.0000,  0.0000]])

>>> a = torch.randn(2, 2)
>>> a
tensor([[ 0.2094, -0.3018],
        [-0.1516,  1.9342]])
>>> torch.diagflat(a)
tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3018,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1516,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.9342]])",305
10059,"How to use torch.lu_unpack, give an example?",">>> A = torch.randn(2, 3, 3)
>>> A_LU, pivots = A.lu()
>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
>>>
>>> # can recover A from factorization
>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))

>>> # LU factorization of a rectangular matrix:
>>> A = torch.randn(2, 3, 2)
>>> A_LU, pivots = A.lu()
>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
>>> P
tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]],

        [[0., 0., 1.],
         [0., 1., 0.],
         [1., 0., 0.]]])
>>> A_L
tensor([[[ 1.0000,  0.0000],
         [ 0.4763,  1.0000],
         [ 0.3683,  0.1135]],

        [[ 1.0000,  0.0000],
         [ 0.2957,  1.0000],
         [-0.9668, -0.3335]]])
>>> A_U
tensor([[[ 2.1962,  1.0881],
         [ 0.0000, -0.8681]],

        [[-1.0947,  0.3736],
         [ 0.0000,  0.5718]]])
>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
>>> torch.norm(A_ - A)
tensor(2.9802e-08)",5496
10060,"How to use torch.addmv, give an example?",">>> M = torch.randn(2)
>>> mat = torch.randn(2, 3)
>>> vec = torch.randn(3)
>>> torch.addmv(M, mat, vec)
tensor([-0.3768, -5.5565])",2543
10061,"How to use torch.nanmedian, give an example?",">>> a = torch.tensor([1, float('nan'), 3, 2])
>>> a.median()
tensor(nan)
>>> a.nanmedian()
tensor(2.)",7548
10062,"How  This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has
one or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the
median of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too., give an example?",">>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])
>>> a
tensor([[2., 3., 1.],
        [nan, 1., nan]])
>>> a.median(0)
torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))
>>> a.nanmedian(0)
torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))",7547
10063,"How to use torch.empty_strided, give an example?",">>> a = torch.empty_strided((2, 3), (1, 2))
>>> a
tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],
        [0.0000e+00, 0.0000e+00, 3.0705e-41]])
>>> a.stride()
(1, 2)
>>> a.size()
torch.Size([2, 3])",289
10064,"How to use torch.isnan, give an example?",">>> torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([False, True, False])",415
10065,"How to use torch.atleast_3d, give an example?",">>> x = torch.tensor(0.5)
>>> x
tensor(0.5000)
>>> torch.atleast_3d(x)
tensor([[[0.5000]]])
>>> y = torch.randn(2,2)
>>> y
tensor([[-0.8079,  0.7460],
        [-1.1647,  1.4734]])
>>> torch.atleast_3d(y)
tensor([[[-0.8079],
        [ 0.7460]],

        [[-1.1647],
        [ 1.4734]]])
>>> x = torch.randn(1,1,1)
>>> x
tensor([[[-1.5689]]])
>>> torch.atleast_3d(x)
tensor([[[-1.5689]]])
>>> x = torch.tensor(0.5)
>>> y = torch.tensor(1.)
>>> torch.atleast_3d((x,y))
(tensor([[[0.5000]]]), tensor([[[1.]]]))",458
10066,"How to use torch.bitwise_xor, give an example?",">>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-2, -2,  0], dtype=torch.int8)
>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, False, False])",400
10067,"How to use torch.sinh, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.5380, -0.8632, -0.1265,  0.9399])
>>> torch.sinh(a)
tensor([ 0.5644, -0.9744, -0.1268,  1.0845])",312
10068,"How to use torch.roll, give an example?",">>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
>>> x
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
>>> torch.roll(x, 1, 0)
tensor([[7, 8],
        [1, 2],
        [3, 4],
        [5, 6]])
>>> torch.roll(x, -1, 0)
tensor([[3, 4],
        [5, 6],
        [7, 8],
        [1, 2]])
>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))
tensor([[6, 5],
        [8, 7],
        [2, 1],
        [4, 3]])",461
10069,"How to use torch.tile, give an example?",">>> x = torch.tensor([1, 2, 3])
>>> x.tile((2,))
tensor([1, 2, 3, 1, 2, 3])
>>> y = torch.tensor([[1, 2], [3, 4]])
>>> torch.tile(y, (2, 2))
tensor([[1, 2, 1, 2],
        [3, 4, 3, 4],
        [1, 2, 1, 2],
        [3, 4, 3, 4]])",1133
10070,"How to use torch.topk, give an example?",">>> x = torch.arange(1., 6.)
>>> x
tensor([ 1.,  2.,  3.,  4.,  5.])
>>> torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))",6980
10071,"How to use torch.sqrt, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-2.0755,  1.0226,  0.0831,  0.4806])
>>> torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])",324
10072,"How to use torch.minimum, give an example?",">>> a = torch.tensor((1, 2, -1))
>>> b = torch.tensor((3, 0, 4))
>>> torch.minimum(a, b)
tensor([1, 0, -1])",331
10073,"How to use torch.floor_divide, give an example?",">>> a = torch.tensor([4.0, 3.0])
>>> b = torch.tensor([2.0, 2.0])
>>> torch.floor_divide(a, b)
tensor([2.0, 1.0])
>>> torch.floor_divide(a, 1.4)
tensor([2.0, 2.0])",6115
10074,"How to use torch.sgn, give an example?",">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])
>>> t.sgn()
tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])",391
10075,"How to use torch.ones_like, give an example?",">>> input = torch.empty(2, 3)
>>> torch.ones_like(input)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])",372
10076,"How to use torch.no_grad, give an example?",">>> x = torch.tensor([1], requires_grad=True)
>>> with torch.no_grad():
...   y = x * 2
>>> y.requires_grad
False
>>> @torch.no_grad()
... def doubler(x):
...     return x * 2
>>> z = doubler(x)
>>> z.requires_grad
False",1103
10077,"How to use torch.view_as_complex, give an example?",">>> x=torch.randn(4, 2)
>>> x
tensor([[ 1.6116, -0.5772],
        [-1.4606, -0.9120],
        [ 0.0786, -1.7497],
        [-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])",470
10078,"How to use torch.swapaxes, give an example?",">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])
>>> x
tensor([[[0, 1],
        [2, 3]],

        [[4, 5],
        [6, 7]]])
>>> torch.swapaxes(x, 0, 1)
tensor([[[0, 1],
        [4, 5]],

        [[2, 3],
        [6, 7]]])
>>> torch.swapaxes(x, 0, 2)
tensor([[[0, 4],
        [2, 6]],

        [[1, 5],
        [3, 7]]])",7544
10079,"How to use torch.logical_xor, give an example?",">>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
tensor([False, False,  True])
>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
>>> torch.logical_xor(a, b)
tensor([ True,  True, False, False])
>>> torch.logical_xor(a.double(), b.double())
tensor([ True,  True, False, False])
>>> torch.logical_xor(a.double(), b)
tensor([ True,  True, False, False])
>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True, False, False])",421
10080,"How to use A floating point scalar operand has dtype torch.get_default_dtype() and an integral
non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect
values when determining the minimum dtypes of an operand.  Quantized and complex types
are not yet supported., give an example?",">>> float_tensor = torch.ones(1, dtype=torch.float)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)
>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)
>>> int_tensor = torch.ones(1, dtype=torch.int)
>>> long_tensor = torch.ones(1, dtype=torch.long)
>>> uint_tensor = torch.ones(1, dtype=torch.uint8)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> bool_tensor = torch.ones(1, dtype=torch.bool)
# zero-dim tensors
>>> long_zerodim = torch.tensor(1, dtype=torch.long)
>>> int_zerodim = torch.tensor(1, dtype=torch.int)

>>> torch.add(5, 5).dtype
torch.int64
# 5 is an int64, but does not have higher category than int_tensor so is not considered.
>>> (int_tensor + 5).dtype
torch.int32
>>> (int_tensor + long_zerodim).dtype
torch.int32
>>> (long_tensor + int_tensor).dtype
torch.int64
>>> (bool_tensor + long_tensor).dtype
torch.int64
>>> (bool_tensor + uint_tensor).dtype
torch.uint8
>>> (float_tensor + double_tensor).dtype
torch.float64
>>> (complex_float_tensor + complex_double_tensor).dtype
torch.complex128
>>> (bool_tensor + int_tensor).dtype
torch.int32
# Since long is a different kind than float, result dtype only needs to be large enough
# to hold the float.
>>> torch.add(long_tensor, float_tensor).dtype
torch.float32",797
10081,"How  A floating point scalar operand has dtype torch.get_default_dtype() and an integral
non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect
values when determining the minimum dtypes of an operand.  Quantized and complex types
are not yet supported., give an example?",">>> float_tensor = torch.ones(1, dtype=torch.float)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)
>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)
>>> int_tensor = torch.ones(1, dtype=torch.int)
>>> long_tensor = torch.ones(1, dtype=torch.long)
>>> uint_tensor = torch.ones(1, dtype=torch.uint8)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> bool_tensor = torch.ones(1, dtype=torch.bool)
# zero-dim tensors
>>> long_zerodim = torch.tensor(1, dtype=torch.long)
>>> int_zerodim = torch.tensor(1, dtype=torch.int)

>>> torch.add(5, 5).dtype
torch.int64
# 5 is an int64, but does not have higher category than int_tensor so is not considered.
>>> (int_tensor + 5).dtype
torch.int32
>>> (int_tensor + long_zerodim).dtype
torch.int32
>>> (long_tensor + int_tensor).dtype
torch.int64
>>> (bool_tensor + long_tensor).dtype
torch.int64
>>> (bool_tensor + uint_tensor).dtype
torch.uint8
>>> (float_tensor + double_tensor).dtype
torch.float64
>>> (complex_float_tensor + complex_double_tensor).dtype
torch.complex128
>>> (bool_tensor + int_tensor).dtype
torch.int32
# Since long is a different kind than float, result dtype only needs to be large enough
# to hold the float.
>>> torch.add(long_tensor, float_tensor).dtype
torch.float32",797
10082,"How to use torch dtype, give an example?","# allowed:
>>> float_tensor *= float_tensor
>>> float_tensor *= int_tensor
>>> float_tensor *= uint_tensor
>>> float_tensor *= bool_tensor
>>> float_tensor *= double_tensor
>>> int_tensor *= long_tensor
>>> int_tensor *= uint_tensor
>>> uint_tensor *= int_tensor

# disallowed (RuntimeError: result type can't be cast to the desired output type):
>>> int_tensor *= float_tensor
>>> bool_tensor *= int_tensor
>>> bool_tensor *= uint_tensor
>>> float_tensor *= complex_float_tensor",45
10083,"How to use A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",">>> torch.device('cuda:0')
device(type='cuda', index=0)

>>> torch.device('cpu')
device(type='cpu')

>>> torch.device('cuda')  # current cuda device
device(type='cuda')",870
10084,"How  A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",">>> torch.device('cuda:0')
device(type='cuda', index=0)

>>> torch.device('cpu')
device(type='cpu')

>>> torch.device('cuda')  # current cuda device
device(type='cuda')",870
10085,"How to use Via a string:Via a string and device ordinal:, give an example?",">>> torch.device('cuda', 0)
device(type='cuda', index=0)

>>> torch.device('cpu', 0)
device(type='cpu', index=0)",8132
10086,"How  Via a string:Via a string and device ordinal:, give an example?",">>> torch.device('cuda', 0)
device(type='cuda', index=0)

>>> torch.device('cpu', 0)
device(type='cpu', index=0)",8132
10087,"How to use NoteThe torch.device argument in functions can generally be substituted with a string.
This allows for fast prototyping of code., give an example?",">>> # Example of a function that takes in a torch.device
>>> cuda1 = torch.device('cuda:1')
>>> torch.randn((2,3), device=cuda1)",7334
10088,"How  The torch.device argument in functions can generally be substituted with a string.
This allows for fast prototyping of code., give an example?",">>> # You can substitute the torch.device with a string
>>> torch.randn((2,3), device='cuda:1')",7335
10089,"How to use NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated
as a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda
tensors and is not supported for cpu tensors., give an example?",">>> torch.device(1)
device(type='cuda', index=1)",2552
10090,"How  For legacy reasons, a device can be constructed via a single device ordinal, which is treated
as a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda
tensors and is not supported for cpu tensors., give an example?",">>> torch.device(1)
device(type='cuda', index=1)",2552
10091,"How to use NoteMethods which take a device will generally accept a (properly formatted) string
or (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",">>> torch.randn((2,3), device=torch.device('cuda:1'))
>>> torch.randn((2,3), device='cuda:1')
>>> torch.randn((2,3), device=1)  # legacy",4243
10092,"How  Methods which take a device will generally accept a (properly formatted) string
or (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",">>> torch.randn((2,3), device=torch.device('cuda:1'))
>>> torch.randn((2,3), device='cuda:1')
>>> torch.randn((2,3), device=1)  # legacy",4243
10093,"How to use torch.strided represents dense Tensors and is the memory layout that
is most commonly used. Each strided tensor has an associated
torch.Storage, which holds its data. These tensors provide
multi-dimensional, strided
view of a storage. Strides are a list of integers: the k-th stride
represents the jump in the memory necessary to go from one element to the
next one in the k-th dimension of the Tensor. This concept makes it possible
to perform many tensor operations efficiently., give an example?",">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
>>> x.stride()
(5, 1)

>>> x.t().stride()
(1, 5)",11226
10094,"How  torch.strided represents dense Tensors and is the memory layout that
is most commonly used. Each strided tensor has an associated
torch.Storage, which holds its data. These tensors provide
multi-dimensional, strided
view of a storage. Strides are a list of integers: the k-th stride
represents the jump in the memory necessary to go from one element to the
next one in the k-th dimension of the Tensor. This concept makes it possible
to perform many tensor operations efficiently., give an example?",">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
>>> x.stride()
(5, 1)

>>> x.t().stride()
(1, 5)",11226
10095,"How to use torch.median, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[ 1.5219, -1.5212,  0.2202]])
>>> torch.median(a)
tensor(0.2202)",301
10096,"How  If keepdim is True, the output tensors are of the same size
as input except in the dimension dim where they are of size 1.
Otherwise, dim is squeezed (see torch.squeeze()), resulting in
the outputs tensor having 1 fewer dimension than input., give an example?",">>> a = torch.randn(4, 5)
>>> a
tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
>>> torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))",3437
10097,"How to use Here is a simple script which exports a pretrained AlexNet as defined in
torchvision into ONNX.  It runs a single round of inference and then
saves the resulting traced model to alexnet.onnx:, give an example?","import torch
import torchvision

dummy_input = torch.randn(10, 3, 224, 224, device='cuda')
model = torchvision.models.alexnet(pretrained=True).cuda()

# Providing input and output names sets the display names for values
# within the model's graph. Setting these does not change the semantics
# of the graph; it is only for readability.
#
# The inputs to the network consist of the flat list of inputs (i.e.
# the values you would pass to the forward() method) followed by the
# flat list of parameters. You can partially specify names, i.e. provide
# a list here shorter than the number of inputs to the model, and we will
# only set that subset of names, starting from the beginning.
input_names = [ ""actual_input_1"" ] + [ ""learned_%d"" % i for i in range(16) ]
output_names = [ ""output1"" ]

torch.onnx.export(model, dummy_input, ""alexnet.onnx"", verbose=True, input_names=input_names, output_names=output_names)",2782
10098,"How to use Here is a simple script which exports a pretrained AlexNet as defined in
torchvision into ONNX.  It runs a single round of inference and then
saves the resulting traced model to alexnet.onnx:The resulting alexnet.onnx is a binary protobuf file which contains both
the network structure and parameters of the model you exported
(in this case, AlexNet).  The keyword argument verbose=True causes the
exporter to print out a human-readable representation of the network:, give an example?","# These are the inputs and parameters to the network, which have taken on
# the names we specified earlier.
graph(%actual_input_1 : Float(10, 3, 224, 224)
      %learned_0 : Float(64, 3, 11, 11)
      %learned_1 : Float(64)
      %learned_2 : Float(192, 64, 5, 5)
      %learned_3 : Float(192)
      # ---- omitted for brevity ----
      %learned_14 : Float(1000, 4096)
      %learned_15 : Float(1000)) {
  # Every statement consists of some output tensors (and their types),
  # the operator to be run (with its attributes, e.g., kernels, strides,
  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)
  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]
  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]
  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]
  # ---- omitted for brevity ----
  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]
  # Dynamic means that the shape is not known. This may be because of a
  # limitation of our implementation (which we would like to fix in a
  # future release) or shapes which are truly dynamic.
  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet
  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet
  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet
  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet
  # ---- omitted for brevity ----
  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]
  return (%output1);
}",7254
10099,"How to use The resulting alexnet.onnx is a binary protobuf file which contains both
the network structure and parameters of the model you exported
(in this case, AlexNet).  The keyword argument verbose=True causes the
exporter to print out a human-readable representation of the network:You can also verify the protobuf using the ONNX library.
You can install ONNX with conda:, give an example?",conda install -c conda-forge onnx,7257
10100,"How to use You can also verify the protobuf using the ONNX library.
You can install ONNX with conda:Then, you can run:, give an example?","import onnx

# Load the ONNX model
model = onnx.load(""alexnet.onnx"")

# Check that the IR is well formed
onnx.checker.check_model(model)

# Print a human readable representation of the graph
onnx.helper.printable_graph(model.graph)",8567
10101,"How to use To run the exported script with caffe2, you will need to install caffe2: If you don’t have one already, Please follow the install instructions.Once these are installed, you can use the backend for Caffe2:, give an example?","# ...continuing from above
import caffe2.python.onnx.backend as backend
import numpy as np

rep = backend.prepare(model, device=""CUDA:0"") # or ""CPU""
# For the Caffe2 backend:
#     rep.predict_net is the Caffe2 protobuf for the network
#     rep.workspace is the Caffe2 workspace for the network
#       (see the class caffe2.python.onnx.backend.Workspace)
outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))
# To run networks with more than one input, pass a tuple
# rather than a single numpy ndarray.
print(outputs[0])",7918
10102,"How to use You can also run the exported model with ONNX Runtime,
you will need to install ONNX Runtime: please follow these instructions.Once these are installed, you can use the backend for ONNX Runtime:, give an example?","# ...continuing from above
import onnxruntime as ort

ort_session = ort.InferenceSession('alexnet.onnx')

outputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})

print(outputs[0])",8562
10103,"How to use The ONNX exporter can be both trace-based and script-based exporter.We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements
of a part of a model.  Checkout this example:, give an example?","import torch

# Trace-based only

class LoopModel(torch.nn.Module):
    def forward(self, x, y):
        for i in range(y):
            x = x + i
        return x

model = LoopModel()
dummy_input = torch.ones(2, 3, dtype=torch.long)
loop_count = torch.tensor(5, dtype=torch.long)

torch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)",8233
10104,"How to use We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements
of a part of a model.  Checkout this example:With trace-based exporter, we get the result ONNX graph which unrolls the for loop:, give an example?","graph(%0 : Long(2, 3),
      %1 : Long()):
  %2 : Tensor = onnx::Constant[value={1}]()
  %3 : Tensor = onnx::Add(%0, %2)
  %4 : Tensor = onnx::Constant[value={2}]()
  %5 : Tensor = onnx::Add(%3, %4)
  %6 : Tensor = onnx::Constant[value={3}]()
  %7 : Tensor = onnx::Add(%5, %6)
  %8 : Tensor = onnx::Constant[value={4}]()
  %9 : Tensor = onnx::Add(%7, %8)
  return (%9)",8234
10105,"How to use With trace-based exporter, we get the result ONNX graph which unrolls the for loop:To utilize script-based exporter for capturing the dynamic loop,
we can write the loop in script, and call it from the regular nn.Module:, give an example?","# Mixing tracing and scripting

@torch.jit.script
def loop(x, y):
    for i in range(int(y)):
        x = x + i
    return x

class LoopModel2(torch.nn.Module):
    def forward(self, x, y):
        return loop(x, y)

model = LoopModel2()
dummy_input = torch.ones(2, 3, dtype=torch.long)
loop_count = torch.tensor(5, dtype=torch.long)
torch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,
                  input_names=['input_data', 'loop_range'])",8428
10106,"How to use To utilize script-based exporter for capturing the dynamic loop,
we can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:, give an example?","graph(%input_data : Long(2, 3),
      %loop_range : Long()):
  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop
  %3 : Tensor = onnx::Cast[to=9](%2)
  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop
    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):
      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop
      %9 : Tensor = onnx::Cast[to=9](%2)
      -> (%9, %8)
  return (%4)",7932
10107,"How to use Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range., give an example?","import caffe2.python.onnx.backend as backend
import numpy as np
import onnx
model = onnx.load('loop.onnx')

rep = backend.prepare(model)
outputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))
print(outputs[0])
#[[37 37 37]
# [37 37 37]]


import onnxruntime as ort
ort_sess = ort.InferenceSession('loop.onnx')
outputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),
                              'loop_range': np.array(9).astype(np.int64)})
print(outputs)
#[array([[37, 37, 37],
#       [37, 37, 37]], dtype=int64)]",4542
10108,"How to use The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please
avoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.
E.g.:, give an example?","class LoopModel(torch.nn.Module):
    def forward(self, x, y):
        res = []
        arr = x.split(2, 0)
        for i in range(int(y)):
            res += [arr[i].sum(0, False)]
        return torch.stack(res)

model = torch.jit.script(LoopModel())
inputs = (torch.randn(16), torch.tensor(8))

out = model(*inputs)
torch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)",7038
10109,"How to use TorchScript only supports a subset of Python types. You can find more details about type annotation
here.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.
By default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,
its type should be specified using MyPy-style annotations., give an example?","import torch

class Module(torch.nn.Module):
    def forward(self, x, tup):
        # type: (int, Tuple[Tensor, Tensor]) -> Tensor
        t0, t1 = tup
        return t0 + t1 + x",2216
10110,"How to use Due to optimization purposes, TorchScript only supports variables with single static types for script functions.
By default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,
its type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below., give an example?","RuntimeError:
Tensor (inferred) cannot be used as a tuple:
  File <filename>
        def forward(self, x, tup):
            t0, t1 = tup
                     ~~~ <--- HERE
            return t0 + t1 + x",2217
10111,"How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.
For the trace-based exporter, tracing treats the numpy values as the constant node,
therefore it calculates the wrong result if we change the input.
So the PyTorch model need implement using torch operators.
For example, do not use numpy operators on numpy tensors:, give an example?","np.concatenate((x, y, z), axis=1)",4967
10112,"How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.
For the trace-based exporter, tracing treats the numpy values as the constant node,
therefore it calculates the wrong result if we change the input.
So the PyTorch model need implement using torch operators.
For example, do not use numpy operators on numpy tensors:do not convert to numpy types:, give an example?",y = x.astype(np.int),9190
10113,"How to use do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.
In addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,, give an example?","class MyModule(nn.Module):
    def __init__(self):
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.dropout(x)",9191
10114,"How to use There are two ways to handle models which consist of named parameters or keyword arguments as inputs:For example, in the model:, give an example?","class Model(torch.nn.Module):
  def forward(self, x, y=None, z=None):
    if y is not None:
      return x + y
    if z is not None:
      return x + z
    return x
m = Model()
x = torch.randn(2, 3)
z = torch.randn(2, 3)",2520
10115,"How to use Not using a dictionary for the keyword arguments and passing all the inputs in the same order
as required by the model, give an example?","torch.onnx.export(model, (x, None, z), ‘test.onnx’)",4350
10116,"How to use Using a dictionary to represent the keyword arguments. This dictionary is always passed in
addition to the non-keyword arguments and is always the last argument in the args tuple., give an example?","torch.onnx.export(model, (x, {'y': None, 'z': z}), ‘test.onnx’)",8106
10117,"How to use There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an
empty or no dictionary. For example,, give an example?","torch.onnx.export(model, (x, {}), ‘test.onnx’)
or
torch.onnx.export(model, (x, ), ‘test.onnx’)",7409
10118,"How to use For cases in which there are no keyword arguments, models can be exported with either an
empty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.
In these cases it is mandatory to have an empty dictionary as the last argument in the
args tuple. For example,, give an example?","class Model(torch.nn.Module):
  def forward(self, k, x):
    ...
    return x
m = Model()
k = torch.randn(2, 3)
x = {torch.tensor(1.): torch.randn(2, 3)}",2506
10119,"How to use An exception to this rule are cases in which the last input is also of a dictionary type.
In these cases it is mandatory to have an empty dictionary as the last argument in the
args tuple. For example,Without the presence of the empty dictionary, the export call assumes that the
‘x’ input is intended to represent the optional dictionary consisting of named arguments.
In order to prevent this from being an issue a constraint is placed to provide an empty
dictionary as the last input in the tuple args in such cases.
The new call would look like this., give an example?","torch.onnx.export(model, (k, x, {}), ‘test.onnx’)",1126
10120,"How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:, give an example?","data = torch.randn(3, 4)
index = torch.tensor([1, 2])

# RHS indexing is supported in ONNX opset >= 11.
class RHSIndexing(torch.nn.Module):
    def forward(self, data, index):
        return data[index]

out = RHSIndexing()(data, index)

torch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)

# onnxruntime
import onnxruntime
sess = onnxruntime.InferenceSession('indexing.onnx')
out_ort = sess.run(None, {
    sess.get_inputs()[0].name: data.numpy(),
    sess.get_inputs()[1].name: index.numpy(),
})

assert torch.all(torch.eq(out, torch.tensor(out_ort)))",7824
10121,"How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:Below is the list of supported patterns for RHS indexing., give an example?","# Scalar indices
data[0, 1]

# Slice indices
data[:3]

# Tensor indices
data[torch.tensor([[1, 2], [2, 3]])]
data[torch.tensor([2, 3]), torch.tensor([1, 2])]
data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]
data[torch.tensor([2, 3]), :, torch.tensor([1, 2])]

# Ellipsis followed by tensor indexing
# Not supported in scripting
# i.e. torch.jit.script(model) will fail if model contains this pattern.
# Export is supported under tracing
# i.e. torch.onnx.export(model)
data[..., torch.tensor([2, 1])]

# The combination of above
data[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]

# Boolean mask (supported for ONNX opset version >= 11)
data[data != 1]",1438
10122,"How to use Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing., give an example?","# Tensor indices that includes negative values.
data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]",1439
10123,"How to use In code, this type of indexing occurs on the LHS.
Export is supported for ONNX opset version >= 11. E.g.:, give an example?","data = torch.zeros(3, 4)
new_data = torch.arange(4).to(torch.float32)

# LHS indexing is supported in ONNX opset >= 11.
class LHSIndexing(torch.nn.Module):
    def forward(self, data, new_data):
        data[1] = new_data
        return data

out = LHSIndexing()(data, new_data)

data = torch.zeros(3, 4)
new_data = torch.arange(4).to(torch.float32)
torch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)

# onnxruntime
import onnxruntime
sess = onnxruntime.InferenceSession('inplace_assign.onnx')
out_ort = sess.run(None, {
    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),
    sess.get_inputs()[1].name: new_data.numpy(),
})

assert torch.all(torch.eq(out, torch.tensor(out_ort)))",3711
10124,"How to use In code, this type of indexing occurs on the LHS.
Export is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing., give an example?","# Scalar indices
data[0, 1] = new_data

# Slice indices
data[:3] = new_data

# Tensor indices
# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.
data[torch.tensor([[1, 2], [2, 3]])] = new_data
data[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data

# Ellipsis followed by tensor indexing
# Not supported to export in script modules
# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.
# Export is supported under tracing
# i.e. torch.onnx.export(model)
data[..., torch.tensor([2, 1])] = new_data

# The combination of above
data[2, ..., torch.tensor([2, 1, 3]), 2:4] += update

# Boolean mask
data[data != 1] = new_data",1436
10125,"How to use Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing., give an example?","# Multiple tensor indices if any has rank >= 2
data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data

# Multiple tensor indices that are not consecutive
data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data

# Tensor indices that includes negative values.
data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data",1437
10126,"How to use If the operator is a non-ATen operator, the symbolic function has to be
added in the corresponding PyTorch Function class. Please read the following
instructions:Symbolic functions should be implemented in Python. All of these functions interact
with Python methods which are implemented via C++-Python bindings,
but intuitively the interface they provide looks like this:, give an example?","def operator/symbolic(g, *inputs):
  """"""
  Modifies Graph (e.g., using ""op""), adding the ONNX operations representing
  this PyTorch function, and returning a Value or tuple of Values specifying the
  ONNX outputs whose values correspond to the original PyTorch return values
  of the autograd Function (or None if an output is not supported by ONNX).

  Args:
    g (Graph): graph to write the ONNX representation into
    inputs (Value...): list of values representing the variables which contain
        the inputs for this function
  """"""

class Value(object):
  """"""Represents an intermediate tensor value computed in ONNX.""""""
  def type(self):
    """"""Returns the Type of the value.""""""

class Type(object):
  def sizes(self):
    """"""Returns a tuple of ints representing the shape of a tensor this describes.""""""

class Graph(object):
  def op(self, opname, *inputs, **attrs):
    """"""
    Create an ONNX operator 'opname', taking 'args' as inputs
    and attributes 'kwargs' and add it as a node to the current graph,
    returning the value representing the single output of this
    operator (see the `outputs` keyword argument for multi-return
    nodes).

    The set of operators and the inputs/attributes they take
    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md

    Args:
        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.
        args (Value...): The inputs to the operator; usually provided
            as arguments to the `symbolic` definition.
        kwargs: The attributes of the ONNX operator, with keys named
            according to the following convention: `alpha_f` indicates
            the `alpha` attribute with type `f`.  The valid type specifiers are
            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute
            specified with type float accepts either a single float, or a
            list of floats (e.g., you would say `dims_i` for a `dims` attribute
            that takes a list of integers).
        outputs (int, optional):  The number of outputs this operator returns;
            by default an operator is assumed to return a single output.
            If `outputs` is greater than one, this functions returns a tuple
            of output `Value`, representing each output of the ONNX operator
            in positional.
    """"""",6166
10127,"How to use The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.
We try to export the model and see the error message as below:, give an example?","UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist
RuntimeError: ONNX export failed: Couldn't export operator elu",6936
10128,"How to use Here is an example of handling missing symbolic function for elu operator.
We try to export the model and see the error message as below:The export fails because PyTorch does not support exporting elu operator.
We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;
in VariableType.h. This means elu is an ATen operator.
We check the ONNX operator list,
and confirm that Elu is standardized in ONNX.
We add the following lines to symbolic_opset9.py:, give an example?","def elu(g, input, alpha, inplace=False):
    return g.op(""Elu"", input, alpha_f=_scalar(alpha))",2785
10129,"How to use Following this tutorial Extending TorchScript with Custom C++ Operators,
you can create and register your own custom ops implementation in PyTorch. Here’s how to export such model to ONNX.:, give an example?","# Create custom symbolic function
from torch.onnx.symbolic_helper import parse_args
@parse_args('v', 'v', 'f', 'i')
def symbolic_foo_forward(g, input1, input2, attr1, attr2):
    return g.op(""Foo"", input1, input2, attr1_f=attr1, attr2_i=attr2)

# Register custom symbolic function
from torch.onnx import register_custom_op_symbolic
register_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)

class FooModel(torch.nn.Module):
    def __init__(self, attr1, attr2):
        super(FooModule, self).__init__()
        self.attr1 = attr1
        self.attr2 = attr2

    def forward(self, input1, input2):
        # Calling custom op
        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)

model = FooModel(attr1, attr2)
torch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={""custom_domain"": 2})",2478
10130,"How to use This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode., give an example?","Example torch ir graph:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):
    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)
    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)
    return (%4)

Is exported as:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):
    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)
    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)
    return (%2)",7742
10131,"How to use This mode is used to export all operators as ATen ops, and avoid conversion to ONNX., give an example?","Example torch ir graph:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):
    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)
    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)
    return (%4)

Is exported as:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):
    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=""exp""](%0)
    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=""div""](%0, %1)
    return (%2)",7739
10132,"How to use To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.
In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator., give an example?","Example torch ir graph:

  graph(%0 : Float):
    %3 : int = prim::Constant[value=0]()
    %4 : Float = aten::triu(%0, %3) # unsupported op
    %5 : Float = aten::mul(%4, %0) # registered op
    return (%5)

is exported as:

  graph(%0 : Float):
    %1 : Long() = onnx::Constant[value={0}]()
    %2 : Float = aten::ATen[operator=""triu""](%0, %1) # unsupported op
    %3 : Float = onnx::Mul(%2, %0) # registered op
    return (%3)",7894
10133,"How to use To export a raw ir., give an example?","Example torch ir graph:

  graph(%x.1 : Float(1, strides=[1])):
    %1 : Tensor = aten::exp(%x.1)
    %2 : Tensor = aten::div(%x.1, %1)
    %y.1 : Tensor[] = prim::ListConstruct(%2)
    return (%y.1)

is exported as:

  graph(%x.1 : Float(1, strides=[1])):
    %1 : Tensor = aten::exp(%x.1)
    %2 : Tensor = aten::div(%x.1, %1)
    %y.1 : Tensor[] = prim::ListConstruct(%2)
    return (%y.1)",7892
10134,"How to use This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.
Exported falls through and exports the operator as is, as custom op. Exporting custom operators
enables users to register and implement the operator as part of their runtime backend., give an example?","Example torch ir graph:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),
        %1 : Float(2, 3, 4, strides=[12, 4, 1])):
    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op
    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op
    return (%7))

is exported as:

  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),
        %1 : Float(2, 3, 4, strides=[12, 4, 1])):
    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op
    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op
    return (%3",7737
10135,"How to use The tracer records the example inputs shape in the graph. In case the model should accept
inputs of dynamic shape, you can utilize the parameter dynamic_axes in export api., give an example?","layer_count = 4

model = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)
model.eval()

with torch.no_grad():
    input = torch.randn(5, 3, 10)
    h0 = torch.randn(layer_count * 2, 3, 20)
    c0 = torch.randn(layer_count * 2, 3, 20)
    output, (hn, cn) = model(input, (h0, c0))

    # default export
    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')
    onnx_model = onnx.load('lstm.onnx')
    # input shape [5, 3, 10]
    print(onnx_model.graph.input[0])

    # export with `dynamic_axes`
    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',
                    input_names=['input', 'h0', 'c0'],
                    output_names=['output', 'hn', 'cn'],
                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})
    onnx_model = onnx.load('lstm.onnx')
    # input shape ['sequence', 3, 10]
    print(onnx_model.graph.input[0])",7341
10136,"How to use No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.
The exporter will try to figure out the right datatype for scalars.  However for cases that it failed
to do so, you will need to manually provide the datatype information.  This often happens with scripted models,
where the datatypes are not recorded.  We are trying to improve the datatype
propagation in the exporter such that manual changes are not required in the future., give an example?","class ImplicitCastType(torch.jit.ScriptModule):
    @torch.jit.script_method
    def forward(self, x):
        # Exporter knows x is float32, will export '2' as float32 as well.
        y = x + 2
        # Without type propagation, exporter doesn't know the datatype of y.
        # Thus '3' is exported as int64 by default.
        return y + 3
        # The following will export correctly.
        # return y + torch.tensor([3], dtype=torch.float32)

x = torch.tensor([1.0], dtype=torch.float32)
torch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',
                  example_outputs=ImplicitCastType()(x))",4319
10137,"How to use Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.
Similar to list, Sequence is a data type that contains arbitrary number of Tensors.
Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.
However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace
add operator.
E.g.:, give an example?","class ListLoopModel(torch.nn.Module):
    def forward(self, x):
        res = []
        res1 = []
        arr = x.split(2, 0)
        res2 = torch.zeros(3, 4, dtype=torch.long)
        for i in range(len(arr)):
            res += [arr[i].sum(0, False)]
            res1 += [arr[-1 - i].sum(0, False)]
            res2 += 1
        return torch.stack(res), torch.stack(res1), res2

model = torch.jit.script(ListLoopModel())
inputs = torch.randn(16)

out = model(inputs)
torch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)

# onnxruntime
import onnxruntime
sess = onnxruntime.InferenceSession('loop_and_list.onnx')
out_ort = sess.run(None, {
    sess.get_inputs()[0].name: inputs.numpy(),
})

assert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]",8528
10138,"How to use use_external_data_format argument in export API enables export of models in ONNX external
data format. With this option enabled, the exporter stores some model parameters in external
binary files, rather than the ONNX file itself. These external binary files are stored in the
same location as the ONNX file. Argument ‘f’ must be a string specifying the location of the model., give an example?","model = torchvision.models.mobilenet_v2(pretrained=True)
input = torch.randn(2, 3, 224, 224, requires_grad=True)
torch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)",11316
10139,"How to use torch.onnx.export, give an example?","""args = (x, y, z)""",4559
10140,"How  A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:, give an example?","""args = (x,
        {
        'y': input_y,
        'z': input_z
        })""",773
10141,"How  is exported as:, give an example?","graph(%x.1 : Long(1, strides=[1]))::
  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)
  %y.1 : Long() = prim::ListConstruct(%1)
  return (%y.1)",9703
10142,"How to use Functions, give an example?","shape(input_1) = ('b', 3, 'w', 'h')
and shape(input_2) = ('b', 4)
and shape(output)  = ('b', 'd', 5)",10680
10143,"How  ONLY INDICES:, give an example?","``dynamic_axes = {'input_1':[0, 2, 3],
                  'input_2':[0],
                  'output':[0, 1]}``
where automatic names will be generated for exported dynamic axes",4566
10144,"How  INDICES WITH CORRESPONDING NAMES:, give an example?","``dynamic_axes = {'input_1':{0:'batch',
                             1:'width',
                             2:'height'},
                  'input_2':{0:'batch'},
                  'output':{0:'batch',
                            1:'detections'}}``
where provided names will be applied to exported dynamic axes",3371
10145,"How  MIXED MODE OF (1) and (2):, give an example?","``dynamic_axes = {'input_1':[0, 2, 3],
                  'input_2':{0:'batch'},
                  'output':[0,1]}``",4166
10146,"How to use torch.tril_indices, give an example?",">>> a = torch.tril_indices(3, 3)
>>> a
tensor([[0, 1, 1, 2, 2, 2],
        [0, 0, 1, 0, 1, 2]])

>>> a = torch.tril_indices(4, 3, -1)
>>> a
tensor([[1, 2, 2, 3, 3, 3],
        [0, 0, 1, 0, 1, 2]])

>>> a = torch.tril_indices(4, 3, 1)
>>> a
tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])",6965
10147,"How to use torch.sub, give an example?",">>> a = torch.tensor((1, 2))
>>> b = torch.tensor((0, 1))
>>> torch.sub(a, b, alpha=2)
tensor([1, 0])",6114
10148,"How to use torch.cuda.amp.autocast, give an example?","# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

for input, target in data:
    optimizer.zero_grad()

    # Enables autocasting for the forward pass (model + loss)
    with autocast():
        output = model(input)
        loss = loss_fn(output, target)

    # Exits the context manager before backward()
    loss.backward()
    optimizer.step()",8923
10149,"How to use autocast can also be used as a decorator, e.g., on the forward method of your model:, give an example?","class AutocastModel(nn.Module):
    ...
    @autocast()
    def forward(self, input):
        ...",5781
10150,"How to use Floating-point Tensors produced in an autocast-enabled region may be float16.
After returning to an autocast-disabled region, using them with floating-point
Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
produced in the autocast region back to float32 (or other dtype if desired).
If a Tensor from the autocast region is already float32, the cast is a no-op,
and incurs no additional overhead.  Example:, give an example?","# Creates some tensors in default dtype (here assumed to be float32)
a_float32 = torch.rand((8, 8), device=""cuda"")
b_float32 = torch.rand((8, 8), device=""cuda"")
c_float32 = torch.rand((8, 8), device=""cuda"")
d_float32 = torch.rand((8, 8), device=""cuda"")

with autocast():
    # torch.mm is on autocast's list of ops that should run in float16.
    # Inputs are float32, but the op runs in float16 and produces float16 output.
    # No manual casts are required.
    e_float16 = torch.mm(a_float32, b_float32)
    # Also handles mixed input types
    f_float16 = torch.mm(d_float32, e_float16)

# After exiting autocast, calls f_float16.float() to use with d_float32
g_float32 = torch.mm(d_float32, f_float16.float())",8922
10151,"How to use autocast(enabled=False) subregions can be nested in autocast-enabled regions.
Locally disabling autocast can be useful, for example, if you want to force a subregion
to run in a particular dtype.  Disabling autocast gives you explicit control over
the execution type.  In the subregion, inputs from the surrounding region
should be cast to dtype before use:, give an example?","# Creates some tensors in default dtype (here assumed to be float32)
a_float32 = torch.rand((8, 8), device=""cuda"")
b_float32 = torch.rand((8, 8), device=""cuda"")
c_float32 = torch.rand((8, 8), device=""cuda"")
d_float32 = torch.rand((8, 8), device=""cuda"")

with autocast():
    e_float16 = torch.mm(a_float32, b_float32)

    with autocast(enabled=False):
        # Calls e_float16.float() to ensure float32 execution
        # (necessary because e_float16 was created in an autocasted region)
        f_float32 = torch.mm(c_float32, e_float16.float())

    # No manual casts are required when re-entering the autocast-enabled region.
    # torch.mm again runs in float16 and produces float16 output, regardless of input types.
    g_float16 = torch.mm(d_float32, f_float32)",8034
10152,"How to use torch.cuda.amp.GradScaler.unscale_, give an example?","...
scaler.scale(loss).backward()
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
scaler.step(optimizer)
scaler.update()",11285
10153,"How to use torch.logdet, give an example?",">>> A = torch.randn(3, 3)
>>> torch.det(A)
tensor(0.2611)
>>> torch.logdet(A)
tensor(-1.3430)
>>> A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
>>> A.det()
tensor([1.1990, 0.4099, 0.7386])
>>> A.det().log()
tensor([ 0.1815, -0.8917, -0.3031])",282
10154,"How to use torch.lt, give an example?",">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])",7271
10155,"How to use torch.quantize_per_tensor, give an example?",">>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)
tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
tensor([ 0, 10, 20, 30], dtype=torch.uint8)",425
10156,"How to use torch.square, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-2.0755,  1.0226,  0.0831,  0.4806])
>>> torch.square(a)
tensor([ 4.3077,  1.0457,  0.0069,  0.2310])",325
10157,"How to use torch.take, give an example?",">>> src = torch.tensor([[4, 3, 5],
...                     [6, 7, 8]])
>>> torch.take(src, torch.tensor([0, 2, 5]))
tensor([ 4,  5,  8])",383
10158,"How to use torch.normal, give an example?",">>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,
          8.0505,   8.1408,   9.0563,  10.0566])",7289
10159,"How  Similar to the function above, but the means are shared among all drawn
elements., give an example?",">>> torch.normal(mean=0.5, std=torch.arange(1., 6.))
tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])",5911
10160,"How  Similar to the function above, but the standard deviations are shared among
all drawn elements., give an example?",">>> torch.normal(mean=torch.arange(1., 6.))
tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])",5912
10161,"How  Similar to the function above, but the means and standard deviations are shared
among all drawn elements. The resulting tensor has size given by size., give an example?",">>> torch.normal(2, 3, size=(1, 4))
tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])",5910
10162,"How to use torch.std_mean, give an example?",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std_mean(a, unbiased=False)
(tensor(0.4188), tensor(-0.8509))",3534
10163,"How to use torch.swapdims, give an example?",">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])
>>> x
tensor([[[0, 1],
        [2, 3]],

        [[4, 5],
        [6, 7]]])
>>> torch.swapdims(x, 0, 1)
tensor([[[0, 1],
        [4, 5]],

        [[2, 3],
        [6, 7]]])
>>> torch.swapdims(x, 0, 2)
tensor([[[0, 4],
        [2, 6]],

        [[1, 5],
        [3, 7]]])",7545
10164,"How to use torch.dstack, give an example?",">>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> torch.dstack((a,b))
tensor([[[1, 4],
         [2, 5],
         [3, 6]]])
>>> a = torch.tensor([[1],[2],[3]])
>>> b = torch.tensor([[4],[5],[6]])
>>> torch.dstack((a,b))
tensor([[[1, 4]],
        [[2, 5]],
        [[3, 6]]])",7638
10165,"How to use torch.special.entr, give an example?",">>> a = torch.arange(-0.5, 1, 0.5)
>>> a
tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])",286
10166,"How to use torch.special.erf, give an example?",">>> torch.special.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427,  1.0000])",429
10167,"How to use torch.special.erfc, give an example?",">>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])",430
10168,"How to use torch.special.erfinv, give an example?",">>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000,  0.4769,    -inf])",431
10169,"How to use torch.special.expit, give an example?",">>> t = torch.randn(4)
>>> t
tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
>>> torch.special.expit(t)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])",388
10170,"How to use torch.special.expm1, give an example?",">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])",433
10171,"How to use torch.special.exp2, give an example?",">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))
tensor([ 1.,  2.,  8., 16.])",432
10172,"How to use torch.special.gammaln, give an example?",">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.special.gammaln(a)
tensor([ 0.5724,  0.0000, -0.1208])",288
10173,"How to use torch.special.i0e, give an example?",">>> torch.special.i0e(torch.arange(5, dtype=torch.float32))
tensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])",434
10174,"How to use torch.special.logit, give an example?",">>> a = torch.rand(5)
>>> a
tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])",294
10175,"How to use torch.special.xlog1py, give an example?",">>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlog1py(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlog1py(x, y)
tensor([1.3863, 2.1972, 2.0794])
>>> torch.special.xlog1py(x, 4)
tensor([1.6094, 3.2189, 4.8283])
>>> torch.special.xlog1py(2, y)
tensor([2.7726, 2.1972, 1.3863])",5903
10176,"How  Similar to SciPy’s scipy.special.xlog1py., give an example?",">>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlog1py(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlog1py(x, y)
tensor([1.3863, 2.1972, 2.0794])
>>> torch.special.xlog1py(x, 4)
tensor([1.6094, 3.2189, 4.8283])
>>> torch.special.xlog1py(2, y)
tensor([2.7726, 2.1972, 1.3863])",5903
10177,"How to use torch.isneginf, give an example?",">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
>>> torch.isneginf(a)
tensor([ True, False, False])",332
10178,"How to use torch.tensordot, give an example?",">>> a = torch.arange(60.).reshape(3, 4, 5)
>>> b = torch.arange(24.).reshape(4, 3, 2)
>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))
tensor([[4400., 4730.],
        [4532., 4874.],
        [4664., 5018.],
        [4796., 5162.],
        [4928., 5306.]])

>>> a = torch.randn(3, 4, 5, device='cuda')
>>> b = torch.randn(4, 5, 6, device='cuda')
>>> c = torch.tensordot(a, b, dims=2).cpu()
tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])

>>> a = torch.randn(3, 5, 4, 6)
>>> b = torch.randn(6, 4, 5, 3)
>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))
tensor([[  7.7193,  -2.4867, -10.3204],
        [  1.5513, -14.4737,  -6.5113],
        [ -0.2850,   4.2573,  -3.5997]])",8349
10179,"How to use torch.clamp, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-1.7120,  0.1734, -0.0478, -0.0922])
>>> torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000,  0.1734, -0.0478, -0.0922])

>>> min = torch.linspace(-1, 1, steps=4)
>>> torch.clamp(a, min=min)
tensor([-1.0000,  0.1734,  0.3333,  1.0000])",3452
10180,"How to use torch.renorm, give an example?",">>> x = torch.ones(3, 3)
>>> x[1].fill_(2)
tensor([ 2.,  2.,  2.])
>>> x[2].fill_(3)
tensor([ 3.,  3.,  3.])
>>> x
tensor([[ 1.,  1.,  1.],
        [ 2.,  2.,  2.],
        [ 3.,  3.,  3.]])
>>> torch.renorm(x, 1, 0, 5)
tensor([[ 1.0000,  1.0000,  1.0000],
        [ 1.6667,  1.6667,  1.6667],
        [ 1.6667,  1.6667,  1.6667]])",452
10181,"How to use torch.logcumsumexp, give an example?",">>> a = torch.randn(10)
>>> torch.logcumsumexp(a, dim=0)
tensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,
         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))",2576
10182,"How to use torch.cat, give an example?",">>> x = torch.randn(2, 3)
>>> x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])",11020
10183,"How to use torch.cdist, give an example?",">>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])
>>> a
tensor([[ 0.9041,  0.0196],
        [-0.3108, -2.4423],
        [-0.4821,  1.0590]])
>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])
>>> b
tensor([[-2.1763, -0.4713],
        [-0.6986,  1.3702]])
>>> torch.cdist(a, b, p=2)
tensor([[3.1193, 2.0959],
        [2.7138, 3.8322],
        [2.2830, 0.3791]])",7546
10184,"How to use torch.index_select, give an example?",">>> x = torch.randn(3, 4)
>>> x
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-0.4664,  0.2647, -0.1228, -1.1068],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
>>> indices = torch.tensor([0, 2])
>>> torch.index_select(x, 0, indices)
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
>>> torch.index_select(x, 1, indices)
tensor([[ 0.1427, -0.5414],
        [-0.4664, -0.1228],
        [-1.1734,  0.7230]])",7260
10185,"How to use torch.cholesky_solve, give an example?",">>> a = torch.randn(3, 3)
>>> a = torch.mm(a, a.t()) # make symmetric positive definite
>>> u = torch.cholesky(a)
>>> a
tensor([[ 0.7747, -1.9549,  1.3086],
        [-1.9549,  6.7546, -5.4114],
        [ 1.3086, -5.4114,  4.8733]])
>>> b = torch.randn(3, 2)
>>> b
tensor([[-0.6355,  0.9891],
        [ 0.1974,  1.4706],
        [-0.4115, -0.6225]])
>>> torch.cholesky_solve(b, u)
tensor([[ -8.1625,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
>>> torch.mm(a.inverse(), b)
tensor([[ -8.1626,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])",6133
10186,"How to use torch.argmin, give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
>>> torch.argmin(a)
tensor(13)
>>> torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])
>>> torch.argmin(a, dim=1, keepdim=True)
tensor([[2],
        [1],
        [3],
        [1]])",7694
10187,"How to use where θ\thetaθ are the parameters, α\alphaα is the learning rate,
rrr is the reward and p(a∣πθ(s))p(a|\pi^\theta(s))p(a∣πθ(s)) is the probability of
taking action aaa in state sss given policy πθ\pi^\thetaπθ.In practice we would sample an action from the output of a network, apply this
action in an environment, and then use log_prob to construct an equivalent
loss function. Note that we use a negative because optimizers use gradient
descent, whilst the rule above assumes gradient ascent. With a categorical
policy, the code for implementing REINFORCE would be as follows:, give an example?","probs = policy_network(state)
# Note that this is equivalent to what used to be called multinomial
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()",11379
10188,"How to use The other way to implement these stochastic/policy gradients would be to use the
reparameterization trick from the
rsample() method, where the
parameterized random variable can be constructed via a parameterized
deterministic function of a parameter-free random variable. The reparameterized
sample therefore becomes differentiable. The code for implementing the pathwise
derivative would be as follows:, give an example?","params = policy_network(state)
m = Normal(*params)
# Any distribution with .has_rsample == True could work based on the application
action = m.rsample()
next_state, reward = env.step(action)  # Assuming that reward is differentiable
loss = -reward
loss.backward()",7213
10189,"How to use torch.distributions.bernoulli.Bernoulli, give an example?",">>> m = Bernoulli(torch.tensor([0.3]))
>>> m.sample()  # 30% chance 1; 70% chance 0
tensor([ 0.])",5733
10190,"How to use torch.distributions.beta.Beta, give an example?",">>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))
>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0
tensor([ 0.1046])",1440
10191,"How to use torch.distributions.binomial.Binomial, give an example?",">>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))
>>> x = m.sample()
tensor([   0.,   22.,   71.,  100.])

>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))
>>> x = m.sample()
tensor([[ 4.,  5.],
        [ 7.,  6.]])",1957
10192,"How to use torch.distributions.categorical.Categorical, give an example?",">>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
>>> m.sample()  # equal probability of 0, 1, 2, 3
tensor(3)",5773
10193,"How to use torch.distributions.cauchy.Cauchy, give an example?",">>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1
tensor([ 2.3214])",5736
10194,"How to use torch.distributions.chi2.Chi2, give an example?",">>> m = Chi2(torch.tensor([1.0]))
>>> m.sample()  # Chi2 distributed with shape df=1
tensor([ 0.1046])",1958
10195,"How to use torch.distributions.continuous_bernoulli.ContinuousBernoulli, give an example?",">>> m = ContinuousBernoulli(torch.tensor([0.3]))
>>> m.sample()
tensor([ 0.2538])",7031
10196,"How to use torch.distributions.dirichlet.Dirichlet, give an example?",">>> m = Dirichlet(torch.tensor([0.5, 0.5]))
>>> m.sample()  # Dirichlet distributed with concentrarion concentration
tensor([ 0.1046,  0.8954])",1959
10197,"How to use torch.distributions.exponential.Exponential, give an example?",">>> m = Exponential(torch.tensor([1.0]))
>>> m.sample()  # Exponential distributed with rate=1
tensor([ 0.1046])",1960
10198,"How to use torch.distributions.fishersnedecor.FisherSnedecor, give an example?",">>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))
>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2
tensor([ 0.2453])",1961
10199,"How to use torch.distributions.gamma.Gamma, give an example?",">>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample()  # Gamma distributed with concentration=1 and rate=1
tensor([ 0.1046])",1962
10200,"How to use torch.distributions.geometric.Geometric, give an example?",">>> m = Geometric(torch.tensor([0.3]))
>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0
tensor([ 2.])",5734
10201,"How to use torch.distributions.gumbel.Gumbel, give an example?",">>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))
>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2
tensor([ 1.0124])",5737
10202,"How to use Creates a half-Cauchy distribution parameterized by scale where:, give an example?","X ~ Cauchy(0, scale)
Y = |X| ~ HalfCauchy(scale)",1991
10203,"How to use torch.distributions.half_cauchy.HalfCauchy, give an example?",">>> m = HalfCauchy(torch.tensor([1.0]))
>>> m.sample()  # half-cauchy distributed with scale=1
tensor([ 2.3214])",1990
10204,"How to use Creates a half-normal distribution parameterized by scale where:, give an example?","X ~ Normal(0, scale)
Y = |X| ~ HalfNormal(scale)",1993
10205,"How to use torch.distributions.half_normal.HalfNormal, give an example?",">>> m = HalfNormal(torch.tensor([1.0]))
>>> m.sample()  # half-normal distributed with scale=1
tensor([ 0.1046])",1992
10206,"How to use This is mainly useful for changing the shape of the result of
log_prob(). For example to create a diagonal Normal distribution with
the same shape as a Multivariate Normal distribution (so they are
interchangeable), you can:, give an example?",">>> loc = torch.zeros(3)
>>> scale = torch.ones(3)
>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))
>>> [mvn.batch_shape, mvn.event_shape]
[torch.Size(()), torch.Size((3,))]
>>> normal = Normal(loc, scale)
>>> [normal.batch_shape, normal.event_shape]
[torch.Size((3,)), torch.Size(())]
>>> diagn = Independent(normal, 1)
>>> [diagn.batch_shape, diagn.event_shape]
[torch.Size(()), torch.Size((3,))]",5130
10207,"How to use torch.distributions.kumaraswamy.Kumaraswamy, give an example?",">>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1
tensor([ 0.1729])",5738
10208,"How to use torch.distributions.lkj_cholesky.LKJCholesky, give an example?",">>> l = LKJCholesky(3, 0.5)
>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix
tensor([[ 1.0000,  0.0000,  0.0000],
        [ 0.3516,  0.9361,  0.0000],
        [-0.1899,  0.4748,  0.8593]])",4057
10209,"How to use torch.distributions.laplace.Laplace, give an example?",">>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample()  # Laplace distributed with loc=0, scale=1
tensor([ 0.1046])",1963
10210,"How to use Creates a log-normal distribution parameterized by
loc and scale where:, give an example?","X ~ Normal(loc, scale)
Y = exp(X) ~ LogNormal(loc, scale)",1995
10211,"How to use torch.distributions.log_normal.LogNormal, give an example?",">>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample()  # log-normal distributed with mean=0 and stddev=1
tensor([ 0.1046])",1994
10212,"How to use Creates a multivariate normal distribution with covariance matrix having a low-rank form
parameterized by cov_factor and cov_diag:, give an example?",covariance_matrix = cov_factor @ cov_factor.T + cov_diag,1997
10213,"How to use torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal, give an example?",">>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))
>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`
tensor([-0.2102, -0.5429])",1996
10214,"How to use The computation for determinant and inverse of covariance matrix is avoided when
cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and
matrix determinant lemma.
Thanks to these formulas, we just need to compute the determinant and inverse of
the small size “capacitance” matrix:, give an example?",capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor,6996
10215,"How to use torch.distributions.mixture_same_family.MixtureSameFamily, give an example?","# Construct Gaussian Mixture Model in 1D consisting of 5 equally
# weighted normal distributions
>>> mix = D.Categorical(torch.ones(5,))
>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))
>>> gmm = MixtureSameFamily(mix, comp)

# Construct Gaussian Mixture Modle in 2D consisting of 5 equally
# weighted bivariate normal distributions
>>> mix = D.Categorical(torch.ones(5,))
>>> comp = D.Independent(D.Normal(
             torch.randn(5,2), torch.rand(5,2)), 1)
>>> gmm = MixtureSameFamily(mix, comp)

# Construct a batch of 3 Gaussian Mixture Models in 2D each
# consisting of 5 random weighted bivariate normal distributions
>>> mix = D.Categorical(torch.rand(3,5))
>>> comp = D.Independent(D.Normal(
            torch.randn(3,5,2), torch.rand(3,5,2)), 1)
>>> gmm = MixtureSameFamily(mix, comp)",6928
10216,"How to use torch.distributions.multinomial.Multinomial, give an example?",">>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))
>>> x = m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 21.,  24.,  30.,  25.])

>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)
tensor([-4.1338])",4498
10217,"How to use torch.distributions.multivariate_normal.MultivariateNormal, give an example?",">>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))
>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
tensor([-0.2102, -0.5429])",7191
10218,"How to use torch.distributions.normal.Normal, give an example?",">>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
>>> m.sample()  # normally distributed with loc=0 and scale=1
tensor([ 0.1046])",2009
10219,"How to use torch.distributions.one_hot_categorical.OneHotCategorical, give an example?",">>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
>>> m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 0.,  0.,  0.,  1.])",5772
10220,"How to use torch.distributions.pareto.Pareto, give an example?",">>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1
tensor([ 1.5623])",5739
10221,"How to use torch.distributions.poisson.Poisson, give an example?",">>> m = Poisson(torch.tensor([4]))
>>> m.sample()
tensor([ 3.])",5735
10222,"How to use torch.distributions.relaxed_bernoulli.RelaxedBernoulli, give an example?",">>> m = RelaxedBernoulli(torch.tensor([2.2]),
                         torch.tensor([0.1, 0.2, 0.3, 0.99]))
>>> m.sample()
tensor([ 0.2951,  0.3442,  0.8918,  0.9021])",1964
10223,"How to use torch.distributions.relaxed_categorical.RelaxedOneHotCategorical, give an example?",">>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),
                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))
>>> m.sample()
tensor([ 0.1294,  0.2324,  0.3859,  0.2523])",1965
10224,"How to use torch.distributions.studentT.StudentT, give an example?",">>> m = StudentT(torch.tensor([2.0]))
>>> m.sample()  # Student's t-distributed with degrees of freedom=2
tensor([ 0.1046])",1966
10225,"How to use Extension of the Distribution class, which applies a sequence of Transforms
to a base distribution.  Let f be the composition of transforms applied:, give an example?","X ~ BaseDistribution
Y = f(X) ~ TransformedDistribution(BaseDistribution, f)
log p(Y) = log p(X) + log |det (dX/dY)|",2363
10226,"How to use An example for the usage of TransformedDistribution would be:, give an example?","# Building a Logistic Distribution
# X ~ Uniform(0, 1)
# f = a + b * logit(X)
# Y ~ f(X) ~ Logistic(a, b)
base_distribution = Uniform(0, 1)
transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]
logistic = TransformedDistribution(base_distribution, transforms)",4481
10227,"How to use torch.distributions.uniform.Uniform, give an example?",">>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))
>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)
tensor([ 2.3418])",2686
10228,"How to use VonMises, give an example?",">>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample() # von Mises distributed with loc=1 and concentration=1
tensor([1.9777])",377
10229,"How to use torch.distributions.weibull.Weibull, give an example?",">>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))
>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1
tensor([ 0.4784])",5740
10230,"How to use torch.distributions.kl.register_kl, give an example?","@register_kl(Normal, Normal)
def kl_normal_normal(p, q):
    # insert implementation here",2086
10231,"How  Lookup returns the most specific (type,type) match ordered by subclass. If
the match is ambiguous, a RuntimeWarning is raised. For example to
resolve the ambiguous situation:, give an example?","@register_kl(BaseP, DerivedQ)
def kl_version1(p, q): ...
@register_kl(DerivedP, BaseQ)
def kl_version2(p, q): ...",4157
10232,"How  Lookup returns the most specific (type,type) match ordered by subclass. If
the match is ambiguous, a RuntimeWarning is raised. For example to
resolve the ambiguous situation:you should register a third most-specific implementation, e.g.:, give an example?","register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.",4158
10233,"How to use Caching is useful for transforms whose inverses are either expensive or
numerically unstable. Note that care must be taken with memoized values
since the autograd graph may be reversed. For example while the following
works with or without caching:, give an example?","y = t(x)
t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.",1511
10234,"How to use However the following will error when caching due to dependency reversal:, give an example?","y = t(x)
z = t.inv(y)
grad(z.sum(), [y])  # error because z is x",1512
10235,"How to use PyTorch provides two global ConstraintRegistry objects that link
Constraint objects to
Transform objects. These objects both
input constraints and return transforms, but they have different guarantees on
bijectivity.The transform_to() registry is useful for performing unconstrained
optimization on constrained parameters of probability distributions, which are
indicated by each distribution’s .arg_constraints dict. These transforms often
overparameterize a space in order to avoid rotation; they are thus more
suitable for coordinate-wise optimization algorithms like Adam:, give an example?","loc = torch.zeros(100, requires_grad=True)
unconstrained = torch.zeros(100, requires_grad=True)
scale = transform_to(Normal.arg_constraints['scale'])(unconstrained)
loss = -Normal(loc, scale).log_prob(data).sum()",7344
10236,"How to use The transform_to() registry is useful for performing unconstrained
optimization on constrained parameters of probability distributions, which are
indicated by each distribution’s .arg_constraints dict. These transforms often
overparameterize a space in order to avoid rotation; they are thus more
suitable for coordinate-wise optimization algorithms like Adam:The biject_to() registry is useful for Hamiltonian Monte Carlo, where
samples from a probability distribution with constrained .support are
propagated in an unconstrained space, and algorithms are typically rotation
invariant.:, give an example?","dist = Exponential(rate)
unconstrained = torch.zeros(100, requires_grad=True)
sample = biject_to(dist.support)(unconstrained)
potential_energy = -dist.log_prob(sample).sum()",7345
10237,"How to use The biject_to() registry is useful for Hamiltonian Monte Carlo, where
samples from a probability distribution with constrained .support are
propagated in an unconstrained space, and algorithms are typically rotation
invariant.:The biject_to and transform_to objects can be extended by user-defined
constraints and transforms using their .register() method either as a
function on singleton constraints:, give an example?","transform_to.register(my_constraint, my_transform)",6977
10238,"How to use The biject_to and transform_to objects can be extended by user-defined
constraints and transforms using their .register() method either as a
function on singleton constraints:or as a decorator on parameterized constraints:, give an example?","@transform_to.register(MyConstraintClass)
def my_factory(constraint):
    assert isinstance(constraint, MyConstraintClass)
    return MyTransform(constraint.param1, constraint.param2)",6976
10239,"How to use torch.distributions.constraint_registry.ConstraintRegistry.register, give an example?","@my_registry.register(MyConstraintClass)
def construct_transform(constraint):
    assert isinstance(constraint, MyConstraint)
    return MyTransform(constraint.arg_constraints)",5124
10240,"How to use torch.quasirandom.SobolEngine, give an example?",">>> soboleng = torch.quasirandom.SobolEngine(dimension=5)
>>> soboleng.draw(3)
tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])",5117
10241,"How to use torch.unique_consecutive, give an example?",">>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
>>> output = torch.unique_consecutive(x)
>>> output
tensor([1, 2, 3, 1, 2])

>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
>>> output
tensor([1, 2, 3, 1, 2])
>>> inverse_indices
tensor([0, 0, 1, 1, 2, 3, 3, 4])

>>> output, counts = torch.unique_consecutive(x, return_counts=True)
>>> output
tensor([1, 2, 3, 1, 2])
>>> counts
tensor([2, 2, 1, 2, 1])",460
10242,"How to use torch.signbit, give an example?",">>> a = torch.tensor([0.7, -1.2, 0., 2.3])
>>> torch.signbit(a)
tensor([ False, True,  False,  False])",335
10243,"How to use torch.isinf, give an example?",">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])",414
10244,"How to use torch.log1p, give an example?",">>> a = torch.randn(5)
>>> a
tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
>>> torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])",329
10245,"How to use torch.randperm, give an example?",">>> torch.randperm(4)
tensor([2, 1, 0, 3])",426
10246,"How to use torch.diff, give an example?",">>> a = torch.tensor([1, 3, 2])
>>> torch.diff(a)
tensor([ 2, -1])
>>> b = torch.tensor([4, 5])
>>> torch.diff(a, append=b)
tensor([ 2, -1,  2,  1])
>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])
>>> torch.diff(c, dim=0)
tensor([[2, 2, 2]])
>>> torch.diff(c, dim=1)
tensor([[1, 1],
        [1, 1]])",7065
10247,"How to use torch.bernoulli, give an example?",">>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
>>> a
tensor([[ 0.1737,  0.0950,  0.3609],
        [ 0.7148,  0.0289,  0.2676],
        [ 0.9456,  0.8937,  0.7202]])
>>> torch.bernoulli(a)
tensor([[ 1.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 1.,  1.,  1.]])

>>> a = torch.ones(3, 3) # probability of drawing ""1"" is 1
>>> torch.bernoulli(a)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
>>> a = torch.zeros(3, 3) # probability of drawing ""1"" is 0
>>> torch.bernoulli(a)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])",10345
10248,"How to use torch.amin, give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.6451, -0.4866,  0.2987, -1.3312],
        [-0.5744,  1.2980,  1.8397, -0.2713],
        [ 0.9128,  0.9214, -1.7268, -0.2995],
        [ 0.9023,  0.4853,  0.9075, -1.6165]])
>>> torch.amin(a, 1)
tensor([-1.3312, -0.5744, -1.7268, -1.6165])",3442
10249,"How to use torch.equal, give an example?",">>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
True",405
10250,"How to use torch.logical_not, give an example?",">>> torch.logical_not(torch.tensor([True, False]))
tensor([False,  True])
>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))
tensor([ True, False, False])
>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))
tensor([ True, False, False])
>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))
tensor([1, 0, 0], dtype=torch.int16)",419
10251,"How to use torch.acos, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
>>> torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])",311
10252,"How to use torch.abs, give an example?",">>> torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])",395
10253,"How to use torch.from_numpy, give an example?",">>> a = numpy.array([1, 2, 3])
>>> t = torch.from_numpy(a)
>>> t
tensor([ 1,  2,  3])
>>> t[0] = -1
>>> a
array([-1,  2,  3])",3905
10254,"How to use torch.ravel, give an example?",">>> t = torch.tensor([[[1, 2],
...                    [3, 4]],
...                   [[5, 6],
...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])",392
10255,"How to use torch.sinc, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.2252, -0.2948,  1.0267, -1.1566])
>>> torch.sinc(a)
tensor([ 0.9186,  0.8631, -0.0259, -0.1300])",309
10256,"How to use String to distinguish measurements with identical label and
sub_label. The principal use of description is to signal to
Compare the columns of data. For instance one might set it
based on the input size  to create a table of the form:, give an example?","| n=1 | n=4 | ...
                        ------------- ...
ReLU(x + 1): (float)    | ... | ... | ...
ReLU(x + 1): (int)      | ... | ... | ...",6080
10257,"How to use torch.utils.benchmark.Timer.blocked_autorange, give an example?","`setup`

total_time = 0
while total_time < min_run_time
    start = timer()
    for _ in range(block_size):
        `stmt`
    total_time += (timer() - start)",1343
10258,"How to use torch.utils.benchmark.CallgrindStats.as_standardized, give an example?","23234231 /tmp/first_build_dir/thing.c:foo(...)
 9823794 /tmp/first_build_dir/thing.c:bar(...)
  ...
   53453 .../aten/src/Aten/...:function_that_actually_changed(...)
  ...
 -9823794 /tmp/second_build_dir/thing.c:bar(...)
-23234231 /tmp/second_build_dir/thing.c:foo(...)",8350
10259,"How to use torch.save, give an example?",">>> # Save to file
>>> x = torch.tensor([0, 1, 2, 3, 4])
>>> torch.save(x, 'tensor.pt')
>>> # Save to io.BytesIO buffer
>>> buffer = io.BytesIO()
>>> torch.save(x, buffer)",5771
10260,"How to use torch.set_grad_enabled, give an example?",">>> x = torch.tensor([1], requires_grad=True)
>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...   y = x * 2
>>> y.requires_grad
False
>>> torch.set_grad_enabled(True)
>>> y = x * 2
>>> y.requires_grad
True
>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False",7481
10261,"How to use torch.combinations, give an example?",">>> a = [1, 2, 3]
>>> list(itertools.combinations(a, r=2))
[(1, 2), (1, 3), (2, 3)]
>>> list(itertools.combinations(a, r=3))
[(1, 2, 3)]
>>> list(itertools.combinations_with_replacement(a, r=2))
[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
>>> tensor_a = torch.tensor(a)
>>> torch.combinations(tensor_a)
tensor([[1, 2],
        [1, 3],
        [2, 3]])
>>> torch.combinations(tensor_a, r=3)
tensor([[1, 2, 3]])
>>> torch.combinations(tensor_a, with_replacement=True)
tensor([[1, 1],
        [1, 2],
        [1, 3],
        [2, 2],
        [2, 3],
        [3, 3]])",284
10262,"How to use torch.mv, give an example?",">>> mat = torch.randn(2, 3)
>>> vec = torch.randn(3)
>>> torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])",3423
10263,"How to use torch.amax, give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.8177,  1.4878, -0.2491,  0.9130],
        [-0.7158,  1.1775,  2.0992,  0.4817],
        [-0.0053,  0.0164, -1.3738, -0.0507],
        [ 1.9700,  1.1106, -1.0318, -1.0816]])
>>> torch.amax(a, 1)
tensor([1.4878, 2.0992, 0.0164, 1.9700])",3445
10264,"How to use torch.numel, give an example?",">>> a = torch.randn(1, 2, 3, 4, 5)
>>> torch.numel(a)
120
>>> a = torch.zeros(4,4)
>>> torch.numel(a)
16",296
10265,"How to use torch.logspace, give an example?",">>> torch.logspace(start=-10, end=10, steps=5)
tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])
>>> torch.logspace(start=0.1, end=1.0, steps=5)
tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])
>>> torch.logspace(start=0.1, end=1.0, steps=1)
tensor([1.2589])
>>> torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])",422
10266,"How to use torch.acosh, give an example?",">>> a = torch.randn(4).uniform_(1, 2)
>>> a
tensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])
>>> torch.acosh(a)
tensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])",327
10267,"How to use The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should
work for exploring the contents. Some common ways to interact with ZIP files:, give an example?","$ unzip my_package.pt && tree my_package
my_package
├── .data
│   ├── 94304870911616.storage
│   ├── 94304900784016.storage
│   ├── extern_modules
│   └── version
├── models
│   └── model_1.pkl
└── torchvision
    └── models
        ├── resnet.py
        └── utils.py
~ cd my_package && cat torchvision/models/resnet.py
...",7002
10268,"How  The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should
work for exploring the contents. Some common ways to interact with ZIP files:, give an example?","# add this to your .vimrc to treat `*.pt` files as zip files
au BufReadCmd *.pt call zip#Browse(expand(""<amatch>""))

~ vi my_package.pt",7001
10269,"How to use PackageImporter and PackageExporter provide a file_structure() method, which will return a printable
and queryable Folder object. The Folder object is a simple directory structure that you can use to explore the
current contents of a torch.package.The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,
use the glob-style include and exclude filtering arguments., give an example?","with PackageExporter('my_package.pt', verbose=False) as pe:
    pe.save_pickle('models', 'model_1.pkl', mod)
    # can limit printed items with include/exclude args
    print(pe.file_structure(include=[""**/utils.py"", ""**/*.pkl""], exclude=""**/*.storages""))

importer = PackageImporter('my_package.pt')
print(importer.file_structure()) # will print out all files",6922
10270,"How to use The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,
use the glob-style include and exclude filtering arguments.Output:, give an example?","# filtered with glob pattern:
#    include=[""**/utils.py"", ""**/*.pkl""], exclude=""**/*.storages""
─── my_package.pt
    ├── models
    │   └── model_1.pkl
    └── torchvision
        └── models
            └── utils.py

# all files
─── my_package.pt
    ├── .data
    │   ├── 94304870911616.storage
    │   ├── 94304900784016.storage
    │   ├── extern_modules
    │   └── version
    ├── models
    │   └── model_1.pkl
    └── torchvision
        └── models
            ├── resnet.py
            └── utils.py",6923
10271,"How to use Output:You can also query Folder objects with the has_file() method., give an example?","exporter_file_structure = exporter.file_structure()
found: bool = exporter_file_structure.has_file(""package_a/subpackage.py"")",4699
10272,"How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save
Python objects, text, and binary data to a package., give an example?","with torch.PackageExporter(""package.pt"") as exporter:
    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.
    exporter.save_pickle(""my_resources"", ""tensor.pkl"", torch.randn(4))
    exporter.save_text(""config_stuff"", ""words.txt"", ""a sample string"")
    exporter.save_binary(""raw_data"", ""binary"", my_bytes)",4714
10273,"How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save
Python objects, text, and binary data to a package.PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load
Python objects, text and binary data from a package., give an example?","importer = torch.PackageImporter(""package.pt"")
my_tensor = importer.load_pickle(""my_resources"", ""tensor.pkl"")
text = importer.load_text(""config_stuff"", ""words.txt"")
binary = importer.load_binary(""raw_data"", ""binary"")",4716
10274,"How to use torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method
__reduce_package__ on a class and by defining a corresponding de-packaging function. This is similar to defining __reduce__ for
Python’s normal pickling process.Steps:, give an example?","# foo.py [Example of customizing how class Foo is packaged]
from torch.package import PackageExporter, PackageImporter
import time


class Foo:
    def __init__(self, my_string: str):
        super().__init__()
        self.my_string = my_string
        self.time_imported = 0
        self.time_exported = 0

    def __reduce_package__(self, exporter: PackageExporter):
        """"""
        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when
        saving an instance of this object. This method should do the work to save this
        object inside of the ``torch.package`` archive.

        Returns function w/ arguments to load the object from a
        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.
        """"""

        # use this pattern to ensure no naming conflicts with normal dependencies,
        # anything saved under this module name shouldn't conflict with other
        # items in the package
        generated_module_name = f""foo-generated._{exporter.get_unique_id()}""
        exporter.save_text(
            generated_module_name,
            ""foo.txt"",
            self.my_string + "", with exporter modification!"",
        )
        time_exported = time.clock_gettime(1)

        # returns de-packaging function w/ arguments to invoke with
        return (unpackage_foo, (generated_module_name, time_exported,))


def unpackage_foo(
    importer: PackageImporter, generated_module_name: str, time_exported: float
) -> Foo:
    """"""
    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function
    when depickling a Foo object.
    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.
    """"""
    time_imported = time.clock_gettime(1)
    foo = Foo(importer.load_text(generated_module_name, ""foo.txt""))
    foo.time_imported = time_imported
    foo.time_exported = time_exported
    return foo",6075
10275,"How  Steps:, give an example?","# output of running above script
─── foo_package
    ├── foo-generated
    │   ├── _0
    │   │   └── foo.txt
    │   └── _1
    │       └── foo.txt
    ├── foo_collection
    │   ├── foo1.pkl
    │   └── foo2.pkl
    └── foo.py

foo_1 string: 'foo_1 initial string, with reduction modification!'
foo_1 export time: 9857706.650140837
foo_1 import time: 9857706.652698385",6076
10276,"How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the
presence of this attribute to determine whether it is executing in a packaged context or not., give an example?","# In foo/bar.py:

if ""__torch_package__"" in dir():  # true if the code is being loaded from a package
    def is_in_package():
        return True

    UserException = Exception
else:
    def is_in_package():
        return False

    UserException = UnpackageableException",771
10277,"How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the
presence of this attribute to determine whether it is executing in a packaged context or not.Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a
torch.package., give an example?","from foo.bar import is_in_package

print(is_in_package())  # False

loaded_module = PackageImporter(my_pacakge).import_module(""foo.bar"")
loaded_module.is_in_package()  # True",4543
10278,"How to use PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing., give an example?","with PackageExporter(f) as exporter:
    # Save the my_module.foo available in your current Python environment.
    exporter.save_module(""my_module.foo"")

    # This saves the provided string to my_module/foo.py in the package archive.
    # It will override the my_module.foo that was previously saved.
    exporter.save_source_string(""my_module.foo"", textwrap.dedent(
        """"""\
        def my_function():
            print('hello world')
        """"""
    ))

    # If you want to treat my_module.bar as a package
    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)
    # pass is_package=True,
    exporter.save_source_string(""my_module.bar"",
                                ""def foo(): print('hello')\n"",
                                is_package=True)

importer = PackageImporter(f)
importer.import_module(""my_module.foo"").my_function()  # prints 'hello world'",4715
10279,"How to use PackageImporter implements the
importlib.resources
API for accessing resources from inside a package., give an example?","with PackageExporter(f) as exporter:
    # saves text to one/a.txt in the archive
    exporter.save_text(""my_resource"", ""a.txt"", ""hello world!"")
    # saves the tensor to my_pickle/obj.pkl
    exporter.save_pickle(""my_pickle"", ""obj.pkl"", torch.ones(2, 2))

    # see below for module contents
    exporter.save_module(""foo"")
    exporter.save_module(""bar"")",4717
10280,"How to use PackageImporter implements the
importlib.resources
API for accessing resources from inside a package.The importlib.resources API allows access to resources from within packaged code., give an example?","# foo.py:
import importlib.resources
import my_resource

# returns ""hello world!""
def get_my_resource():
    return importlib.resources.read_text(my_resource, ""a.txt"")",7122
10281,"How to use The importlib.resources API allows access to resources from within packaged code.Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies
with the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within
packaged code., give an example?","# bar.py:
import torch_package_importer # this is the PackageImporter that imported this module.

# Prints ""hello world!"", equivalient to importlib.resources.read_text
def get_my_resource():
    return torch_package_importer.load_text(""my_resource"", ""a.txt"")

# You also do things that the importlib.resources API does not support, like loading
# a pickled object from the package.
def get_my_pickle():
    return torch_package_importer.load_pickle(""my_pickle"", ""obj.pkl"")",7123
10282,"How to use To tell if an object’s code is from a torch.package, use the torch.package.is_from_package() function.
Note: if an object is from a package but its definition is from a module marked extern or from stdlib,
this check will return False., give an example?","importer = PackageImporter(f)
mod = importer.import_module('foo')
obj = importer.load_pickle('model', 'model.pkl')
txt = importer.load_text('text', 'my_test.txt')

assert is_from_package(mod)
assert is_from_package(obj)
assert not is_from_package(txt) # str is from stdlib, so this will return False",7923
10283,"How to use To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter
aware of the original PackageImporter so that it can find source code for your object’s dependencies., give an example?","importer = PackageImporter(f)
obj = importer.load_pickle(""model"", ""model.pkl"")

# re-export obj in a new package
with PackageExporter(f2, importer=(importer, sys_importer)) as exporter:
    exporter.save_pickle(""model"", ""model.pkl"", obj)",7912
10284,"How to use To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object.
Saving TorchScript objects that are attributes or submodules is supported as well with no extra work., give an example?","# save TorchScript just like any other object
with PackageExporter(file_name, verbose=True) as e:
    e.save_pickle(""res"", ""script_model.pkl"", scripted_model)
    e.save_pickle(""res"", ""mixed_model.pkl"", python_model_with_scripted_submodule)
# load as normal
importer = PackageImporter(file_name)
loaded_script = importer.load_pickle(""res"", ""script_model.pkl"")
loaded_mixed = importer.load_pickle(""res"", ""mixed_model.pkl""",7909
10285,"How to use A torch.package file is a ZIP archive which conventionally uses the .pt extension. Inside the ZIP archive, there are two kinds of files:As an example, this is what a fully packaged ResNet model from torchvision looks like:, give an example?","resnet
├── .data  # All framework-specific data is stored here.
│   │      # It's named to avoid conflicts with user-serialized code.
│   ├── 94286146172688.storage  # tensor data
│   ├── 94286146172784.storage
│   ├── extern_modules  # text file with names of extern modules (e.g. 'torch')
│   ├── version         # version metadata
│   ├── ...
├── model  # the pickled model
│   └── model.pkl
└── torchvision  # all code dependencies are captured as source files
    └── models
        ├── resnet.py
        └── utils.py",1293
10286,"How to use The .data/ directory is owned by torch.package, and its contents are considered to be a private implementation detail.
The torch.package format makes no guarantees about the contents of .data/, but any changes made will be backward compatible
(that is, newer version of PyTorch will always be able to load older torch.packages).Currently, the .data/ directory contains the following items:, give an example?",".data
├── 94286146172688.storage
├── 94286146172784.storage
├── extern_modules
├── version
├── ...",2053
10287,"How to use All other files in the archive were put there by a user. The layout is identical to a Python
regular package. For a deeper dive in how Python packaging works,
please consult this essay (it’s slightly out of date, so double-check implementation details
with the Python reference documentation)., give an example?","<package root>
├── model  # the pickled model
│   └── model.pkl
├── another_package
│   ├── __init__.py
│   ├── foo.txt         # a resource file , see importlib.resources
│   └── ...
└── torchvision
    └── models
        ├── resnet.py   # torchvision.models.resnet
        └── utils.py    # torchvision.models.utils",1088
10288,"How to use When you issue a save_pickle(obj, ...) call, PackageExporter will pickle the object normally. Then, it uses the
pickletools standard library module to parse the pickle bytecode.In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object’s type, like:, give an example?",GLOBAL 'torchvision.models.resnet Resnet`,3690
10289,"How to use Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from module and leave the rest out.
This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a
module, so that’s what torch.package uses.Actions are applied to modules using patterns. Patterns can either be module names (""foo.bar"") or globs (like ""foo.**""). You associate a pattern
with an action using methods on PackageImporter, e.g., give an example?","my_exporter.intern(""torchvision.**"")
my_exporter.extern(""numpy"")",4458
10290,"How to use When specifying actions, you can pass multiple patterns, e.g., give an example?","exporter.intern([""torchvision.models.**"", ""torchvision.utils.**""])",8378
10291,"How to use A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g., give an example?","exporter.mock(""**"", exclude=[""torchvision.**""])",814
10292,"How to use Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example:, give an example?","from foo import MyClass

my_class_instance = MyClass()

with PackageExporter(f) as exporter:
    exporter.save_module(""foo"")

importer = PackageImporter(f)
imported_MyClass = importer.import_module(""foo"").MyClass

assert isinstance(my_class_instance, MyClass)  # works
assert isinstance(my_class_instance, imported_MyClass)  # ERROR!",1149
10293,"How to use In this example, MyClass and import_MyClass are not the same type. In this specific example, MyClass and import_MyClass have exactly the
same implementation, so you might thing it’s okay to consider them the same class. But consider the situation where import_MyClass is coming from an
older package with an entirely different implementation of MyClass — in that case, it’s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:, give an example?","print(MyClass.__name__)  # prints ""foo.MyClass""
print(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass",3803
10294,"How to use torch.package.PackageExporter.close, give an example?","with PackageExporter(""file.zip"") as e:
    ...",8444
10295,"How to use torch.package.PackageExporter.register_extern_hook, give an example?","hook(exporter: PackageExporter, module_name: str) -> None",7103
10296,"How to use torch.package.PackageExporter.register_intern_hook, give an example?","hook(exporter: PackageExporter, module_name: str) -> None",7104
10297,"How to use torch.package.PackageExporter.register_mock_hook, give an example?","hook(exporter: PackageExporter, module_name: str) -> None",7102
10298,"How to use torch.package.PackageImporter.id, give an example?",<torch_package_0>,5507
10299,"How to use torch.cumsum, give an example?",">>> a = torch.randn(10)
>>> a
tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
         0.1850, -1.1571, -0.4243])
>>> torch.cumsum(a, dim=0)
tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
        -1.8209, -2.9780, -3.4022])",2517
10300,"How to use torch.linalg.matrix_power, give an example?","matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B",1826
10301,"How  If n= 0, it returns the identity matrix (or batch) of the same shape
as A. If n is negative, it returns the inverse of each matrix
(if invertible) raised to the power of abs(n)., give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[-0.2270,  0.6663, -1.3515],
        [-0.9838, -0.4002, -1.9313],
        [-0.7886, -0.0450,  0.0528]])
>>> torch.linalg.matrix_power(a, 0)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
>>> torch.linalg.matrix_power(a, 3)
tensor([[ 1.0756,  0.4980,  0.0100],
        [-1.6617,  1.4994, -1.9980],
        [-0.4509,  0.2731,  0.8001]])
>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)
tensor([[[ 0.2640,  0.4571, -0.5511],
        [-1.0163,  0.3491, -1.5292],
        [-0.4899,  0.0822,  0.2773]],
        [[ 0.2640,  0.4571, -0.5511],
        [-1.0163,  0.3491, -1.5292],
        [-0.4899,  0.0822,  0.2773]]])",3456
10302,"How to use torch.profiler.profile, give an example?","with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    code_to_profile()
print(p.key_averages().table(
    sort_by=""self_cuda_time_total"", row_limit=-1))",11411
10303,"How to use Using the profiler’s schedule, on_trace_ready and step functions:, give an example?","# Non-default profiler schedule allows user to turn profiler on and off
# on different iterations of the training loop;
# trace_handler is called every time a new trace becomes available
def trace_handler(prof):
    print(prof.key_averages().table(
        sort_by=""self_cuda_time_total"", row_limit=-1))
    # prof.export_chrome_trace(""/tmp/test_trace_"" + str(prof.step_num) + "".json"")

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],

    # In this example with wait=1, warmup=1, active=2,
    # profiler will skip the first step/iteration,
    # start warming up on the second, record
    # the third and the forth iterations,
    # after which the trace will become available
    # and on_trace_ready (when set) is called;
    # the cycle repeats starting with the next step

    schedule=torch.profiler.schedule(
        wait=1,
        warmup=1,
        active=2),
    on_trace_ready=trace_handler
    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')
    # used when outputting for tensorboard
    ) as p:
        for iter in range(N):
            code_iteration_to_profile(iter)
            # send a signal to the profiler that the next iteration has started
            p.step()",8115
10304,"How to use torch.unique, give an example?",">>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
>>> output
tensor([ 2,  3,  1])

>>> output, inverse_indices = torch.unique(
...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
>>> output
tensor([ 1,  2,  3])
>>> inverse_indices
tensor([ 0,  2,  1,  2])

>>> output, inverse_indices = torch.unique(
...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
>>> output
tensor([ 1,  2,  3])
>>> inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])",378
10305,"How to use torch.asin, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.5962,  1.4985, -0.4396,  1.4525])
>>> torch.asin(a)
tensor([-0.6387,     nan, -0.4552,     nan])",320
10306,"How to use To construct an Optimizer you have to give it an iterable containing the
parameters (all should be Variable s) to optimize. Then,
you can specify optimizer-specific options such as the learning rate, weight decay, etc., give an example?","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)",10317
10307,"How to use Optimizer s also support specifying per-parameter options. To do this, instead
of passing an iterable of Variable s, pass in an iterable of
dict s. Each of them will define a separate parameter group, and should contain
a params key, containing a list of parameters belonging to it. Other keys
should match the keyword arguments accepted by the optimizers, and will be used
as optimization options for this group.For example, this is very useful when one wants to specify per-layer learning rates:, give an example?","optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9)",2538
10308,"How to use This is a simplified version supported by most optimizers. The function can be
called once the gradients are computed using e.g.
backward()., give an example?","for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()",9374
10309,"How to use Some optimization algorithms such as Conjugate Gradient and LBFGS need to
reevaluate the function multiple times, so you have to pass in a closure that
allows them to recompute your model. The closure should clear the gradients,
compute the loss, and return it., give an example?","for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)",9373
10310,"How to use Learning rate scheduling should be applied after optimizer’s update; e.g., you
should write your code this way:, give an example?","model = [Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()",4088
10311,"How to use Most learning rate schedulers can be called back-to-back (also referred to as
chaining schedulers). The result is that each scheduler is applied one after the
other on the learning rate obtained by the one preceding it., give an example?","model = [Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler1.step()
    scheduler2.step()",4270
10312,"How to use In many places in the documentation, we will use the following template to refer to schedulers
algorithms., give an example?",">>> scheduler = ...
>>> for epoch in range(100):
>>>     train(...)
>>>     validate(...)
>>>     scheduler.step()",3740
10313,"How to use AveragedModel class serves to compute the weights of the SWA model. You can create an
averaged model by running:, give an example?",>>> swa_model = AveragedModel(model),1382
10314,"How to use AveragedModel class serves to compute the weights of the SWA model. You can create an
averaged model by running:Here the model model can be an arbitrary torch.nn.Module object. swa_model
will keep track of the running averages of the parameters of the model. To update these
averages, you can use the update_parameters() function:, give an example?",>>> swa_model.update_parameters(model),2789
10315,"How to use Typically, in SWA the learning rate is set to a high constant value. SWALR is a
learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it
constant. For example, the following code creates a scheduler that linearly anneals the
learning rate from its initial value to 0.05 in 5 epochs within each parameter group:, give an example?",">>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \
>>>         anneal_strategy=""linear"", anneal_epochs=5, swa_lr=0.05)",8039
10316,"How to use update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model
on a given dataloader loader at the end of training:, give an example?",">>> torch.optim.swa_utils.update_bn(loader, swa_model)",11289
10317,"How to use By default, torch.optim.swa_utils.AveragedModel computes a running equal average of
the parameters that you provide, but you can also use custom averaging functions with the
avg_fn parameter. In the following example ema_model computes an exponential moving average., give an example?",">>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)",344
10318,"How to use In the example below, swa_model is the SWA model that accumulates the averages of the weights.
We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule
and start to collect SWA averages of the parameters at epoch 160:, give an example?",">>> loader, optimizer, model, loss_fn = ...
>>> swa_model = torch.optim.swa_utils.AveragedModel(model)
>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
>>> swa_start = 160
>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)
>>>
>>> for epoch in range(300):
>>>       for input, target in loader:
>>>           optimizer.zero_grad()
>>>           loss_fn(model(input), target).backward()
>>>           optimizer.step()
>>>       if epoch > swa_start:
>>>           swa_model.update_parameters(model)
>>>           swa_scheduler.step()
>>>       else:
>>>           scheduler.step()
>>>
>>> # Update bn statistics for the swa_model at the end
>>> torch.optim.swa_utils.update_bn(loader, swa_model)
>>> # Use swa_model to make predictions on test data
>>> preds = swa_model(test_input)",3774
10319,"How to use torch.nanquantile, give an example?",">>> t = torch.tensor([float('nan'), 1, 2])
>>> t.quantile(0.5)
tensor(nan)
>>> t.nanquantile(0.5)
tensor(1.5000)
>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])
>>> t
tensor([[nan, nan],
        [1., 2.]])
>>> t.nanquantile(0.5, dim=0)
tensor([1., 2.])
>>> t.nanquantile(0.5, dim=1)
tensor([   nan, 1.5000])",393
10320,"How to use torch.symeig, give an example?","UPLO = ""U"" if upper else ""L""
L = torch.linalg.eigvalsh(A, UPLO=UPLO)",11230
10321,"How  L, _ = torch.symeig(A, upper=upper) should be replaced withL, V = torch.symeig(A, eigenvectors=True, upper=upper) should be replaced with, give an example?","UPLO = ""U"" if upper else ""L""
L, V = torch.linalg.eigh(A, UPLO=UPLO)",4049
10322,"How  If upper is False, then lower triangular portion is used., give an example?",">>> a = torch.randn(5, 5)
>>> a = a + a.t()  # To make a symmetric
>>> a
tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],
        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],
        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],
        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],
        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])
>>> e, v = torch.symeig(a, eigenvectors=True)
>>> e
tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])
>>> v
tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],
        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],
        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],
        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],
        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])
>>> a_big = torch.randn(5, 2, 2)
>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric
>>> e, v = a_big.symeig(eigenvectors=True)
>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True",3540
10323,"How to use torch.gcd, give an example?",">>> a = torch.tensor([5, 10, 15])
>>> b = torch.tensor([3, 4, 5])
>>> torch.gcd(a, b)
tensor([1, 2, 5])
>>> c = torch.tensor([3])
>>> torch.gcd(a, c)
tensor([1, 1, 3])",1456
10324,"How to use torch.matrix_exp, give an example?",">>> a = torch.randn(2, 2, 2)
>>> a[0, :, :] = torch.eye(2, 2)
>>> a[1, :, :] = 2 * torch.eye(2, 2)
>>> a
tensor([[[1., 0.],
         [0., 1.]],

        [[2., 0.],
         [0., 2.]]])
>>> torch.matrix_exp(a)
tensor([[[2.7183, 0.0000],
         [0.0000, 2.7183]],

         [[7.3891, 0.0000],
          [0.0000, 7.3891]]])

>>> import math
>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])
>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]
tensor([[ 0.5000,  0.8660],
        [-0.8660,  0.5000]])",1399
10325,"How to use Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)
to a github repository by adding a simple hubconf.py file;hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function
(example: a pre-trained model you want to publish)., give an example?","def entrypoint_name(*args, **kwargs):
    # args & kwargs are optional, for models which take positional/keyword arguments.
    ...",9549
10326,"How to use Here is a code snippet specifies an entrypoint for resnet18 model if we expand
the implementation in pytorch/vision/hubconf.py.
In most case importing the right function in hubconf.py is sufficient. Here we
just want to use the expanded version as an example to show how it works.
You can see the full script in
pytorch/vision repo, give an example?","dependencies = ['torch']
from torchvision.models.resnet import resnet18 as _resnet18

# resnet18 is the name of entrypoint
def resnet18(pretrained=False, **kwargs):
    """""" # This docstring shows up in hub.help()
    Resnet18 model
    pretrained (bool): kwargs, load pretrained weights into the model
    """"""
    # Call the model, load pretrained weights
    model = _resnet18(pretrained=pretrained, **kwargs)
    return model",2775
10327,"How  Here is a code snippet specifies an entrypoint for resnet18 model if we expand
the implementation in pytorch/vision/hubconf.py.
In most case importing the right function in hubconf.py is sufficient. Here we
just want to use the expanded version as an example to show how it works.
You can see the full script in
pytorch/vision repo, give an example?","if pretrained:
    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth
    dirname = os.path.dirname(__file__)
    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)
    state_dict = torch.load(checkpoint)
    model.load_state_dict(state_dict)

    # For checkpoint saved elsewhere
    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'
    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))",2776
10328,"How to use torch.hub.list, give an example?",">>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)",345
10329,"How to use torch.hub.help, give an example?",">>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))",379
10330,"How to use torch.hub.load, give an example?",">>> # from a github repo
>>> repo = 'pytorch/vision'
>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)
>>> # from a local directory
>>> path = '/some/local/path/pytorch/vision'
>>> model = torch.hub.load(path, 'resnet50', pretrained=True)",3481
10331,"How to use torch.hub.download_url_to_file, give an example?",">>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')",411
10332,"How to use torch.hub.load_state_dict_from_url, give an example?",>>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'),3496
10333,"How to use torch.utils.model_zoo.load_url, give an example?",>>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'),3496
10334,"How to use torch.log10, give an example?",">>> a = torch.rand(5)
>>> a
tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])


>>> torch.log10(a)
tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])",292
10335,"How to use torch.fmin, give an example?",">>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])
>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])
>>> torch.fmin(a, b)
tensor([-9.3000, 0.1000, 2.1000,    nan])",6111
10336,"How to use torch.outer, give an example?",">>> v1 = torch.arange(1., 5.)
>>> v2 = torch.arange(1., 4.)
>>> torch.outer(v1, v2)
tensor([[  1.,   2.,   3.],
        [  2.,   4.,   6.],
        [  3.,   6.,   9.],
        [  4.,   8.,  12.]])",438
10337,"How to use torch.tan, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-1.2027, -1.7687,  0.4412, -1.3856])
>>> torch.tan(a)
tensor([-2.5930,  4.9859,  0.4722, -5.3366])",323
10338,"How to use torch.addr, give an example?",">>> vec1 = torch.arange(1., 4.)
>>> vec2 = torch.arange(1., 3.)
>>> M = torch.zeros(3, 2)
>>> torch.addr(M, vec1, vec2)
tensor([[ 1.,  2.],
        [ 2.,  4.],
        [ 3.,  6.]])",3545
10339,"How to use torch.cumprod, give an example?",">>> a = torch.randn(10)
>>> a
tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
        -0.2129, -0.4206,  0.1968])
>>> torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
         0.0014, -0.0006, -0.0001])

>>> a[5] = 0.0
>>> torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
         0.0000, -0.0000, -0.0000])",2516
10340,"How to use torch.inner, give an example?","# Dot product
>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))
tensor(7)

# Multidimensional input tensors
>>> a = torch.randn(2, 3)
>>> a
tensor([[0.8173, 1.0874, 1.1784],
        [0.3279, 0.1234, 2.7894]])
>>> b = torch.randn(2, 4, 3)
>>> b
tensor([[[-0.4682, -0.7159,  0.1506],
        [ 0.4034, -0.3657,  1.0387],
        [ 0.9892, -0.6684,  0.1774],
        [ 0.9482,  1.3261,  0.3917]],

        [[ 0.4537,  0.7493,  1.1724],
        [ 0.2291,  0.5749, -0.2267],
        [-0.7920,  0.3607, -0.3701],
        [ 1.3666, -0.5850, -1.7242]]])
>>> torch.inner(a, b)
tensor([[[-0.9837,  1.1560,  0.2907,  2.6785],
        [ 2.5671,  0.5452, -0.6912, -1.5509]],

        [[ 0.1782,  2.9843,  0.7366,  1.5672],
        [ 3.5115, -0.4864, -1.2476, -4.4337]]])

# Scalar input
>>> torch.inner(a, torch.tensor(2))
tensor([[1.6347, 2.1748, 2.3567],
        [0.6558, 0.2469, 5.5787]])",44
10341,"How to use torch.isclose, give an example?",">>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))
tensor([ True, False, False])
>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)
tensor([True, True])",11373
10342,"How to use torch.rsqrt, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.0370,  0.2970,  1.5420, -0.9105])
>>> torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])",317
10343,"How to use torch.searchsorted, give an example?",">>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])
>>> sorted_sequence
tensor([[ 1,  3,  5,  7,  9],
        [ 2,  4,  6,  8, 10]])
>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])
>>> values
tensor([[3, 6, 9],
        [3, 6, 9]])
>>> torch.searchsorted(sorted_sequence, values)
tensor([[1, 3, 4],
        [1, 2, 4]])
>>> torch.searchsorted(sorted_sequence, values, right=True)
tensor([[2, 3, 5],
        [1, 3, 4]])

>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])
>>> sorted_sequence_1d
tensor([1, 3, 5, 7, 9])
>>> torch.searchsorted(sorted_sequence_1d, values)
tensor([[1, 3, 4],
        [1, 3, 4]])",382
10344,"How to use torch.block_diag, give an example?",">>> import torch
>>> A = torch.tensor([[0, 1], [1, 0]])
>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])
>>> C = torch.tensor(7)
>>> D = torch.tensor([1, 2, 3])
>>> E = torch.tensor([[4], [5], [6]])
>>> torch.block_diag(A, B, C, D, E)
tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],
        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])",361
10345,"How to use torch.linalg.inv, give an example?","torch.linalg.solve(A, B) == A.inv() @ B",1827
10346,"How  Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if A is a batch of matrices
then the output has the same batch dimensions., give an example?",">>> x = torch.rand(4, 4)
>>> y = torch.linalg.inv(x)
>>> z = x @ y
>>> z
tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000, -0.0000, -0.0000,  1.0000]])
>>> torch.dist(z, torch.eye(4))
tensor(1.1921e-07)

>>> # Batched inverse example
>>> x = torch.randn(2, 3, 4, 4)
>>> y = torch.linalg.inv(x)
>>> z = x @ y
>>> torch.dist(z, torch.eye(4).expand_as(x))
tensor(1.9073e-06)

>>> x = torch.rand(4, 4, dtype=torch.cdouble)
>>> y = torch.linalg.inv(x)
>>> z = x @ y
>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))
tensor(7.5107e-16, dtype=torch.float64)",6123
10347,"How to use torch.bitwise_or, give an example?",">>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-1, -2,  3], dtype=torch.int8)
>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, True, False])",399
10348,"How to use torch.igammac, give an example?",">>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.igammac(a1, a2)
tensor([0.6472, 0.4335, 0.2650])
>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)
tensor([1., 1., 1.])",6105
10349,"How to use torch.randint, give an example?",">>> torch.randint(3, 5, (3,))
tensor([4, 3, 4])


>>> torch.randint(10, (2, 2))
tensor([[0, 2],
        [5, 5]])


>>> torch.randint(3, 10, (2, 2))
tensor([[4, 5],
        [6, 7]])",7277
10350,"How to use torch.masked_select, give an example?",">>> x = torch.randn(3, 4)
>>> x
tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
        [-1.2035,  1.2252,  0.5002,  0.6248],
        [ 0.1307, -2.0608,  0.1244,  2.0139]])
>>> mask = x.ge(0.5)
>>> mask
tensor([[False, False, False, False],
        [False, True, True, True],
        [False, False, False, True]])
>>> torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])",7291
10351,"How to use torch.mm, give an example?",">>> mat1 = torch.randn(2, 3)
>>> mat2 = torch.randn(3, 3)
>>> torch.mm(mat1, mat2)
tensor([[ 0.4851,  0.5037, -0.3633],
        [-0.0760, -3.6705,  2.4784]])",7779
10352,"How to use torch.movedim, give an example?",">>> t = torch.randn(3,2,1)
>>> t
tensor([[[-0.3362],
        [-0.8437]],

        [[-0.9627],
        [ 0.1727]],

        [[ 0.5173],
        [-0.1398]]])
>>> torch.movedim(t, 1, 0).shape
torch.Size([2, 3, 1])
>>> torch.movedim(t, 1, 0)
tensor([[[-0.3362],
        [-0.9627],
        [ 0.5173]],

        [[-0.8437],
        [ 0.1727],
        [-0.1398]]])
>>> torch.movedim(t, (1, 2), (0, 1)).shape
torch.Size([2, 1, 3])
>>> torch.movedim(t, (1, 2), (0, 1))
tensor([[[-0.3362, -0.9627,  0.5173]],

        [[-0.8437,  0.1727, -0.1398]]])",4693
10353,"How to use torch.addmm, give an example?",">>> M = torch.randn(2, 3)
>>> mat1 = torch.randn(2, 3)
>>> mat2 = torch.randn(3, 3)
>>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
        [ 0.7573, -3.9555, -2.8681]])",7776
10354,"How to use torch.prod, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[-0.8020,  0.5428, -1.5854]])
>>> torch.prod(a)
tensor(0.6902)",302
10355,"How  If keepdim is True, the output tensor is of the same size
as input except in the dimension dim where it is of size 1.
Otherwise, dim is squeezed (see torch.squeeze()), resulting in
the output tensor having 1 fewer dimension than input., give an example?",">>> a = torch.rand(4, 2).bool()
>>> a
tensor([[True, True],
        [True, False],
        [True, True],
        [True, True]], dtype=torch.bool)
>>> torch.all(a, dim=1)
tensor([ True, False,  True,  True], dtype=torch.bool)
>>> torch.all(a, dim=0)
tensor([ True, False], dtype=torch.bool)",3433
10356,"How to use torch.where, give an example?",">>> x = torch.randn(3, 2)
>>> y = torch.ones(3, 2)
>>> x
tensor([[-0.4620,  0.3139],
        [ 0.3898, -0.7197],
        [ 0.0478, -0.1657]])
>>> torch.where(x > 0, x, y)
tensor([[ 1.0000,  0.3139],
        [ 0.3898,  1.0000],
        [ 0.0478,  1.0000]])
>>> x = torch.randn(2, 2, dtype=torch.double)
>>> x
tensor([[ 1.0779,  0.0383],
        [-0.8785, -1.1089]], dtype=torch.float64)
>>> torch.where(x > 0, x, 0.)
tensor([[1.0779, 0.0383],
        [0.0000, 0.0000]], dtype=torch.float64)",7211
10357,"How to use torch.empty_like, give an example?",">>> torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])",404
10358,"How to use torch.bmm, give an example?",">>> input = torch.randn(10, 3, 4)
>>> mat2 = torch.randn(10, 4, 5)
>>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])",7778
10359,"How to use torch.randn, give an example?",">>> torch.randn(4)
tensor([-2.1436,  0.9966,  2.3426, -0.6366])
>>> torch.randn(2, 3)
tensor([[ 1.5954,  2.8929, -1.0923],
        [ 1.1719, -0.4709, -0.1996]])",7278
10360,"How to use torch.poisson, give an example?",">>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5
>>> torch.poisson(rates)
tensor([[9., 1., 3., 5.],
        [8., 6., 6., 0.],
        [0., 4., 5., 3.],
        [2., 1., 4., 2.]])",380
10361,"How to use torch.vstack, give an example?",">>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> torch.vstack((a,b))
tensor([[1, 2, 3],
        [4, 5, 6]])
>>> a = torch.tensor([[1],[2],[3]])
>>> b = torch.tensor([[4],[5],[6]])
>>> torch.vstack((a,b))
tensor([[1],
        [2],
        [3],
        [4],
        [5],
        [6]])",7636
10362,"How to use torch.reshape, give an example?",">>> a = torch.arange(4.)
>>> torch.reshape(a, (2, 2))
tensor([[ 0.,  1.],
        [ 2.,  3.]])
>>> b = torch.tensor([[0, 1], [2, 3]])
>>> torch.reshape(b, (-1,))
tensor([ 0,  1,  2,  3])",842
10363,"How to use torch.bucketize, give an example?",">>> boundaries = torch.tensor([1, 3, 5, 7, 9])
>>> boundaries
tensor([1, 3, 5, 7, 9])
>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])
>>> v
tensor([[3, 6, 9],
        [3, 6, 9]])
>>> torch.bucketize(v, boundaries)
tensor([[1, 3, 4],
        [1, 3, 4]])
>>> torch.bucketize(v, boundaries, right=True)
tensor([[2, 3, 5],
        [2, 3, 5]])",342
10364,"How to use torch.copysign, give an example?",">>> a = torch.randn(5)
>>> a
tensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])
>>> torch.copysign(a, 1)
tensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])
>>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.7079,  0.2778, -1.0249,  0.5719],
        [-0.0059, -0.2600, -0.4475, -1.3948],
        [ 0.3667, -0.9567, -2.5757, -0.1751],
        [ 0.2046, -0.0742,  0.2998, -0.1054]])
>>> b = torch.randn(4)
tensor([ 0.2373,  0.3120,  0.3190, -1.1128])
>>> torch.copysign(a, b)
tensor([[ 0.7079,  0.2778,  1.0249, -0.5719],
        [ 0.0059,  0.2600,  0.4475, -1.3948],
        [ 0.3667,  0.9567,  2.5757, -0.1751],
        [ 0.2046,  0.0742,  0.2998, -0.1054]])",6106
10365,"How to use torch.kthvalue, give an example?",">>> x = torch.arange(1., 6.)
>>> x
tensor([ 1.,  2.,  3.,  4.,  5.])
>>> torch.kthvalue(x, 4)
torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))

>>> x=torch.arange(1.,7.).resize_(2,3)
>>> x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))",3432
10366,"How to use torch.cholesky, give an example?",L = torch.linalg.cholesky(A),11022
10367,"How  L = torch.cholesky(A) should be replaced withU = torch.cholesky(A, upper=True) should be replaced with, give an example?","U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()",4046
10368,"How  If upper is True, and AAA is a batch of symmetric positive-definite
matrices, then the returned tensor will be composed of upper-triangular Cholesky factors
of each of the individual matrices. Similarly, when upper is False, the returned
tensor will be composed of lower-triangular Cholesky factors of each of the individual
matrices., give an example?",">>> a = torch.randn(3, 3)
>>> a = torch.mm(a, a.t()) # make symmetric positive-definite
>>> l = torch.cholesky(a)
>>> a
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
>>> l
tensor([[ 1.5528,  0.0000,  0.0000],
        [-0.4821,  1.0592,  0.0000],
        [ 0.9371,  0.5487,  0.7023]])
>>> torch.mm(l, l.t())
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
>>> a = torch.randn(3, 2, 2)
>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
>>> l = torch.cholesky(a)
>>> z = torch.matmul(l, l.transpose(-1, -2))
>>> torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)",3543
10369,"How to use torch.histc, give an example?",">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])",2252
10370,"How to use torch.eq, give an example?",">>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ True, False],
        [False, True]])",7267
10371,"How to use torch.unsqueeze, give an example?",">>> x = torch.tensor([1, 2, 3, 4])
>>> torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
>>> torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])",794
10372,"How to use torch.matrix_rank, give an example?",">>> a = torch.eye(10)
>>> torch.matrix_rank(a)
tensor(10)
>>> b = torch.eye(10)
>>> b[0, 0] = 0
>>> torch.matrix_rank(b)
tensor(9)",10931
10373,"How to use torch.argmax, give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
>>> torch.argmax(a)
tensor(0)",7692
10374,"How  This is the second value returned by torch.max(). See its
documentation for the exact semantics of this method., give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
>>> torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])",7693
10375,"How to use torch.imag, give an example?",">>> x=torch.randn(4, dtype=torch.cfloat)
>>> x
tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])
>>> x.imag
tensor([ 0.3553, -0.7896, -0.0633, -0.8119])",471
10376,"How to use torch.exp, give an example?",">>> torch.exp(torch.tensor([0, math.log(2.)]))
tensor([ 1.,  2.])",406
10377,"How to use torch.angle, give an example?",">>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159
tensor([ 135.,  135,  -45])",396
10378,"How to use torch.Tensor.scatter_, give an example?","self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2",2487
10379,"How  Additionally accepts an optional reduce argument that allows
specification of an optional reduction operation, which is applied to all
values in the tensor src into self at the indicies
specified in the index. For each value in src, the reduction
operation is applied to an index in self which is specified by
its index in src for dimension != dim and by the corresponding
value in index for dimension = dim.Given a 3-D tensor and reduction using the multiplication operation, self
is updated as:, give an example?","self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2",957
10380,"How  Reducing with the addition operation is the same as using
scatter_add_()., give an example?",">>> src = torch.arange(1, 11).reshape((2, 5))
>>> src
tensor([[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10]])
>>> index = torch.tensor([[0, 1, 2, 0]])
>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)
tensor([[1, 0, 0, 4, 0],
        [0, 2, 0, 0, 0],
        [0, 0, 3, 0, 0]])
>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])
>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)
tensor([[1, 2, 3, 0, 0],
        [6, 7, 0, 0, 8],
        [0, 0, 0, 0, 0]])

>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),
...            1.23, reduce='multiply')
tensor([[2.0000, 2.0000, 2.4600, 2.0000],
        [2.0000, 2.0000, 2.0000, 2.4600]])
>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),
...            1.23, reduce='add')
tensor([[2.0000, 2.0000, 3.2300, 2.0000],
        [2.0000, 2.0000, 2.0000, 3.2300]])",5114
10381,"How to use torch.range, give an example?",">>> torch.range(1, 4)
tensor([ 1.,  2.,  3.,  4.])
>>> torch.range(1, 4, 0.5)
tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])",427
10382,"How to use torch.broadcast_to, give an example?",">>> x = torch.tensor([1, 2, 3])
>>> torch.broadcast_to(x, (3, 3))
tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])",462
10383,"How to use torch.squeeze, give an example?",">>> x = torch.zeros(2, 1, 2, 1, 2)
>>> x.size()
torch.Size([2, 1, 2, 1, 2])
>>> y = torch.squeeze(x)
>>> y.size()
torch.Size([2, 2, 2])
>>> y = torch.squeeze(x, 0)
>>> y.size()
torch.Size([2, 1, 2, 1, 2])
>>> y = torch.squeeze(x, 1)
>>> y.size()
torch.Size([2, 2, 1, 2])",8357
10384,"How to use You can use torch.manual_seed() to seed the RNG for all devices (both
CPU and CUDA):, give an example?","import torch
torch.manual_seed(0)",8606
10385,"How to use For custom operators, you might need to set python seed as well:, give an example?","import random
random.seed(0)",2510
10386,"How to use If you or any of the libraries you are using rely on NumPy, you can seed the global
NumPy RNG with:, give an example?","import numpy as np
np.random.seed(0)",3573
10387,"How to use Please check the documentation for torch.use_deterministic_algorithms()
for a full list of affected operations. If an operation does not act correctly
according to the documentation, or if you need a deterministic implementation
of an operation that does not have one, please submit an issue:
https://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()
will throw an error:, give an example?",">>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ...",4847
10388,"How to use For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()
will throw an error:When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a
nondeterministic algorithm, but when the deterministic flag is turned on, its alternate
deterministic implementation will be used:, give an example?",">>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
tensor([[[ 1.1900, -2.3409],
         [ 0.4796,  0.8003]],
        [[ 0.1509,  1.8027],
         [ 0.0333, -1.1444]]], device='cuda:0')",2523
10389,"How to use DataLoader will reseed workers following Randomness in multi-process data loading algorithm.
Use worker_init_fn() and generator to preserve reproducibility:, give an example?","def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

DataLoader(
    train_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    worker_init_fn=seed_worker
    generator=g,
)",2065
10390,"How to use torch.dsplit, give an example?",">>> t = torch.arange(16.0).reshape(2, 2, 4)
>>> t
tensor([[[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.]],
        [[ 8.,  9., 10., 11.],
         [12., 13., 14., 15.]]])
>>> torch.dsplit(t, 2)
(tensor([[[ 0.,  1.],
        [ 4.,  5.]],
       [[ 8.,  9.],
        [12., 13.]]]),
 tensor([[[ 2.,  3.],
          [ 6.,  7.]],
         [[10., 11.],
          [14., 15.]]]))",384
10391,"How to use torch.can_cast, give an example?",">>> torch.can_cast(torch.double, torch.float)
True
>>> torch.can_cast(torch.float, torch.int)
False",401
10392,"How to use torch.linalg.slogdet, give an example?",">>> A = torch.randn(3, 3)
>>> A
tensor([[ 0.0032, -0.2239, -1.1219],
        [-0.6690,  0.1161,  0.4053],
        [-1.6218, -0.9273, -0.0082]])
>>> torch.linalg.det(A)
tensor(-0.7576)
>>> torch.linalg.logdet(A)
tensor(nan)
>>> torch.linalg.slogdet(A)
torch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))",6124
10393,"How to use torch.qr, give an example?","Q, R = torch.linalg.qr(A)",11204
10394,"How  Q, R = torch.qr(A) should be replaced withQ, R = torch.qr(A, some=False) should be replaced with, give an example?","Q, R = torch.linalg.qr(A, mode=""complete"")",5031
10395,"How  If some is True, then this function returns the thin (reduced) QR factorization.
Otherwise, if some is False, this function returns the complete QR factorization., give an example?",">>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
>>> q, r = torch.qr(a)
>>> q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
>>> r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
>>> torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
>>> torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
>>> a = torch.randn(3, 4, 5)
>>> q, r = torch.qr(a, some=False)
>>> torch.allclose(torch.matmul(q, r), a)
True
>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True",3480
10396,"How to use torch.nan_to_num, give an example?",">>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])
>>> torch.nan_to_num(x)
tensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])
>>> torch.nan_to_num(x, nan=2.0)
tensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])
>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)
tensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])",467
10397,"How to use torch.div, give an example?",">>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
>>> torch.div(x, 0.5)
tensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])

>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
...                   [ 0.1815, -1.0111,  0.9805, -1.5923],
...                   [ 0.1062,  1.4581,  0.7759, -1.2344],
...                   [-0.1830, -0.0313,  1.1908, -1.4757]])
>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
>>> torch.div(a, b)
tensor([[-0.4620, -6.6051,  0.5676,  1.2639],
        [ 0.2260, -3.4509, -1.2086,  6.8990],
        [ 0.1322,  4.9764, -0.9564,  5.3484],
        [-0.2278, -0.1068, -1.4678,  6.3938]])

>>> torch.div(a, b, rounding_mode='trunc')
tensor([[-0., -6.,  0.,  1.],
        [ 0., -3., -1.,  6.],
        [ 0.,  4., -0.,  5.],
        [-0., -0., -1.,  6.]])

>>> torch.div(a, b, rounding_mode='floor')
tensor([[-1., -7.,  0.,  1.],
        [ 0., -4., -2.,  6.],
        [ 0.,  4., -1.,  5.],
        [-1., -1., -2.,  6.]])",6113
10398,"How to use Mixing Tracing and Scripting, give an example?","import torch

def foo(x, y):
    return 2 * x + y

traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))

@torch.jit.script
def bar(x):
    return traced_foo(x, x)",9596
10399,"How to use Setting the environment variable PYTORCH_JIT=0 will disable all script
and tracing annotations. If there is hard-to-debug error in one of your
TorchScript models, you can use this flag to force everything to run using native
Python. Since TorchScript (scripting and tracing) is disabled with this flag,
you can use tools like pdb to debug the model code.  For example:, give an example?","@torch.jit.script
def scripted_fn(x : torch.Tensor):
    for i in range(12):
        x = x + x
    return x

def fn(x):
    x = torch.neg(x)
    import pdb; pdb.set_trace()
    return scripted_fn(x)

traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))
traced_fn(torch.rand(3, 4))",5874
10400,"How to use Setting the environment variable PYTORCH_JIT=0 will disable all script
and tracing annotations. If there is hard-to-debug error in one of your
TorchScript models, you can use this flag to force everything to run using native
Python. Since TorchScript (scripting and tracing) is disabled with this flag,
you can use tools like pdb to debug the model code.  For example:Debugging this script with pdb works except for when we invoke the
@torch.jit.script function. We can globally disable
JIT, so that we can call the @torch.jit.script
function as a normal Python function and not compile it. If the above script
is called disable_jit_example.py, we can invoke it like so:, give an example?",$ PYTORCH_JIT=0 python disable_jit_example.py,2073
10401,"How to use Inspecting Code, give an example?","@torch.jit.script
def foo(len):
    # type: (int) -> torch.Tensor
    rv = torch.zeros(3, 4)
    for i in range(len):
        if i < 10:
            rv = rv - 1.0
        else:
            rv = rv + 1.0
    return rv

print(foo.code)",722
10402,"How to use TorchScript provides a code pretty-printer for all ScriptModule instances. This
pretty-printer gives an interpretation of the script method’s code as valid
Python syntax. For example:A ScriptModule with a single forward method will have an attribute
code, which you can use to inspect the ScriptModule’s code.
If the ScriptModule has more than one method, you will need to access
.code on the method itself and not the module. We can inspect the
code of a method named foo on a ScriptModule by accessing .foo.code.
The example above produces this output:, give an example?","def foo(len: int) -> Tensor:
    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)
    rv0 = rv
    for i in range(len):
        if torch.lt(i, 10):
            rv1 = torch.sub(rv0, 1., 1)
        else:
            rv1 = torch.add(rv0, 1., 1)
        rv0 = rv1
    return rv0",772
10403,"How to use Interpreting Graphs, give an example?","@torch.jit.script
def foo(len):
    # type: (int) -> torch.Tensor
    rv = torch.zeros(3, 4)
    for i in range(len):
        if i < 10:
            rv = rv - 1.0
        else:
            rv = rv + 1.0
    return rv

print(foo.graph)",723
10404,"How to use graph follows the same rules described in the Inspecting Code section
with regard to forward method lookup.The example script above produces the graph:, give an example?","graph(%len.1 : int):
  %24 : int = prim::Constant[value=1]()
  %17 : bool = prim::Constant[value=1]() # test.py:10:5
  %12 : bool? = prim::Constant()
  %10 : Device? = prim::Constant()
  %6 : int? = prim::Constant()
  %1 : int = prim::Constant[value=3]() # test.py:9:22
  %2 : int = prim::Constant[value=4]() # test.py:9:25
  %20 : int = prim::Constant[value=10]() # test.py:11:16
  %23 : float = prim::Constant[value=1]() # test.py:12:23
  %4 : int[] = prim::ListConstruct(%1, %2)
  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10
  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5
    block0(%i.1 : int, %rv.14 : Tensor):
      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12
      %rv.13 : Tensor = prim::If(%21) # test.py:11:9
        block0():
          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18
          -> (%rv.3)
        block1():
          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18
          -> (%rv.6)
      -> (%17, %rv.13)
  return (%rv)",9488
10405,"How to use One way to automatically catch many errors in traces is by using check_inputs
on the torch.jit.trace() API. check_inputs takes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example:, give an example?","def loop_in_traced_fn(x):
    result = x[0]
    for i in range(x.size(0)):
        result = result * x[i]
    return result

inputs = (torch.rand(3, 4, 5),)
check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]

traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)",4662
10406,"How to use One way to automatically catch many errors in traces is by using check_inputs
on the torch.jit.trace() API. check_inputs takes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example:Gives us the following diagnostic information:, give an example?","ERROR: Graphs differed across invocations!
Graph diff:

            graph(%x : Tensor) {
            %1 : int = prim::Constant[value=0]()
            %2 : int = prim::Constant[value=0]()
            %result.1 : Tensor = aten::select(%x, %1, %2)
            %4 : int = prim::Constant[value=0]()
            %5 : int = prim::Constant[value=0]()
            %6 : Tensor = aten::select(%x, %4, %5)
            %result.2 : Tensor = aten::mul(%result.1, %6)
            %8 : int = prim::Constant[value=0]()
            %9 : int = prim::Constant[value=1]()
            %10 : Tensor = aten::select(%x, %8, %9)
        -   %result : Tensor = aten::mul(%result.2, %10)
        +   %result.3 : Tensor = aten::mul(%result.2, %10)
        ?          ++
            %12 : int = prim::Constant[value=0]()
            %13 : int = prim::Constant[value=2]()
            %14 : Tensor = aten::select(%x, %12, %13)
        +   %result : Tensor = aten::mul(%result.3, %14)
        +   %16 : int = prim::Constant[value=0]()
        +   %17 : int = prim::Constant[value=3]()
        +   %18 : Tensor = aten::select(%x, %16, %17)
        -   %15 : Tensor = aten::mul(%result, %14)
        ?     ^                                 ^
        +   %19 : Tensor = aten::mul(%result, %18)
        ?     ^                                 ^
        -   return (%15);
        ?             ^
        +   return (%19);
        ?             ^
            }",2725
10407,"How to use  , give an example?","def fn(x):
    result = x[0]
    for i in range(x.size(0)):
        result = result * x[i]
    return result

inputs = (torch.rand(3, 4, 5),)
check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]

scripted_fn = torch.jit.script(fn)
print(scripted_fn.graph)
#print(str(scripted_fn.graph).strip())

for input_tuple in [inputs] + check_inputs:
    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))",9152
10408,"How to use In this case, data-dependent control flow like this can be captured using
torch.jit.script() instead:Which produces:, give an example?","graph(%x : Tensor) {
    %5 : bool = prim::Constant[value=1]()
    %1 : int = prim::Constant[value=0]()
    %result.1 : Tensor = aten::select(%x, %1, %1)
    %4 : int = aten::size(%x, %1)
    %result : Tensor = prim::Loop(%4, %5, %result.1)
    block0(%i : int, %7 : Tensor) {
        %10 : Tensor = aten::select(%x, %1, %i)
        %result.2 : Tensor = aten::mul(%7, %10)
        -> (%5, %result.2)
    }
    return (%result);
}",3796
10409,"How to use The tracer produces warnings for several problematic patterns in traced
computation. As an example, take a trace of a function that contains an
in-place assignment on a slice (a view) of a Tensor:Produces several warnings and a graph which simply returns the input:, give an example?","fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.
    x[0] = torch.rand(*x.shape[1:2])
fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:
Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)
    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
graph(%0 : Float(3, 4)) {
    return (%0);
}",4905
10410,"How to use First convert your model from GPU to CPU and then save it, like so:, give an example?","cpu_model = gpu_model.cpu()
sample_input_cpu = sample_input_gpu.cpu()
traced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)
torch.jit.save(traced_cpu, ""cpu.pt"")

traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)
torch.jit.save(traced_gpu, ""gpu.pt"")

# ... later, when using the model:

if use_gpu:
  model = torch.jit.load(""gpu.pt"")
else:
  model = torch.jit.load(""cpu.pt"")

model(input)",2450
10411,"How to use Frequently Asked Questions, give an example?","import torch

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.x = 2

    def forward(self):
        return self.x

m = torch.jit.script(Model())",9595
10412,"How to use Migrating to PyTorch 1 2 Recursive Scripting API, give an example?","import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

my_model = Model()
my_scripted_model = torch.jit.script(my_model)",9597
10413,"How to use Old API:New API:, give an example?","try:
    from typing_extensions import Final
except:
    # If you don't have `typing_extensions` installed, you can use a
    # polyfill from `torch.jit`.
    from torch.jit import Final

class MyModule(torch.nn.Module):

    my_constant: Final[int]

    def __init__(self):
        super(MyModule, self).__init__()
        self.my_constant = 2

    def forward(self):
        pass

m = torch.jit.script(MyModule())",4620
10414,"How to use torch.column_stack, give an example?",">>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> torch.column_stack((a, b))
tensor([[1, 4],
    [2, 5],
    [3, 6]])
>>> a = torch.arange(5)
>>> b = torch.arange(10).reshape(5, 2)
>>> torch.column_stack((a, b, b))
tensor([[0, 0, 1, 0, 1],
        [1, 2, 3, 2, 3],
        [2, 4, 5, 4, 5],
        [3, 6, 7, 6, 7],
        [4, 8, 9, 8, 9]])",2262
10415,"How to use torch.is_tensor, give an example?",">>> x=torch.tensor([1,2,3])
>>> torch.is_tensor(x)
True",4492
10416,"How  Note that this function is simply doing isinstance(obj, Tensor).
Using that isinstance check is better for typechecking with mypy,
and more explicit - so it’s recommended to use that instead of
is_tensor., give an example?",">>> x=torch.tensor([1,2,3])
>>> torch.is_tensor(x)
True",4492
10417,"How to use torch.set_flush_denormal, give an example?",">>> torch.set_flush_denormal(True)
True
>>> torch.tensor([1e-323], dtype=torch.float64)
tensor([ 0.], dtype=torch.float64)
>>> torch.set_flush_denormal(False)
True
>>> torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)",5244
10418,"How to use torch.einsum, give an example?","# trace
>>> torch.einsum('ii', torch.randn(4, 4))
tensor(-1.2104)

# diagonal
>>> torch.einsum('ii->i', torch.randn(4, 4))
tensor([-0.1034,  0.7952, -0.2433,  0.4545])

# outer product
>>> x = torch.randn(5)
>>> y = torch.randn(4)
>>> torch.einsum('i,j->ij', x, y)
tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],
        [-0.3744,  0.9381,  1.2685, -1.6070],
        [ 0.7208, -1.8058, -2.4419,  3.0936],
        [ 0.1713, -0.4291, -0.5802,  0.7350],
        [ 0.5704, -1.4290, -1.9323,  2.4480]])

# batch matrix multiplication
>>> As = torch.randn(3,2,5)
>>> Bs = torch.randn(3,5,4)
>>> torch.einsum('bij,bjk->bik', As, Bs)
tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
        [-1.6706, -0.8097, -0.8025, -2.1183]],

        [[ 4.2239,  0.3107, -0.5756, -0.2354],
        [-1.4558, -0.3460,  1.5087, -0.8530]],

        [[ 2.8153,  1.8787, -4.3839, -1.2112],
        [ 0.3728, -2.1131,  0.0921,  0.8305]]])

# batch permute
>>> A = torch.randn(2, 3, 4, 5)
>>> torch.einsum('...ij->...ji', A).shape
torch.Size([2, 3, 5, 4])

# equivalent to torch.nn.functional.bilinear
>>> A = torch.randn(3,5,4)
>>> l = torch.randn(2,5)
>>> r = torch.randn(2,4)
>>> torch.einsum('bn,anm,bm->ba', l, A, r)
tensor([[-0.3430, -5.2405,  0.4494],
        [ 0.3311,  5.5201, -3.0356]])",2260
10419,"How to use torch.lerp, give an example?",">>> start = torch.arange(1., 5.)
>>> end = torch.empty(4).fill_(10)
>>> start
tensor([ 1.,  2.,  3.,  4.])
>>> end
tensor([ 10.,  10.,  10.,  10.])
>>> torch.lerp(start, end, 0.5)
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
>>> torch.lerp(start, end, torch.full_like(start, 0.5))
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])",7290
10420,"How to use torch.maximum, give an example?",">>> a = torch.tensor((1, 2, -1))
>>> b = torch.tensor((3, 0, 4))
>>> torch.maximum(a, b)
tensor([3, 2, 4])",330
10421,"How to use torch.norm, give an example?",">>> import torch
>>> a = torch.arange(9, dtype= torch.float) - 4
>>> b = a.reshape((3, 3))
>>> torch.norm(a)
tensor(7.7460)
>>> torch.norm(b)
tensor(7.7460)
>>> torch.norm(a, float('inf'))
tensor(4.)
>>> torch.norm(b, float('inf'))
tensor(4.)
>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
>>> torch.norm(c, dim=0)
tensor([1.4142, 2.2361, 5.0000])
>>> torch.norm(c, dim=1)
tensor([3.7417, 4.2426])
>>> torch.norm(c, p=1, dim=1)
tensor([6., 6.])
>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
>>> torch.norm(d, dim=(1,2))
tensor([ 3.7417, 11.2250])
>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
(tensor(3.7417), tensor(11.2250))",362
10422,"How to use torch.zeros, give an example?",">>> torch.zeros(2, 3)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

>>> torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])",437
10423,"How to use torch.any, give an example?",">>> a = torch.rand(1, 2).bool()
>>> a
tensor([[False, True]], dtype=torch.bool)
>>> torch.any(a)
tensor(True, dtype=torch.bool)
>>> a = torch.arange(0, 3)
>>> a
tensor([0, 1, 2])
>>> torch.any(a)
tensor(True)",291
10424,"How to use torch.lu_solve, give an example?",">>> A = torch.randn(2, 3, 3)
>>> b = torch.randn(2, 3, 1)
>>> A_LU = torch.lu(A)
>>> x = torch.lu_solve(b, *A_LU)
>>> torch.norm(torch.bmm(A, x) - b)
tensor(1.00000e-07 *
       2.8312)",7565
10425,"How to use torch.argsort, give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
        [ 0.1598,  0.0788, -0.0745, -1.2700],
        [ 1.2208,  1.0722, -0.7064,  1.2564],
        [ 0.0669, -0.2318, -0.8229, -0.9280]])


>>> torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])",7695
10426,"How to use torch.cummax, give an example?",">>> a = torch.randn(10)
>>> a
tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,
     1.9946, -0.8209])
>>> torch.cummax(a, dim=0)
torch.return_types.cummax(
    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,
     1.9946,  1.9946]),
    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))",304
10427,"How to use torch.gather, give an example?","out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2",2485
10428,"How  input and index must have the same number of dimensions.
It is also required that index.size(d) <= input.size(d) for all
dimensions d != dim.  out will have the same shape as index.
Note that input and index do not broadcast against each other., give an example?",">>> t = torch.tensor([[1, 2], [3, 4]])
>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))
tensor([[ 1,  1],
        [ 4,  3]])",9636
10429,"How to use torch.isreal, give an example?",">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))
tensor([True, False, True])",416
10430,"How to use torch.nn.init.calculate_gain, give an example?",">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2",352
10431,"How to use torch.nn.init.uniform_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.uniform_(w)",446
10432,"How to use torch.nn.init.normal_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.normal_(w)",442
10433,"How to use torch.nn.init.constant_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.constant_(w, 0.3)",440
10434,"How to use torch.nn.init.ones_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.ones_(w)",443
10435,"How to use torch.nn.init.zeros_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.zeros_(w)",447
10436,"How to use torch.nn.init.eye_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.eye_(w)",441
10437,"How to use torch.nn.init.dirac_, give an example?",">>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
>>> w = torch.empty(3, 24, 5, 5)
>>> nn.init.dirac_(w, 3)",439
10438,"How to use torch.nn.init.xavier_uniform_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))",1105
10439,"How to use torch.nn.init.xavier_normal_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.xavier_normal_(w)",1104
10440,"How to use torch.nn.init.kaiming_uniform_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')",1107
10441,"How to use torch.nn.init.kaiming_normal_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')",1106
10442,"How to use torch.nn.init.orthogonal_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.orthogonal_(w)",444
10443,"How to use torch.nn.init.sparse_, give an example?",">>> w = torch.empty(3, 5)
>>> nn.init.sparse_(w, sparsity=0.1)",445
10444,"How to use torch.addbmm, give an example?",">>> M = torch.randn(3, 5)
>>> batch1 = torch.randn(10, 3, 4)
>>> batch2 = torch.randn(10, 4, 5)
>>> torch.addbmm(M, batch1, batch2)
tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])",7777
10445,"How to use torch.linalg.pinv, give an example?","torch.linalg.lstsq(A, B).solution == A.pinv() @ B",1825
10446,"How  The singular values (or the norm of the eigenvalues when hermitian= True)
that are below the specified rcond threshold are treated as zero and discarded in the computation., give an example?",">>> A = torch.randn(3, 5)
>>> A
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
>>> torch.linalg.pinv(A)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])

Batched linalg.pinv example
>>> A = torch.randn(2, 6, 3)
>>> B = torch.linalg.pinv(A)
>>> torch.matmul(B, A).round()
tensor([[[1., -0., 0.],
         [0., 1., -0.],
         [0., 0., 1.]],

        [[1., -0., 0.],
         [-0., 1., 0.],
         [-0., -0., 1.]]])

Hermitian input example
>>> A = torch.randn(3, 3, dtype=torch.complex64)
>>> A = A + A.t().conj()  # creates a Hermitian matrix
>>> B = torch.linalg.pinv(A, hermitian=True)
>>> torch.matmul(B, A)
tensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,
        5.9605e-08-2.3842e-07j],
        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,
        -4.7684e-07+1.1921e-07j],
        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,
        1.0000e+00-1.7897e-07j]])

Non-default rcond example
>>> rcond = 0.5
>>> A = torch.randn(3, 3)
>>> torch.linalg.pinv(A)
tensor([[ 0.2971, -0.4280, -2.0111],
        [-0.0090,  0.6426, -0.1116],
        [-0.7832, -0.2465,  1.0994]])
>>> torch.linalg.pinv(A, rcond)
tensor([[-0.2672, -0.2351, -0.0539],
        [-0.0211,  0.6467, -0.0698],
        [-0.4400, -0.3638, -0.0910]])

Matrix-wise rcond example
>>> A = torch.randn(5, 6, 2, 3, 3)
>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]
>>> torch.linalg.pinv(A, rcond)
>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'
>>> torch.linalg.pinv(A, rcond)",7296
10447,"How to use torch.result_type, give an example?",">>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)
torch.float32
>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
torch.uint8",428
10448,"How to use torch.set_default_dtype, give an example?",">>> # initial default for floating point is torch.float32
>>> torch.tensor([1.2, 3]).dtype
torch.float32
>>> # initial default for floating point is torch.complex64
>>> torch.tensor([1.2, 3j]).dtype
torch.complex64
>>> torch.set_default_dtype(torch.float64)
>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
torch.float64
>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor
torch.complex128",7022
10449,"How to use torch.cummin, give an example?",">>> a = torch.randn(10)
>>> a
tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,
     0.9165,  1.6684])
>>> torch.cummin(a, dim=0)
torch.return_types.cummin(
    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,
    -1.3298, -1.3298]),
    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))",303
10450,"How to use torch.mode, give an example?",">>> a = torch.randint(10, (5,))
>>> a
tensor([6, 5, 1, 0, 2])
>>> b = a + (torch.randn(50, 1) * 5).long()
>>> torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))",3440
10451,"How to use torch.distributed.optim.ZeroRedundancyOptimizer, give an example?",">>> import torch.nn as nn
>>> from torch.distributed.optim import ZeroRedundancyOptimizer
>>> from torch.nn.parallel import DistributedDataParallel as DDP

>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])
>>> ddp = DDP(model, device_ids=[rank])
>>> opt = ZeroRedundancyOptimizer(
>>>     ddp.parameters(),
>>>     optimizer_class=torch.optim.Adam,
>>>     lr=0.01
>>> )
>>> ddp(inputs).sum().backward()
>>> opt.step()",8690
10452,"How to use torch.arange, give an example?",">>> torch.arange(5)
tensor([ 0,  1,  2,  3,  4])
>>> torch.arange(1, 4)
tensor([ 1,  2,  3])
>>> torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])",4475
10453,"How to use torch.hypot, give an example?",">>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))
tensor([5.0000, 5.6569, 6.4031])",7281
10454,"How to use torch.as_strided, give an example?",">>> x = torch.randn(3, 3)
>>> x
tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
>>> t = torch.as_strided(x, (2, 2), (1, 2))
>>> t
tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])",456
10455,"How to use torch.transpose, give an example?",">>> x = torch.randn(2, 3)
>>> x
tensor([[ 1.0028, -0.9893,  0.5809],
        [-0.1669,  0.7299,  0.4942]])
>>> torch.transpose(x, 0, 1)
tensor([[ 1.0028, -0.1669],
        [-0.9893,  0.7299],
        [ 0.5809,  0.4942]])",7259
10456,"How to use torch.cross, give an example?",">>> a = torch.randn(4, 3)
>>> a
tensor([[-0.3956,  1.1455,  1.6895],
        [-0.5849,  1.3672,  0.3599],
        [-1.1626,  0.7180, -0.0521],
        [-0.1339,  0.9902, -2.0225]])
>>> b = torch.randn(4, 3)
>>> b
tensor([[-0.0257, -1.4725, -1.2251],
        [-1.1479, -0.7005, -1.9757],
        [-1.3904,  0.3726, -1.1836],
        [-0.9688, -0.7153,  0.2159]])
>>> torch.cross(a, b, dim=1)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
>>> torch.cross(a, b)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])",3408
10457,"How to use torch.rot90, give an example?",">>> x = torch.arange(4).view(2, 2)
>>> x
tensor([[0, 1],
        [2, 3]])
>>> torch.rot90(x, 1, [0, 1])
tensor([[1, 3],
        [0, 2]])

>>> x = torch.arange(8).view(2, 2, 2)
>>> x
tensor([[[0, 1],
         [2, 3]],

        [[4, 5],
         [6, 7]]])
>>> torch.rot90(x, 1, [1, 2])
tensor([[[1, 3],
         [0, 2]],

        [[5, 7],
         [4, 6]]])",449
10458,"How to use torch.Tensor.scatter_add_, give an example?","self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2",2486
10459,"How  self, index and src should have same number of
dimensions. It is also required that index.size(d) <= src.size(d) for all
dimensions d, and that index.size(d) <= self.size(d) for all dimensions
d != dim. Note that index and src do not broadcast., give an example?",">>> src = torch.ones((2, 5))
>>> index = torch.tensor([[0, 1, 2, 0, 0]])
>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)
tensor([[1., 0., 0., 1., 1.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.]])
>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])
>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)
tensor([[2., 0., 0., 1., 1.],
        [0., 2., 0., 0., 0.],
        [0., 0., 2., 1., 1.]])",10661
10460,"How to use torch.triu, give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.3480, -0.5211, -0.4573]])
>>> torch.triu(a)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.0000, -1.0680,  0.6602],
        [ 0.0000,  0.0000, -0.4573]])
>>> torch.triu(a, diagonal=1)
tensor([[ 0.0000,  0.5207,  2.0049],
        [ 0.0000,  0.0000,  0.6602],
        [ 0.0000,  0.0000,  0.0000]])
>>> torch.triu(a, diagonal=-1)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.0000, -0.5211, -0.4573]])

>>> b = torch.randn(4, 6)
>>> b
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])
>>> torch.triu(b, diagonal=1)
tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])
>>> torch.triu(b, diagonal=-1)
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])",6962
10461,"How to use torch.polygamma, give an example?",">>> a = torch.tensor([1, 0.5])
>>> torch.polygamma(1, a)
tensor([1.64493, 4.9348])
>>> torch.polygamma(2, a)
tensor([ -2.4041, -16.8288])
>>> torch.polygamma(3, a)
tensor([ 6.4939, 97.4091])
>>> torch.polygamma(4, a)
tensor([ -24.8863, -771.4742])",337
10462,"How to use torch.var, give an example?",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var(a, unbiased=False)
tensor(0.1754)",3535
10463,"How to use torch.allclose, give an example?",">>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
False
>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
True
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))
False
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True",9244
10464,"How to use torch.matmul, give an example?",">>> # vector x vector
>>> tensor1 = torch.randn(3)
>>> tensor2 = torch.randn(3)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([])
>>> # matrix x vector
>>> tensor1 = torch.randn(3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([3])
>>> # batched matrix x broadcasted vector
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
>>> # batched matrix x batched matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(10, 4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
>>> # batched matrix x broadcasted matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])",7774
10465,"How to use torch.count_nonzero, give an example?",">>> x = torch.zeros(3,3)
>>> x[torch.randn(3,3) > 0.5] = 1
>>> x
tensor([[0., 1., 1.],
        [0., 0., 0.],
        [0., 0., 1.]])
>>> torch.count_nonzero(x)
tensor(3)
>>> torch.count_nonzero(x, dim=0)
tensor([0, 1, 2])",469
10466,"How to use torch.meshgrid, give an example?",">>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([4, 5, 6])
>>> grid_x, grid_y = torch.meshgrid(x, y)
>>> grid_x
tensor([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]])
>>> grid_y
tensor([[4, 5, 6],
        [4, 5, 6],
        [4, 5, 6]])",464
10467,"How to use torch.is_nonzero, give an example?",">>> torch.is_nonzero(torch.tensor([0.]))
False
>>> torch.is_nonzero(torch.tensor([1.5]))
True
>>> torch.is_nonzero(torch.tensor([False]))
False
>>> torch.is_nonzero(torch.tensor([3]))
True
>>> torch.is_nonzero(torch.tensor([1, 3, 5]))
Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with more than one value is ambiguous
>>> torch.is_nonzero(torch.tensor([]))
Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with no values is ambiguous",413
10468,"How to use A tensor can be constructed from a Python list or sequence using the
torch.tensor() constructor:, give an example?",">>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])",858
10469,"How  A tensor can be constructed from a Python list or sequence using the
torch.tensor() constructor:, give an example?",">>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])",858
10470,"How to use A tensor can be constructed from a Python list or sequence using the
torch.tensor() constructor:A tensor of specific data type can be constructed by passing a
torch.dtype and/or a torch.device to a
constructor or tensor creation op:, give an example?",">>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",865
10471,"How  A tensor of specific data type can be constructed by passing a
torch.dtype and/or a torch.device to a
constructor or tensor creation op:, give an example?",">>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",865
10472,"How to use For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:, give an example?",">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])",2558
10473,"How  For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:, give an example?",">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])",2558
10474,"How to use The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a
single value:, give an example?",">>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5",7007
10475,"How  The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a
single value:, give an example?",">>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5",7007
10476,"How to use For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that
torch.autograd records operations on them for automatic differentiation., give an example?",">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])",2560
10477,"How  For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that
torch.autograd records operations on them for automatic differentiation., give an example?",">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])",2560
10478,"How to use A sparse COO tensor can be constructed by providing the two tensors of
indices and values, as well as the size of the sparse tensor (when it
cannot be inferred from the indices and values tensors) to a function
torch.sparse_coo_tensor().Suppose we want to define a sparse tensor with the entry 3 at location
(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).
Unspecified elements are assumed to have the same value, fill value,
which is zero by default. We would then write:, give an example?",">>> i = [[0, 1, 1],
         [2, 0, 2]]
>>> v =  [3, 4, 5]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3))
>>> s
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3, 4, 5]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)
>>> s.to_dense()
tensor([[0, 0, 3],
        [4, 0, 5]])",6145
10479,"How to use Suppose we want to define a sparse tensor with the entry 3 at location
(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).
Unspecified elements are assumed to have the same value, fill value,
which is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want
to write your indices this way, you should transpose before passing them to
the sparse constructor:, give an example?",">>> i = [[0, 2], [1, 0], [1, 2]]
>>> v =  [3,      4,      5    ]
>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))
>>> # Or another equivalent formulation to get s
>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))
>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()
tensor([[0, 0, 3],
        [4, 0, 5]])",6151
10480,"How to use Note that the input i is NOT a list of index tuples.  If you want
to write your indices this way, you should transpose before passing them to
the sparse constructor:An empty sparse COO tensor can be constructed by specifying its size
only:, give an example?",">>> torch.sparse_coo_tensor(size=(2, 3))
tensor(indices=tensor([], size=(2, 0)),
       values=tensor([], size=(0,)),
       size=(2, 3), nnz=0, layout=torch.sparse_coo)",4485
10481,"How to use PyTorch hybrid COO tensor extends the sparse COO tensor by allowing
the values tensor to be a multi-dimensional tensor so that we
have:Suppose we want to create a (2 + 1)-dimensional tensor with the entry
[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry
[7, 8] at location (1, 2). We would write, give an example?",">>> i = [[0, 1, 1],
         [2, 0, 2]]
>>> v =  [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))
>>> s
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([[3, 4],
                      [5, 6],
                      [7, 8]]),
       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)",4957
10482,"How  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing
the values tensor to be a multi-dimensional tensor so that we
have:Suppose we want to create a (2 + 1)-dimensional tensor with the entry
[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry
[7, 8] at location (1, 2). We would write, give an example?",">>> s.to_dense()
tensor([[[0, 0],
         [0, 0],
         [3, 4]],
        [[5, 6],
         [0, 0],
         [7, 8]]])",4958
10483,"How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,
where there may be duplicate coordinates in the indices; in this case,
the interpretation is that the value at that index is the sum of all
duplicate value entries. For example, one can specify multiple values,
3 and 4, for the same index 1, that leads to an 1-D
uncoalesced tensor:, give an example?",">>> i = [[1, 1]]
>>> v =  [3, 4]
>>> s=torch.sparse_coo_tensor(i, v, (3,))
>>> s
tensor(indices=tensor([[1, 1]]),
       values=tensor(  [3, 4]),
       size=(3,), nnz=2, layout=torch.sparse_coo)",4981
10484,"How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,
where there may be duplicate coordinates in the indices; in this case,
the interpretation is that the value at that index is the sum of all
duplicate value entries. For example, one can specify multiple values,
3 and 4, for the same index 1, that leads to an 1-D
uncoalesced tensor:while the coalescing process will accumulate the multi-valued elements
into a single value using summation:, give an example?",">>> s.coalesce()
tensor(indices=tensor([[1]]),
       values=tensor([7]),
       size=(3,), nnz=1, layout=torch.sparse_coo)",11386
10485,"How to use However, some operations can be implemented more efficiently on
uncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by
simply concatenating the indices and values tensors:, give an example?",">>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))
>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))
>>> a + b
tensor(indices=tensor([[0, 0, 1, 1]]),
       values=tensor([7, 8, 5, 6]),
       size=(2,), nnz=4, layout=torch.sparse_coo)",2972
10486,"How to use Let’s consider the following example:, give an example?",">>> i = [[0, 1, 1],
         [2, 0, 2]]
>>> v =  [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))",4099
10487,"How to use Let’s consider the following example:As mentioned above, a sparse COO tensor is a torch.Tensor
instance and to distinguish it from the Tensor instances that use
some other layout, on can use torch.Tensor.is_sparse or
torch.Tensor.layout properties:, give an example?",">>> isinstance(s, torch.Tensor)
True
>>> s.is_sparse
True
>>> s.layout == torch.sparse_coo
True",1304
10488,"How to use As mentioned above, a sparse COO tensor is a torch.Tensor
instance and to distinguish it from the Tensor instances that use
some other layout, on can use torch.Tensor.is_sparse or
torch.Tensor.layout properties:The number of sparse and dense dimensions can be acquired using
methods torch.Tensor.sparse_dim() and
torch.Tensor.dense_dim(), respectively. For instance:, give an example?",">>> s.sparse_dim(), s.dense_dim()
(2, 1)",1306
10489,"How to use NoteCurrently, one can acquire the COO format data only when the tensor
instance is coalesced:, give an example?",">>> s.indices()
RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first",2051
10490,"How to use Currently, one can acquire the COO format data only when the tensor
instance is coalesced:For acquiring the COO format data of an uncoalesced tensor, use
torch.Tensor._values() and torch.Tensor._indices():, give an example?",">>> s._indices()
tensor([[0, 1, 1],
        [2, 0, 2]])",2052
10491,"How to use If s is a sparse COO tensor then its COO format data can be
acquired using methods torch.Tensor.indices() and
torch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not
coalesced:, give an example?",">>> s.is_coalesced()
False",3479
10492,"How to use Constructing a new sparse COO tensor results a tensor that is not
coalesced:but one can construct a coalesced copy of a sparse COO tensor using
the torch.Tensor.coalesce() method:, give an example?",">>> s2 = s.coalesce()
>>> s2.indices()
tensor([[0, 1, 1],
       [2, 0, 2]])",1834
10493,"How to use When working with uncoalesced sparse COO tensors, one must take into
an account the additive nature of uncoalesced data: the values of the
same indices are the terms of a sum that evaluation gives the value of
the corresponding tensor element. For example, the scalar
multiplication on an uncoalesced sparse tensor could be implemented by
multiplying all the uncoalesced values with the scalar because c *
(a + b) == c * a + c * b holds. However, any nonlinear operation,
say, a square root, cannot be implemented by applying the operation to
uncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not
hold in general.Slicing (with positive step) of a sparse COO tensor is supported only
for dense dimensions. Indexing is supported for both sparse and dense
dimensions:, give an example?",">>> s[1]
tensor(indices=tensor([[0, 2]]),
       values=tensor([[5, 6],
                      [7, 8]]),
       size=(3, 2), nnz=2, layout=torch.sparse_coo)
>>> s[1, 0, 1]
tensor(6)
>>> s[1, 0, 1:]
tensor([6])",8388
10494,"How to use Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()
method. The user must supply the row and column indices and values tensors separately.
The size argument is optional and will be deduced from the the crow_indices
and col_indices if it is not present., give an example?",">>> crow_indices = torch.tensor([0, 2, 4])
>>> col_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)
>>> csr
tensor(crow_indices=tensor([0, 2, 4]),
      col_indices=tensor([0, 1, 0, 1]),
      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
      dtype=torch.float64)
>>> csr.to_dense()
tensor([[1., 2.],
        [3., 4.]], dtype=torch.float64)",6001
10495,"How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO
tensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will
be interpreted as missing values in the sparse tensor:, give an example?",">>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)
>>> sp = a._to_sparse_csr()
>>> sp
tensor(crow_indices=tensor([0, 1, 3, 3]),
      col_indices=tensor([2, 0, 1]),
      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)",7293
10496,"How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO
tensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will
be interpreted as missing values in the sparse tensor:The sparse matrix-vector multiplication can be performed with the
tensor.matmul() method. This is currently the only math operation
supported on CSR tensors., give an example?",">>> vec = torch.randn(4, 1, dtype=torch.float64)
>>> sp.matmul(vec)
tensor([[0.9078],
        [1.3180],
        [0.0000]], dtype=torch.float64)",7304
10497,"How to use torch.sin, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.5461,  0.1347, -2.7266, -0.2746])
>>> torch.sin(a)
tensor([-0.5194,  0.1343, -0.4032, -0.2711])",319
10498,"How to use torch.gradient, give an example?",">>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)
>>> torch.gradient(t)
tensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])
>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)
>>> torch.gradient(t, spacing=(coords,))
tensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])",390
10499,"How to use torch.bincount, give an example?",">>> input = torch.randint(0, 8, (5,), dtype=torch.int64)
>>> weights = torch.linspace(0, 1, steps=5)
>>> input, weights
(tensor([4, 3, 6, 3, 4]),
 tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

>>> torch.bincount(input)
tensor([0, 0, 0, 2, 2, 0, 1])

>>> input.bincount(weights)
tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])",7202
10500,"How to use torch.conj, give an example?",">>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])",402
10501,"How to use torch.positive, give an example?",">>> t = torch.randn(5)
>>> t
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
>>> torch.positive(t)
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])",389
10502,"How to use torch.atan, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
>>> torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])",310
10503,"How to use torch.tensor, give an example?",">>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

>>> torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],
...              dtype=torch.float64,
...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
tensor(3.1416)

>>> torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])",435
10504,"How to use torch.triu_indices, give an example?",">>> a = torch.triu_indices(3, 3)
>>> a
tensor([[0, 0, 0, 1, 1, 2],
        [0, 1, 2, 1, 2, 2]])

>>> a = torch.triu_indices(4, 3, -1)
>>> a
tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],
        [0, 1, 2, 0, 1, 2, 1, 2, 2]])

>>> a = torch.triu_indices(4, 3, 1)
>>> a
tensor([[0, 0, 1],
        [1, 2, 2]])",6964
10505,"How to use torch.remainder, give an example?",">>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([ 1.,  0.,  1.,  1.,  0.,  1.])
>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])",6108
10506,"How to use torch.eye, give an example?",">>> torch.eye(3)
tensor([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.],
        [ 0.,  0.,  1.]])",407
10507,"How to use torch.trunc, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 3.4742,  0.5466, -0.8008, -0.9079])
>>> torch.trunc(a)
tensor([ 3.,  0., -0., -0.])",316
10508,"How to use torch.log2, give an example?",">>> a = torch.rand(5)
>>> a
tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])


>>> torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])",293
10509,"How to use torch.sparse_coo_tensor, give an example?",">>> i = torch.tensor([[0, 1, 1],
...                   [2, 0, 2]])
>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)
>>> torch.sparse_coo_tensor(i, v, [2, 4])
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

>>> torch.sparse_coo_tensor(i, v)  # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

>>> torch.sparse_coo_tensor(i, v, [2, 4],
...                         dtype=torch.float64,
...                         device=torch.device('cuda:0'))
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])
#
# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)",353
10510,"How to use torch.logical_or, give an example?",">>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
tensor([ True, False,  True])
>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
>>> torch.logical_or(a, b)
tensor([ True,  True,  True, False])
>>> torch.logical_or(a.double(), b.double())
tensor([ True,  True,  True, False])
>>> torch.logical_or(a.double(), b)
tensor([ True,  True,  True, False])
>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True,  True, False])",420
10511,"How to use torch.solve, give an example?","X = torch.linalg.solve(A, B)",11216
10512,"How  Supports real-valued and complex-valued inputs., give an example?",">>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
...                   [-6.05, -3.30,  5.36, -4.44,  1.08],
...                   [-0.45,  2.58, -2.70,  0.27,  9.04],
...                   [8.32,  2.71,  4.35,  -7.17,  2.14],
...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
...                   [-1.56,  4.00, -8.67,  1.75,  2.86],
...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
>>> X, LU = torch.solve(B, A)
>>> torch.dist(B, torch.mm(A, X))
tensor(1.00000e-06 *
       7.0977)

>>> # Batched solver example
>>> A = torch.randn(2, 3, 1, 4, 4)
>>> B = torch.randn(2, 3, 1, 4, 6)
>>> X, LU = torch.solve(B, A)
>>> torch.dist(B, A.matmul(X))
tensor(1.00000e-06 *
   3.6386)",6134
10513,"How to use torch.vander, give an example?",">>> x = torch.tensor([1, 2, 3, 5])
>>> torch.vander(x)
tensor([[  1,   1,   1,   1],
        [  8,   4,   2,   1],
        [ 27,   9,   3,   1],
        [125,  25,   5,   1]])
>>> torch.vander(x, N=3)
tensor([[ 1,  1,  1],
        [ 4,  2,  1],
        [ 9,  3,  1],
        [25,  5,  1]])
>>> torch.vander(x, N=3, increasing=True)
tensor([[ 1,  1,  1],
        [ 1,  2,  4],
        [ 1,  3,  9],
        [ 1,  5, 25]])",6992
10514,"How to use torch.frexp, give an example?",">>> x = torch.arange(9.)
>>> mantissa, exponent = torch.frexp(x)
>>> mantissa
tensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])
>>> exponent
tensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)
>>> torch.ldexp(mantissa, exponent)
tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])",6117
10515,"How to use torch.addcdiv, give an example?",">>> t = torch.randn(1, 3)
>>> t1 = torch.randn(3, 1)
>>> t2 = torch.randn(1, 3)
>>> torch.addcdiv(t, t1, t2, value=0.1)
tensor([[-0.2312, -3.6496,  0.1312],
        [-1.0428,  3.4292, -0.1030],
        [-0.5369, -0.9829,  0.0430]])",2545
10516,"How to use torch.moveaxis, give an example?",">>> t = torch.randn(3,2,1)
>>> t
tensor([[[-0.3362],
        [-0.8437]],

        [[-0.9627],
        [ 0.1727]],

        [[ 0.5173],
        [-0.1398]]])
>>> torch.moveaxis(t, 1, 0).shape
torch.Size([2, 3, 1])
>>> torch.moveaxis(t, 1, 0)
tensor([[[-0.3362],
        [-0.9627],
        [ 0.5173]],

        [[-0.8437],
        [ 0.1727],
        [-0.1398]]])
>>> torch.moveaxis(t, (1, 2), (0, 1)).shape
torch.Size([2, 1, 3])
>>> torch.moveaxis(t, (1, 2), (0, 1))
tensor([[[-0.3362, -0.9627,  0.5173]],

        [[-0.8437,  0.1727, -0.1398]]])",7543
10517,"How to use torch.lgamma, give an example?",">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])",287
10518,"How to use torch.futures.Future.add_done_callback, give an example?",">>> import torch
>>>
>>> def callback(fut):
>>>     print(f""This will run after the future has finished."")
>>>     print(fut.wait())
>>>
>>> fut = torch.futures.Future()
>>> fut.add_done_callback(callback)
>>> fut.set_result(5)
>>>
>>> # Outputs are:
>>> This will run after the future has finished.
>>> 5",358
10519,"How to use torch.futures.Future.set_exception, give an example?",">>> import torch
>>>
>>> fut = torch.futures.Future()
>>> fut.set_exception(ValueError(""foo""))
>>> fut.wait()
>>>
>>> # Output:
>>> # This will run after the future has finished.
>>> ValueError: foo",359
10520,"How to use torch.futures.Future.set_result, give an example?",">>> import threading
>>> import time
>>> import torch
>>>
>>> def slow_set_future(fut, value):
>>>     time.sleep(0.5)
>>>     fut.set_result(value)
>>>
>>> fut = torch.futures.Future()
>>> t = threading.Thread(
>>>     target=slow_set_future,
>>>     args=(fut, torch.ones(2) * 3)
>>> )
>>> t.start()
>>>
>>> print(fut.wait())  # tensor([3., 3.])
>>> t.join()",356
10521,"How to use torch.futures.Future.then, give an example?",">>> import torch
>>>
>>> def callback(fut):
>>>     print(f""RPC return value is {fut.wait()}."")
>>>
>>> fut = torch.futures.Future()
>>> # The inserted callback will print the return value when
>>> # receiving the response from ""worker1""
>>> cb_fut = fut.then(callback)
>>> chain_cb_fut = cb_fut.then(
>>>     lambda x : print(f""Chained cb done. {x.wait()}"")
>>> )
>>> fut.set_result(5)
>>>
>>> # Outputs are:
>>> # RPC return value is 5.
>>> # Chained cb done. None",357
10522,"How to use torch.futures.collect_all, give an example?",">>> import torch
>>>
>>> fut0 = torch.futures.Future()
>>> fut1 = torch.futures.Future()
>>>
>>> fut = torch.futures.collect_all([fut0, fut1])
>>>
>>> fut0.set_result(0)
>>> fut1.set_result(1)
>>>
>>> fut_list = fut.wait()
>>> print(f""fut0 result = {fut_list[0].wait()}"")
>>> print(f""fut1 result = {fut_list[1].wait()}"")
>>> # outputs:
>>> # fut0 result = 0
>>> # fut1 result = 1",360
10523,"How to use torch.fmod, give an example?",">>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([-1., -0., -1.,  1.,  0.,  1.])
>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)
tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])",6107
10524,"How to use torch.dist, give an example?",">>> x = torch.randn(4)
>>> x
tensor([-1.5393, -0.8675,  0.5916,  1.6321])
>>> y = torch.randn(4)
>>> y
tensor([ 0.0967, -1.0511,  0.6295,  0.8360])
>>> torch.dist(x, y, 3.5)
tensor(1.6727)
>>> torch.dist(x, y, 3)
tensor(1.6973)
>>> torch.dist(x, y, 0)
tensor(inf)
>>> torch.dist(x, y, 1)
tensor(2.6537)",7285
10525,"How to use torch.empty, give an example?",">>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')
>>> torch.empty_like(a)
tensor([[0, 0, 0],
        [0, 0, 0]], device='cuda:0', dtype=torch.int32)",341
10526,"How to use torch.nansum, give an example?",">>> a = torch.tensor([1., 2., float('nan'), 4.])
>>> torch.nansum(a)
tensor(7.)",338
10527,"How  If keepdim is True, the output tensor is of the same size
as input except in the dimension(s) dim where it is of size 1.
Otherwise, dim is squeezed (see torch.squeeze()), resulting in the
output tensor having 1 (or len(dim)) fewer dimension(s)., give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
        [-0.2993,  0.9138,  0.9337, -1.6864],
        [ 0.1132,  0.7892, -0.1003,  0.5688],
        [ 0.3637, -0.9906, -0.4752, -1.5197]])
>>> torch.sum(a, 1)
tensor([-0.4598, -0.1381,  1.3708, -2.6217])
>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)
>>> torch.sum(b, (2, 1))
tensor([  435.,  1335.,  2235.,  3135.])",3435
10528,"How to use torch.diagonal, give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[-1.0854,  1.1431, -0.1752],
        [ 0.8536, -0.0905,  0.0360],
        [ 0.6927, -0.3735, -0.4945]])


>>> torch.diagonal(a, 0)
tensor([-1.0854, -0.0905, -0.4945])


>>> torch.diagonal(a, 1)
tensor([ 1.1431,  0.0360])


>>> x = torch.randn(2, 5, 4, 2)
>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)
tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
         [-1.1065,  1.0401, -0.2235, -0.7938]],

        [[-1.7325, -0.3081,  0.6166,  0.2335],
         [ 1.0500,  0.7336, -0.3836, -1.1015]]])",1264
10529,"How to use torch.le, give an example?",">>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, False], [True, True]])",7270
10530,"How to use torch.rand, give an example?",">>> torch.rand(4)
tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
>>> torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
        [ 0.3816,  0.7249,  0.0998]])",7276
10531,"How to use torch.take_along_dim, give an example?",">>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])
>>> max_idx = torch.argmax(t)
>>> torch.take_along_dim(t, max_idx)
tensor([60])
>>> sorted_idx = torch.argsort(t, dim=1)
>>> torch.take_along_dim(t, sorted_idx, dim=1)
tensor([[10, 20, 30],
        [40, 50, 60]])",2661
10532,"How to use torch.isposinf, give an example?",">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
>>> torch.isposinf(a)
tensor([False,  True, False])",333
10533,"How to use torch.logical_and, give an example?",">>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
tensor([ True, False, False])
>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
>>> torch.logical_and(a, b)
tensor([False, False,  True, False])
>>> torch.logical_and(a.double(), b.double())
tensor([False, False,  True, False])
>>> torch.logical_and(a.double(), b)
tensor([False, False,  True, False])
>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([False, False,  True, False])",418
10534,"How to use torch.neg, give an example?",">>> a = torch.randn(5)
>>> a
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
>>> torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])",328
10535,"How to use torch.trace, give an example?",">>> x = torch.arange(1., 10.).view(3, 3)
>>> x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.]])
>>> torch.trace(x)
tensor(15.)",448
10536,"How to use There are two ways to initialize using TCP, both requiring a network address
reachable from all processes and a desired world_size. The first way
requires specifying an address that belongs to the rank 0 process. This
initialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed
package. group_name is deprecated as well., give an example?","import torch.distributed as dist

# Use address of one of the machines
dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',
                        rank=args.rank, world_size=4)",4474
10537,"How to use Another initialization method makes use of a file system that is shared and
visible from all machines in a group, along with a desired world_size. The URL should start
with file:// and contain a path to a non-existent file (in an existing
directory) on a shared file system. File-system initialization will automatically
create that file if it doesn’t exist, but will not delete the file. Therefore, it
is your responsibility to make sure that the file is cleaned up before the next
init_process_group() call on the same file path/name.Note that automatic rank assignment is not supported anymore in the latest
distributed package and group_name is deprecated as well., give an example?","import torch.distributed as dist

# rank should always be specified
dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',
                        world_size=4, rank=args.rank)",4465
10538,"How to use Distributed Key Value Store, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Run on process 1 (server)
>>> server_store = dist.TCPStore(""127.0.0.1"", 1234, 2, True, timedelta(seconds=30))
>>> # Run on process 2 (client)
>>> client_store = dist.TCPStore(""127.0.0.1"", 1234, 2, False)
>>> # Use any of the store methods from either the client or server after initialization
>>> server_store.set(""first_key"", ""first_value"")
>>> client_store.get(""first_key"")",364
10539,"How to use torch.distributed.Store.set, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.set(""first_key"", ""first_value"")
>>> # Should return ""first_value""
>>> store.get(""first_key"")",370
10540,"How to use torch.distributed.Store.get, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.set(""first_key"", ""first_value"")
>>> # Should return ""first_value""
>>> store.get(""first_key"")",370
10541,"How to use torch.distributed.Store.add, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.add(""first_key"", 1)
>>> store.add(""first_key"", 6)
>>> # Should return 7
>>> store.get(""first_key"")",367
10542,"How to use torch.distributed.Store.wait, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> # This will throw an exception after 30 seconds
>>> store.wait([""bad_key""])",366
10543,"How to use torch.distributed.Store.num_keys, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.set(""first_key"", ""first_value"")
>>> # This should return 2
>>> store.num_keys()",368
10544,"How to use torch.distributed.Store.delete_key, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, HashStore can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.set(""first_key"")
>>> # This should return true
>>> store.delete_key(""first_key"")
>>> # This should return false
>>> store.delete_key(""bad_key"")",365
10545,"How to use torch.distributed.Store.set_timeout, give an example?",">>> import torch.distributed as dist
>>> from datetime import timedelta
>>> # Using TCPStore as an example, other store types can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, 1, True, timedelta(seconds=30))
>>> store.set_timeout(timedelta(seconds=10))
>>> # This will throw an exception after 10 seconds
>>> store.wait([""bad_key""])",369
10546,"How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.
It shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?","# Code runs on each rank.
dist.init_process_group(""nccl"", rank=rank, world_size=2)
output = torch.tensor([rank]).cuda(rank)
s = torch.cuda.Stream()
handle = dist.all_reduce(output, async_op=True)
# Wait ensures the operation is enqueued, but not necessarily complete.
handle.wait()
# Using result on non-default stream.
with torch.cuda.stream(s):
    s.wait_stream(torch.cuda.default_stream())
    output.add_(100)
if rank == 0:
    # if the explicit call to wait_stream was omitted, the output below will be
    # non-deterministically 1 or 101, depending on whether the allreduce overwrote
    # the value after the add completed.
    print(output)",7080
10547,"How to use torch.distributed.broadcast_object_list, give an example?",">>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> if dist.get_rank() == 0:
>>>     # Assumes world_size of 3.
>>>     objects = [""foo"", 12, {1: 2}] # any picklable object
>>> else:
>>>     objects = [None, None, None]
>>> dist.broadcast_object_list(objects, src=0)
>>> broadcast_objects
['foo', 12, {1: 2}]",280
10548,"How to use torch.distributed.all_reduce, give an example?",">>> # All tensors below are of torch.int64 type.
>>> # We have 2 process groups, 2 ranks.
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
>>> tensor
tensor([1, 2]) # Rank 0
tensor([3, 4]) # Rank 1
>>> dist.all_reduce(tensor, op=ReduceOp.SUM)
>>> tensor
tensor([4, 6]) # Rank 0
tensor([4, 6]) # Rank 1",1631
10549,"How  Complex tensors are supported., give an example?",">>> # All tensors below are of torch.cfloat dtype.
>>> # We have 2 process groups, 2 ranks.
>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]
>>> tensor_list
[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1
>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)
>>> tensor
tensor([1.+1.j, 2.+2.j]) # Rank 0
tensor([3.+3.j, 4.+4.j]) # Rank 1
>>> dist.all_gather(tensor_list, tensor)
>>> tensor_list
[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0
[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1",1629
10550,"How to use torch.distributed.all_gather, give an example?",">>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]
>>> tensor_list
[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
>>> tensor
tensor([1, 2]) # Rank 0
tensor([3, 4]) # Rank 1
>>> dist.all_gather(tensor_list, tensor)
>>> tensor_list
[tensor([1, 2]), tensor([3, 4])] # Rank 0
[tensor([1, 2]), tensor([3, 4])] # Rank 1",1630
10551,"How to use torch.distributed.all_gather_object, give an example?",">>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes world_size of 3.
>>> gather_objects = [""foo"", 12, {1: 2}] # any picklable object
>>> output = [None for _ in gather_objects]
>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])
>>> output
['foo', 12, {1: 2}]",277
10552,"How to use torch.distributed.gather_object, give an example?",">>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> # Assumes world_size of 3.
>>> gather_objects = [""foo"", 12, {1: 2}] # any picklable object
>>> output = [None for _ in gather_objects]
>>> dist.gather_object(
        gather_objects[dist.get_rank()],
        output if dist.get_rank() == 0 else None,
        dst=0
    )
>>> # On rank 0
>>> output
['foo', 12, {1: 2}]",278
10553,"How to use torch.distributed.scatter_object_list, give an example?",">>> # Note: Process group initialization omitted on each rank.
>>> import torch.distributed as dist
>>> if dist.get_rank() == 0:
>>>     # Assumes world_size of 3.
>>>     objects = [""foo"", 12, {1: 2}] # any picklable object
>>> else:
>>>     # Can be any list on non-src ranks, elements are not used.
>>>     objects = [None, None, None]
>>> output_list = [None]
>>> dist.scatter_object_list(output_list, objects, src=0)
>>> # Rank i gets objects[i]. For example, on rank 2:
>>> output_list
[{1: 2}]",279
10554,"How to use torch.distributed.all_to_all, give an example?",">>> input = torch.arange(4) + rank * 4
>>> input = list(input.chunk(4))
>>> input
[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))
>>> dist.all_to_all(output, input)
>>> output
[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3",371
10555,"How to use Note that you can use torch.profiler (recommended, only available after 1.8.1)  or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,
nccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?","import torch
import torch.distributed as dist
with torch.profiler():
    tensor = torch.randn(20, 10)
    dist.all_reduce(tensor)",4505
10556,"How to use For example, if the system we use for distributed training has 2 nodes, each
of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would
like to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?","import torch
import torch.distributed as dist

dist.init_process_group(backend=""nccl"",
                        init_method=""file:///distributed_test"",
                        world_size=2,
                        rank=0)
tensor_list = []
for dev_idx in range(torch.cuda.device_count()):
    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))

dist.all_reduce_multigpu(tensor_list)",2518
10557,"How to use Code running on Node 0Code running on Node 1, give an example?","import torch
import torch.distributed as dist

dist.init_process_group(backend=""nccl"",
                        init_method=""file:///distributed_test"",
                        world_size=2,
                        rank=1)
tensor_list = []
for dev_idx in range(torch.cuda.device_count()):
    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))

dist.all_reduce_multigpu(tensor_list)",1615
10558,"How to use In both cases of single-node distributed training or multi-node distributed
training, this utility will launch the given number of processes per node
(--nproc_per_node). If used for GPU training, this number needs to be less
or equal to the number of GPUs on the current system (nproc_per_node),
and each process will be operating on a single GPU from GPU 0 to
GPU (nproc_per_node - 1).How to use this module:, give an example?",">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
           arguments of your training script)",3703
10559,"How to use How to use this module:Node 1: (IP: 192.168.1.1, and has a free port: 1234), give an example?",">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
           --nnodes=2 --node_rank=0 --master_addr=""192.168.1.1""
           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
           and all other arguments of your training script)",2963
10560,"How to use Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
           --nnodes=2 --node_rank=1 --master_addr=""192.168.1.1""
           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
           and all other arguments of your training script)",4332
10561,"How  Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",>>> python -m torch.distributed.launch --help,4331
10562,"How to use 2. In your training program, you must parse the command-line argument:
--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.
If your training program uses GPUs, you should ensure that your code only
runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?",">>> import argparse
>>> parser = argparse.ArgumentParser()
>>> parser.add_argument(""--local_rank"", type=int)
>>> args = parser.parse_args()",133
10563,"How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?",>>> torch.cuda.set_device(args.local_rank)  # before your code runs,4750
10564,"How to use Set your device to local rank using eitheror, give an example?",">>> with torch.cuda.device(args.local_rank):
>>>    # your code to run",5834
10565,"How to use or3. In your training program, you are supposed to call the following function
at the beginning to start the distributed backend. You need to make sure that
the init_method uses env://, which is the only supported init_method
by this module., give an example?","torch.distributed.init_process_group(backend='YOUR BACKEND',
                                     init_method='env://')",10326
10566,"How to use 3. In your training program, you are supposed to call the following function
at the beginning to start the distributed backend. You need to make sure that
the init_method uses env://, which is the only supported init_method
by this module.4. In your training program, you can either use regular distributed functions
or use torch.nn.parallel.DistributedDataParallel() module. If your
training program uses GPUs for training and you would like to use
torch.nn.parallel.DistributedDataParallel() module,
here is how to configure it., give an example?","model = torch.nn.parallel.DistributedDataParallel(model,
                                                  device_ids=[args.local_rank],
                                                  output_device=args.local_rank)",142
10567,"How to use torch.ceil, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.6341, -1.4208, -1.0900,  0.5826])
>>> torch.ceil(a)
tensor([-0., -1., -1.,  1.])",321
10568,"How to use torch.linalg.det, give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[ 0.9478,  0.9158, -1.1295],
        [ 0.9701,  0.7346, -1.8044],
        [-0.2337,  0.0557,  0.6929]])
>>> torch.linalg.det(a)
tensor(0.0934)

>>> out = torch.empty(0)
>>> torch.linalg.det(a, out=out)
tensor(0.0934)
>>> out
tensor(0.0934)

>>> a = torch.randn(3, 2, 2)
>>> a
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
>>> torch.linalg.det(a)
tensor([1.1990, 0.4099, 0.7386])",6125
10569,"How to use torch.nonzero, give an example?",">>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))
tensor([[ 0],
        [ 1],
        [ 2],
        [ 4]])
>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
...                             [0.0, 0.4, 0.0, 0.0],
...                             [0.0, 0.0, 1.2, 0.0],
...                             [0.0, 0.0, 0.0,-0.4]]))
tensor([[ 0,  0],
        [ 1,  1],
        [ 2,  2],
        [ 3,  3]])
>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)
(tensor([0, 1, 2, 4]),)
>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
...                             [0.0, 0.4, 0.0, 0.0],
...                             [0.0, 0.0, 1.2, 0.0],
...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)
(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))
>>> torch.nonzero(torch.tensor(5), as_tuple=True)
(tensor([0]),)",1288
10570,"How to use torch.reciprocal, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.4595, -2.1219, -1.4314,  0.7298])
>>> torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])",318
10571,"How to use torch.sort, give an example?",">>> x = torch.randn(3, 4)
>>> sorted, indices = torch.sort(x)
>>> sorted
tensor([[-0.2162,  0.0608,  0.6719,  2.3332],
        [-0.5793,  0.0061,  0.6058,  0.9497],
        [-0.5071,  0.3343,  0.9553,  1.0960]])
>>> indices
tensor([[ 1,  0,  2,  3],
        [ 3,  1,  0,  2],
        [ 0,  3,  1,  2]])

>>> sorted, indices = torch.sort(x, 0)
>>> sorted
tensor([[-0.5071, -0.2162,  0.6719, -0.5793],
        [ 0.0608,  0.0061,  0.9497,  0.3343],
        [ 0.6058,  0.9553,  1.0960,  2.3332]])
>>> indices
tensor([[ 2,  0,  0,  1],
        [ 0,  1,  1,  2],
        [ 1,  2,  2,  0]])
>>> x = torch.tensor([0, 1] * 9)
>>> x.sort()
torch.return_types.sort(
    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))
>>> x.sort(stable=True)
torch.return_types.sort(
    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))",822
10572,"How to use torch.float_power, give an example?",">>> a = torch.randint(10, (4,))
>>> a
tensor([6, 4, 7, 1])
>>> torch.float_power(a, 2)
tensor([36., 16., 49.,  1.], dtype=torch.float64)

>>> a = torch.arange(1, 5)
>>> a
tensor([ 1,  2,  3,  4])
>>> exp = torch.tensor([2, -3, 4, -5])
>>> exp
tensor([ 2, -3,  4, -5])
>>> torch.float_power(a, exp)
tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)",295
10573,"How to use torch.diag_embed, give an example?",">>> a = torch.randn(2, 3)
>>> torch.diag_embed(a)
tensor([[[ 1.5410,  0.0000,  0.0000],
         [ 0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -2.1788]],

        [[ 0.5684,  0.0000,  0.0000],
         [ 0.0000, -1.0845,  0.0000],
         [ 0.0000,  0.0000, -1.3986]]])

>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)
tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],
         [ 0.0000,  0.5684,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -1.0845,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000, -2.1788],
         [ 0.0000,  0.0000,  0.0000, -1.3986]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000]]])",1265
10574,"How to use torch.frac, give an example?",">>> torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])",408
10575,"How to use torch.linalg.householder_product, give an example?",">>> a = torch.randn(2, 2)
>>> h, tau = torch.geqrf(a)
>>> q = torch.linalg.householder_product(h, tau)
>>> torch.allclose(q, torch.linalg.qr(a)[0])
True

>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)
>>> tau = torch.randn(3, 1, dtype=torch.complex128)
>>> q = torch.linalg.householder_product(h, tau)
>>> q
tensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],
        [-0.6853+0.7953j,  2.0790+0.5620j]],

        [[ 1.4581+1.6989j, -1.5360+0.1193j],
        [ 1.3877-0.6691j,  1.3512+1.3024j]],

        [[ 1.4766+0.5783j,  0.0361+0.6587j],
        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)",6132
10576,"How to use torch.tril, give an example?",">>> a = torch.randn(3, 3)
>>> a
tensor([[-1.0813, -0.8619,  0.7105],
        [ 0.0935,  0.1380,  2.2112],
        [-0.3409, -0.9828,  0.0289]])
>>> torch.tril(a)
tensor([[-1.0813,  0.0000,  0.0000],
        [ 0.0935,  0.1380,  0.0000],
        [-0.3409, -0.9828,  0.0289]])

>>> b = torch.randn(4, 6)
>>> b
tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],
        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])
>>> torch.tril(b, diagonal=1)
tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])
>>> torch.tril(b, diagonal=-1)
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])",6963
10577,"How to use torch.i0, give an example?",">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])",412
10578,"How to use torch.hstack, give an example?",">>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> torch.hstack((a,b))
tensor([1, 2, 3, 4, 5, 6])
>>> a = torch.tensor([[1],[2],[3]])
>>> b = torch.tensor([[4],[5],[6]])
>>> torch.hstack((a,b))
tensor([[1, 4],
        [2, 5],
        [3, 6]])",7637
10579,"How to use torch.pow, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
>>> torch.pow(a, 2)
tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
>>> exp = torch.arange(1., 5.)

>>> a = torch.arange(1., 5.)
>>> a
tensor([ 1.,  2.,  3.,  4.])
>>> exp
tensor([ 1.,  2.,  3.,  4.])
>>> torch.pow(a, exp)
tensor([   1.,    4.,   27.,  256.])",8358
10580,"How  The operation applied is:, give an example?",">>> exp = torch.arange(1., 5.)
>>> base = 2
>>> torch.pow(base, exp)
tensor([  2.,   4.,   8.,  16.])",7210
10581,"How to use torch.floor, give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.8166,  1.5308, -0.2530, -0.2091])
>>> torch.floor(a)
tensor([-1.,  1., -1., -1.])",322
10582,"How to use inference mode, give an example?",">>> import torch
>>> x = torch.ones(1, 2, 3, requires_grad=True)
>>> with torch.inference_mode():
...   y = x * x
>>> y.requires_grad
False
>>> y._version
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Inference tensors do not track version counter.
>>> @torch.inference_mode()
... def func(x):
...   return x * x
>>> out = func(x)
>>> out.requires_grad
False",363
10583,"How to use torch.as_tensor, give an example?",">>> a = numpy.array([1, 2, 3])
>>> t = torch.as_tensor(a)
>>> t
tensor([ 1,  2,  3])
>>> t[0] = -1
>>> a
array([-1,  2,  3])

>>> a = numpy.array([1, 2, 3])
>>> t = torch.as_tensor(a, device=torch.device('cuda'))
>>> t
tensor([ 1,  2,  3])
>>> t[0] = -1
>>> a
array([1,  2,  3])",285
10584,"How to use torch.trapz, give an example?",">>> y = torch.randn((2, 3))
>>> y
tensor([[-2.1156,  0.6857, -0.2700],
        [-1.2145,  0.5540,  2.0431]])
>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])
>>> torch.trapz(y, x)
tensor([-1.2220,  0.9683])",474
10585,"How to use torch.mul, give an example?",">>> a = torch.randn(3)
>>> a
tensor([ 0.2015, -0.4255,  2.6087])
>>> torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])",3424
10586,"How  The shapes of input and other must be
broadcastable., give an example?",">>> a = torch.randn(4, 1)
>>> a
tensor([[ 1.1207],
        [-0.3137],
        [ 0.0700],
        [ 0.8378]])
>>> b = torch.randn(1, 4)
>>> b
tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
>>> torch.mul(a, b)
tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
        [-0.1614, -0.0382,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4312,  0.1019, -0.4394,  1.8753]])",7283
10587,"How to use torch.logsumexp, give an example?",">>> a = torch.randn(3, 3)
>>> torch.logsumexp(a, 1)
tensor([ 0.8442,  1.4322,  0.8711])",3434
10588,"How to use torch.max, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[ 0.6763,  0.7445, -2.2369]])
>>> torch.max(a)
tensor(0.7445)",300
10589,"How  If keepdim is True, the output tensors are of the same size
as input except in the dimension dim where they are of size 1.
Otherwise, dim is squeezed (see torch.squeeze()), resulting
in the output tensors having 1 fewer dimension than input., give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
        [ 1.1949, -1.1127, -2.2379, -0.6702],
        [ 1.5717, -0.9207,  0.1297, -1.8768],
        [-0.6172,  1.0036, -0.6060, -0.2432]])
>>> torch.max(a, 1)
torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))",3436
10590,"How to use torch.mean, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[ 0.2294, -0.5481,  1.3288]])
>>> torch.mean(a)
tensor(0.3367)",298
10591,"How to use torch.quantile, give an example?",">>> a = torch.randn(2, 3)
>>> a
tensor([[ 0.0795, -1.2117,  0.9765],
        [ 1.1707,  0.6706,  0.4884]])
>>> q = torch.tensor([0.25, 0.5, 0.75])
>>> torch.quantile(a, q, dim=1, keepdim=True)
tensor([[[-0.5661],
        [ 0.5795]],

        [[ 0.0795],
        [ 0.6706]],

        [[ 0.5280],
        [ 0.9206]]])
>>> torch.quantile(a, q, dim=1, keepdim=True).shape
torch.Size([3, 2, 1])
>>> a = torch.arange(4.)
>>> a
tensor([0., 1., 2., 3.])",3473
10592,"How to use torch.vdot, give an example?",">>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))
tensor(7)
>>> a = torch.tensor((1 +2j, 3 - 1j))
>>> b = torch.tensor((2 +1j, 4 - 0j))
>>> torch.vdot(a, b)
tensor([16.+1.j])
>>> torch.vdot(b, a)
tensor([16.-1.j])",436
10593,"How to use torch.asinh, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])
>>> torch.asinh(a)
tensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])",307
10594,"How to use torch.lstsq, give an example?","X = torch.linalg.lstsq(A, B).solution",11090
10595,"How  Returned tensor XXX has shape (max⁡(m,n)×k)(\max(m, n) \times k)(max(m,n)×k). The first nnn
rows of XXX contains the solution. If m≥nm \geq nm≥n, the residual sum of squares
for the solution in each column is given by the sum of squares of elements in the
remaining m−nm - nm−n rows of that column., give an example?",">>> A = torch.tensor([[1., 1, 1],
...                   [2, 3, 4],
...                   [3, 5, 2],
...                   [4, 2, 5],
...                   [5, 4, 3]])
>>> B = torch.tensor([[-10., -3],
...                   [ 12, 14],
...                   [ 14, 12],
...                   [ 16, 16],
...                   [ 18, 16]])
>>> X, _ = torch.lstsq(B, A)
>>> X
tensor([[  2.0000,   1.0000],
        [  1.0000,   1.0000],
        [  1.0000,   2.0000],
        [ 10.9635,   4.8501],
        [  8.9332,   5.2418]])",5206
10596,"How to use torch.view_as_real, give an example?",">>> x=torch.randn(4, dtype=torch.cfloat)
>>> x
tensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])
>>> torch.view_as_real(x)
tensor([[ 0.4737, -0.3839],
        [-0.2098, -0.6699],
        [ 0.3470, -0.9451],
        [-0.5174, -1.3136]])",473
10597,"How to use torch.isfinite, give an example?",">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])",5104
10598,"How to use torch.flipud, give an example?",">>> x = torch.arange(4).view(2, 2)
>>> x
tensor([[0, 1],
        [2, 3]])
>>> torch.flipud(x)
tensor([[2, 3],
        [0, 1]])",2475
10599,"How to use torch.full, give an example?",">>> torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416]])",409
10600,"How to use torch.Generator, give an example?",">>> g_cpu = torch.Generator()
>>> g_cuda = torch.Generator(device='cuda')",351
10601,"How  Gets the current device of the generator., give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu.device
device(type='cpu')",2701
10602,"How to use torch.Generator.get_state, give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu.get_state()",346
10603,"How to use torch.Generator.initial_seed, give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu.initial_seed()
2147483647",347
10604,"How to use torch.Generator.manual_seed, give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu.manual_seed(2147483647)",348
10605,"How to use torch.Generator.seed, give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu.seed()
1516516984916",349
10606,"How to use torch.Generator.set_state, give an example?",">>> g_cpu = torch.Generator()
>>> g_cpu_other = torch.Generator()
>>> g_cpu.set_state(g_cpu_other.get_state())",350
10607,"How to use torch.deg2rad, give an example?",">>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])
>>> torch.deg2rad(a)
tensor([[ 3.1416, -3.1416],
        [ 6.2832, -6.2832],
        [ 1.5708, -1.5708]])",339
10608,"How to use torch.var_mean, give an example?",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var_mean(a, unbiased=False)
(tensor(0.1754), tensor(-0.8509))",3536
10609,"How to use torch.complex, give an example?",">>> real = torch.tensor([1, 2], dtype=torch.float32)
>>> imag = torch.tensor([3, 4], dtype=torch.float32)
>>> z = torch.complex(real, imag)
>>> z
tensor([(1.+3.j), (2.+4.j)])
>>> z.dtype
torch.complex64",381
10610,"How to use torch.triangular_solve, give an example?",">>> A = torch.randn(2, 2).triu()
>>> A
tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]])
>>> b = torch.randn(2, 3)
>>> b
tensor([[-0.0210,  2.3513, -1.5492],
        [ 1.5429,  0.7403, -1.0243]])
>>> torch.triangular_solve(b, A)
torch.return_types.triangular_solve(
solution=tensor([[ 1.7841,  2.9046, -2.5405],
        [ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]]))",6119
10611,"How to use torch.ones, give an example?",">>> torch.ones(2, 3)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

>>> torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])",423
10612,"How to use torch.t, give an example?",">>> x = torch.randn(())
>>> x
tensor(0.1995)
>>> torch.t(x)
tensor(0.1995)
>>> x = torch.randn(3)
>>> x
tensor([ 2.4320, -0.4608,  0.7702])
>>> torch.t(x)
tensor([ 2.4320, -0.4608,  0.7702])
>>> x = torch.randn(2, 3)
>>> x
tensor([[ 0.4875,  0.9158, -0.5872],
        [ 0.3938, -0.6929,  0.6932]])
>>> torch.t(x)
tensor([[ 0.4875,  0.3938],
        [ 0.9158, -0.6929],
        [-0.5872,  0.6932]])",88
10613,"How to use torch.gt, give an example?",">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])",7269
10614,"How to use torch.round, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
>>> torch.round(a)
tensor([ 1.,  1.,  1., -1.])",314
10615,"How to use torch.pca_lowrank, give an example?","- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding
  structure with randomness: probabilistic algorithms for
  constructing approximate matrix decompositions,
  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at
  `arXiv <http://arxiv.org/abs/0909.4061>`_).",7557
10616,"How to use torch.narrow, give an example?",">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> torch.narrow(x, 0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
>>> torch.narrow(x, 1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])",466
10617,"How to use torch.min, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[ 0.6750,  1.0857,  1.7197]])
>>> torch.min(a)
tensor(0.6750)",299
10618,"How  If keepdim is True, the output tensors are of the same size as
input except in the dimension dim where they are of size 1.
Otherwise, dim is squeezed (see torch.squeeze()), resulting in
the output tensors having 1 fewer dimension than input., give an example?",">>> a = torch.randn(4, 4)
>>> a
tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
        [-1.4644, -0.2635, -0.3651,  0.6134],
        [ 0.2457,  0.0384,  1.0128,  0.7015],
        [-0.1153,  2.9849,  2.1458,  0.5788]])
>>> torch.min(a, 1)
torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))",3441
10619,"How to use torch.multinomial, give an example?",">>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights
>>> torch.multinomial(weights, 2)
tensor([1, 2])
>>> torch.multinomial(weights, 4) # ERROR!
RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,
not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
>>> torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])",3462
10620,"How to use To get started, let’s look at a simpler, custom version of PyTorch’s Linear module.
This module applies an affine transformation to its input., give an example?","import torch
from torch import nn

class MyLinear(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(in_features, out_features))
    self.bias = nn.Parameter(torch.randn(out_features))

  def forward(self, input):
    return (input @ self.weight) + self.bias",7897
10621,"How to use This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be
constructed and called:, give an example?","m = MyLinear(4, 3)
sample_input = torch.randn(4)
m(sample_input)
: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)",7815
10622,"How to use Note that the module itself is callable, and that calling it invokes its forward() function.
This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
The “forward pass” is responsible for applying the computation represented by the module
to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of
module outputs with respect to its inputs, which can be used for “training” parameters through gradient
descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it
is not required to manually implement a backward() function for each module. The process of training
module parameters through successive forward / backward passes is covered in detail in
Neural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call to
parameters() or named_parameters(),
where the latter includes each parameter’s name:, give an example?","for parameter in m.named_parameters():
  print(parameter)
: ('weight', Parameter containing:
tensor([[ 1.0597,  1.1796,  0.8247],
        [-0.5080, -1.2635, -1.1045],
        [ 0.0593,  0.2469, -1.4299],
        [-0.4926, -0.5457,  0.4793]], requires_grad=True))
('bias', Parameter containing:
tensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))",4488
10623,"How to use Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.
The simplest way to do this is using the Sequential module. It allows us to chain together
multiple modules:, give an example?","net = nn.Sequential(
  MyLinear(4, 3),
  nn.ReLU(),
  MyLinear(3, 1)
)

sample_input = torch.randn(4)
net(sample_input)
: tensor([-0.6749], grad_fn=<AddBackward0>)",4257
10624,"How to use In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives
full flexibility on how submodules are used for a module’s computation.For example, here’s a simple neural network implemented as a custom module:, give an example?","import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.l0 = MyLinear(4, 3)
    self.l1 = MyLinear(3, 1)
  def forward(self, x):
    x = self.l0(x)
    x = F.relu(x)
    x = self.l1(x)
    return x",3721
10625,"How to use For example, here’s a simple neural network implemented as a custom module:This module is composed of two “children” or “submodules” (l0 and l1) that define the layers of
the neural network and are utilized for computation within the module’s forward() method. Immediate
children of a module can be iterated through via a call to children() or
named_children():, give an example?","net = Net()
for child in net.named_children():
  print(child)
: ('l0', MyLinear())
('l1', MyLinear())",2515
10626,"How to use This module is composed of two “children” or “submodules” (l0 and l1) that define the layers of
the neural network and are utilized for computation within the module’s forward() method. Immediate
children of a module can be iterated through via a call to children() or
named_children():To go deeper than just the immediate children, modules() and
named_modules() recursively iterate through a module and its child modules:, give an example?","class BigNet(nn.Module):
  def __init__(self):
    super().__init__()
    self.l1 = MyLinear(5, 4)
    self.net = Net()
  def forward(self, x):
    return self.net(self.l1(x))

big_net = BigNet()
for module in big_net.named_modules():
  print(module)
: ('', BigNet(
  (l1): MyLinear()
  (net): Net(
    (l0): MyLinear()
    (l1): MyLinear()
  )
))
('l1', MyLinear())
('net', Net(
  (l0): MyLinear()
  (l1): MyLinear()
))
('net.l0', MyLinear())
('net.l1', MyLinear())",7755
10627,"How to use To go deeper than just the immediate children, modules() and
named_modules() recursively iterate through a module and its child modules:Sometimes, it’s necessary for a module to dynamically define submodules.
The ModuleList and ModuleDict modules are useful here; they
register submodules from a list or dict:, give an example?","class DynamicNet(nn.Module):
  def __init__(self, num_layers):
    super().__init__()
    self.linears = nn.ModuleList(
      [MyLinear(4, 4) for _ in range(num_layers)])
    self.activations = nn.ModuleDict({
      'relu': nn.ReLU(),
      'lrelu': nn.LeakyReLU()
    })
    self.final = MyLinear(4, 1)
  def forward(self, x, act):
    for linear in self.linears:
      x = linear(x)
    x = self.activations[act](x)
    x = self.final(x)
    return x

dynamic_net = DynamicNet(3)
sample_input = torch.randn(4)
output = dynamic_net(sample_input, 'relu')",7900
10628,"How to use Sometimes, it’s necessary for a module to dynamically define submodules.
The ModuleList and ModuleDict modules are useful here; they
register submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.
This means that calls to parameters() and named_parameters() will
recursively include child parameters, allowing for convenient optimization of all parameters within the network:, give an example?","for parameter in dynamic_net.named_parameters():
  print(parameter)
: ('linears.0.weight', Parameter containing:
tensor([[-1.2051,  0.7601,  1.1065,  0.1963],
        [ 3.0592,  0.4354,  1.6598,  0.9828],
        [-0.4446,  0.4628,  0.8774,  1.6848],
        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))
('linears.0.bias', Parameter containing:
tensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))
('linears.1.weight', Parameter containing:
tensor([[ 2.1113, -0.0623, -1.0806,  0.3508],
        [-0.0550,  1.5317,  1.1064, -0.5562],
        [-0.4028, -0.6942,  1.5793, -1.0140],
        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))
('linears.1.bias', Parameter containing:
tensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))
('linears.2.weight', Parameter containing:
tensor([[-2.6340, -0.3887, -0.9979,  0.0767],
        [-0.3526,  0.8756, -1.5847, -0.6016],
        [-0.3269, -0.1608,  0.2897, -2.0829],
        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))
('linears.2.bias', Parameter containing:
tensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))
('final.weight', Parameter containing:
tensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))
('final.bias', Parameter containing:
tensor([0.3381], requires_grad=True))",5977
10629,"How to use For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.
This means that calls to parameters() and named_parameters() will
recursively include child parameters, allowing for convenient optimization of all parameters within the network:It’s also easy to move all parameters to a different device or change their precision using
to():, give an example?","# Move all parameters to a CUDA device
dynamic_net.to(device='cuda')

# Change precision of all parameters
dynamic_net.to(dtype=torch.float64)

dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))
: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)",2501
10630,"How to use Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s
Optimizers from torch.optim:, give an example?","# Create the network (from previous section) and optimizer
net = Net()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)

# Run a sample training loop that ""teaches"" the network
# to output the constant zero function
for _ in range(10000):
  input = torch.randn(4)
  output = net(input)
  loss = torch.abs(output)
  net.zero_grad()
  loss.backward()
  optimizer.step()",4637
10631,"How to use In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according
to its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the
key parts of training are present:After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the
value of l1’s weight parameter shows that its values are now much closer to 0 (as may be expected):, give an example?","print(net.l1.weight)
: Parameter containing:
tensor([[-0.0013],
        [ 0.0030],
        [-0.0008]], requires_grad=True)",3805
10632,"How to use In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.
Now, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. “state dictionary”):, give an example?","# Save the module
torch.save(net.state_dict(), 'net.pt')

...

# Load the module later on
new_net = Net()
new_net.load_state_dict(torch.load('net.pt'))
: <All keys matched successfully>",3791
10633,"How to use A module’s state_dict contains state that affects its computation. This includes, but is not limited to, the
module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module
computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”
and “non-persistent”. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want
the current value of the running mean to be considered part of the module’s state_dict so that it will be
restored when loading a serialized form of the module, but we don’t want it to be learnable.
This snippet shows how to use register_buffer() to accomplish this:, give an example?","class RunningMean(nn.Module):
  def __init__(self, num_features, momentum=0.9):
    super().__init__()
    self.momentum = momentum
    self.register_buffer('mean', torch.zeros(num_features))
  def forward(self, x):
    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x
    return self.mean",815
10634,"How to use As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want
the current value of the running mean to be considered part of the module’s state_dict so that it will be
restored when loading a serialized form of the module, but we don’t want it to be learnable.
This snippet shows how to use register_buffer() to accomplish this:Now, the current value of the running mean is considered part of the module’s state_dict
and will be properly restored when loading the module from disk:, give an example?","m = RunningMean(4)
for _ in range(10):
  input = torch.randn(4)
  m(input)

print(m.state_dict())
: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))

# Serialized form will contain the 'mean' tensor
torch.save(m.state_dict(), 'mean.pt')

m_loaded = RunningMean(4)
m_loaded.load_state_dict(torch.load('mean.pt'))
assert(torch.all(m.mean == m_loaded.mean))",1283
10635,"How to use Now, the current value of the running mean is considered part of the module’s state_dict
and will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module’s state_dict by marking them as non-persistent:, give an example?","self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)",4546
10636,"How to use As mentioned previously, buffers can be left out of the module’s state_dict by marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with
to():, give an example?","# Moves all module parameters and buffers to the specified device / dtype
m.to(device='cuda', dtype=torch.float64)",1308
10637,"How to use torch.flip, give an example?",">>> x = torch.arange(8).view(2, 2, 2)
>>> x
tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]]])
>>> torch.flip(x, [0, 1])
tensor([[[ 6,  7],
         [ 4,  5]],

        [[ 2,  3],
         [ 0,  1]]])",451
10638,"How to use torch.svd, give an example?","U, S, Vh = torch.linalg.svd(A, full_matrices=not some)
V = Vh.transpose(-2, -1).conj()",11227
10639,"How  U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with, give an example?",S = torch.svdvals(A),8042
10640,"How  Supports input of float, double, cfloat and cdouble data types.
The dtypes of U and V are the same as input’s. S will
always be real-valued, even if input is complex., give an example?",">>> a = torch.randn(5, 3)
>>> a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
>>> u, s, v = torch.svd(a)
>>> u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
>>> s
tensor([2.3289, 2.0315, 0.7806])
>>> v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
>>> a_big = torch.randn(7, 5, 3)
>>> u, s, v = torch.svd(a_big)
>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
tensor(2.6503e-06)",6118
10641,"How to use torch.real, give an example?",">>> x=torch.randn(4, dtype=torch.cfloat)
>>> x
tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])
>>> x.real
tensor([ 0.3100, -0.5445, -1.6492, -0.0638])",472
10642,"How to use torch.xlogy, give an example?",">>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.xlogy(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.xlogy(x, y)
tensor([1.0986, 1.3863, 0.0000])
>>> torch.xlogy(x, 4)
tensor([1.3863, 2.7726, 4.1589])
>>> torch.xlogy(2, y)
tensor([2.1972, 1.3863, 0.0000])",5904
10643,"How to use torch.quantize_per_channel, give an example?",">>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])
>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)
tensor([[-1.,  0.],
        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_channel_affine,
       scale=tensor([0.1000, 0.0100], dtype=torch.float64),
       zero_point=tensor([10,  0]), axis=0)
>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
tensor([[  0,  10],
        [100, 200]], dtype=torch.uint8)",465
10644,"How to use torch.enable_grad, give an example?",">>> x = torch.tensor([1.], requires_grad=True)
>>> with torch.no_grad():
...   with torch.enable_grad():
...     y = x * 2
>>> y.requires_grad
True
>>> y.backward()
>>> x.grad
>>> @torch.enable_grad()
... def doubler(x):
...     return x * 2
>>> with torch.no_grad():
...     z = doubler(x)
>>> z.requires_grad
True",1102
10645,"How to use torch.eig, give an example?",L_complex = torch.linalg.eigvals(A),11052
10646,"How  L, _ = torch.eig(A) should be replaced withL, V = torch.eig(A, eigenvectors=True) should be replaced with, give an example?","L_complex, V_complex = torch.linalg.eig(A)",4048
10647,"How to use At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader
class.  It represents a Python iterable over a dataset, with support forThese options are configured by the constructor arguments of a
DataLoader, which has signature:, give an example?","DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)",7439
10648,"How to use After fetching a list of samples using the indices from sampler, the function
passed as the collate_fn argument is used to collate lists of samples
into batches.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?","for indices in batch_sampler:
    yield collate_fn([dataset[i] for i in indices])",978
10649,"How to use In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?","dataset_iter = iter(dataset)
for indices in batch_sampler:
    yield collate_fn([next(dataset_iter) for _ in indices])",3799
10650,"How to use When automatic batching is disabled, the default collate_fn simply
converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?","for index in sampler:
    yield collate_fn(dataset[index])",8347
10651,"How  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?","for data in iter(dataset):
    yield collate_fn(data)",3800
10652,"How to use See the example below., give an example?","class SimpleCustomBatch:
    def __init__(self, data):
        transposed_data = list(zip(*data))
        self.inp = torch.stack(transposed_data[0], 0)
        self.tgt = torch.stack(transposed_data[1], 0)

    # custom memory pinning method on custom type
    def pin_memory(self):
        self.inp = self.inp.pin_memory()
        self.tgt = self.tgt.pin_memory()
        return self

def collate_wrapper(batch):
    return SimpleCustomBatch(batch)

inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
dataset = TensorDataset(inps, tgts)

loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,
                    pin_memory=True)

for batch_ndx, sample in enumerate(loader):
    print(sample.inp.is_pinned())
    print(sample.tgt.is_pinned())",5783
10653,"How to use torch.utils.data.IterableDataset, give an example?",">>> class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end > start, ""this example code only works with end >= start""
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         worker_info = torch.utils.data.get_worker_info()
...         if worker_info is None:  # single-process data loading, return the full iterator
...             iter_start = self.start
...             iter_end = self.end
...         else:  # in a worker process
...             # split workload
...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
...             worker_id = worker_info.id
...             iter_start = self.start + worker_id * per_worker
...             iter_end = min(iter_start + per_worker, self.end)
...         return iter(range(iter_start, iter_end))
...
>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
>>> ds = MyIterableDataset(start=3, end=7)

>>> # Single-process loading
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]

>>> # Mult-process loading with two worker processes
>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 5, 4, 6]

>>> # With even more workers
>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))
[3, 4, 5, 6]",8345
10654,"How to use torch.utils.data.random_split, give an example?",">>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))",5094
10655,"How to use torch.utils.data.WeightedRandomSampler, give an example?",">>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))
[4, 4, 1, 4, 5]
>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))
[0, 1, 4, 3, 2]",376
10656,"How to use torch.utils.data.BatchSampler, give an example?",">>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]",375
10657,"How to use torch.utils.data.distributed.DistributedSampler, give an example?",">>> sampler = DistributedSampler(dataset) if is_distributed else None
>>> loader = DataLoader(dataset, shuffle=(sampler is None),
...                     sampler=sampler)
>>> for epoch in range(start_epoch, n_epochs):
...     if is_distributed:
...         sampler.set_epoch(epoch)
...     train(loader)",3933
10658,"How to use torch.sum, give an example?",">>> a = torch.randn(1, 3)
>>> a
tensor([[ 0.1133, -0.9567,  0.2958]])
>>> torch.sum(a)
tensor(-0.5475)",297
10659,"How to use torch.bitwise_not, give an example?",">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)",398
10660,"How to use torch.vsplit, give an example?",">>> t = torch.arange(16.0).reshape(4,4)
>>> t
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.]])
>>> torch.vsplit(t, 2)
(tensor([[0., 1., 2., 3.],
         [4., 5., 6., 7.]]),
 tensor([[ 8.,  9., 10., 11.],
         [12., 13., 14., 15.]]))
>>> torch.vsplit(t, [3, 6])
(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]),
 tensor([[12., 13., 14., 15.]]),
 tensor([], size=(0, 4)))",386
10661,"How to use torch.hsplit, give an example?",">>> t = torch.arange(16.0).reshape(4,4)
>>> t
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.]])
>>> torch.hsplit(t, 2)
(tensor([[ 0.,  1.],
         [ 4.,  5.],
         [ 8.,  9.],
         [12., 13.]]),
 tensor([[ 2.,  3.],
         [ 6.,  7.],
         [10., 11.],
         [14., 15.]]))
>>> torch.hsplit(t, [3, 6])
(tensor([[ 0.,  1.,  2.],
         [ 4.,  5.,  6.],
         [ 8.,  9., 10.],
         [12., 13., 14.]]),
 tensor([[ 3.],
         [ 7.],
         [11.],
         [15.]]),
 tensor([], size=(4, 0)))",385
10662,"How to use torch.fliplr, give an example?",">>> x = torch.arange(4).view(2, 2)
>>> x
tensor([[0, 1],
        [2, 3]])
>>> torch.fliplr(x)
tensor([[1, 0],
        [3, 2]])",2476
10663,"How to use torch.cosh, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.1632,  1.1835, -0.6979, -0.7325])
>>> torch.cosh(a)
tensor([ 1.0133,  1.7860,  1.2536,  1.2805])",308
10664,"How to use torch.rad2deg, give an example?",">>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])
>>> torch.rad2deg(a)
tensor([[ 180.0233, -180.0233],
        [ 359.9894, -359.9894],
        [  89.9544,  -89.9544]])",340
10665,"How to use torch.broadcast_shapes, give an example?",">>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))
torch.Size([1, 3, 2])",7634
10666,"How to use torch.get_default_dtype, give an example?",">>> torch.get_default_dtype()  # initial default for floating point is torch.float32
torch.float32
>>> torch.set_default_dtype(torch.float64)
>>> torch.get_default_dtype()  # default is now changed to torch.float64
torch.float64
>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this
>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32",410
10667,"How to use torch.cartesian_prod, give an example?",">>> a = [1, 2, 3]
>>> b = [4, 5]
>>> list(itertools.product(a, b))
[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
>>> tensor_a = torch.tensor(a)
>>> tensor_b = torch.tensor(b)
>>> torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])",283
10668,"How to use torch.zeros_like, give an example?",">>> input = torch.empty(2, 3)
>>> torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])",373
10669,"How to use torch.flatten, give an example?",">>> t = torch.tensor([[[1, 2],
...                    [3, 4]],
...                   [[5, 6],
...                    [7, 8]]])
>>> torch.flatten(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
>>> torch.flatten(t, start_dim=1)
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])",8058
10670,"How to use torch.bitwise_and, give an example?",">>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([1, 0,  3], dtype=torch.int8)
>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ False, True, False])",397
10671,"How to use torch.digamma, give an example?",">>> a = torch.tensor([1, 0.5])
>>> torch.digamma(a)
tensor([-0.5772, -1.9635])",336
10672,"How to use This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transform nn.Module
instances. FX consists of three main components: a symbolic tracer,
an intermediate representation, and Python code generation. A
demonstration of these components in action:, give an example?","import torch
# Simple module for demonstration
class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.rand(3, 4))
        self.linear = torch.nn.Linear(4, 5)

    def forward(self, x):
        return self.linear(x + self.param).clamp(min=0.0, max=1.0)

module = MyModule()

from torch.fx import symbolic_trace
# Symbolic tracing frontend - captures the semantics of the module
symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)

# High-level intermediate representation (IR) - Graph representation
print(symbolic_traced.graph)
""""""
graph(x):
    %param : [#users=1] = self.param
    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})
    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})
    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})
    return clamp_1
""""""

# Code generation - valid Python code
print(symbolic_traced.code)
""""""
def forward(self, x):
    param = self.param
    add_1 = x + param;  x = param = None
    linear_1 = self.linear(add_1);  add_1 = None
    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None
    return clamp_1
""""""",2384
10673,"How to use What is an FX transform? Essentially, it’s a function that looks like this., give an example?","import torch
import torch.fx

def transform(m: nn.Module,
              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:
    # Step 1: Acquire a Graph representing the code in `m`

    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to
    # fx.Tracer.trace and constructing a GraphModule. We'll
    # split that out in our transform to allow the caller to
    # customize tracing behavior.
    graph : torch.fx.Graph = tracer_class().trace(m)

    # Step 2: Modify this Graph or create a new one
    graph = ...

    # Step 3: Construct a Module to return
    return torch.fx.GraphModule(m, graph)",8332
10674,"How to use NoteIt is also possible to modify an existing GraphModule instead of
creating a new one, like so:, give an example?","import torch
import torch.fx

def transform(m : nn.Module) -> nn.Module:
    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)

    # Modify gm.graph
    # <...>

    # Recompile the forward() method of `gm` from its Graph
    gm.recompile()

    return gm",3927
10675,"How to use Full treatment of the semantics of graphs can be found in the Graph
documentation, but we are going to cover the basics here. A Graph is
a data structure that represents a method on a GraphModule. The
information that this requires is:All three of these concepts are represented with Node instances.
Let’s see what we mean by that with a short example:, give an example?","import torch
import torch.fx

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.rand(3, 4))
        self.linear = torch.nn.Linear(4, 5)

    def forward(self, x):
        return torch.topk(torch.sum(
            self.linear(x + self.linear.weight).relu(), dim=-1), 3)

m = MyModule()
gm = torch.fx.symbolic_trace(m)

gm.graph.print_tabular()",1094
10676,"How to use One approach to building this new Graph is to directly manipulate your old
one. To aid in this, we can simply take the Graph we obtain from symbolic
tracing and modify it. For example, let’s say we desire to replace
torch.add() calls with torch.mul() calls., give an example?","import torch
import torch.fx

# Sample module
class M(torch.nn.Module):
    def forward(self, x, y):
        return torch.add(x, y)

def transform(m: torch.nn.Module,
              tracer_class : type = fx.Tracer) -> torch.nn.Module:
    graph : fx.Graph = tracer_class().trace(m)
    # FX represents its Graph as an ordered list of
    # nodes, so we can iterate through them.
    for node in graph.nodes:
        # Checks if we're calling a function (i.e:
        # torch.add)
        if node.op == 'call_function':
            # The target attribute is the function
            # that call_function calls.
            if node.target == torch.add:
                node.target = torch.mul

    graph.lint() # Does some checks to make sure the
                 # Graph is well-formed.

    return fx.GraphModule(m, graph)",4652
10677,"How to use One approach to building this new Graph is to directly manipulate your old
one. To aid in this, we can simply take the Graph we obtain from symbolic
tracing and modify it. For example, let’s say we desire to replace
torch.add() calls with torch.mul() calls.We can also do more involved Graph rewrites, such as
deleting or appending nodes. To aid in these transformations,
FX has utility functions for transforming the graph that can
be found in the Graph documentation. An
example of using these APIs to append a torch.relu() call
can be found below., give an example?","# Specifies the insertion point. Any nodes added to the
# Graph within this scope will be inserted after `node`
with traced.graph.inserting_after(node):
    # Insert a new `call_function` node calling `torch.relu`
    new_node = traced.graph.call_function(
        torch.relu, args=(node,))

    # We want all places that used the value of `node` to
    # now use that value after the `relu` call we've added.
    # We use the `replace_all_uses_with` API to do this.
    node.replace_all_uses_with(new_node)",8241
10678,"How to use Another way of manipulating Graphs is by reusing the Proxy
machinery used in symbolic tracing. For example, let’s
imagine that we wanted to write a transformation that decomposed
PyTorch functions into smaller operations. It would transform every
F.relu(x) call into (x > 0) * x. One possibility would be to
perform the requisite graph rewriting to insert the comparison and
multiplication after the F.relu, and then clean up the original
F.relu. However, we can automate this process by using Proxy
objects to automatically record operations into the Graph.To use this method, we write the operations that we want inserted as regular
PyTorch code and invoke that code with Proxy objects as arugments.
These Proxy objects will capture the operations that are performed
on them and append them to the Graph., give an example?","# Note that this decomposition rule can be read as regular Python
def relu_decomposition(x):
    return (x > 0) * x

decomposition_rules = {}
decomposition_rules[F.relu] = relu_decomposition

def decompose(model: torch.nn.Module,
              tracer_class : type = fx.Tracer) -> torch.nn.Module:
    """"""
    Decompose `model` into smaller constituent operations.
    Currently,this only supports decomposing ReLU into its
    mathematical definition: (x > 0) * x
    """"""
    graph : fx.Graph = tracer_class().trace(model)
    new_graph = fx.Graph()
    env = {}
    for node in graph.nodes:
        if node.op == 'call_function' and node.target in decomposition_rules:
            # By wrapping the arguments with proxies,
            # we can dispatch to the appropriate
            # decomposition rule and implicitly add it
            # to the Graph by symbolically tracing it.
            proxy_args = [
                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]
            output_proxy = decomposition_rules[node.target](*proxy_args)

            # Operations on `Proxy` always yield new `Proxy`s, and the
            # return value of our decomposition rule is no exception.
            # We need to extract the underlying `Node` from the `Proxy`
            # to use it in subsequent iterations of this transform.
            new_node = output_proxy.node
            env[node.name] = new_node
        else:
            # Default case: we don't have a decomposition rule for this
            # node, so just copy the node over into the new graph.
            new_node = new_graph.node_copy(node, lambda x: env[x.name])
            env[node.name] = new_node
    return fx.GraphModule(model, new_graph)",7927
10679,"How to use A useful code organizational pattern in FX is to loop over all the Nodes
in a Graph and execute them. This can be used for several things including
runtime analysis of values flowing through the graph or transformation of the code
via retracing with Proxys. For example, suppose we want to run a
GraphModule and record the torch.Tensor shape and dtype
properties on the nodes as we see them at runtime. That might look like:, give an example?","import torch
import torch.fx
from torch.fx.node import Node

from typing import Dict

class ShapeProp:
    """"""
    Shape propagation. This class takes a `GraphModule`.
    Then, its `propagate` method executes the `GraphModule`
    node-by-node with the given arguments. As each operation
    executes, the ShapeProp class stores away the shape and
    element type for the output values of each operation on
    the `shape` and `dtype` attributes of the operation's
    `Node`.
    """"""
    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def propagate(self, *args):
        args_iter = iter(args)
        env : Dict[str, Node] = {}

        def load_arg(a):
            return torch.fx.graph.map_arg(a, lambda n: env[n.name])

        def fetch_attr(target : str):
            target_atoms = target.split('.')
            attr_itr = self.mod
            for i, atom in enumerate(target_atoms):
                if not hasattr(attr_itr, atom):
                    raise RuntimeError(f""Node referenced nonexistant target {'.'.join(target_atoms[:i])}"")
                attr_itr = getattr(attr_itr, atom)
            return attr_itr

        for node in self.graph.nodes:
            if node.op == 'placeholder':
                result = next(args_iter)
            elif node.op == 'get_attr':
                result = fetch_attr(node.target)
            elif node.op == 'call_function':
                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
            elif node.op == 'call_method':
                self_obj, *args = load_arg(node.args)
                kwargs = load_arg(node.kwargs)
                result = getattr(self_obj, node.target)(*args, **kwargs)
            elif node.op == 'call_module':
                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))

            # This is the only code specific to shape propagation.
            # you can delete this `if` branch and this becomes
            # a generic GraphModule interpreter.
            if isinstance(result, torch.Tensor):
                node.shape = result.shape
                node.dtype = result.dtype

            env[node.name] = result

        return load_arg(self.graph.result)",881
10680,"How to use Because the output of most deep learning modules consists of floating
point torch.Tensor instances, checking for equivalence between
the results of two torch.nn.Module is not as straightforward
as doing a simple equality check. To motivate this, let’s use an
example:, give an example?","import torch
import torch.fx
import torchvision.models as models

def transform(m : torch.nn.Module) -> torch.nn.Module:
    gm = torch.fx.symbolic_trace(m)

    # Imagine we're doing some transforms here
    # <...>

    gm.recompile()

    return gm

resnet18 = models.resnet18()
transformed_resnet18 = transform(resnet18)

input_image = torch.randn(5, 3, 224, 224)

assert resnet18(input_image) == transformed_resnet18(input_image)
""""""
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
""""""",1425
10681,"How to use Because the output of most deep learning modules consists of floating
point torch.Tensor instances, checking for equivalence between
the results of two torch.nn.Module is not as straightforward
as doing a simple equality check. To motivate this, let’s use an
example:Here, we’ve tried to check equality of the values of two deep learning
models with the == equality operator. However, this is not well-
defined both due to the issue of that operator returning a tensor
and not a bool, but also because comparison of floating point values
should use a margin of error (or epsilon) to account for the
non-commutativity of floating point operations (see
here for more
details). We can use torch.allclose() instead, which will give
us an approximate comparison taking into account a relative and
absolute tolerance threshold:, give an example?","assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))",2800
10682,"How to use Invoke pdb to step into the running program. Although the code that
represents the Graph is not in any source file, we can still step
into it manually using pdb when the forward pass is invoked., give an example?","import torch
import torch.fx
import torchvision.models as models

def my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
    graph = tracer_class().trace(inp)
    # Transformation logic here
    # <...>

    # Return new Module
    return fx.GraphModule(inp, graph)

my_module = models.resnet18()
my_module_transformed = my_pass(my_module)

input_value = torch.randn(5, 3, 224, 224)

# When this line is executed at runtime, we will be dropped into an
# interactive `pdb` prompt. We can use the `step` or `s` command to
# step into the execution of the next line
import pdb; pdb.set_trace()

my_module_transformed(input_value)",3865
10683,"How to use If you’d like to run the same code multiple times, then it can be
a bit tedious to step to the right code with pdb. In that case, one
approach is to simply copy-paste the generated forward pass into
your code and examine it from there., give an example?","# Assume that `traced` is a GraphModule that has undergone some
# number of transforms

# Copy this code for later
print(traced)
# Print the code generated from symbolic tracing. This outputs:
""""""
def forward(self, y):
    x = self.x
    add_1 = x + y;  x = y = None
    return add_1
""""""

# Subclass the original Module
class SubclassM(M):
    def __init__(self):
        super().__init__()

    # Paste the generated `forward` function (the one we printed and
    # copied above) here
    def forward(self, y):
        x = self.x
        add_1 = x + y;  x = y = None
        return add_1

# Create an instance of the original, untraced Module. Then, create an
# instance of the Module with the copied `forward` function. We can
# now compare the output of both the original and the traced version.
pre_trace = M()
post_trace = SubclassM()",3592
10684,"How to use GraphModule.to_folder() is a method in GraphModule that allows
you to dump out the generated FX code to a folder. Although copying the
forward pass into the code often suffices as in Print the Generated Code,
it may be easier to examine modules and parameters using to_folder., give an example?","m = symbolic_trace(M())
m.to_folder(""foo"", ""Bar"")
from foo import Bar
y = Bar()",2742
10685,"How to use Now that we’ve identified that a transformation is creating incorrect
code, it’s time to debug the transformation itself. First, we’ll check
the Limitations of Symbolic Tracing section in the documentation.
Once we verify that tracing is working as expected, the goal
becomes figuring out what went wrong during our GraphModule
transformation. There may be a quick answer in
Writing Transformations, but, if not, there are several ways to
examine our traced module:, give an example?","# Sample Module
class M(torch.nn.Module):
    def forward(self, x, y):
        return x + y

# Create an instance of `M`
m = M()

# Symbolically trace an instance of `M` (returns a GraphModule). In
# this example, we'll only be discussing how to inspect a
# GraphModule, so we aren't showing any sample transforms for the
# sake of brevity.
traced = symbolic_trace(m)

# Print the code produced by tracing the module.
print(traced)
# The generated `forward` function is:
""""""
def forward(self, x, y):
    add_1 = x + y;  x = y = None
    return add_1
""""""

# Print the internal Graph.
print(traced.graph)
# This print-out returns:
""""""
graph(x, y):
    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})
    return add_1
""""""

# Print a tabular representation of the internal Graph.
traced.graph.print_tabular()
# This gives us:
""""""
opcode         name    target                   args      kwargs
-------------  ------  -----------------------  --------  --------
placeholder    x       x                        ()        {}
placeholder    y       y                        ()        {}
call_function  add_1   <built-in function add>  (x, y)    {}
""""""",4539
10686,"How to use Using the utility functions above, we can compare our traced Module
before and after we’ve applied our transformations. Sometimes, a
simple visual comparison is enough to trace down a bug. If it’s still
not clear what’s going wrong, a debugger like pdb can be a good
next step.Going off of the example above, consider the following code:, give an example?","# Sample user-defined function
def transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
    # Get the Graph from our traced Module
    g = tracer_class().trace(module)

    """"""
    Transformations on `g` go here
    """"""

    return fx.GraphModule(module, g)

# Transform the Graph
transformed = transform_graph(traced)

# Print the new code after our transforms. Check to see if it was
# what we expected
print(transformed)",8120
10687,"How to use The main limitation of symbolic tracing is it does not currently support
dynamic control flow. That is, loops or if statements where the
condition may depend on the input values of the program.For example, let’s examine the following program:, give an example?","def func_to_trace(x):
    if x.sum() > 0:
        return torch.relu(x)
    else:
        return torch.neg(x)

traced = torch.fx.symbolic_trace(func_to_trace)
""""""
  <...>
  File ""dyn.py"", line 6, in func_to_trace
    if x.sum() > 0:
  File ""pytorch/torch/fx/proxy.py"", line 155, in __bool__
    return self.tracer.to_bool(self)
  File ""pytorch/torch/fx/proxy.py"", line 85, in to_bool
    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow
""""""",2522
10688,"How to use On the other hand, so-called static control flow is supported. Static
control flow is loops or if statements whose value cannot change
across invocations. Typically, in PyTorch programs, this control flow
arises for code making decisions about a model’s architecture based on
hyper-parameters. As a concrete example:, give an example?","import torch
import torch.fx

class MyModule(torch.nn.Module):
    def __init__(self, do_activation : bool = False):
        super().__init__()
        self.do_activation = do_activation
        self.linear = torch.nn.Linear(512, 512)

    def forward(self, x):
        x = self.linear(x)
        # This if-statement is so-called static control flow.
        # Its condition does not depend on any input values
        if self.do_activation:
            x = torch.relu(x)
        return x

without_activation = MyModule(do_activation=False)
with_activation = MyModule(do_activation=True)

traced_without_activation = torch.fx.symbolic_trace(without_activation)
print(traced_without_activation.code)
""""""
def forward(self, x):
    linear_1 = self.linear(x);  x = None
    return linear_1
""""""

traced_with_activation = torch.fx.symbolic_trace(with_activation)
print(traced_with_activation.code)
""""""
import torch
def forward(self, x):
    linear_1 = self.linear(x);  x = None
    relu_1 = torch.relu(linear_1);  linear_1 = None
    return relu_1
""""""",4632
10689,"How to use The if-statement if self.do_activation does not depend on any
function inputs, thus it is static. do_activation can be considered
to be a hyper-parameter, and the traces of different instances of
MyModule with different values for that parameter have different
code. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control
flow. These instances can be made to support symbolic tracing by
removing the data dependencies on input values, for example by moving
values to Module attributes or by binding concrete values to arguments
during symbolic tracing:, give an example?","def f(x, flag):
    if flag: return x
    else: return x*2

fx.symbolic_trace(f) # Fails!

fx.symbolic_trace(f, concrete_args={'flag': True})",7110
10690,"How to use FX uses __torch_function__ as the mechanism by which it intercepts
calls (see the technical
overview
for more information about this). Some functions, such as builtin Python
functions or those in the math module, are not covered by
__torch_function__, but we would still like to capture them in
symbolic tracing. For example:, give an example?","import torch
import torch.fx
from math import sqrt

def normalize(x):
    """"""
    Normalize `x` by the size of the batch dimension
    """"""
    return x / sqrt(len(x))

# It's valid Python code
normalize(torch.rand(3, 4))

traced = torch.fx.symbolic_trace(normalize)
""""""
  <...>
  File ""sqrt.py"", line 9, in normalize
    return x / sqrt(len(x))
  File ""pytorch/torch/fx/proxy.py"", line 161, in __len__
    raise RuntimeError(""'len' is not supported in symbolic tracing by default. If you want ""
RuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope
""""""",2388
10691,"How to use FX uses __torch_function__ as the mechanism by which it intercepts
calls (see the technical
overview
for more information about this). Some functions, such as builtin Python
functions or those in the math module, are not covered by
__torch_function__, but we would still like to capture them in
symbolic tracing. For example:The error tells us that the built-in function len is not supported.
We can make it so that functions like this are recorded in the trace as
direct calls using the wrap() API:, give an example?","torch.fx.wrap('len')
torch.fx.wrap('sqrt')

traced = torch.fx.symbolic_trace(normalize)

print(traced.code)
""""""
import math
def forward(self, x):
    len_1 = len(x)
    sqrt_1 = math.sqrt(len_1);  len_1 = None
    truediv = x / sqrt_1;  x = sqrt_1 = None
    return truediv
""""""",7050
10692,"How to use The Tracer class is the class that underlies the
implementation of symbolic_trace. The behavior of tracing can be
customized by subclassing Tracer, like so:, give an example?","class MyCustomTracer(torch.fx.Tracer):
    # Inside here you can override various methods
    # to customize tracing. See the `Tracer` API
    # reference
    pass


# Let's use this custom tracer to trace through this module
class MyModule(torch.nn.Module):
    def forward(self, x):
        return torch.relu(x) + torch.ones(3, 4)

mod = MyModule()

traced_graph = MyCustomTracer().trace(mod)
# trace() returns a Graph. Let's wrap it up in a
# GraphModule to make it runnable
traced = torch.fx.GraphModule(mod, traced_graph)",6953
10693,"How to use Leaf Modules are the modules that appear as calls in the symbolic trace
rather than being traced through. The default set of leaf modules is the
set of standard torch.nn module instances. For example:, give an example?","class MySpecialSubmodule(torch.nn.Module):
    def forward(self, x):
        return torch.neg(x)

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 4)
        self.submod = MySpecialSubmodule()

    def forward(self, x):
        return self.submod(self.linear(x))

traced = torch.fx.symbolic_trace(MyModule())
print(traced.code)
# `linear` is preserved as a call, yet `submod` is traced though.
# This is because the default set of ""Leaf Modules"" includes all
# standard `torch.nn` modules.
""""""
import torch
def forward(self, x):
    linear_1 = self.linear(x);  x = None
    neg_1 = torch.neg(linear_1);  linear_1 = None
    return neg_1
""""""",4070
10694,"How to use Miscellanea, give an example?","@torch.fx.wrap
def torch_randn(x, shape):
    return torch.randn(shape)

def f(x):
    return x + torch_randn(x, 5)
fx.symbolic_trace(f)",721
10695,"How to use torch.fx.symbolic_trace, give an example?","def f(a, b):
    if b == True:
        return a
    else:
        return a*2",9110
10696,"How  Note that although you can still pass in different values of b, they will be ignored.We can also use concrete_args to eliminate data-structure handling from
our function. This will use pytrees to flatten your input. To avoid
overspecializing, pass in fx.PH for values that shouldn’t be
specialized. For example:, give an example?","def f(x):
    out = 0
    for v in x.values():
        out += v
    return out
f = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})
assert f({'a': 1, 'b': 2, 'c': 4}) == 7",4464
10697,"How to use torch.fx.wrap, give an example?","# foo/bar/baz.py
def my_custom_function(x, y):
    return x * x + y * y

torch.fx.wrap('my_custom_function')

def fn_to_be_traced(x, y):
    # When symbolic tracing, the below call to my_custom_function will be inserted into
    # the graph rather than tracing it.
    return my_custom_function(x, y)",7516
10698,"How  This function can also equivalently be used as a decorator:, give an example?","# foo/bar/baz.py
@torch.fx.wrap
def my_custom_function(x, y):
    return x * x + y * y",7511
10699,"How to use For example, the following code, give an example?","import torch
import torch.fx

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.rand(3, 4))
        self.linear = torch.nn.Linear(4, 5)

    def forward(self, x):
        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)

m = MyModule()
gm = torch.fx.symbolic_trace(m)",2527
10700,"How to use Will produce the following Graph:, give an example?",print(gm.graph),2528
10701,"How to use API Reference, give an example?","graph(x):
    %linear_weight : [#users=1] = self.linear.weight
    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})
    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})
    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})
    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})
    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})
    return topk_1",9489
10702,"How to use torch.fx.Graph.eliminate_dead_code, give an example?","def forward(self, x):
    a = x + 1
    return x + self.attr_1",1432
10703,"How  Before dead code is eliminated, a from a = x + 1 below has no users
and thus can be eliminated from the graph without having an effect.After dead code is eliminated, a = x + 1 has been removed, and the rest
of forward remains., give an example?","def forward(self, x):
    return x + self.attr_1",1433
10704,"How to use torch.fx.Graph.inserting_after, give an example?","with g.inserting_after(n):
    ... # inserting after node n
... # insert point restored to what it was previously
g.inserting_after(n) #  set the insert point permanently",5826
10705,"How to use torch.fx.Graph.inserting_before, give an example?","with g.inserting_before(n):
    ... # inserting before node n
... # insert point restored to what it was previously
g.inserting_before(n) #  set the insert point permanently",5827
10706,"How to use torch.fx.Graph.node_copy, give an example?","# Copying all the nodes in `g` into `new_graph`
g : torch.fx.Graph = ...
new_graph = torch.fx.graph()
value_remap = {}
for node in g.nodes:
    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])",43
10707,"How to use torch.fx.Node.prepend, give an example?","Before: p -> self
        bx -> x -> ax
After:  p -> x -> self
        bx -> ax",1434
10708,"How to use Methods in the Interpreter class can be overridden to customize
the behavior of execution. The map of overrideable methods
in terms of call hierarchy:, give an example?","run()
    +-- run_node
        +-- placeholder()
        +-- get_attr()
        +-- call_function()
        +-- call_method()
        +-- call_module()
        +-- output()",4240
10709,"How to use Suppose we want to swap all instances of torch.neg with
torch.sigmoid and vice versa (including their Tensor
method equivalents). We could subclass Interpreter like so:, give an example?","class NegSigmSwapInterpreter(Interpreter):
    def call_function(self, target : Target,
                      args : Tuple, kwargs : Dict) -> Any:
        if target == torch.sigmoid:
            return torch.neg(*args, **kwargs)
        return super().call_function(n)

    def call_method(self, target : Target,
                    args : Tuple, kwargs : Dict) -> Any:
        if target == 'neg':
            call_self, *args_tail = args
            return call_self.sigmoid(*args_tail, **kwargs)
        return super().call_method(n)

def fn(x):
    return torch.sigmoid(x).neg()

gm = torch.fx.symbolic_trace(fn)
input = torch.randn(3, 4)
result = NegSigmSwapInterpreter(gm).run(input)
torch.testing.assert_allclose(result, torch.neg(input).sigmoid())",6152
10710,"How to use Suppose we want to swap all instances of torch.neg with
torch.sigmoid and vice versa (including their Tensor
method equivalents). We could subclass Transformer like so:, give an example?","class NegSigmSwapXformer(Transformer):
    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
        if target == torch.sigmoid:
            return torch.neg(*args, **kwargs)
        return super().call_function(n)

    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
        if target == 'neg':
            call_self, *args_tail = args
            return call_self.sigmoid(*args_tail, **kwargs)
        return super().call_method(n)

def fn(x):
    return torch.sigmoid(x).neg()

gm = torch.fx.symbolic_trace(fn)

transformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()
input = torch.randn(3, 4)
torch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())",6153
10711,"How to use torch.fx.replace_pattern, give an example?","class Match(NamedTuple):
    # Node from which the match was found
    anchor: Node
    # Maps nodes in the pattern subgraph to nodes in the larger graph
    nodes_map: Dict[Node, Node]",810
10712,"How  When the pattern is matched, it will be removed from the larger
function and replaced by replacement. If there are multiple
matches for pattern in the larger function, each non-overlapping
match will be replaced. In the case of a match overlap, the first
found match in the set of overlapping matches will be replaced.
(“First” here being defined as the first in a topological ordering
of the Nodes’ use-def relationships. In most cases, the first Node
is the parameter that appears directly after self, while the
last Node is whatever the function returns.)One important thing to note is that the parameters of the
pattern Callable must be used in the Callable itself,
and the parameters of the replacement Callable must match
the pattern. The first rule is why, in the above code block, the
forward function has parameters x, w1, w2, but the
pattern function only has parameters w1, w2. pattern
doesn’t use x, so it shouldn’t specify x as a parameter.
As an example of the second rule, consider replacing, give an example?","def pattern(x, y):
    return torch.neg(x) + torch.relu(y)",8380
10713,"How  One important thing to note is that the parameters of the
pattern Callable must be used in the Callable itself,
and the parameters of the replacement Callable must match
the pattern. The first rule is why, in the above code block, the
forward function has parameters x, w1, w2, but the
pattern function only has parameters w1, w2. pattern
doesn’t use x, so it shouldn’t specify x as a parameter.
As an example of the second rule, consider replacingwith, give an example?","def replacement(x, y):
    return torch.relu(x)",4656
10714,"How  In this case, replacement needs the same number of parameters
as pattern (both x and y), even though the parameter
y isn’t used in replacement.After calling subgraph_rewriter.replace_pattern, the generated
Python code looks like this:, give an example?","def forward(self, x, w1, w2):
    stack_1 = torch.stack([w1, w2])
    sum_1 = stack_1.sum()
    stack_2 = torch.stack([w1, w2])
    sum_2 = stack_2.sum()
    max_1 = torch.max(sum_1)
    add_1 = x + max_1
    max_2 = torch.max(sum_2)
    add_2 = add_1 + max_2
    return add_2",3801
10715,"How to use torch.split, give an example?",">>> a = torch.arange(10).reshape(5,2)
>>> a
tensor([[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]])
>>> torch.split(a, 2)
(tensor([[0, 1],
         [2, 3]]),
 tensor([[4, 5],
         [6, 7]]),
 tensor([[8, 9]]))
>>> torch.split(a, [1,4])
(tensor([[0, 1]]),
 tensor([[2, 3],
         [4, 5],
         [6, 7],
         [8, 9]]))",3482
10716,"How to use torch.sign, give an example?",">>> a = torch.tensor([0.7, -1.2, 0., 2.3])
>>> a
tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
>>> torch.sign(a)
tensor([ 1., -1.,  0.,  1.])",334
10717,"How to use torch.utils.cpp_extension.CppExtension, give an example?",">>> from setuptools import setup
>>> from torch.utils.cpp_extension import BuildExtension, CppExtension
>>> setup(
        name='extension',
        ext_modules=[
            CppExtension(
                name='extension',
                sources=['extension.cpp'],
                extra_compile_args=['-g']),
        ],
        cmdclass={
            'build_ext': BuildExtension
        })",1068
10718,"How to use torch.utils.cpp_extension.CUDAExtension, give an example?",">>> from setuptools import setup
>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension
>>> setup(
        name='cuda_extension',
        ext_modules=[
            CUDAExtension(
                    name='cuda_extension',
                    sources=['extension.cpp', 'extension_kernel.cu'],
                    extra_compile_args={'cxx': ['-g'],
                                        'nvcc': ['-O2']})
        ],
        cmdclass={
            'build_ext': BuildExtension
        })",1067
10719,"How to use torch.utils.cpp_extension.load, give an example?",">>> from torch.utils.cpp_extension import load
>>> module = load(
        name='extension',
        sources=['extension.cpp', 'extension_kernel.cu'],
        extra_cflags=['-O2'],
        verbose=True)",1510
10720,"How to use torch.utils.cpp_extension.load_inline, give an example?",">>> from torch.utils.cpp_extension import load_inline
>>> source = \'\'\'
at::Tensor sin_add(at::Tensor x, at::Tensor y) {
  return x.sin() + y.sin();
}
\'\'\'
>>> module = load_inline(name='inline_extension',
                         cpp_sources=[source],
                         functions=['sin_add'])",5780
10721,"How to use torch.repeat_interleave, give an example?",">>> x = torch.tensor([1, 2, 3])
>>> x.repeat_interleave(2)
tensor([1, 1, 2, 2, 3, 3])
>>> y = torch.tensor([[1, 2], [3, 4]])
>>> torch.repeat_interleave(y, 2)
tensor([1, 1, 2, 2, 3, 3, 4, 4])
>>> torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]])
>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],
        [3, 4]])",463
10722,"How to use torch.add, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
>>> torch.add(a, 20)
tensor([ 20.0202,  21.0985,  21.3506,  19.3944])",3425
10723,"How  If other is of type FloatTensor or DoubleTensor, alpha must be
a real number, otherwise it should be an integer., give an example?",">>> a = torch.randn(4)
>>> a
tensor([-0.9732, -0.3497,  0.6245,  0.4022])
>>> b = torch.randn(4, 1)
>>> b
tensor([[ 0.3743],
        [-1.7724],
        [-0.5811],
        [-0.8017]])
>>> torch.add(a, b, alpha=10)
tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
        [-18.6971, -18.0736, -17.0994, -17.3216],
        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])",3466
10724,"How to use torch.kron, give an example?",">>> mat1 = torch.eye(2)
>>> mat2 = torch.ones(2, 2)
>>> torch.kron(mat1, mat2)
tensor([[1., 1., 0., 0.],
        [1., 1., 0., 0.],
        [0., 0., 1., 1.],
        [0., 0., 1., 1.]])

>>> mat1 = torch.eye(2)
>>> mat2 = torch.arange(1, 5).reshape(2, 2)
>>> torch.kron(mat1, mat2)
tensor([[1., 2., 0., 0.],
        [3., 4., 0., 0.],
        [0., 0., 1., 2.],
        [0., 0., 3., 4.]])",6135
10725,"How to use torch.polar, give an example?",">>> import numpy as np
>>> abs = torch.tensor([1, 2], dtype=torch.float64)
>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)
>>> z = torch.polar(abs, angle)
>>> z
tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)",355
10726,"How to use torch.use_deterministic_algorithms, give an example?",">>> torch.use_deterministic_algorithms(True)

# Forward mode nondeterministic error
>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))
...
RuntimeError: index_copy does not have a deterministic implementation...

# Backward mode nondeterministic error
>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()
...
RuntimeError: index_add_cuda_ does not have a deterministic implementation...",4466
10727,"How to use torch.addcmul, give an example?",">>> t = torch.randn(1, 3)
>>> t1 = torch.randn(3, 1)
>>> t2 = torch.randn(1, 3)
>>> torch.addcmul(t, t1, t2, value=0.1)
tensor([[-0.8635, -0.6391,  1.6174],
        [-0.7617, -0.5879,  1.7388],
        [-0.8353, -0.6249,  1.6511]])",2546
10728,"How to use torch.fmax, give an example?",">>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])
>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])
>>> torch.fmax(a, b)
tensor([9.7000, 0.5000, 3.1000,    nan])",6112
10729,"How to use torch.tensor_split, give an example?",">>> x = torch.arange(8)
>>> torch.tensor_split(x, 3)
(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))

>>> x = torch.arange(7)
>>> torch.tensor_split(x, 3)
(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))
>>> torch.tensor_split(x, (1, 6))
(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))

>>> x = torch.arange(14).reshape(2, 7)
>>> x
tensor([[ 0,  1,  2,  3,  4,  5,  6],
        [ 7,  8,  9, 10, 11, 12, 13]])
>>> torch.tensor_split(x, 3, dim=1)
(tensor([[0, 1, 2],
        [7, 8, 9]]),
 tensor([[ 3,  4],
        [10, 11]]),
 tensor([[ 5,  6],
        [12, 13]]))
>>> torch.tensor_split(x, (1, 6), dim=1)
(tensor([[0],
        [7]]),
 tensor([[ 1,  2,  3,  4,  5],
        [ 8,  9, 10, 11, 12]]),
 tensor([[ 6],
        [13]]))",450
10730,"How to use torch.std, give an example?",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std(a, unbiased=False)
tensor(0.4188)",3533
10731,"How to use torch.cos, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
>>> torch.cos(a)
tensor([ 0.1395,  0.2957,  0.6553,  0.5574])",315
10732,"How to use This is the simplest to apply form of quantization where the weights are
quantized ahead of time but the activations are dynamically quantized
during inference. This is used for situations where the model execution time
is dominated by loading weights from memory rather than computing the matrix
multiplications. This is true for for LSTM and Transformer type models with
small batch size.Diagram:, give an example?","# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                 /
linear_weight_fp32

# dynamically quantized model
# linear and LSTM weights are in int8
previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32
                     /
   linear_weight_int8",2125
10733,"How to use Diagram:API example:, give an example?","import torch

# define a floating point model
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.fc = torch.nn.Linear(4, 4)

    def forward(self, x):
        x = self.fc(x)
        return x

# create a model instance
model_fp32 = M()
# create a quantized model instance
model_int8 = torch.quantization.quantize_dynamic(
    model_fp32,  # the original model
    {torch.nn.Linear},  # a set of layers to dynamically quantize
    dtype=torch.qint8)  # the target dtype for quantized weights

# run the model
input_fp32 = torch.randn(4, 4, 4, 4)
res = model_int8(input_fp32)",2132
10734,"How to use Static quantization quantizes the weights and activations of the model.  It
fuses activations into preceding layers where possible.  It requires
calibration with a representative dataset to determine optimal quantization
parameters for activations. Post Training Quantization is typically used when
both memory bandwidth and compute savings are important with CNNs being a
typical use case.  Static quantization is also known as Post Training
Quantization or PTQ.Diagram:, give an example?","# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                    /
    linear_weight_fp32

# statically quantized model
# weights and activations are in int8
previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
                    /
  linear_weight_int8",2124
10735,"How to use Diagram:, give an example?","import torch

# define a floating point model where some layers could be statically quantized
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        # QuantStub converts tensors from floating point to quantized
        self.quant = torch.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.relu = torch.nn.ReLU()
        # DeQuantStub converts tensors from quantized to floating point
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        # manually specify where tensors will be converted from floating
        # point to quantized in the quantized model
        x = self.quant(x)
        x = self.conv(x)
        x = self.relu(x)
        # manually specify where tensors will be converted from quantized
        # to floating point in the quantized model
        x = self.dequant(x)
        return x

# create a model instance
model_fp32 = M()

# model must be set to eval mode for static quantization logic to work
model_fp32.eval()

# attach a global qconfig, which contains information about what kind
# of observers to attach. Use 'fbgemm' for server inference and
# 'qnnpack' for mobile inference. Other quantization configurations such
# as selecting symmetric or assymetric quantization and MinMax or L2Norm
# calibration techniques can be specified here.
model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# Fuse the activations to preceding layers, where applicable.
# This needs to be done manually depending on the model architecture.
# Common fusions include `conv + relu` and `conv + batchnorm + relu`
model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])

# Prepare the model for static quantization. This inserts observers in
# the model that will observe activation tensors during calibration.
model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)

# calibrate the prepared model to determine quantization parameters for activations
# in a real world setting, the calibration would be done with a representative dataset
input_fp32 = torch.randn(4, 1, 4, 4)
model_fp32_prepared(input_fp32)

# Convert the observed model to a quantized model. This does several things:
# quantizes the weights, computes and stores the scale and bias value to be
# used with each activation tensor, and replaces key operators with quantized
# implementations.
model_int8 = torch.quantization.convert(model_fp32_prepared)

# run the model, relevant calculations will happen in int8
res = model_int8(input_fp32)",2130
10736,"How to use Quantization Aware Training models the effects of quantization during training
allowing for higher accuracy compared to other quantization methods.  During
training, all calculations are done in floating point, with fake_quant modules
modeling the effects of quantization by clamping and rounding to simulate the
effects of INT8.  After model conversion, weights and
activations are quantized, and activations are fused into the preceding layer
where possible.  It is commonly used with CNNs and yields a higher accuracy
compared to static quantization.  Quantization Aware Training is also known as
QAT.Diagram:, give an example?","# original model
# all tensors and computations are in floating point
previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                      /
    linear_weight_fp32

# model with fake_quants for modeling quantization numerics during training
previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32
                           /
   linear_weight_fp32 -- fq

# quantized model
# weights and activations are in int8
previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
                     /
   linear_weight_int8",2123
10737,"How  Diagram:, give an example?","import torch

# define a floating point model where some layers could benefit from QAT
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        # QuantStub converts tensors from floating point to quantized
        self.quant = torch.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.bn = torch.nn.BatchNorm2d(1)
        self.relu = torch.nn.ReLU()
        # DeQuantStub converts tensors from quantized to floating point
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.dequant(x)
        return x

# create a model instance
model_fp32 = M()

# model must be set to train mode for QAT logic to work
model_fp32.train()

# attach a global qconfig, which contains information about what kind
# of observers to attach. Use 'fbgemm' for server inference and
# 'qnnpack' for mobile inference. Other quantization configurations such
# as selecting symmetric or assymetric quantization and MinMax or L2Norm
# calibration techniques can be specified here.
model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')

# fuse the activations to preceding layers, where applicable
# this needs to be done manually depending on the model architecture
model_fp32_fused = torch.quantization.fuse_modules(model_fp32,
    [['conv', 'bn', 'relu']])

# Prepare the model for QAT. This inserts observers and fake_quants in
# the model that will observe weight and activation tensors during calibration.
model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)

# run the training loop (not shown)
training_loop(model_fp32_prepared)

# Convert the observed model to a quantized model. This does several things:
# quantizes the weights, computes and stores the scale and bias value to be
# used with each activation tensor, fuses modules where appropriate,
# and replaces key operators with quantized implementations.
model_fp32_prepared.eval()
model_int8 = torch.quantization.convert(model_fp32_prepared)

# run the model, relevant calculations will happen in int8
res = model_int8(input_fp32)",2131
10738,"How to use There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function)., give an example?","import torch.quantization.quantize_fx as quantize_fx
import copy

model_fp = UserModel(...)

#
# post training dynamic/weight_only quantization
#

# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model
model_to_quantize = copy.deepcopy(model_fp)
model_to_quantize.eval()
qconfig_dict = {"""": torch.quantization.default_dynamic_qconfig}
# prepare
model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)
# no calibration needed when we only have dynamici/weight_only quantization
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)

#
# post training static quantization
#

model_to_quantize = copy.deepcopy(model_fp)
qconfig_dict = {"""": torch.quantization.get_default_qconfig('qnnpack')}
model_to_quantize.eval()
# prepare
model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)
# calibrate (not shown)
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)

#
# quantization aware training for static quantization
#

model_to_quantize = copy.deepcopy(model_fp)
qconfig_dict = {"""": torch.quantization.get_default_qat_qconfig('qnnpack')}
model_to_quantize.train()
# prepare
model_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)
# training loop (not shown)
# quantize
model_quantized = quantize_fx.convert_fx(model_prepared)

#
# fusion
#
model_to_quantize = copy.deepcopy(model_fp)
model_fused = quantize_fx.fuse_fx(model_to_quantize)",7402
10739,"How to use If you see an error similar to:, give an example?",RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...,3577
10740,"How to use If you see an error similar to:This means that you are trying to pass a non-quantized Tensor to a quantized
kernel. A common workaround is to use torch.quantization.QuantStub to
quantize the tensor.  This needs to be done manually in Eager mode quantization.
An e2e example:, give an example?","class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = torch.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)

    def forward(self, x):
        # during the convert step, this will be replaced with a
        # `quantize_per_tensor` call
        x = self.quant(x)
        x = self.conv(x)
        return x",7723
10741,"How  If you see an error similar to:, give an example?",RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.,3576
10742,"How to use If you see an error similar to:This means that you are trying to pass a quantized Tensor to a non-quantized
kernel. A common workaround is to use torch.quantization.DeQuantStub to
dequantize the tensor.  This needs to be done manually in Eager mode quantization.
An e2e example:, give an example?","class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = torch.quantization.QuantStub()
        self.conv1 = torch.nn.Conv2d(1, 1, 1)
        # this module will not be quantized (see `qconfig = None` logic below)
        self.conv2 = torch.nn.Conv2d(1, 1, 1)
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        # during the convert step, this will be replaced with a
        # `quantize_per_tensor` call
        x = self.quant(x)
        x = self.conv1(x)
        # during the convert step, this will be replaced with a
        # `dequantize` call
        x = self.dequant(x)
        x = self.conv2(x)
        return x

m = M()
m.qconfig = some_qconfig
# turn off quantization for conv2
m.conv2.qconfig = None",7724
10743,"How to use torch.fake_quantize_per_tensor_affine, give an example?",">>> x = torch.randn(4)
>>> x
tensor([ 0.0552,  0.9730,  0.3973, -1.0780])
>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)
tensor([0.1000, 1.0000, 0.4000, 0.0000])",457
10744,"How to use torch.baddbmm, give an example?",">>> M = torch.randn(10, 3, 5)
>>> batch1 = torch.randn(10, 3, 4)
>>> batch2 = torch.randn(10, 4, 5)
>>> torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])",7775
10745,"How to use torch.ge, give an example?",">>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])",7268
10746,"How to use torch.all, give an example?",">>> a = torch.rand(1, 2).bool()
>>> a
tensor([[False, True]], dtype=torch.bool)
>>> torch.all(a)
tensor(False, dtype=torch.bool)
>>> a = torch.arange(0, 3)
>>> a
tensor([0, 1, 2])
>>> torch.all(a)
tensor(False)",290
10747,"How to use torch.dot, give an example?",">>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
tensor(7)",403
10748,"How to use Once you’ve installed TensorBoard, these utilities let you log PyTorch models
and metrics into a directory for visualization within the TensorBoard UI.
Scalars, images, histograms, graphs, and embedding visualizations are all
supported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption
and visualization by TensorBoard. For example:, give an example?","import torch
import torchvision
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms

# Writer will output to ./runs/ directory by default
writer = SummaryWriter()

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
model = torchvision.models.resnet50(False)
# Have ResNet model take in grayscale rather than RGB
model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
images, labels = next(iter(trainloader))

grid = torchvision.utils.make_grid(images)
writer.add_image('images', grid, 0)
writer.add_graph(model, images)
writer.close()",4648
10749,"How to use The SummaryWriter class is your main entry to log data for consumption
and visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable
and runnable with:, give an example?","pip install tensorboard
tensorboard --logdir=runs",6949
10750,"How to use This can then be visualized with TensorBoard, which should be installable
and runnable with:Lots of information can be logged for one experiment. To avoid cluttering
the UI and have better result clustering, we can group plots by naming them
hierarchically. For example, “Loss/train” and “Loss/test” will be grouped
together, while “Accuracy/train” and “Accuracy/test” will be grouped separately
in the TensorBoard interface., give an example?","from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)",7474
10751,"How to use torch.utils.tensorboard.writer.SummaryWriter.__init__, give an example?","from torch.utils.tensorboard import SummaryWriter

# create a summary writer with automatically generated folder name.
writer = SummaryWriter()
# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/

# create a summary writer using the specified folder name.
writer = SummaryWriter(""my_experiment"")
# folder location: my_experiment

# create a summary writer with comment appended.
writer = SummaryWriter(comment=""LR_0.1_BATCH_16"")
# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/",9398
10752,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalar, give an example?","from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
x = range(100)
for i in x:
    writer.add_scalar('y=2x', i * 2, i)
writer.close()",9404
10753,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalars, give an example?","from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
r = 5
for i in range(100):
    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),
                                    'xcosx':i*np.cos(i/r),
                                    'tanx': np.tan(i/r)}, i)
writer.close()
# This call adds three values to the same scalar plot with the tag
# 'run_14h' in TensorBoard's scalar section.",9403
10754,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_histogram, give an example?","from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter()
for i in range(10):
    x = np.random.random(1000)
    writer.add_histogram('distribution centers', x + i, i)
writer.close()",9400
10755,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_image, give an example?","from torch.utils.tensorboard import SummaryWriter
import numpy as np
img = np.zeros((3, 100, 100))
img[0] = np.arange(0, 10000).reshape(100, 100) / 10000
img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000

img_HWC = np.zeros((100, 100, 3))
img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000
img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000

writer = SummaryWriter()
writer.add_image('my_image', img, 0)

# If you have non-default dimension setting, set the dataformats argument.
writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')
writer.close()",4495
10756,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_images, give an example?","from torch.utils.tensorboard import SummaryWriter
import numpy as np

img_batch = np.zeros((16, 3, 100, 100))
for i in range(16):
    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i
    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i

writer = SummaryWriter()
writer.add_images('my_image_batch', img_batch, 0)
writer.close()",4494
10757,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_text, give an example?","writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)",11423
10758,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_embedding, give an example?","import keyword
import torch
meta = []
while len(meta)<100:
    meta = meta+keyword.kwlist # get some strings
meta = meta[:100]

for i, v in enumerate(meta):
    meta[i] = v+str(i)

label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
    label_img[i]*=i/100.0

writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)",9594
10759,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve, give an example?","from torch.utils.tensorboard import SummaryWriter
import numpy as np
labels = np.random.randint(2, size=100)  # binary label
predictions = np.random.rand(100)
writer = SummaryWriter()
writer.add_pr_curve('pr_curve', labels, predictions, 0)
writer.close()",9399
10760,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars, give an example?","layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},
             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],
                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}

writer.add_custom_scalars(layout)",9774
10761,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_mesh, give an example?","from torch.utils.tensorboard import SummaryWriter
vertices_tensor = torch.as_tensor([
    [1, 1, 1],
    [-1, -1, 1],
    [1, -1, -1],
    [-1, 1, -1],
], dtype=torch.float).unsqueeze(0)
colors_tensor = torch.as_tensor([
    [255, 0, 0],
    [0, 255, 0],
    [0, 0, 255],
    [255, 0, 255],
], dtype=torch.int).unsqueeze(0)
faces_tensor = torch.as_tensor([
    [0, 2, 3],
    [0, 3, 1],
    [0, 1, 2],
    [1, 3, 2],
], dtype=torch.int).unsqueeze(0)

writer = SummaryWriter()
writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)

writer.close()",9401
10762,"How to use torch.utils.tensorboard.writer.SummaryWriter.add_hparams, give an example?","from torch.utils.tensorboard import SummaryWriter
with SummaryWriter() as w:
    for i in range(5):
        w.add_hparams({'lr': 0.1*i, 'bsize': i},
                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})",9402
10763,"How to use torch.linspace, give an example?",">>> torch.linspace(3, 10, steps=5)
tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])
>>> torch.linspace(-10, 10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
>>> torch.linspace(start=-10, end=10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
>>> torch.linspace(start=-10, end=10, steps=1)
tensor([-10.])",417
10764,"How to use torch.lcm, give an example?",">>> a = torch.tensor([5, 10, 15])
>>> b = torch.tensor([3, 4, 5])
>>> torch.lcm(a, b)
tensor([15, 20, 15])
>>> c = torch.tensor([3])
>>> torch.lcm(a, c)
tensor([15, 30, 15])",1457
10765,"How to use The default behavior (letting .grads be None before the first
backward(), such that their layout is created according to 1 or 2,
and retained over time according to 3 or 4) is recommended for best performance.
Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad
layouts.In fact, resetting all .grads to None before each
accumulation phase, e.g.:, give an example?","for iterations...
    ...
    for param in model.parameters():
        param.grad = None
    loss.backward()",7021
10766,"How to use torch.autograd.Function, give an example?",">>> class Exp(Function):
>>>
>>>     @staticmethod
>>>     def forward(ctx, i):
>>>         result = i.exp()
>>>         ctx.save_for_backward(result)
>>>         return result
>>>
>>>     @staticmethod
>>>     def backward(ctx, grad_output):
>>>         result, = ctx.saved_tensors
>>>         return grad_output * result
>>>
>>> #Use it by calling the apply method:
>>> output = Exp.apply(input)",4341
10767,"How to use torch.autograd.profiler.profile, give an example?",">>> x = torch.randn((1, 1), requires_grad=True)
>>> with torch.autograd.profiler.profile() as prof:
>>>     for _ in range(100):  # any normal python code, really!
>>>         y = x ** 2
>>          y.backward()
>>> # NOTE: some columns were removed for brevity
>>> print(prof.key_averages().table(sort_by=""self_cpu_time_total""))
-----------------------------------  ---------------  ---------------  ---------------
Name                                 Self CPU total   CPU time avg     Number of Calls
-----------------------------------  ---------------  ---------------  ---------------
mul                                  32.048ms         32.048ms         200
pow                                  27.041ms         27.041ms         200
PowBackward0                         9.727ms          55.483ms         100
torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
torch::autograd::GraphRoot           691.816us        691.816us        100
-----------------------------------  ---------------  ---------------  ---------------",453
10768,"How to use It is useful when running the program under nvprof:, give an example?",nvprof --profile-from-start off -o trace_name.prof -- <regular command here>,3953
10769,"How to use torch.autograd.profiler.emit_nvtx, give an example?",">>> with torch.cuda.profiler.profile():
...     model(x) # Warmup CUDA memory allocator and profiler
...     with torch.autograd.profiler.emit_nvtx():
...         model(x)",8054
10770,"How to use torch.autograd.detect_anomaly, give an example?",">>> import torch
>>> from torch import autograd
>>> class MyFunc(autograd.Function):
...     @staticmethod
...     def forward(ctx, inp):
...         return inp.clone()
...     @staticmethod
...     def backward(ctx, gO):
...         # Error during the backward pass
...         raise RuntimeError(""Some error in backward"")
...         return gO.clone()
>>> def run_fn(a):
...     out = MyFunc.apply(a)
...     return out.sum()
>>> inp = torch.rand(10, 10, requires_grad=True)
>>> out = run_fn(inp)
>>> out.backward()
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File ""<stdin>"", line 8, in backward
    RuntimeError: Some error in backward
>>> with autograd.detect_anomaly():
...     inp = torch.rand(10, 10, requires_grad=True)
...     out = run_fn(inp)
...     out.backward()
    Traceback of forward call that caused the error:
      File ""tmp.py"", line 53, in <module>
        out = run_fn(inp)
      File ""tmp.py"", line 44, in run_fn
        out = MyFunc.apply(a)
    Traceback (most recent call last):
      File ""<stdin>"", line 4, in <module>
      File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File ""<stdin>"", line 8, in backward
    RuntimeError: Some error in backward",7496
10771,"How to use torch.chain_matmul, give an example?",">>> a = torch.randn(3, 4)
>>> b = torch.randn(4, 5)
>>> c = torch.randn(5, 6)
>>> d = torch.randn(6, 7)
>>> torch.chain_matmul(a, b, c, d)
tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])",306
10772,"How to use torch.tanh, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
>>> torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])",313
10773,"How to use torch.atan2, give an example?",">>> a = torch.randn(4)
>>> a
tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
>>> torch.atan2(a, torch.randn(4))
tensor([ 0.9833,  0.0811, -1.9743, -1.4151])",7282
10774,"How to use torch.ne, give an example?",">>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [True, False]])",7272
10775,"How to use torch.msort, give an example?",">>> t = torch.randn(3, 4)
>>> t
tensor([[-0.1321,  0.4370, -1.2631, -1.1289],
        [-2.0527, -1.1250,  0.2275,  0.3077],
        [-0.0881, -0.1259, -0.5495,  1.0284]])
>>> torch.msort(t)
tensor([[-2.0527, -1.1250, -1.2631, -1.1289],
        [-0.1321, -0.1259, -0.5495,  0.3077],
        [-0.0881,  0.4370,  0.2275,  1.0284]])",387
10776,"How to use torch.utils.bottleneck is a tool that can be used as an initial step for
debugging bottlenecks in your program. It summarizes runs of your script with
the Python profiler and PyTorch’s autograd profiler.Run it on the command line with, give an example?",python -m torch.utils.bottleneck /path/to/source/script.py [args],5715
10777,"How to use torch.utils.checkpoint.checkpoint_sequential, give an example?",">>> model = nn.Sequential(...)
>>> input_var = checkpoint_sequential(model, chunks, input_var)",5775
10778,"How to use torch.unbind, give an example?",">>> torch.unbind(torch.tensor([[1, 2, 3],
>>>                            [4, 5, 6],
>>>                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))",5495
10779,"How to use torch.cholesky_inverse, give an example?",">>> a = torch.randn(3, 3)
>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite
>>> u = torch.cholesky(a)
>>> a
tensor([[  0.9935,  -0.6353,   1.5806],
        [ -0.6353,   0.8769,  -1.7183],
        [  1.5806,  -1.7183,  10.6618]])
>>> torch.cholesky_inverse(u)
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
>>> a.inverse()
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])",3542
10780,"How to use torch.lu, give an example?",">>> A = torch.randn(2, 3, 3)
>>> A_LU, pivots = torch.lu(A)
>>> A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
>>> pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
>>> A_LU, pivots, info = torch.lu(A, get_infos=True)
>>> if info.nonzero().size(0) == 0:
...   print('LU factorization succeeded for all samples!')
LU factorization succeeded for all samples!",281
10781,"How to use torch.igamma, give an example?",">>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.igammac(a1, a2)
tensor([0.3528, 0.5665, 0.7350])
tensor([0.3528, 0.5665, 0.7350])
>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)
tensor([1., 1., 1.])",6104
10782,"How to use torch.load, give an example?",">>> torch.load('tensors.pt')
# Load all tensors onto the CPU
>>> torch.load('tensors.pt', map_location=torch.device('cpu'))
# Load all tensors onto the CPU, using a function
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})
# Load tensor from io.BytesIO object
>>> with open('tensor.pt', 'rb') as f:
...     buffer = io.BytesIO(f.read())
>>> torch.load(buffer)
# Load a module with 'ascii' encoding for unpickling
>>> torch.load('module.pt', encoding='ascii')",8093
10783,How many dimensions does each input tensor have?,zero,5273
10784,What is another name for a tuple of Tensors?,output,5273
10785,What may use the Sleef library when input is on the CPU?,torch.sinh,5382
10786,Where can you find details about the Sleef library?,here,5382
10787,Returns what with the hyperbolic sine of the elements of input?,a new tensor,5382
10788,"When input is on the CPU, the implementation of torch.sinh may use what library?",Sleef library,5382
10789,What is shifts a tuple of?,python:ints,9631
10790,"If shifts is a tuple, dims must be what?",a tuple of the same size,9631
10791,What is constructed by repeating the elements of input?,tensor,1847
10792,"If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified?",fewer,3475
10793,"If input has shape (8, 6, 4, 2) and reps is (2, 2), reps is treated as what?","(1, 1, 2, 2).",3475
10794,What are reps treated as if input has shape and reps is 2?,"(1, 1, 2, 2)",3475
10795,What specifies fewer dimensions than input has?,reps,3475
10796,"If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified.",fewer,1848
10797,What is reps treated as if input has shape and reps is 2?,"(1, 1, 2, 2).",1848
10798,"If input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has",if input has fewer dimensions than reps specifies,1848
10799,"If input has shape (what is the number of dimensions) and reps is (3, 3, 2, 2), then input is treated as if it had","4, 2)",1848
10800,What does reps represent?,the number of repetitions per dimension,1848
10801,What is an example of a tensor whose elements to repeat?,Example,1848
10802,What is the size of the input tensor along a given dimension?,k largest elements,5579
10803,"What is the k in ""top-k""?",k,5579
10804,What returns the indices of the elements in the original input tensor?,"A namedtuple of (values, indices)",5579
10805,"If what is not given, the last dimension of the input is chosen?",dim,5580
10806,"If largest is what, the smallest elements of the input tensor are returned?",False,5580
10807,Returns what of the given input tensor along a given dimension?,k largest elements,5580
10808,"The boolean option sorted if True, will make sure that the returned k elements are themselves what?",sorted input,5580
10809,What is chosen if dim is not given?,the last dimension of the input,5580
10810,What are returned if largest is False?,k smallest elements,5580
10811,What option makes sure that the returned k elements are themselves sorted input?,boolean,5580
10812,The boolean option sorted if True will make sure that the returned k elements are themselves sorted?,input (Tensor),6981
10813,"The dimension to sort along largest (bool, optional) – controls whether to return largest or smallest elements sorted (bool, optional)",k,6981
10814,"The boolean option sorted if True, will ma what?",k,6981
10815,"If largest is what, the k smallest elements are returned?",False,3409
10816,The boolean option sorted if True will make sure that the returned k elements are themselves sorted input (Tensor),input tensor,3409
10817,What is the dimension to sort along with the input tensor?,k,3409
10818,"If largest is what, the smallest elements are returned?",False,3446
10819,What is returned if largest is False?,A namedtuple of,3446
10820,The boolean option sorted if True will make sure that the returned k elements are themselves sorted what?,input (Tensor),3446
10821,Which dimension controls whether to return largest or smallest elements?,k,3446
10822,"A namedtuple of (values, indices, etc.) is returned, where the indices are the indices of",indices,821
10823,What is returned where the indices are the indices of the elements in the original input tensor?,"A namedtuple of (values, indices)",821
10824,What type of operation is applied in kHkWkH times kWkHkW regions?,2D average-pooling operation,1233
10825,What type of operation is applied in kHkWkH times kWkHkW regions by step size?,2D average-pooling operation,1221
10826,What type of average-pooling operation is applied in kHkWkH times kWkHkW regions?,2D,1166
10827,What type of average-pooling operation does kTkHkWkT times kH times,3D,1166
10828,What is applied in kTkHkWkT times kH times kWkTk,3D average-pooling operation,1167
10829,What type of operation does kHkWkH times kWkHkW regions by step size?,2D average-pooling,1620
10830,What type of operation is applied in kTkHkWkT times kH times kWk,3D average-pooling,2366
10831,What type of average-pooling operation does kHkWkH times kWkHkW regions by step size?,2D,1188
10832,What type of pooling operation does kHkWkH times kWkHkW regions by step size?,2D average-pooling,1188
10833,What does kTkHkWkT times kH times kWkTkH,3D average-pooling operation,1621
10834,What does the array of sliding local blocks combine into?,a large containing tensor,1621
10835,What does a 3D pooling over an input signal comprised of several input planes do?,Computes a partial inverse of MaxPool1d,1621
10836,What does the 3D pooling function do?,Computes a partial inverse of MaxPool3d,1621
10837,What operation does kHkWkH times kWkHkW regions by step size sHsWs,2D average-pooling,1189
10838,What operation does kTkHkWkT times kH times kWkTkH,3D average-pooling,1189
10839,What does a partial inverse of MaxPool1d. Computes a partial inverse of MaxPool2,Computes a partial inverse of MaxPool3d,1189
10840,What is applied to an input signal composed of several input planes?,1D power-average pooling,1189
10841,Applies 2D fractional max pooling over an input signal composed of what?,several input planes,1189
10842,What does the 3D pooling over an input signal consist of several input planes do?,Computes a partial inverse of MaxPool2d,1170
10843,What does a 3D pooling over an input signal consist of several input planes do?,Computes a partial inverse of MaxPool1d,1197
10844,What does the 3D max pooling over an input signal consist of?,Computes a partial inverse of MaxPool1d,1212
10845,What does the 3D max pooling over an input signal consist of several input planes do?,Computes a partial inverse of MaxPool1d,1226
10846,What is the result of the 3D max pooling over an input signal composed of several input planes?,Computes a partial inverse of MaxPool2d,1226
10847,What does MaxPool1d do?,Computes a partial inverse,1641
10848,What is a partial inverse of MaxPool3d?,Computes a partial inverse of MaxPool3d,1641
10849,What type of power-average pooling over an input signal composed of several input planes?,2D,1203
10850,What operation in kHkWkH times kWkHkW regions by step size sHsWs,2D average-pooling,1203
10851,What does MaxPool2d do?,Computes a partial inverse of MaxPool3d,1227
10852,What does Compute a partial inverse of MaxPool3d do?,Computes a partial inverse of MaxPool3d,1642
10853,What is the name of each element of the input Tensor?,Thresholds,7850
10854,What does threshold apply element-wise?,rectified linear unit function,7850
10855,What is the element-wise function of the input Tensor?,hardswish function,7850
10856,What is the in-place version of threshold()?,relu(),3824
10857,What version of threshold() is used?,In-place,3824
10858,What element-wise function does threshold() use?,rectified linear unit function,3824
10859,What does threshold() apply element-wise?,rectified linear unit function,3824
10860,What is the element-wise function of the HardTanh function?,hardswish function,3824
10861,What is the name of the function that is element-wise?,hardswish function,1228
10862,What is the result of a 3D max pooling over an input signal composed of several input planes?,Computes a partial inverse of MaxPool1d,1228
10863,2D power-average pooling over an input signal composed of what?,several input planes,1228
10864,What does a 1D adaptive max pooling over an input signal consist of?,several input planes,1228
10865,What is each element of the input Tensor?,Thresholds,1228
10866,What is element-wise?,rectified linear unit function,1228
10867,2D adaptive max pooling over an input signal composed of what?,several input planes,1228
10868,What does a 1D adaptive average pooling over an input signal consist of?,several input planes,1228
10869,2D adaptive average pooling over an input signal composed of what?,several input planes,1228
10870,3D adaptive max pooling over an input signal composed of what?,several input planes,1228
10871,3D adaptive average pooling over an input signal composed of what?,several input planes,1228
10872,What element-wise function does relu() use?,rectified linear unit function,1259
10873,What does relu do element-wise?,rectified linear unit function,1259
10874,What is the in-place version of the rectified linear unit function?,relu(),1259
10875,What is the in-place version of HardTanh?,hardtanh(),1259
10876,What is the in-place version of relu()?,hardtanh(),1259
10877,What is the element-wise function that is described in the paper?,hardswish function,1259
10878,How does elu(x) apply?,element-wise,1240
10879,In-place version of elu(). Applies what?,element-wise,3821
10880,"What =max(0,x)+min(0,(exp(x/)1))?",CELU(x),3822
10881,What is the in-place version of leaky_relu()?,elu(),3822
10882,What is the scale of elu()?,1.0507009873554804934193349852946,3822
10883,What is the name of the in-place version of elu()?,LeakyReLU(x),3822
10884,"What =max(0,x)+negative_slopemin(0,x)?",LeakyReLU(x),1239
10885,"What does x =max(0,x)+negative_slopemin(0,x)textLeaky",LeakyReLU,1241
10886,What is the value of the function PReLU(x)?,"0,x",1241
10887,What is the in-place version of rrelu()?,gated linear unit,1241
10888,What is the in-place version of Randomized leaky ReLU?,rrelu(),1241
10889,What type of unit is the LeakyReLU?,gated linear unit,1241
10890,What type of transformation does y=xAT+by = xAT + by=xAT+b?,linear,1237
10891,What type of transformation does y=x1TAx2+by apply to the incoming data?,bilinear,1237
10892,What is the probability of zeroing some of the elements of the input tensor?,probability p,2228
10893,Randomly zeroes some of the elements of the input tensor with probability p using samples from what?,a Bernoulli distribution,2228
10894,"Computes sums, means or maxes of bags of embeddings without what?",instantiating the intermediate embeddings,839
10895,What does the tensor of shape have everywhere except where the index of last dimension matches the corresponding value of the input tensor?,zeros,839
10896,What is the name of the tensor that returns a tensor of shape?,num_classes,839
10897,What type of similarity between x1 and x2 is returned by torch.nn.PairwiseDistance?,cosine,5788
10898,Computes the distance between every pair of row vectors in the input?,p-norm,5788
10899,What type of negative log likelihood loss does CosineEmbeddingLoss combine?,Poisson,2645
10900,What is a function that measures Binary Cross Entropy between target and output logits?,Poisson negative log likelihood loss,2645
10901,What type of loss is HingeEmbeddingLoss?,Gaussian negative log likelihood loss,2645
10902,What criterion combines log_softmax and nll_loss in a single function?,CosineEmbeddingLoss,5760
10903,CosineEmbeddingLoss combines what two criterions in a single function?,log_softmax and nll_loss,5760
10904,What type of negative log likelihood loss is HingeEmbeddingLoss?,Gaussian,5760
10905,What does the Kullback-Leibler divergence Loss Function measure?,Measures the element-wise mean squared error,5760
10906,What does the Kullback-Leibler divergence Loss Function use for details?,MultiLabelSoftMarginLoss,5760
10907,CosineEmbeddingLoss combines what in a single function?,log_softmax and nll_loss,5760
10908,What does a function use if the absolute element-wise error falls below delta?,a squared term,5760
10909,What function uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?,SoftMarginLoss,5760
10910,What is the criterion that combines log_softmax and nll_loss in a single function?,CosineEmbeddingLoss,4870
10911,CosineEmbeddingLoss combines log_softmax and nll_loss in a single function?,Poisson,4870
10912,What is the Kullback-Leibler divergence Loss Function that takes the mean element-wise absolute value difference?,MultiLabelMarginLoss,4870
10913,"Multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=N",negative log likelihood loss,4870
10914,What does a function use if the absolute element-wise error falls below beta and an L1 term otherwise?,a squared term,4870
10915,What is the name of the loss that combines log_softmax and nll_loss in a single function?,Poisson negative log likelihood loss,4870
10916,What is the name of the function that measures Poisson negative log likelihood loss?,CosineEmbeddingLoss,2646
10917,What criterion combines in a single function?,log_softmax and nll_loss,2647
10918,What is another term for Gaussian negative log likelihood loss?,Gaussian negative log likelihood loss,2647
10919,What function uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise?,SoftMarginLoss,2647
10920,What type of negative log likelihood loss does CosineEmbeddingLoss measure?,Poisson,2641
10921,What is the criterion for Gaussian negative log likelihood loss?,Connectionist Temporal Classification loss,2641
10922,What is the term for the loss of log_softmax and nll_loss in a single function?,Connectionist Temporal Classification loss,2641
10923,What type of loss does HingeEmbeddingLoss measure?,Gaussian negative log likelihood loss,2641
10924,CosineEmbeddingLoss combines what two criterion in a single function?,log_softmax and nll_loss,5759
10925,What is the name of the criterion that combines log_softmax and nll_loss in a single function?,Connectionist Temporal Classification loss,4869
10926,What is the Kullback-Leibler divergence Loss Function that measures the element-wise mean squared error?,MarginRankingLoss,4869
10927,What is the loss that combines log_softmax and nll_loss in a single function?,Poisson negative log likelihood loss,4869
10928,What is the name of the function that combines log_softmax and nll_loss in a single function?,CosineEmbeddingLoss,4869
10929,What is the Kullback-Leibler divergence Loss Function called?,MultiLabelMarginLoss,6924
10930,What is the Kullback-Leibler divergence Loss Function?,MultiLabelSoftMarginLoss,6924
10931,What is the result of multi_margin_loss?,negative log likelihood loss,6924
10932,What does this criterion combine in a single function?,log_softmax and nll_loss,7485
10933,This criterion combines log_softmax and what else in a single function?,nll_loss,7485
10934,What is the name of the MultiLabelSoftMarginLoss function?,MultiLabelSoftMarginLoss,6917
10935,What is HingeEmbeddingLoss for details?,Gaussian negative log likelihood loss,6917
10936,What is another name for Gaussian negative log likelihood loss?,HingeEmbeddingLoss,6917
10937,What is HingeEmbeddingLoss?,Gaussian negative log likelihood loss,2677
10938,"What is multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",negative log likelihood loss,5761
10939,What does MarginRankingLoss do?,Measures the element-wise mean squared error,2651
10940,What does MarginRankingLoss measure?,the element-wise mean squared error,4235
10941,What operation is reversed by rearranging elements in a tensor of shape?,PixelShuffle,5105
10942,What is the downscale_factor of a tensor of shape?,Pads tensor,5105
10943,What ranges elements in a tensor of shape?,r,5105
10944,Reverses what operation by rearranging elements in a tensor of shape?,PixelShuffle,5105
10945,What is used to compute the output of a flow-field grid?,input values and pixel locations,5105
10946,What is computed from a flow-field grid?,input values and pixel locations,5105
10947,What is the downscale_factor of the PixelShuffle operation?,r,5701
10948,What is the tensor of the PixelShuffle operation?,Pads,5701
10949,What is the name of the step that samples the input to the given size or the given scale_factor?,Down/up,4720
10950,What is the name of the tensor that Down/up samples the input to either the given size or the given scale_factor?,Pads tensor,4720
10951,What type of theta is used to generate a flow field?,affine matrices,4720
10952,What dimension does the dot product version of this function not support an out parameter?,1-dimensional,7780
10953,What is an alias for?,torch.acos,1007
10954,What is another name for Alias for torch.acos?,Alias for torch.acos,1007
10955,What does it do when a tensor input is not specified?,Counts the number of non-zero values in the tensor input along the given dim,1938
10956,What is a tuple of dims along which to count non-zeros?,python:ints,1938
10957,How much higher performance is torch.linalg.svd() compared to the full-rank SVD implementation?,10-fold,4400
10958,What type of matrices will low-rank SVD be useful for?,sparse matrices,4400
10959,Why should you use the full-rank SVD implementation for dense matrices?,10-fold higher performance characteristics,5201
10960,"Return the singular value decomposition of a matrix, batches of matrices, or a sparse matrix AAA such that A","U, S, V",5201
10961,What is computed for the matrix AMA - MAM?,SVD,5201
10962,What is low-rank SVD useful for?,huge sparse matrices,5201
10963,What must niter be?,a nonnegative integer,5201
10964,What type of matrices is low-rank SVD useful for?,huge sparse matrices,3730
10965,What is a slightly overestimated rank of A?,q,3730
10966,What is useful for huge sparse matrices that torch.linalg.svd() cannot handle?,low-rank SVD,3730
10967,How much higher performance is torch.linalg.svd() compared to full-rank SVD?,10-fold,7126
10968,The low-rank SVD is useful for what type of matrices that torch.linalg.svd() cannot handle?,huge sparse matrices,7126
10969,What will be useful for huge sparse matrices that torch.linalg.svd() cannot handle?,low-rank SVD,4383
10970,What does q mean?,a slightly overestimated rank of A. conduct,759
10971,What grid is defined by expanding the iii th input over dimensions defined by other inputs?,iii th,6179
10972,"If the input has what of size (N1,),(N2,),...,(Nk,)(N_1,), (N_2,",kkk tensors,6179
10973,What is the name for the sequence of Tensors?,seq,6179
10974,What do NNN tensors create?,NNN N-dimensional grids,6179
10975,What can NNN tensors be?,scalar or 1-dimensional vector,6178
10976,What will be treated as tensors of size automatically?,Scalars,6178
10977,What will be treated as tensors of size?,Scalars,10819
10978,Returns True if the input is a single element tensor which is not equal to what?,torch.tensor([0]) or torch.tensor([False]),5238
10979,What throws if torch.numel()!= 1?,RuntimeError,5238
10980,What are the two variants of Torch's tensor types?,CPU and GPU,868
10981,What is the complex of a BFloat16Tensor torch?,128-bit,2063
10982,What type of tensor is a 32-bit floating point torch?,dtype CPU tensor GPU tensor,9210
10983,What is the name of the GPU tensor?,32-bit floating point torch,1498
10984,What is the GPU tensor?,32-bit floating point torch,2667
10985,What bit floating point torch is GPU tensor?,32,2667
10986,What type of torch is float32?,torch,11058
10987,What is another name for a double-tensor torch?,double torch,11060
10988,What type of complex torch is included in the BFloat16Tensor 32-bit complex torch?,8-bit,11042
10989,What is another name for a double torch?,double torch,11059
10990,What is the floating point of a DoubleTensor torch?,16-bit,11038
10991,What is 1 torch.float16 or torch.half torch?,16-bit floating point,117
10992,What are the names of the two types of torch?,torch.float16 or torch.half torch,11055
10993,How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or,32,10953
10994,What is the name of HalfTensor 16-bit floating point 2 torch?,torch.cuda,11045
10995,What is the 32-bit complex torch.complex32 64-bit complex torch.complex64?,32-bit complex torch.complex32 64-bit complex torch.complex64,11012
10996,What type of complex torch is a BFloat16Tensor 32-bit complex torch?,64-bit,10945
10997,What is a torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.byteTensor,32-bit complex torch.complex32 64-bit complex torch.complex64,10945
10998,What type of complex torch is a BFloat16Tensor?,32-bit,162
10999,How many bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch,32,11033
11000,What is the name of the torch that has a 64-bit integer?,LongTensor,11027
11001,What type of torch is LongTensor?,long torch,11027
11002,What type of complex torch is a torch?,64-bit,158
11003,What is the name of the Boolean?,LongTensor Boolean,158
11004,What type of torch is a LongTensor?,Boolean torch,11028
11005,What is a LongTensor Boolean torch.bool?,LongTensor torch.cuda,11028
11006,What type of torch is.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch,128-bit complex torch,113
11007,What is another name for unsigned 8-bit integer torch.uint8 torch?,torch.cdouble,11024
11008,What is a torch.uint8 torch?,8-bit integer,184
11009,What is the name of the torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed),torch.int8,11079
11010,What is a CharTensor 16-bit integer (signed)?,torch.int16 or torch.short torch,11036
11011,Int16 or torch.short torch.ShortTensor torch.cuda.IntTensor 32-bit,16,128
11012,How many 2-bit integers are in a torch?,3,11048
11013,What is a torch.int32 or torch.int torch?,32-bit integer,150
11014,What language can a tensor be constructed from?,Python,859
11015,A tensor of specific data type can be constructed by passing a what?,torch.dtype,859
11016,What are two ways to avoid a copy of a Tensor?,requires_grad_() or detach(),10949
11017,A tensor can be constructed from a Python list or sequence using what constructor?,torch.tensor(),860
11018,What can be used to create a tensor?,requires_grad=True,860
11019,What are two ways to avoid a copy of a tensor?,requires_grad_() or detach(),11232
11020,What type of data does torch.as_tensor() copy?,numpy array,11232
11021,What is the name of the tensor class that defines numeric operations on it?,Note,11232
11022,What holds a tensor's data?,torch.Storage,11232
11023,"If you have a numpy array and want to avoid a copy, use what?",torch.as_tensor(),11049
11024,What always copies data?,Warning torch.tensor(),11049
11025,What are two ways to avoid a copy of a Tensor data?,requires_grad_() or detach(),11049
11026,A tensor of specific data type can be constructed by passing what to a constructor?,torch.dtype and/or a torch.device,11049
11027,"For more information about building Tensors, see what?",Creation Ops,11231
11028,What language can be used to access and modify the contents of a tensor?,Python,7006
11029,What are the contents of a tensor accessed and modified using?,indexing and slicing notation,7006
11030,What does torch.Storage do?,holds its data,7006
11031,What are the three attributes of a torch.Tensor?,"torch.dtype, torch.device, and torch.layout attributes",7006
11032,The contents of a tensor can be accessed and modified using Python’s what?,indexing,866
11033,The contents of a tensor can be accessed and modified using Python's what?,indexing and slicing notation,7005
11034,What is required to create a tensor?,requires_grad=True,863
11035,What method can be used to change an existing tensor's torch.device and/or torch.dtype?,to() method,863
11036,What can a tensor be created with?,requires_grad=True,863
11037,What does the to() method on a tensor do?,Warning,863
11038,What holds data for each tensor?,an associated torch.Storage,2234
11039,"For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor,",Tensor Attributes,2234
11040,What does torch.FloatTensor.abs() compute the result in?,a new tensor,2234
11041,What are methods that mutate a tensor marked with?,underscore suffix,2234
11042,What method is used to change an existing tensor's torch.device and/or torch.dtype?,to() method,2234
11043,What does torch.FloatTensor.abs_() do?,computes the absolute value in-place and returns the modified tensor,2234
11044,What does the current implementation of torch.Tensor introduce?,memory overhead,2234
11045,What are some attributes of a torch.Tensor?,"torch.dtype, torch.device, and torch.layout attributes",2234
11046,What kind of view does the tensor class provide?,multi-dimensional,2234
11047,What does torch.Tensor.item() get a Python number from?,a tensor,8087
11048,What is another name for a tensor class?,Note,8087
11049,What does torch.FloatTensor.abs() do?,Note,8088
11050,What does the to() method do to an existing tensor?,Warning,2559
11051,What does to() method on a tensor do?,Warning,2559
11052,"For more information on the torch.Tensor, see what?",Tensor Attributes,2562
11053,What is the name of the warning that is issued when a tensor is changed?,Warning,2562
11054,What computes the absolute value in-place and returns the modified tensor?,torch.FloatTensor.abs_(),4241
11055,What is the name of the warning that a method that mutates a tensor is marked with an underscore suffix?,Warning,4241
11056,What does the to() method do to change an existing tensor's torch.device and/or torch.dtype?,Warning,4241
11057,What is a warning about a mutated tensor?,Warning,4390
11058,What does the to() method do?,Warning,4390
11059,What kind of memory usage might be caused by the current implementation of torch.Tensor?,unexpectedly high,4430
11060,What does torch.tensor() create a tensor with?,pre-existing data,7878
11061,What type of tensor is created with the same size as another tensor?,tensor,7878
11062,What type of tensor is created with the same type but different size as another tensor?,tensor,7878
11063,What is the name of the tensor creation ops?,Creation Ops,7884
11064,"To create a tensor with specific size, use what?",torch,7884
11065,What does torch. * tensor do?,creation ops,7884
11066,What does torch. *_like tensor do?,creation ops,7884
11067,What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.abs_ In-place version of,Tensor.abs,7884
11068,What is another name for Tensor.abs?,torch.abs,7884
11069,What type of tensor does torch create?,tensor with the same size (and similar types) as another tensor,7392
11070,What type of tensor is similar to another tensor but different in size?,tensor,7392
11071,What does torch create a tensor with?,specific size,7879
11072,What is another name for tensor creation ops?,Creation Ops,7885
11073,"To create a tensor with the same size (and similar types) as another tensor, use what?",torch,7885
11074,Is the Tensor.is_cuda true?,True if the Tensor is stored on the GPU,7885
11075,What is another name for creation ops?,Creation Ops,7885
11076,What does Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing?,real values of the self tensor,7885
11077,"To create a tensor with similar type but different size as another tensor, use what?",tensor.new_* creation ops,7882
11078,What does Tensor.new_empty return a Tensor of size size filled with?,uninitialized data,7882
11079,What is True if the Tensor is a meta tensor?,meta,7882
11080,Is the Tensor.is_cuda True?,True if the Tensor is stored on the GPU,7882
11081,"If n is the number of dimensions in x, x.T is equivalent to what?",x.permute,7883
11082,Is the Tensor.is_cuda True or False if the Tensor is stored on the GPU?,True,7883
11083,"If n is the number of dimensions in what integer, x.T is equivalent to x.permute?",x,7880
11084,What returns a tensor of size size filled with uninitialized data?,Tensor.new_ones,7880
11085,What returns a new Tensor with data as the tensor data?,Tensor,6578
11086,What does a Tensor.new_empty return a Tensor of size size filled with?,uninitialized data,6578
11087,What is the Tensor of size size filled with?,uninitialized data,6578
11088,What Returns a new Tensor with data as the tensor data?,Tensor.new_tensor,6578
11089,What does a scalar or tensor do to self tensor?,Add a scalar or tensor to self tensor,6578
11090,What is another name for add() Tensor?,addbmm,6578
11091,What does tensor.new_full return a Tensor of size size filled with?,fill_value,7881
11092,What does tensor.new_empty return a Tensor of size size filled with?,uninitialized data,7881
11093,What returns a Tensor of size size filled with uninitialized data?,Tensor.device,5284
11094,What does Tensor.new_full return a Tensor of size size filled with?,fill_value,5334
11095,What data does Tensor.new_empty return a Tensor of size size filled with?,uninitialized data,5334
11096,What Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()?,abs(),5334
11097,"If n is the number of dimensions in what, x.T is equivalent to x.permute(n-1, n-2",x,3889
11098,What does the Tensor.new_empty return a Tensor of size size filled with?,uninitialized data,3889
11099,What returns a Tensor of size size filled with 0.,Tensor.is_cuda,3889
11100,Tensor.new_ones Returns a Tensor of size size filled with what value?,1,5285
11101,Tensor.new_zeros Returns a Tensor of size size filled with what value?,0.,5285
11102,What Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_(),absolute,5285
11103,Tensor.new_ones Returns a Tensor of size size filled with what?,1,5288
11104,Is Tensor.is_cuda True or False if the Tensor is stored on the GPU?,True,5288
11105,Tensor.imag Returns a new tensor containing what values of the self tensor?,imaginary values,5288
11106,What is returned a Tensor of size size filled with?,uninitialized data,5288
11107,In-place version of what Alias for abs() Tensor.absolute?,abs() Tensor.absolute,5288
11108,Tensor.new_zeros Returns a Tensor of size size filled with what?,0.,3455
11109,What is x.T equivalent to?,x.permute,3455
11110,Is the Tensor.is_meta True or False if the Tensor is a meta tensor?,True,3455
11111,What is the tensor of size size filled with?,0.,3453
11112,What does the new Tensor return as the tensor data?,data,6577
11113,Tensor.is_cuda Is True if the Tensor is stored on what?,GPU,6577
11114,Is the Tensor.is_quantized True or False if the Tensor is quantized?,True,6459
11115,Is Tensor.is_quantized True or False if the Tensor is quantized?,True,6459
11116,What is added to self tensor?,scalar or tensor,6459
11117,Returns what with data as the tensor data?,a new Tensor,5335
11118,Tensor.is_quantized Is what if the Tensor is quantized?,True,5335
11119,Tensor.new_full Returns a Tensor of size size filled with what?,fill_value,5335
11120,Tensor.grad This attribute is what by default?,None,5335
11121,Tensor.new_empty Returns a Tensor of size size filled with what?,uninitialized data,5335
11122,What does new_empty return a Tensor of size size filled with?,uninitialized data,5335
11123,What is the name of the tensor that returns a new Tensor?,Tensor.abs,5335
11124,Which tensor returns a new tensor containing imaginary values of the self tensor?,Tensor.imag,5335
11125,What is the Tensor true if it is a meta tensor?,meta tensor,6576
11126,Tensor.new_zeros Returns a Tensor of size size filled with how many zeros?,0.,6576
11127,What type of Tensor is True if the Tensor is a meta tensor?,meta tensor,3890
11128,Returns a Tensor of size size filled with what?,1,5283
11129,Tensor.is_cuda Is what?,True if the Tensor is stored on the GPU,5283
11130,Tensor.is_meta Is True if the Tensor is a what?,meta tensor,5283
11131,What is the device where the Tensor is located?,torch,6338
11132,What is the name of the device where this Tensor is?,torch.device,6338
11133,What is the torch.device where the Tensor is located?,the torch.device where this Tensor is,3887
11134,What is the default attribute of Tensor.grad?,None,3887
11135,What Add a scalar or tensor to self tensor?,add,3887
11136,What does the Alias for dim() Tensor.ndim return?,real values of the self tensor,3887
11137,What device is used to store the Tensor?,torch,5287
11138,What is the torch.device where the Tensor is?,Tensor.grad,5282
11139,What is the size size of a Tensor of size size filled with?,1,5282
11140,What is the torch.device where this Tensor is?,Tensor.grad,5282
11141,What does Tensor.new_ones return?,Tensor of size size filled with 1.,6575
11142,What device is used to store a Tensor?,torch,6575
11143,What is the device where the Tensor is stored?,torch,3874
11144,What is the device where the Tensor is?,torch,3454
11145,What is the default value of Tensor.grad?,None,6405
11146,What Alias for dim() Tensor.n Returns a new tensor containing real values of the self tens,dim,6405
11147,What does Tensor.ndim Alias for dim() Tensor.real Return a new tensor containing?,real values of the self tensor,6405
11148,What Alias for dim() Tensor.real?,dim,6452
11149,Is True if the Tensor is a what?,meta tensor,3873
11150,Is True if the Tensor is what?,quantized,3873
11151,What Alias for dim() Tensor.real Returns a new tensor containing real values of the self tens,dim,7464
11152,Is the Tensor quantized?,True,3872
11153,What is the default value of the attribute Tensor.grad?,None,6406
11154,Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing what?,real values of the self tensor,6406
11155,What is the name of the In-place version of abs()?,abs(),6455
11156,What is the In-place version of abs()?,abs(),6455
11157,Is Tensor.is_meta True or False if the Tensor is a meta tensor?,True,6456
11158,What can be added to a self tensor?,a scalar or tensor,6456
11159,Is the Tensor a meta tensor?,True,3871
11160,What is the name of the device where the Tensor is located?,torch.device,3871
11161,Is True if the Tensor is a meta tensor?,if the Tensor is a meta tensor,3871
11162,What is the name of the In-place version of addbmm() Tensor?,addbmm,3871
11163,What is the in-place version of abs() Tensor.absolute?,abs() Tensor.abs,3870
11164,What is the torch.abs() Tensor.abs_ In-place version of?,abs() Tensor.absolute,6572
11165,What is used for abs() Tensor.absolute?,Alias,3886
11166,What Alias does torch.acos() provide for?,abs_() Tensor.acos,6404
11167,What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for,Tensor.abs,6198
11168,What is the name of the Alias for abs?,Tensor.acos,7463
11169,What is the default value of this attribute?,None by default,7465
11170,What does the tensor.ndim Alias for dim() Tensor.real Returns a new tensor,real values of the self tensor,7465
11171,What attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self?,Tensor.abs,7465
11172,What is the name of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self?,addbmm,7465
11173,What Alias for abs() Tensor.absolute_ In-place version of absolute()?,abs() Tensor.absolute,1000
11174,What Alias for abs() Tensor.absolute Returns a new tensor containing real values of the self,abs,6557
11175,Alias for dim() Tensor.real Returns a new tensor containing what?,real values of the self tensor,1002
11176,What does a new tensor do?,add,6615
11177,What Returns a new tensor containing imaginary values of the self tensor?,Tensor.abs,6615
11178,What Returns a new tensor containing real values of the self tensor?,real,1001
11179,What does abs() Tensor.absolute Alias for abs() Tensor.absolute Alias for ab,abs() Tensor,1001
11180,What does add to self tensor?,a scalar or tensor,6558
11181,What type of tensor can be added to a self tensor?,scalar,6427
11182,What is the name of the In-place version of add() Tensor?,add,3875
11183,What is the name of the device where the Tensor is stored?,torch.device,3875
11184,What is the name of the tensor that is stored on the GPU?,addbmm,3875
11185,What is the In-place version of Tensor.add?,add() Tensor,6197
11186,What does add add to self tensor?,scalar or tensor,5340
11187,What is the In-place version of abs() Tensor?,abs() Tensor,6196
11188,What is the name of the in-place version of abs() Tensor?,abs() Tensor,5785
11189,Abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut,abs() Tensor,5785
11190,What is the in-place version of Tensor?,abs() Tensor,6200
11191,What is the name of the in-place version of add() Tensor?,addbmm,3819
11192,What is the In-place version of Alias for abs() Tensor.absolute?,abs() Tensor.absolute,3819
11193,What is the In-place version of add() Tensor?,add,6208
11194,What is the In-place version of absolute() Alias for abs_() Tensor?,absolute,6208
11195,What does Tensor.absolute use for abs?,Alias,6204
11196,What is the name of the In-place version of addcdiv?,addcdiv,6204
11197,What do you add to a scalar or tensor to self tensor?,a scalar or tensor to self tensor,5786
11198,What is a Tensor.add_ In-place version of?,add() Tensor.addbmm,5786
11199,What is the name of the in-place version of absolute() Alias for abs_() Tensor?,Alias for abs() Tensor,998
11200,What does add add add?,a scalar or tensor to self tensor,998
11201,What is the In-place version of absolute() Alias for abs() Tensor?,absolute,998
11202,What is the name of the function that adds a scalar or tensor to self tensor?,addcmul,997
11203,What is used for abs_() Tensor?,Alias,6207
11204,What is the name of the in-place version of addcdiv?,addcmul,6207
11205,What is the In-place version of absolute() Alias for abs_() Tensor.acos?,acos() Tensor.arccos,6207
11206,Alias for what?,torch.mul(),1034
11207,What is another name for Alias for torch.mul?,Alias for torch.mul,1034
11208,What does Alias for torch.mul() do?,Alias for torch.mul(),1034
11209,In what state is the PyTorch API of sparse tensors?,beta,6939
11210,What is the storage format for sparse tensors?,Coordinate format,6939
11211,Which API of sparse tensors is in beta?,PyTorch,6939
11212,What does PyTorch implement as one of the storage formats for implementing sparse tensors?,Coordinate format,4961
11213,What is another name for Coordinate format?,COO format,6938
11214,What is the name of the indices of specified elements in COO format?,torch.int64,6938
11215,In what format are the indices of specified elements collected?,indices tensor of size,4960
11216,PyTorch implements what format for sparse tensors?,Coordinate format,4960
11217,What is the name of the element type in the COO format?,torch,4960
11218,What are the indices of specified elements collected in?,indices tensor of size,6147
11219,What would we use to define a sparse tensor?,i,6147
11220,What is the dimensionality of the tensor and nse is the number of specified elements?,ndim,10858
11221,The memory consumption of a sparse COO tensor is at least what?,nse bytes,10858
11222,What is the indices of specified elements called?,ndim,10858
11223,The indices of specified elements are collected in indices tensor of size (ndim) and with element type torch.int,nse,10857
11224,"In indices tensor of size, what is the dimensionality of the tensor?",ndim,10857
11225,What is another name for indices tensor of size?,nse,8379
11226,What type of tensor is at least product(tensor shape>) * size of element type in bytes>?,strided tensor,8379
11227,What is the name of the storage format for sparse tensors implemented by PyTorch?,Coordinate format,8379
11228,What is the dimensionality of the tensor?,ndim,11378
11229,What is the number of specified elements in a sparse COO tensor?,nse,11378
11230,What is the number of specified elements in a tensor?,nse,11377
11231,What tensor is at least product(tensor shape>) * size of element type in bytes>?,strided tensor,11377
11232,How much memory saving occurs when using the COO storage format?,200 fold,2529
11233,What is the memory consumption of a 10 000 x 10 000 tensor with COO tensor layout?,2 000 000 bytes,2529
11234,What should we note about the input i?,the input i is NOT a list of index tuples,6146
11235,What would we write to define a sparse tensor?,i,6146
11236,The input i is NOT a list of what?,index tuples,4484
11237,How is an empty sparse COO tensor constructed?,"An empty sparse COO tensor can be constructed by specifying its size
only",4484
11238,Which hybrid COO tensor extends the sparse COO tensor?,PyTorch,4956
11239,What extends the sparse COO tensor?,PyTorch hybrid COO tensor,4956
11240,What are the invariants of a sparse COO tensor?,"s.sparse_dim(), K = s.dense_dim()",4956
11241,The indices of specified elements are collected in what indices of size?,tensor,10860
11242,What does M =?,s.sparse_dim(),10860
11243,"The indices of specified elements are collected in indices tensor of size (sparse_dims, dense_",nse,10859
11244,"What are the corresponding values of nse, dense_dims, and with an arbitrary integer or floating point number element type?",tensor,10843
11245,What are the values tensor of size?,"nse, dense_dims",10843
11246,"Where is the entry [5, 6] in a sparse tensor?","(1, 0",4443
11247,What type of tensor is s?,sparse COO tensor,8294
11248,What is the name of the sparse COO tensor?,s.sparse_dim(),8294
11249,What is the sum of the number of sparse and dense dimensions?,dimensionality,4164
11250,"If s is a COO tensor and M = s.sparse_dim(), K = s",sparse,10845
11251,"What is the location of the entry [5, 6] in a 2 + 1-dimensional tensor?","(1, 0",10845
11252,What are the corresponding (tensor) values collected in?,arbitrary integer or floating point number element type,10845
11253,s.values().layout == torch.strided - values are stored as what?,strided tensors,3713
11254,What is the meaning of strided tensors?,Note,3713
11255,What is the dimensionality of a tensor the sum of the number of sparse and dense dimensions?,s.ndim,4163
11256,What will accumulate the multi-valued elements into a single value using summation?,the coalescing process,4163
11257,What permits uncoalesced sparse tensors?,PyTorch sparse COO tensor format,4985
11258,What type of uncoalesced tensor is created when multiple values are specified for the same index 1?,1-D,4985
11259,What does sparse COO tensor format permit?,"uncoalesced sparse tensors,
where there may be duplicate coordinates in the indices",4985
11260,What uncoalesced tensor can be created when multiple values are specified for the same index?,1-D,2105
11261,What is the value at an uncoalesced sparse COO tensor?,the sum of all duplicate value entries,4982
11262,What does the torch.Tensor.is_coalesced() return True?,Note,11387
11263,What is the result of the torch.Tensor.is_coalesced() method?,Note,11387
11264,What are the properties of the output of torch.Tensor.coalesce() method?,the indices of specified tensor elements are unique,10631
11265,PyTorch sparse COO tensor format permits uncoalesced sparse tensors where there may,duplicate coordinates,10631
11266,What are the indices of specified tensor elements?,unique,10631
11267,What does torch.Tensor.is_coalesced() return?,True,10970
11268,Why should sparse tensors be coalesced?,to prevent them from growing too large,10970
11269,What type of sparse tensor will most operations work identically given?,coalesced or uncoalesced sparse tensor,3724
11270,What should you do to your sparse tensors to prevent them from growing too large?,coalesce,3574
11271,What is an example of a lexicographical ordering of indices?,Let’s consider the following example,3574
11272,What is a torch.Tensor instance?,a sparse COO tensor,2970
11273,What methods can be used to acquire the COO format data of a sparse COO tensor?,methods torch.Tensor.indices() and torch.Tensor.values(),2970
11274,"Currently, one can acquire the COO format data only when the tensor instance is what?",coalesced,2970
11275,What is the name of a sparse COO tensor?,torch,4100
11276,What does a sparse COO tensor do?,Note,1305
11277,How can the number of sparse and dense dimensions be acquired?,methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim(),7206
11278,What dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d,sparse and dense,7206
11279,"If s is a what, then its COO format data can be acquired using methods torch.Tensor.indices() and torch",sparse COO tensor,7206
11280,"For acquiring the COO format data of what tensor, use torch.Tensor._values() and torch.T",uncoalesced,7206
11281,What type of tensor is uncoalesced?,coalesced,2494
11282,What can one construct of a sparse COO tensor using the torch.Tensor.coalesce() method?,coalesced copy,2494
11283,What can be used to construct a coalesced copy of a sparse COO tensor?,the torch,8993
11284,What method can be used to create a coalesced copy of a sparse COO tensor?,torch,1832
11285,What can one construct of a sparse COO tensor using the torch?,coalesced copy,1832
11286,What can be used to build a coalesced copy of a sparse COO tensor?,torch,8992
11287,What can be done with the fill value of a sparse tensor?,operations that may interpret the fill value differently,5937
11288,What are the three 1-D tensors in a CSR sparse tensor?,"crow_indices, col_indices and values",761
11289,What does the crow_indices tensor consist of?,compressed row indices,7017
11290,What encodes the index in values and col_indices depending on where the given row starts?,tensor,7017
11291,What does each successive number in the tensor subtracted by the number before it denote?,the number of elements in a given row,7017
11292,What is the size of the crow_indices tensor?,1-D tensor,7017
11293,What is the last element of the crow_indices tensor?,number of non-zeros,7017
11294,What tensor contains the values of the CSR tensor?,values,7017
11295,What is the crow_indices tensor?,1-D tensor of size size[0] + 1,6912
11296,The crow_indices tensor encodes the index in values and col_indices depending on what?,where the given row starts,6912
11297,What is the col_indices tensor?,1-D tensor of size nnz,6912
11298,What does the col_indices tensor contain?,column indices of each value,6912
11299,What tensor contains the column indices of each value?,col_indices,6912
11300,"If you want to use MKL-enabled matrix operations, use what?",torch.int32,6912
11301,The index tensors crow_indices and col_indices should have element type what?,torch.int64,6912
11302,Is the size argument optional or optional?,optional,6912
11303,The values tensor contains the values of the CSR tensor. This is a what?,1-D tensor,6912
11304,The user must supply the row and column indices and values tensors separately or together?,separately,6912
11305,The crow_indices tensor encodes the index in values and what else depending on where the given row starts?,col_indices,7016
11306,The index tensors crow_indices and col_indices should have element type either what?,torch.int64,6991
11307,What is the col_indices tensor of size nnz?,1-D tensor,6991
11308,What is the default element type for index tensors crow_indices and col_indices?,torch.int64,6991
11309,What is the size of the CSR tensor?,1-D,7357
11310,"If you want to use MKL-enabled matrix operations, what should you use?",torch.int32,4403
11311,What is the default element type for index tensors?,torch.int64,4403
11312,What can be directly constructed by using the torch._sparse_csr_tensor() method?,Sparse CSR matrices,4403
11313,What is the default element type for crow_indices and col_indices?,torch.int64,4402
11314,What is the name of the method used to construct sparse CSR matrices?,torch,6002
11315,What is the simplest way of constructing a sparse CSR from a strided or sparse COO tens,tensor,6002
11316,The size argument is optional and will be deduced from what if it is not present?,crow_indices and col_indices,6000
11317,The sparse matrix-vector multiplication can be performed with what method?,tensor.matmul(),7294
11318,The sparse matrix-vector multiplication is the only math operation supported on what?,CSR tensors,7294
11319,Any zeros in the strided tensor will be interpreted as what?,missing values,7294
11320,What does M[layout] denote?,matrix,7095
11321,What denotes a scalar?,f,7095
11322,What is a PyTorch operation?,Sparse grad?,7093
11323,What is Layout signature torch.mv()?,no,7093
11324,What is the name of the Layout signature?,torch.mv(),7093
11325,What does T[layout] denote with a given layout?,a tensor,7094
11326,What function is no longer used for Linear Algebra operations on sparse matrices?,addmm(),7094
11327,What is the default value for M[sparse_coo] at V[strided] -> V[stride,no,4970
11328,What does torch.mv() no M[sparse_coo] @ V[strided] -> V[s,Layout signature,4970
11329,What is the term for a hybrid sparse?,Sparse grad,6004
11330,What is the name of the Sparse grad at V[strided] -> V[strided] torch?,M[sparse_coo],6004
11331,What is the value of M[sparse_coo] at V[strided] -> V[strided,no,4064
11332,What is not present at V[strided] -> V[strided] torch?,M[sparse_coo],4064
11333,What does torch.mv() say about M[sparse_coo] at V[strided] -> V[,no,11101
11334,What does torch.mv() no?,M[sparse_coo],11101
11335,What is the number of M[sparse_coo] at V[strided] -> V[strided,no,10236
11336,What does no M[sparse_coo] @ V[strided] -> V[strided] torch,M[sparse_coo],10236
11337,What does.mv() no M[sparse_csr] @ V[strided] -> V[,M[sparse_coo] @ V[strided] -> V[strided] torch,4179
11338,What does M[sparse_coo] do at V[strided] -> V[strided] torch,M[sparse_coo],4179
11339,What is the value of the M[sparse_csr] at V[strided] -> V[stri,no,11104
11340,What is the name of the node that does not have a T[sparse_coo] at T[strided,addmm,11104
11341,What is the number of M[sparse_csr] at V[strided] -> V[stride,no,10244
11342,What does no M[sparse_coo] @ M[strided] -> M[strided] torch,addmm,10244
11343,What does matmul() no M[sparse_coo] @ M[strided] -> M[stride,M[sparse_csr],4190
11344,Matmul() no what @ M[strided] -> M[strided] torch?,M[sparse_coo],4190
11345,Where does M[sparse_coo] come from?,M[strided],11213
11346,What does torch.smm() no at M[strided] -> M[sparse_coo] torch?,M[sparse_coo],11213
11347,When does no M[sparse_coo] occur?,M[strided],10232
11348,What does M[sparse_coo] do at M[strided] -> M[sparse_co,M[sparse_coo],10232
11349,What does M[sparse_coo] do at M[strided] -> M[strided] torch,M[sparse_coo],4176
11350,Where does M[sparse_csr] come from?,M[strided],4189
11351,What does M[sparse_csr] do at M[strided] -> M[strided],M[sparse_csr],4189
11352,When does no M[sparse_csr] occur?,M[strided],10243
11353,What @ M[strided] -> M[strided] torch?,M[sparse_csr],4188
11354,What does M[sparse_coo] @ M[strided] -> M[strided] torch?,M[sparse_coo],10234
11355,What does torch.sparse.mm() say at M[strided] -> M[strided] torch?,M[sparse_coo],11223
11356,What does torch.sparse.mm() yes?,M[sparse_coo],11223
11357,What does M[strided] -> M[strided] torch have?,M[sparse_coo],4177
11358,What does lobpcg stand for?,lobpcg,11440
11359,What does not exist at M[strided] -> M[sparse_coo] torch?,M[sparse_coo],10233
11360,What does M[sparse_coo] mean?,M[sparse_coo],4175
11361,Where is M[sparse_coo] at?,M[strided],4174
11362,What @ M[strided] -> M[sparse_coo] torch?,M[sparse_coo],4174
11363,What does not exist at M[strided] -> M[hybrid sparse_coo] torch?,M[sparse_coo],10231
11364,What does M[sparse_coo] @ M[strided] -> M[hybrid sparse,no,10230
11365,What does M[hybrid sparse_coo] mean?,M[sparse_coo],4173
11366,What @ M[strided] -> M[hybrid sparse_coo] torch?,M[sparse_coo],4172
11367,What does f * mean?,M[sparse_coo],9314
11368,What does sspaddmm() no?,f * M[sparse_coo],11014
11369,What is the answer to T[sparse_coo] at T[strided]?,no,10247
11370,What is T[sparse_coo] @ T[strided] -> T[strided] torch?,no,10247
11371,"What is the acronym for ""M[sparse_coo]""?",PCA,6177
11372,What does M[strided] + f * mean at M[strided]?,M[sparse_coo],10249
11373,What does f * M[strided] + f * (M[sparse_coo] @ M[s,M[sparse_coo],9315
11374,What does f * M[strided] + f * mean at M[strided]?,M[sparse_coo],11221
11375,What is M[sparse_coo]?,SVD,11442
11376,What does torch.sspaddmm() no f *?,M[sparse_coo],11225
11377,What does torch.sspaddmm() no f * (M[sparse_coo] @ M[stride,M[sparse_coo],11225
11378,What returns the number of dense dimensions in a sparse tensor self?,Tensor.dense_dim,6329
11379,What returns the number of sparse dimensions in a sparse tensor self?,Tensor.sparse_dim,6329
11380,What returns a new sparse tensor with values from a strided tensor self filtered by the indice,Tensor.sparse_mask,6329
11381,What Returns the number of dense dimensions in a sparse tensor self?,Tensor.dense_dim,6329
11382,What returns a coalesced copy of self if self is an uncoalesced tensor?,Tensor.coalesce,6329
11383,"If self is a sparse COO tensor that is coalesced, what type of tensor is returned?",uncoalesced,6329
11384,What returns the tensor containing the compressed row indices of the self tensor when self is a sparse C,Tensor.col_indices,6329
11385,What _ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,6329
11386,What Returns the tensor containing the compressed row indices of the self tensor when self is a sparse,Tensor.crow_indices,6329
11387,What does Tensor.dense_dim return?,the number of sparse dimensions in a sparse tensor,6329
11388,What removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of,Tensor.sparse_resize_and_clear,6329
11389,Tensor.sparse_mask Returns what with values from a strided tensor self filtered by the,a new sparse tensor,6329
11390,"If self is a sparse COO tensor that is coalesced, what does Tensor.is_coales",True if self is a sparse COO tensor that is coalesced,6329
11391,What Returns the tensor containing the compressed row indices of the self tensor?,Tensor.col_indices,6329
11392,What are the values of the strided tensor self filtered by?,indices of the sparse tensor mask,7070
11393,What does Tensor.is_sparse use if the Tensor uses?,sparse storage layout,7070
11394,What is the number of dense dimensions in a sparse tensor self?,dense,6464
11395,What _dim Return the number of sparse dimensions in a sparse tensor self?,sparse,6464
11396,What does the new sparse tensor return values from?,a strided tensor self filtered by the indices of the sparse tensor mask,6464
11397,What is the name of the method that resizes a sparse tensor?,Tensor.sparse_resize_and_clear,6464
11398,What is returned in a sparse tensor self?,number of dense dimensions,5193
11399,Returns a coalesced copy of self if self is an what tensor?,uncoalesced,5193
11400,Return the number of dense dimensions in a what self?,sparse tensor,5193
11401,What are the values from a strided tensor self filtered by?,indices of the sparse tensor mask,6666
11402,What are the values of the sparse tensor mask filtered by?,indices,6463
11403,Is True if the Tensor uses sparse storage layout?,False,3878
11404,What is the name of the number of sparse dimensions in a sparse tensor self?,sparse,3878
11405,What does the Tensor.sparse_mask return?,a new sparse tensor,3878
11406,What is used to convert a tensor to compressed row storage format?,Tensor.indices,3877
11407,What does the sparse tensor mask filter?,indices,3877
11408,What return the number of sparse dimensions in a sparse tensor self?,sparse,6328
11409,What type of tensor is self?,uncoalesced,6328
11410,What does a sparse tensor self return?,the number of sparse dimensions,5195
11411,Return the number of dense dimensions in a what tensor self?,sparse,5192
11412,What does Tensor.sparse_dim return?,the number of sparse dimensions in a sparse tensor self,6667
11413,What is the return value of a sparse tensor?,number of sparse dimensions in a sparse tensor self,5196
11414,What returns true if self is a sparse COO tensor that is coalesced?,Tensor.to_dense,5196
11415,What Returns a new sparse tensor with values from a strided tensor self filtered by the,Tensor.sparse_mask,5196
11416,Where are the values of a new sparse tensor retrieved from?,a strided tensor self filtered by the indices of the sparse tensor mask,5338
11417,What is the sparse tensor mask filtered by?,indices,5338
11418,What are the following Tensor methods specific to sparse COO tensors?,Tensor.coalesce,5338
11419,What type of copy of self does Tensor.coalesce return if self is an uncoalesced tensor?,coalesced,6297
11420,What does Tensor.sparse_resize_ Resize self sparse tensor to?,the desired size and the number of sparse and dense dimensions,6297
11421,The following methods are specific to what type of CSR tensors?,sparse,6297
11422,What Tensor method returns a coalesced copy of self if self is an uncoalesced tensor?,Tensor.sparse_resize_,6719
11423,What is returned if self is an uncoalesced tensor?,self,6719
11424,What type of copy of the tensor does Tensor._to_sparse_csr return?,sparse,5411
11425,What resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,5291
11426,"If self is a sparse COO tensor that is coalesced, what return does Tensor.is_co",True,5291
11427,What type of copy of self is returned if self is an uncoalesced tensor?,coalesced,5291
11428,Returns what type of self if self is an uncoalesced tensor?,coalesced copy,5291
11429,Returns a coalesced copy of self if self is an what type of tensor?,uncoalesced,7076
11430,What returns the tensor containing the column indices of the self tensor when self is a sparse CSR,Tensor.col_indices,7076
11431,What is the name of the Tensor method that resizes a sparse tensor?,Tensor.sparse_resize_and_clear,7071
11432,Where are the values of a sparse tensor filtered by the indices of the sparse tensor mask,a strided tensor self,7071
11433,"If self is an uncoalesced tensor, what is returned?",if self is an uncoalesced tensor,1899
11434,What is the name of the tensor method that resizes a sparse tensor?,Tensor.sparse_resize_and_clear,5188
11435,What is the name of the Tensor method that resizes a sparse COO tensor?,Tensor.sparse_resize_and_clear,5188
11436,Return the tensor of a sparse COO tensor?,indices,5188
11437,What does Tensor.sparse_resize_ Resize?,self sparse tensor,6438
11438,What is another name for sparse tensors?,Tensor.sparse_resize_and_clear,6438
11439,What does Tensor.sparse_resize resize?,self sparse tensor,6674
11440,What happens when a sparse tensor is removed?,resizes self to the desired size and the number of sparse and dense dimensions,6674
11441,What does return the value of a sparse COO tensor?,values tensor of a sparse COO tensor,5204
11442,"If self is a sparse COO tensor, return a coalesced copy of self if self is what?",uncoalesced,5204
11443,What removes all specified elements from a sparse tensor?,Tensor.sparse_resize_and_clear,5164
11444,What resizes to the desired size and the number of sparse and dense dimensions?,self sparse tensor,5164
11445,Resizes self sparse tensor to what size?,the desired size,5164
11446,What is returned by the value tensor of a sparse COO tensor?,Return the values tensor of a sparse COO tensor,5203
11447,"If self is a sparse COO tensor, what type of tensor is it?",uncoalesced,7075
11448,What is _coalesced?,Tensor.is,7075
11449,When does Tensor.coalesce return a coalesced copy of self?,if self is an uncoalesced tensor,7075
11450,"If self is a sparse COO tensor that is coalesced, what returns true?",False,6673
11451,What does Tensor.sparse_resize_and_clear_ do to a sparse tensor?,Removes all specified elements,5339
11452,Returns what with values from a strided tensor self filtered by the indices of the sparse tens,a new sparse tensor,5339
11453,The following methods are specific to what?,sparse CSR tensors,5149
11454,What does a sparse tensor self do?,Removes all specified elements,5149
11455,What Returns the tensor containing the column indices of the self tensor when self is a sparse C,Tensor.col_indices,5149
11456,"If self is a sparse COO tensor that is coalesced, what value does Tensor.is_co",True,6446
11457,What does the tensor return when self is a sparse CSR tensor of layout sparse_csr,column indices,6446
11458,The following methods are specific to what CSR tensors?,sparse,6446
11459,Is self a sparse COO tensor that is coalesced?,False,6445
11460,What does tensor.col_indices return when self is a sparse CSR tensor of layout sparse,column indices of the self tensor,6445
11461,What methods are specific to sparse CSR tensors?,Tensor.crow_indices,6678
11462,What happens to a sparse tensor?,resizes self to the desired size and the number of sparse and dense dimensions,6679
11463,"If self is a sparse COO tensor that is coalesced, what return value is returned?",False,6679
11464,What removes all specified elements from a sparse tensor self?,Tensor.sparse_resize_and_clear,6679
11465,What does Tensor.col_indices return when self is a sparse CSR tensor of layout sparse_,column indices of the self tensor,6679
11466,The following methods are specific to sparse what?,CSR tensors,6720
11467,What does resize self to the desired size and the number of sparse and dense dimensions?,Removes all specified elements,5148
11468,"Returns True if self is a sparse COO tensor that is coalesced, what else?",False,5219
11469,Returns what if self is a sparse COO tensor that is coalesced?,True,5219
11470,What does Tensor.crow_indices return when self is a sparse CSR tensor of layout sparse,tensor containing the compressed row indices of the self tensor,6716
11471,What indices of the self tensor is returned when self is a sparse CSR tensor of layout spars,column,6315
11472,When self is a sparse CSR tensor of layout what?,sparse_csr,6315
11473,Creates what copy of self?,strided copy,2027
11474,What indices does the tensor return when self is a sparse CSR tensor of layout sparse_,column,2027
11475,What does return when self is a sparse CSR tensor of layout sparse_csr?,the tensor containing the compressed row indices,5673
11476,Returns the tensor containing what indices of the self tensor when self is a sparse CSR,column,5673
11477,Self is a sparse CSR tensor of what?,layout sparse_csr,6300
11478,"When self is a sparse CSR tensor of layout, what does Tensor.col_indices return?",sparse_csr,6300
11479,What is a sparse CSR tensor of layout sparse_csr?,self,5671
11480,Returns what when self is a sparse CSR tensor of layout sparse_csr?,the tensor containing the column indices of the self tensor,5671
11481,What is the name of the method that supports sparse COO tensors?,t_(),7079
11482,What are the specified values for a sparse tensor in CSR?,crow_indices and col_indices,1840
11483,What ensions dim does sparse.sum return the sum of each row of the sparse tensor input in?,dim,1840
11484,What does sparse.addmm do exactly the same thing as?,torch.addmm(),1840
11485,What returns the sum of each row of the sparse tensor input in the given dimensions dim?,sparse.sum,1840
11486,What does sparse.mm perform a matrix multiplication of?,sparse matrix mat1 and the (sparse or strided) matrix mat2,1840
11487,What multiplies a sparse tensor mat1 with a dense tensor mat2?,Matrix,1840
11488,What performs a sparse COO matrix mat1 and a strided matrix mat2?,matrix multiplication,1840
11489,What applies a softmax function?,softmax,1840
11490,Matrix performs a matrix multiplication of what?,sparse COO matrix mat1 and a strided matrix mat2,1840
11491,What Applies a softmax function followed by logarithm?,sparse.log_softmax,1840
11492,Performs a matrix multiplication of what?,sparse matrix input with the dense matrix mat,1840
11493,What is the name of the function that supports backward for sparse matrix mat1?,sparse.mm,1839
11494,What are the specified values for the sparse tensor?,crow_indices and col_indices,1845
11495,What type of ensions dim does sparse.sum return?,dim,1845
11496,Returns the sum of each row of the sparse tensor input in what ensions dim?,dim,5669
11497,What does sparse.addmm return?,the sum of each row of the sparse tensor input,5669
11498,What does sparse.mm perform of the sparse matrix mat1 and the (sparse or strided) matrix mat2,matrix multiplication,10731
11499,Matrix performs a matrix multiplication of a sparse COO matrix mat1 and what else?,a strided matrix mat2,7527
11500,What does this function do exactly the same thing as in the forward?,torch.addmm(),7527
11501,What type of matrix does matrix multiplication perform?,sparse,7527
11502,What does this function support backward for?,sparse matrix mat1,7527
11503,What performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2,sparse.mm,10735
11504,What does sparse.mm perform of a sparse COO matrix mat1 and a strided matrix mat2?,matrix multiplication,10735
11505,What is used to perform a matrix multiplication of the sparse matrix input?,dense matrix mat,10735
11506,What does sparse.log_apply?,softmax,10735
11507,What do sparse.mm perform a matrix multiplication of?,sparse COO matrix mat1 and a strided matrix mat2,10735
11508,What does sparse.mm perform a matrix multiplication of with the dense matrix mat?,sparse matrix input,10735
11509,What is performed of the sparse matrix mat1 and the (sparse or strided) matrix mat2?,matrix multiplication,4797
11510,Matrix multiplies sparse matrix input with what?,dense matrix mat,4797
11511,Which torch function supports sparse tensors?,hstack(),7098
11512,What is_signed() is_tensor()?,same_size(),7098
11513,What is the name of the torch function that supports sparse tensors?,svd_lowrank(),7098
11514,What do the following torch functions support?,sparse tensors,7098
11515,Returns what with the sine of the elements of input?,a new tensor,5407
11516,"What does torch.full_like(input, fill_value, layout=input.layout, device=input.device",fill_value,5479
11517,What is fill_value?,the number to fill the output tensor with,5479
11518,What happens if None?,defaults to the dtype of input,5479
11519,What is equivalent to torch.full?,torch.full_like,5480
11520,What is returned with the same size as input filled with fill_value?,tensor,5480
11521,"What is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=",torch.full,5480
11522,"Default: if None, what does torch.full_like(input.size(), fill_value, input.dtype",defaults to the dtype of input,5480
11523,"dtype (torch.dtype, optional) – what?",desired data type of returned tensor,8352
11524,What variable does torch.tensor() construct when data is a tensor x?,leaf,8352
11525,What is equivalent to x.clone().detach()?,torch.tensor(x),8352
11526,What are the equivalents of torch.tensor?,clone() and detach(),8352
11527,What can initial data for the tensor be?,"a list, tuple, NumPy ndarray, scalar, and other types",8352
11528,"Default: if what, infers data type from data. device (torch.device, optional) – the desired device",None,8352
11529,What determines size of the output tensor?,input,9633
11530,The STFT computes the Fourier transform of short overlapping windows of the input to give what of the signal as they change over time?,frequency components,6944
11531,What does the STFT compute without the optional batch dimension?,the following expression,6944
11532,Input must be either a time sequence or a 2-D batch of time sequences?,1-D time sequence or a 2-D batch of time sequences,3656
11533,"If hop_length is None, it is treated as equal to what?",floor(n_fft / 4),11376
11534,"If win_length is None, it is treated as equal to what?",n_fft,11376
11535,"If window is None (default), it is treated as if having what number everywhere in the window?",111,11376
11536,What begins at time thop_lengtht times texthop_lengththop_length?,ttt-th frame,11376
11537,What determines the padding method used on input when center is True?,pad_mode,11376
11538,What is the name of the function that determines the padding method used on input when center is True?,torch.nn.functional.pad(),11376
11539,What is the default setting for the padding method used on input when center is True?,"""reflect""",11376
11540,for torch.stft input must be what?,1-D time sequence or a 2-D batch of time sequences,11376
11541,"For all available options, see what for all available options.",torch.nn.functional.pad(),11376
11542,When will the window be padded on both sides to length n_fft before being applied.,"If win_length<n_fft window will be padded on
both sides to length n_fft before being applied.",11376
11543,What determines padding method used?,pad_mode,11376
11544,"If hop_length is None (default), it is treated as equal to what?",floor(n_fft / 4),3416
11545,"If what is True (default for real input), only values for omega are returned",onesided,3416
11546,Input must be a what?,1-D time sequence or a 2-D batch of time sequences,11374
11547,"If win_length is what, it is treated as equal to n_fft?",None,9638
11548,What is the default value for input when center is True?,reflect,9638
11549,"If win_length is None (default), it is treated as equal to what?",n_fft,3548
11550,"If normalized is True, what is the default?",If normalized is True,3548
11551,What is the win_length of a window?,1-D tensor of size,3546
11552,"If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides",padded,3546
11553,What happens to window if win_lengthn_ffttextwin_lengthn_fft?,padded on both sides to length n_fft,3414
11554,What can a window be a size of win_length?,1-D tensor,3415
11555,"If True, input will be padded on both sides so that the ttt-th frame is centered at time thop_",center,3415
11556,What is the default value for the padding method used on input when center is True?,reflect,3415
11557,"If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides?",padded,11400
11558,What happens to window if win_lengthn_ffttextwin_length  textn,padded on both sides to length n_fft,11400
11559,"If window is None (default), it is treated as if it has what number everywhere in the window?",111,11400
11560,"If window is what (default) value, it is treated as if having 111 everywhere in the window?",None,11400
11561,What can a window be of size win_length?,1-D tensor,11401
11562,"If center is what, input will be padded on both sides so that the ttt-th frame is centered?",True,11401
11563,"If win_length  textn_fftwin_lengthn_fft, window will be",n_fft,11401
11564,What is centered at time thop_lengtht times texthop_lengththop_length,ttt-th frame,9957
11565,What is the default value for Win_length?,floor(n_fft / 4),9957
11566,What frame is centered at time thop_lengtht times texthop_lengththop_,ttt-th,9957
11567,What is the default to return half of results to avoid redundancy for real inputs?,False onesided,9957
11568,What is the default padding method used when center is True?,reflect,10369
11569,"If center is True, input will be padded on both sides so that which frame is centered at time thop_lengtht",ttt-th frame,3400
11570,What is the name of the padding method used on input when center is True?,torch.nn.functional.pad(),3547
11571,"If what is the default, input will be padded on both sides so that the ttt-th frame is centered?",center is True,3547
11572,What is the default setting for a window when center is True?,reflect,3547
11573,What is the default setting for padding when center is True?,"""reflect""",3658
11574,"If window is None (default),what is it treated as?", it is treated as if having 111 everywhere in the window.,3658
11575,If what is true (default for real input) only values for omega are used?,one sided is True,3658
11576,If onesided is what (default for real input)?,True,3464
11577,"If what is true, the function returns the normalized STFT results?",normalized is True,3460
11578,Returns what if return_complex is true?,a complex tensor of size,3460
11579,Calling with what may cause error or return incorrect result?,previous signature,3460
11580,The input tensor n_fft (int) – the input tensor n_fft (,Fourier transform,3460
11581,"If return_complex is True, the return is what?",input.dim(),3465
11582,What is the number of frequencies where STFT is applied?,NNN,3465
11583,If onesided True what values are used for omega?,"only values for
ω\omegaω in [0,1,2,…,⌊n_fft2⌋+1]",3465
11584,"If normalized is True, the function returns what?",normalized STFT results,3465
11585,"If return_complex is False, the output is what?",input.dim(),3477
11586,What is the return if return_complex is True?,input.dim(),3477
11587,Returns either a complex tensor of size (NT)(* times N times T,a real tensor of size,3477
11588,What is the default value of normalized?,False,3459
11589,What is the last dimension of the input.dim() + 2 dimensional real tensor?,the last dimension represents the real and imaginary components,3476
11590,"If what is true, returns a complex tensor of size?",return_complex,5503
11591,What is the total number of frames used?,TTT,5504
11592,What is the default value for the window function?,None,5504
11593,Returns either a complex tensor of size (NT)(* times N times T),if return_complex is true,3478
11594,What is the default value for window of all 111 s?,None,3478
11595,n_fft (int) – the input tensor n_fft (int) – size,Fourier transform,3478
11596,What is the default value for n_fft?,None,3478
11597,What is the output if return_complex is False?,input.dim() + 2 dimensional real tensor,3478
11598,What is the default value for return half of results to avoid redundancy for real inputs?,False onesided,3478
11599,What is the default value for the size of window frame and STFT filter?,None,7520
11600,"What is window (Tensor, optional)?",optional window function,11394
11601,What controls the padding method used when center is True?,True pad_mode,11394
11602,What is the default for real input and window?,True,11394
11603,What is the default for STFT results?,"""reflect"" normalized",11394
11604,Default: None (treated as window of all how many s),111,9522
11605,What is hop_length equal to?,floor(n_fft / 4),9522
11606,What is the default window of all 111 s?,None,11393
11607,What is the default name of the padding method used when center is True?,"""reflect""",11393
11608,What is the default value for the padding method used when center is True?,reflect,11393
11609,n_fft (int) – size of what?,Fourier transform hop_length,9956
11610,What is n_fft equal to?,floor,9956
11611,"Default: None (treated as equal to n_fft) window (Tensor, what) – the optional window function",optional,9956
11612,Default: None (treated as window of all?,111 s,7521
11613,What is the default value for normalized STFT results?,False,7521
11614,What is the default value for return normalized STFT results?,False,7521
11615,What is the window function?,optional,11399
11616,Window (Tensor) – the optional window function. Default: None (treated as window of all 111 s),optional,11399
11617,What is the default setting for return normalized STFT results?,False,11399
11618,What is center?,whether to pad input on both sides,9054
11619,What do you want to avoid with real inputs?,redundancy,10368
11620,"Default: False onesided (bool, optional) – controls whether to return what percentage of results?",half,10266
11621,What is the default value for real input and window?,True,10266
11622,A tensor containing the STFT result with what described above Tensor containing the STFT result with?,shape,10266
11623,"Default: False onesided (bool, optional) – controls whether to return half of results to avoid what for real inputs?",redundancy,10266
11624,"What bool controls whether to return a complex tensor, or a real tensor with an extra last dimension for the",return_complex,10266
11625,What does dim represent?,the dimension or dimensions to approximate the gradient over,7536
11626,What does the number of bins in an array of non-negative ints do?,Count the frequency of each value in an array of non-negative ints,1933
11627,What does the number of bins in an array of non-negative ints need to be one larger than the largest value in input?,Note,1933
11628,"If what is specified, the number of bins is at least minlength and if input is empty, the result is tensor of size",minlength,7203
11629,What is the result of the number of bins (size 1) being one larger than the largest value in input unless input is empty?,tensor of size 0.,7203
11630,What does the number of bins (size 1) have to be larger than the largest value in input?,Note,7203
11631,What is the value at position i?,n,1934
11632,What happens to a tensor of shape Size([max(input) + 1])?,if input is non-empty,1934
11633,What is the result if the number of bins is one larger than the largest value in input unless input is empty?,a tensor of size 0.,1934
11634,"If what is specified, the number of bins is at least minlength and if input is empty, the result is a tensor",minlength,1934
11635,"Input (Tensor) – optional, weight for each value in the input tensor. Should be of same size as input",1-d int tensor weights,1934
11636,What should the weight for each value in the input tensor be of?,same size,1934
11637,Minlength (int) – what?,"optional, minimum number of bins",1934
11638,"Minlength (int) – optional, minimum number of bins. Should be what?",non-negative,1934
11639,What should the minimum number of bins be?,non-negative,4428
11640,What does torch.conj() do?,Computes the element-wise conjugate,1742
11641,What has a non-complex dtype?,input,1742
11642,What may return a non-writeable view for an input of non-complex dtype?,torch.conj(),1742
11643,What function returns a non-writeable view for an input of non-complex dtype?,torch.conj(),1742
11644,What is the name of the alias?,torch.vstack(),1063
11645,What is the name of the file that is used by Alias?,Alias of torch.vstack(),1063
11646,What is the name of the function that is used to create a vstack?,Alias of torch.vstack(),1063
11647,Fills self tensor with elements samples from the normal distribution parameterized by what?,mean and std,2415
11648,What fills with elements from the normal distribution?,self tensor,2415
11649,Fills what with elements samples from the normal distribution parameterized by mean and std?,self tensor,2415
11650,What does torch.as_tensor() copy data from?,NumPy ndarray,11233
11651,What type of data does torch.Tensor.requires_grad_() or torch.Tensor.detach,Tensor,11233
11652,What does torch.tensor do?,always copies data,11233
11653,What is a warning about a copy of data by a Tensor?,Warning,11233
11654,What do you use if you want to avoid a copy of a Tensor data?,torch.Tensor.requires_grad_() or torch.Tensor.detach(),11233
11655,What ndarray does torch.as_tensor() avoid a copy of?,NumPy,11233
11656,What does torch.as_tensor() do?,Warning,11233
11657,What does torch.tensor() always copy data?,a Tensor data,11035
11658,"If you want to change the required_grad flag of a Tensor, use what?",requires_grad_() or detach(),11035
11659,What does torch.as_tensor() use if you want to avoid a copy of data?,NumPy ndarray,8220
11660,What does torch.tensor always copy data?,Warning,8220
11661,What can data (array_like) – Initial data for the tensor be?,"a list, tuple, NumPy ndarray, scalar, and other types",8205
11662,What infers data type from data?,if None,9207
11663,What will device be for CPU tensor types?,CPU,5266
11664,What is the default number of columns in a 2-D tensor?,n out,5266
11665,What is the global default of a 2-D tensor?,Default,5266
11666,What is the default setting of a 2-D tensor?,torch.strided,5266
11667,What is the device for CPU tensor types?,CPU,3549
11668,What does window_length (int) mean?,length of the window,3549
11669,What is the name of the window that is suitable for use in spectral analysis?,periodic,3549
11670,What device will be the CPU for CPU tensor types?,current CUDA device,3549
11671,"If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors.",pin_memory,10588
11672,What is the default setting for pin_memory?,False,10588
11673,What is an example of a return tensor?,Example:,9165
11674,Returns what of the upper triangular part of a row by col matrix in a 2-by-N Tensor?,the indices,5563
11675,What is defined as the elements on and above the diagonal?,The upper triangular part,5563
11676,What value controls which diagonal to consider?,"offset = 0,",5563
11677,"What is the set of indices (i,i)lbrace (i, i) rbrace",main diagonal,5563
11678,What does row (int) mean?,row (int) – number of rows in the 2-D matrix,5563
11679,The dividend and divisor may contain both for what?,integer and floating point numbers,1753
11680,The remainder of division has the same what as the divisor other?,sign,1753
11681,"What is supported to a common shape, type promotion, and integer and float inputs?",broadcasting,1753
11682,What has the same sign as the divisor other?,The remainder,7035
11683,Supports what type of programming?,broadcasting,7035
11684,What is the input for the dividend?,Tensor,7033
11685,What does the dividend support?,broadcasting,7033
11686,The remainder of the dividend and divisor has the same sign as what?,dividend input,7033
11687,"Supports what to a common shape, type promotion, and integer and float inputs?",broadcasting,6110
11688,"When the divisor is zero, returns what for floating point dtypes on both CPU and GPU?",,6110
11689,What is input (Tensor) –?,the dividend,6110
11690,"In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with what?",complex numbers,6109
11691,"What is the input (Tensor) – the divisor out (Tensor, optional) – the output tens",the dividend,6109
11692,What computes the element-wise remainder of division equivalently to the C library function fmod()?,torch.fmod(),4362
11693,What is the input (Tensor) referred to as?,the dividend,4362
11694,What type of tensor does torch return?,2-D,5265
11695,What is the number of rows in the output tensor?,n,5265
11696,What is the default setting of the returned Tensor?,torch.strided,5265
11697,What is returned with ones on the diagonal and zeros elsewhere?,a 2-D tensor,5265
11698,What is the default number of columns in the output tensor?,n out,5265
11699,What is the default setting for a 2-D tensor?,torch.strided,5265
11700,What is the default value of torch.set_default_tensor_type()?,if None,4091
11701,What is the zeroth order modified Bessel function of the first kind?,L,4091
11702,What is equivalent to calling torch.kaiser_window?,Calling torch.kaiser_window,4091
11703,What is the periodic argument intended for?,a helpful shorthand,4091
11704,"If window_length is one, what is the returned window?",a single element tensor containing a one,4091
11705,"What does beta (float, optional) provide for the window?",shape parameter,4091
11706,"What is equivalent to calling torch.kaiser_window(L, B, periodic=True)[:-1])?","torch.kaiser_window(L, B, periodic=True)",4091
11707,"Default: if None, uses what?",global default,4091
11708,"Default: if None, uses a what default?",global,1678
11709,What is window_length?,length of the window,1678
11710,What is the name of the window suitable for use in spectral analysis?,periodic,1678
11711,"What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1])?","torch.kaiser_window(L, B, periodic=True)",1678
11712,What is the window length of the Kaiser window?,window_length,1678
11713,What argument is intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?,periodic,1678
11714,What is the default layout of the returned window tensor?,torch.strided,1678
11715,What does n out represent?,the output tensor,9952
11716,What is the default setting for the returned Tensor?,torch.strided,9952
11717,"n (int) – the nu of rows m (int, optional) – the number of columns with default being n",m,9952
11718,What is the default setting of returned Tensor?,torch.strided,9952
11719,"If None, uses a global default (see torch.set_default_tensor_type()).",Default,10342
11720,"Out (Tensor, optional) - what is the name of the output tensor?",output tensor,10342
11721,What is the default setting for the output tensor?,torch.strided,10342
11722,What is the default setting for the current device for the default tensor type?,if None,9772
11723,What is a tensor with ones on the diagonal and zeros elsewhere?,2-D tensor,9164
11724,Returns what with the truncated integer values of the elements of input?,a new tensor,5409
11725,What is an example of a new tensor?,Example,5409
11726,Returns what with the logarithm to the base 2 of the elements of input?,a new tensor,5403
11727,What is the name of the Alias for?,torch.special.expm1,1036
11728,What is the name of the site?,Alias for torch.special.expm1,1036
11729,What does Alias for torch.special.expm1() do?,Alias for torch.special.expm1(),1036
11730,What is the window_length?,window length,1677
11731,What does torch.i0() do?,Let I_0 be the zeroth order modified Bessel function of the first kind,1677
11732,What type of window is returned if False?,symmetric,1677
11733,Computes the Kaiser window with window length window_length and what other parameter?,shape parameter beta,1676
11734,Computes the Kaiser window with what parameter?,window length,1676
11735,What is the periodic argument intended to produce a periodic window as input to functions like torch.stft()?,Note,4090
11736,What is the name of the zeroth order modified Bessel function of the first kind?,L,4090
11737,What is the periodic argument intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?,Note,4090
11738,"What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]?","torch.kaiser_window(L, B, periodic=True)",1516
11739,"What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)?",Calling torch.kaiser_window,1516
11740,What is the returned window if window_length is one?,a single element tensor containing a one,4375
11741,The periodic argument is intended as a useful shorthand to produce a periodic window as input to functions like what?,torch.stft(),1517
11742,"Default: if None, uses a what?",global default,1517
11743,What is the term for dense layout?,torch.strided,1517
11744,What is the window returned if window_length is one?,a single element tensor,4374
11745,"What does beta (float, optional) represent for the window?",shape parameter,11406
11746,What is the default layout of a returned window tensor?,torch.strided,11406
11747,What does torch.set_default_tensor_type() use?,global default,10402
11748,What is the periodic window suitable for?,spectral analysis,10402
11749,What is dtype?,the desired data type of returned tensor,10402
11750,What is the only option that supports dense layout?,torch.strided,8954
11751,"What parameter does beta (float, optional) provide for the window?",shape parameter,8954
11752,What is the name for a dense layout?,torch.strided,9209
11753,What is the name of the desired layout of returned window tensor?,torch.layout,9773
11754,What is the default layout of a window tensor?,torch.strided,9773
11755,What is the layout of returned window tensor?,desired layout,9773
11756,What is the default layout of returned window tensor?,torch.strided,9773
11757,Device will be what for CPU tensor types and the current CUDA device for CUDA tensor types?,CPU,9773
11758,Computes what of a square matrix for an integer n?,n-th power,1788
11759,What is the first column of a product of Householder matrices?,first n columns,1788
11760,Efficiently multiplies two or more matrices by what?,reordering,1788
11761,What is the power of a square matrix for an integer n?,n-th power,1713
11762,"Computes the solution to the system torch.tensordot(A, X) = B.",X,1713
11763,What does torch.tensordot() compute?,multiplicative inverse,1786
11764,"Computes the solution X to the system torch.tensordot(A, X) = what?",B,1786
11765,What type of tensor does this function return?,uncoalesced tensor,4425
11766,What types of indices can be used for the tensor?,"a list, tuple, NumPy ndarray, scalar, and other types",4425
11767,What is the name of the internal tensor?,LongTensor,4425
11768,What is cast to the LongTensor internally?,torch,7558
11769,What can indices be?,"a list, tuple, NumPy ndarray, scalar, and other types",7558
11770,"Can be a list, NumPy ndarray, scalar, and other types?",tuple,7558
11771,What is the name of the tensor internally?,LongTensor,7558
11772,What is the first dimension of the indices?,the number of tensor dimensions,9610
11773,What are some other types of indices?,"tuple, NumPy ndarray, scalar",9610
11774,What will be cast to the LongTensor internally?,torch,9610
11775,What is the initial value for?,tensor,9610
11776,What are some other types of initial values for the tensor?,"tuple, NumPy ndarray, scalar",9610
11777,What is optional for a sparse tensor?,size,10694
11778,What default infers data type from values?,if None,10694
11779,What is the size of the sparse tensor if not provided?,minimum size big enough to hold all non-zero elements,10694
11780,"Default: if what, infers data type from values?",None,10694
11781,What infers data type from values?,if None,9208
11782,"Default: if what, infers data type from values. device (torch.device, optional) – the desired device",None,9208
11783,What is treated as False?,Zeros,1744
11784,"What is the tensor to compute OR with out (Tensor, optional) – the output tensor?",other,1744
11785,What is an example of a tensor to compute OR with out?,Example,1744
11786,What contains L and U factors for LU factorization of A?,LU,7563
11787,"What types of inputs does torch.solve(B, A) support?",real-valued and complex-valued inputs,7563
11788,What is the LU factorization of A in order?,namedtuple solution,7563
11789,What does LU contain for LU factorization of A?,L and U factors,7563
11790,What does torch.linalg.solve() not return?,LU factorization,7563
11791,"What should torch.solve(B, A) be replaced with?",Note,7563
11792,"What should X = torch.solve(B, A)solution be replaced with?",Note,7563
11793,What function replaces torch.solve()?,torch.linalg.solve(),7563
11794,What function returns the LU factorization of the input?,torch.lu(),7563
11795,"What type of inputs can torch.solve(B, A) take?",2D,4058
11796,"What does torch.solve(B, A) do when it takes inputs that are batches of 2D matrices?",Warning,4058
11797,"What type of inputs can torch.solve(B, A) take in?",2D,11218
11798,"What does torch.solve(B, A) provide?",Warning,11218
11799,What can be used with torch.lu_solve() and torch.lu_unpack() to get the LU factorization?,torch.lu(),6136
11800,What types of inputs does PyTorch support?,real-valued and complex-valued inputs,6136
11801,What is the replacement for torch.solve()?,torch.linalg.solve(),8217
11802,What does torch.linalg.solve() do?,does not return the LU factorization of the input,11215
11803,What is used to get the LU factorization of the input?,torch.lu(),11215
11804,What is torch.solve() replaced with?,torch.linalg.solve(),11215
11805,What is torch.solve() replaced by?,torch.linalg.solve(),11215
11806,What is the name of the strides used to transpose the returned matrices?,"B.contiguous().transpose(-1, -2)",4385
11807,What is *?,zero or more batch dimensions,4385
11808,What will be transposed regardless of the original strides?,the returned matrices solution and LU,4385
11809,"What is the name of the input square matrix of size (,m,m)(*, m, m)(,m",A,4385
11810,"Out ((Tensor, Tensor)) – optional output tuple.",optional,4385
11811,"What is the input square matrix of size (,m,m)(*, m, m)(,m,m)",A,8449
11812,What is the pointer to current cuBLAS handle?,cublasHandle_t,7782
11813,What is checked if the Context-manager selects a given stream?,peer access between two devices,7782
11814,What does cublasHandle_t return for a given device?,currently selected Stream,3938
11815,What does the cublasHandle_t pointer return for a given device?,the currently selected Stream,1879
11816,What does the Context-manager return for a given device?,default Stream,5527
11817,What does this library get?,the cuda capability of a device,3937
11818,What does the library get from a device?,properties,7783
11819,What is the name of a device. Gets the properties of a device?,Gets the cuda capability of a device,7783
11820,What was this library compiled with?,NVCC gencode flags,5499
11821,Returns the currently selected Stream for a given device. Returns what for a given device?,default Stream,5499
11822,Gets the cuda capability of a device. Gets what?,name of a device,5499
11823,What Scatters tensors from multiple GPUs?,comm.scatter,5499
11824,What does PyTorch return for a given device?,default Stream,5531
11825,What type of memory allocator statistics does nvidia-smi return?,CUDA,5136
11826,What does memory_reserved do?,Set memory fraction for a process,5138
11827,What does memory_reserved() do for a process?,Set memory fraction,5138
11828,Set memory fraction for a process. Deprecated; see what?,memory_reserved(),5138
11829,Deprecated; see what?,max_memory_reserved(),5138
11830,What is memory_reserved()?,Deprecated,5306
11831,What does memory_reserved do for a process?,Set memory fraction,5306
11832,What does memory_reserved() do?,Set memory fraction for a process,5306
11833,What is a deprecated function that returns the maximum GPU memory occupied by tensors in bytes for a given device,max_memory_reserved,5308
11834,What is a deprecated function that returns the maximum GPU memory occupied by tensors for a given device?,max_memory_reserved(),5163
11835,What is the name of the deprecated function that returns the maximum GPU memory managed by the caching allocator for a given device,max_memory_reserved(),5525
11836,What is modeled after SciPy's special module?,The torch.special module,7336
11837,What does the torch.special module compute on input?,entropy,7336
11838,Computes the entropy on input in what way?,elementwise,1754
11839,"What is defined in the range (1,1)(-1, 1)(1,1)?",inverse error function,1754
11840,What is the inverse error function of input defined as?,input (Tensor),1754
11841,When may some functions change?,PyTorch releases,7758
11842,Computes what on input?,entropy,7758
11843,What does Computes the error function of input do?,Computes the complementary error function of input,7758
11844,What is the name of each function in PyTorch?,documentation,7757
11845,"Out (Tensor, optional) – the output tensor.",optional,9629
11846,Computes the exponential of the elements minus what amount of input?,1,9629
11847,Computes the natural what of the absolute value of the gamma function on input?,logarithm,9629
11848,What does Compute the error function of input do?,Computes the complementary error function of input,9626
11849,"What error function is defined in the range (1,1)(-1, 1)(1,1)?",inverse error function,1761
11850,"What is out (Tensor, optional) defined as?",output tensor,9627
11851,Computes the exponential of the elements minus what number of input?,1,9627
11852,What is the function that provides greater precision than exp(x) - 1 for small values of x?,Computes the exponential of the elements minus 1 of input,9627
11853,What does Computes the exponential of the elements minus 1 of input provide?,greater precision,9627
11854,"What is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) – the input",inverse error function,9628
11855,What does torch.special.i0eWhat do?,Computes the exponentially scaled zeroth order modified Bessel function of the first kind for each element of input.,9628
11856,What is the name of the function that computes the complementary error function of input?,torch.special.erfc,9628
11857,What is the name of the function that computes the error function of input?,torch.special.erf,9628
11858,torch.special.logit returns what?,Returns a new tensor with the logit of the elements of input,9628
11859,"What function is defined in the range (1,1)(-1, 1)(1,1)?",inverse error function,10338
11860,What is the inverse error function of input called?,input (Tensor),2307
11861,"What function is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) – the",inverse error function,2307
11862,Out (Tensor) - the output tensor.,optional,2304
11863,What is the natural value of the absolute value of the gamma function on input?,logarithm,2306
11864,What natural function is computed of the absolute value of the gamma function on input?,logarithm,7555
11865,What order modified Bessel function of the first kind is computed for each element of input?,zeroth,1695
11866,What type of Tensor is the output tensor?,optional,1695
11867,Computes what of input?,base two exponential function,1695
11868,Computes what of the absolute value of the gamma function on input?,natural logarithm,1755
11869,What is the absolute value of the gamma function on input?,natural logarithm,10337
11870,"Out (Tensor, optional) - the output tensor.",optional,1789
11871,What is input clamped to when eps is not None?,eps,1696
11872,"When eps is None and input  0 or input > 1, the function will yield what?",,1696
11873,What does input (Number or Tensor) represent?,Multiplier,1696
11874,What does the function do for each element of input?,Computes the exponentially scaled zeroth order modified Bessel function of the first kind,1765
11875,What is the input with the following cases?,* log1p(other),10340
11876,What is scipy.special.xlog1py similar to?,SciPy,2301
11877,What is the default output tensor?,None out,5404
11878,What does the function compute with the following cases?,* log1p(other),5404
11879,"If increasing is true, the order of the columns is reversed x0,x1,...,x(N1),x(N",True,2682
11880,What type of input tensor is x (Tensor)?,1-D,2682
11881,"If increasing is what, the order of the columns is reversed?",True,6993
11882,What are the powers of the input vector?,elementwise,6993
11883,"If increasing is what, the order of the columns is reversed x0,x1,...,x(N1)x0,",True,6993
11884,What is x (Tensor)?,1-D input tensor,11425
11885,What does N stand for?,Number of columns in the output,11425
11886,"What is increasing (bool, optional)?",Order of the powers of the columns,2683
11887,"What is N (int, optional)?",Number of columns in the output,4292
11888,What is N?,Number of columns in the output,4292
11889,"What does increasing (bool, optional) mean?",Order of the powers of the columns,4292
11890,What does mantissa and exponent tensors do?,Decomposes input,2084
11891,What type of input does mantissa support?,float inputs,2084
11892,What is the input tensor out?,input (Tensor),2084
11893,What is the name of the function that performs the element-wise division of tensor1 by tensor2?,Warning,4811
11894,How does the element-wise division of tensor1 by tensor2 work?,multiply the result by the scalar value,4811
11895,What is the result of the element-wise division of tensor1 by tensor2?,Warning,4811
11896,"The shapes of input, tensor1, and tensor2 must be what?",broadcastable,4812
11897,What value does addcdiv multiply the result of the element-wise division of tensor1 by tensor2?,scalar,4812
11898,What is no longer supported with addcdiv?,Warning Integer division,4812
11899,For what type of inputs can addcdiv be implemented as (input + value * tensor1 / tensor2),float inputs,4812
11900,What is the tensor to be added tensor1 (Tensor)?,input,4812
11901,Input (Tensor) – what is to be added tensor1 (Tensor)?,tensor,7287
11902,What is an example of a tensor that must be broadcastable?,Example,7287
11903,What does the Future type encapsulate?,asynchronous execution,7786
11904,What is the Future type primarily used by?,Distributed RPC Framework,7786
11905,What is an example of an asynchronous execution of a callable?,rpc_async(),7786
11906,What is GPU support a beta feature?,Warning,7786
11907,What can a callback function use to get the value of a Future?,value() method,7786
11908,What does the torch._C.Future expose to add callback functions and set results?,APIs,7786
11909,What happens if this Future is done?,Return True,7786
11910,What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects?,Wrapper around a torch._C.Future ,7786
11911,What does the Future type expose to add callback functions and set results?,APIs,7785
11912,What does torch._C.Future encapsulate?,asynchronous execution of a callable,8439
11913,"What can be added to the same Future, but the order in which they will be executed cannot be guaranteed?",Multiple callbacks,8439
11914,"If the Future is already completed, the given callback will be run what?",inline,8439
11915,When will the given callback function be run?,when the Future is completed,8439
11916,Which method provides a way to synchronize after your callback has completed?,then(),8439
11917,What can be cheaper if your callback does not return anything?,add_done_callback,8439
11918,What does add_done_callback behave in the same way as then()?,GPU tensors,8439
11919,"What is a Callable that takes in one argument, is the reference to this Future?",callback,8439
11920,What do both then() and add_done_callback use under the hood?,same,8439
11921,What does Future.done() return if this Future is done?,True,8439
11922,When is a Future done?,if it has a result or an exception,8439
11923,What does torch._C.Future expose?,APIs,8439
11924,What is Warning about GPU support?,"GPU support is a beta feature, subject to changes",8439
11925,What does torch._C.Future expose to add callback functions and set results?,APIs,8437
11926,What version of Warning GPU support is subject to changes?,beta,8154
11927,When will the given callback function run?,when the Future is completed,2665
11928,What cannot be guaranteed when multiple callbacks are added to the same Future?,the order in which they will be executed,1157
11929,What kind of callback registration API do both then() and add_done_callback use?,same,1157
11930,What will be run when the Future is completed?,Append the given callback function to this Future,1157
11931,"If this Future is already completed, the given callback will be run what?",inline,1157
11932,What is the name of the callable that takes in one argument?,Note,8283
11933,"What is the name of the callback that takes in one argument, is the reference to this Future?",which,8283
11934,What method provides a way to synchronize after your callback has completed?,then(),8285
11935,What is the name of the callback that takes in one argument?,Future,8422
11936,What does this method behave in the same way as then()?,GPU tensors,8422
11937,What happens if a Future is done?,Return True,8422
11938,What is the name of the Callable that references the Future?,Note,8421
11939,"What is a callback for a Callable that takes in one argument, is the reference to this Future?",Future,8421
11940,"What is a callback that takes in one argument, is the reference to this Future?",Future,9014
11941,What function returns True if the value contains tensors that reside on GPUs?,Future.done(),9014
11942,What is a callback that takes in one argument?,Note,9013
11943,What does Future.done() return if the Future is done?,True,5177
11944,A Future is done if it has what?,a result or an exception,5177
11945,"If the result is already usable, what does Future.done() do?",synchronizations,5177
11946,"If the value contains what that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are popul",tensors,5178
11947,"When calling wait()/value() on this Future, the exception set here will be raised what?",inline,5178
11948,What will return True if the value contains tensors that reside on GPUs?,Future.done(),3516
11949,What does a Future not do?,a Future cannot be marked completed twice,3516
11950,What does Future.done() do?,record events on all the relevant current streams,3516
11951,What will be run immediately inline if this Future is already completed?,the given callback,3516
11952,What does Future.done() do to ensure that the result is already usable?,synchronizations,3514
11953,What will mark this Future as completed with an error and trigger all attached callbacks?,this Future,5820
11954,"When calling wait()/value() on this Future, the exception set here will be raised what way?",inline,5820
11955,What is the exception for this Future?,result,10596
11956,What does a Future cannot be marked completed twice?,a Future cannot be marked completed twice,10596
11957,What does setting the result for this Future do?,mark this Future as completed and trigger all attached callbacks,10596
11958,"Set the result for this Future, which will mark this Future as what?",completed,5819
11959,What does the result for this Future do?,mark this Future as completed,10594
11960,What will mark this Future as completed and trigger all attached callbacks?,Set the result for this Future,5830
11961,How many times can a Future be marked completed?,twice,5830
11962,"Set the result for this Future, which will do what?",mark this Future as completed and trigger all attached callbacks,5830
11963,Is a Future able to be completed twice?,a Future cannot be marked completed twice,5830
11964,What will this method record events on?,all the relevant current streams,5830
11965,What happens when a Future is marked as completed?,trigger all attached callbacks,5829
11966,What does it mean that a Future cannot be marked completed twice?,a Future cannot be marked completed twice,5829
11967,What is the result object of the Future?,object,10597
11968,What is the result object of this Future?,object,3511
11969,This method will record events on what?,all the relevant current streams,3511
11970,What is a Callable that takes this Future as the only argument?,Callable,5919
11971,What is the value held by this Future?,The value held by this Future,5919
11972,"If the value contains tensors that reside on GPUs, this method will not perform any additional synchronization. This should be done separately",wait(),5919
11973,A new Future object that holds what of the callback will be marked as completed when the given callback finishes?,return value,825
11974,Obtain the value of an already-completed future. This method should only be called after a call to what?,wait() has completed,825
11975,What does a Callable that takes this Future as the only argument do?,Note,5918
11976,When will a new Future object be marked as completed?,completed when the given callback finishes,5918
11977,What is the only argument for a callback?,Future,9010
11978,When a new Future object holds the return value of the callback is marked as what?,completed,9010
11979,What does a Callable that takes this Future object hold the return value of the callback and will be marked as completed when the given callback finishes,Note,9010
11980,What is a new Future object that holds the return value of a callback that will be marked as completed when the given callback finishes?,Note,824
11981,What is a new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes?,Note,824
11982,What holds the return value of the callback and will be marked as completed when the given callback finishes?,A new Future object,826
11983,What method does not perform any additional synchronization if the value contains tensors that reside on GPUs?,wait(),826
11984,The future returned by fut.wait() will be marked appropriately with what?,the encountered error,4467
11985,What happens if the callback function throws an error?,the future returned by then will be marked appropriately,4391
11986,What could happen if the method is called after a call to wait() has completed?,Future may not yet hold a value,4575
11987,Where should a method be called to obtain the value of an already-completed future?,inside a callback function,4575
11988,Obtain the value of an already-completed future. This method should only be called after a call to what function has completed?,wait(),4577
11989,What is done when all of the sub-futures are completed?,Collects the provided Future objects into a single combined Future,4577
11990,What happens if any of the futures encounter an error?,exit early,4577
11991,What could cause value() to fail?,this Future may not yet hold a value,4577
11992,What does the value() method do?,Block until the value of this Future is ready,4577
11993,"If the function (callback or RPC) creating the value has thrown an error, what method will also throw an error?",this wait method,4577
11994,"If the value contains what that reside on GPUs, this method will not perform any additional synchronization.",tensors,4577
11995,"If the value contains tensors that reside on GPUs, what is performed with the kernels?",synchronization,4577
11996,What method inserts the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the,wait(),4577
11997,What may not yet hold a value?,Future,7731
11998,What could happen if a call to value() is called after a call to wait() has completed?,Future may not yet hold a value,7731
11999,Where do tensors reside?,GPUs,7732
12000,Where should this method be called after a call to wait() has completed?,inside a callback function,7732
12001,What could cause the call to value() to fail?,this Future may not yet hold a value,7732
12002,How long should the value() method be called?,until the value of this Future is ready,7732
12003,What method should be used to perform additional synchronization if the value contains tensors that reside on GPUs?,wait(),4468
12004,What does this method do to obtain the value of an already-completed future?,Obtain the value of an already-completed future,4468
12005,What could happen if a call to value() fails?,Future may not yet hold a value,4468
12006,"If the value contains what that reside on GPUs, this method will not perform any additional synchronization?",tensors,4468
12007,"If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done",wait(),9012
12008,What does a callback do?,Obtain the value of an already-completed future,9012
12009,What does a callback take as the only argument?,Future,9012
12010,The synchronization of tensors that reside on GPUs should be done separately through a call to what?,wait(),3519
12011,What happens to the value held by this Future?,the function (callback or RPC) creating the value has thrown an error,3519
12012,What does the value() method do until the value of this Future is ready?,Block,3519
12013,What is the name of the method that performs synchronization of tensors that reside on GPUs?,wait(),3519
12014,How long should the value() method block?,until the value of this Future is ready,3519
12015,How long is the value held by this Future blocked?,until the value of this Future is ready,7349
12016,What is the name of the block until the value of the Future is ready?,Block,1445
12017,How long until the value of this Future is ready?,Block,1445
12018,What does the wait method return if the function creating the value has thrown an error?,The value held by this Future,7352
12019,What is the function that collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed,Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed,7352
12020,Returns a Future object to what?,a list of the passed in Futures,7352
12021,What is a futures (list)?,list of Future object,7352
12022,When will this method throw an error?,if wait on any Future throws,7352
12023,Futures (list) – a list of what?,completed Future results,7352
12024,What does the wait method throw an error on?,The value held by this Future,7352
12025,What does the wait method do?,Waits for all provided futures to be complete,7352
12026,"If any of the futures encounter an error, the method will report the error not waiting for other futures to complete?",exit early,7352
12027,What functions create the value of a Future?,callback or RPC,7351
12028,What does this return a Future object to?,Returns a Future object to a list of the passed in Futures,7351
12029,What is the value held by a Future?,value held by this Future,7351
12030,What function created the value has thrown an error?,callback or RPC,7351
12031,What is a list of Future objects?,list,3517
12032,What will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the as,wait(),3517
12033,What happens when all of the sub-futures are completed?,Collects the provided Future objects into a single combined Future,3517
12034,When is a single combined Future completed?,when all of the sub-futures are completed,1618
12035,What does the method do?,Waits for all provided futures to be complete,1618
12036,"If any of the futures encounter an error, the method will do what?",exit early,1618
12037,What does futures return?,Returns a Future object to a list of the passed in Futures,9433
12038,What is futures (list)?,a list of Future object,9433
12039,What does this method do if wait on any Future throws?,throw an error,9433
12040,What does the dividend and divisor compute?,the element-wise remainder of division,1752
12041,What is the dividend input?,input (Tensor),1752
12042,Returns what of (input - other) The shapes of input and other must be broadcastable?,p-norm,5650
12043,What is the other (Tensor)?,the Right-hand-side input tensor,5650
12044,"Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes called what?",deconvolution,1202
12045,What does deconvolution combine an array of sliding local blocks into?,large containing tensor,1234
12046,What does Compute a partial inverse of MaxPool2d?,Computes a partial inverse of MaxPool3d,1234
12047,What does Computes a partial inverse of MaxPool1d?,Computes a partial inverse of MaxPool2d,2367
12048,Applies a 1D power-average pooling over an input signal composed of what?,several input planes,2367
12049,Applies what operation in kTkHkWkT times kH times kWkT,3D average-pooling,1171
12050,What type of pooling over an input signal composed of several input planes?,1D power-average,1171
12051,Computes a partial inverse of MaxPool2d. Computes a partial inverse of MaxPool,Computes a partial inverse of MaxPool2d,1171
12052,Applies what element-wise function?,hardswish function,1171
12053,What does 3D fractional max pooling over an input signal composed of several input planes?,2D fractional max pooling,1643
12054,"Applies the element-wise function ReLU6(x)=min(max(0,x),6)textReLU6",hardswish,1255
12055,What is the in-place version of elu()?,elu(),1255
12056,A squared term is used if the absolute element-wise error falls below what?,beta,2642
12057,"If the first argument is 2-dimensional and the second argument is what, the matrix-vector product is returned?",1-dimensional,3490
12058,"If the first argument is 2-dimensional and the second argument is 1-dimensional, what dimension is prepended to its dimension for the purpose of the batched matrix",1,3490
12059,What type of dimensions are broadcasted?,non-matrix,6971
12060,"If the second argument is at least what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after",1-dimensional,3398
12061,What is a torch.Tensor?,multi-dimensional matrix containing elements of a single data type,869
12062,What type of tensor does Torch define?,LongTensor Boolean,7939
12063,What is another name for a doubletensor torch?,double torch,11043
12064,What is the sign of an IntTensor 6?,4-bit integer,11039
12065,What is a bfloat16 torch?,16-bit floating point 2 torch,119
12066,What is the unsigned name of the IntTensor torch?,/ quantized 4-bit integer,11034
12067,What is the sign for torch.int8 torch?,8-bit integer,177
12068,What is an alias for the default tensor type?,torch.Tensor,151
12069,What can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor,A tensor of specific data type,8219
12070,Add what to self tensor?,scalar or tensor,5341
12071,What provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type?,PyTorch,4975
12072,What is stored contiguously in memory?,array elements,4975
12073,What does PyTorch use for those array elements that are actually stored?,specified elements,4975
12074,What is a sparse storage format?,Note,4974
12075,What format can be advantageous only when the size and sparsity levels of arrays are high?,sparse storage,8107
12076,What is at least product(tensor shape>) * size of element type in bytes>?,strided tensor,7174
12077,What is NOT a list of index tuples?,the input i,846
12078,What type of sparse COO tensor can be constructed by specifying its size only?,empty,846
12079,What is the input i of a sparse COO tensor?,i,847
12080,What is the input i NOT a list of?,index tuples,6148
12081,What extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional ten,PyTorch,1122
12082,How does PyTorch hybrid COO tensor extend the sparse COO tensor?,allowing the values tensor to be a multi-dimensional tensor,1122
12083,PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tens,multi-dimensional tensor,5028
12084,What does M stand for?,s.sparse_dim(),5028
12085,What are the corresponding tensor values collected with?,arbitrary integer or floating point number element type,10844
12086,"If s is a sparse COO tensor and M = s.sparse_dim(), what",K,10844
12087,What does PyTorch sparse COO tensor format permit?,uncoalesced sparse tensors,3714
12088,PyTorch sparse COO tensor format permits what?,uncoalesced sparse tensors,4984
12089,How many values can be specified for the same index?,1,4983
12090,How many values can be specified for the same index 1?,3 and 4,4983
12091,What are two values that lead to a 1-D uncoalesced tensor?,3 and 4,2106
12092,How are sparse COO tensors implemented?,by simply concatenating the indices and values tensors,2969
12093,What is a sparse COO tensor a instance of?,torch.Tensor,2580
12094,How will most operations work with a coalesced or uncoalesced sparse tensor?,most operations will work identically,2580
12095,What should you do to prevent sparse tensors from growing too large?,coalesce,3575
12096,What type of COO tensor is a torch.Tensor instance?,sparse,2548
12097,What can be acquired using methods torch.Tensor.indices() and torch.Tensor.values()?,COO format data,4635
12098,What is used to distinguish sparse instances from other layouts?,Use torch.Tensor.is_sparse,4635
12099,What can one acquire only when the tensor instance is coalesced?,COO format data,4635
12100,What could be implemented by multiplying all the uncoalesced values with the scalar?,scalar multiplication,4635
12101,What type of operation cannot be implemented by a square root?,nonlinear operation,4635
12102,What dimension can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d,sparse,7207
12103,What does not hold in general?,sqrt(a + b) == sqrt(a) + sqrt(b),8387
12104,The values of the same indices are what?,terms of a sum,2495
12105,What is the CSR tensor of size nnz?,1-D tensor,7358
12106,What does svd_lowrank() yes indicate if the PyTorch operation supports backward with respect to sparse matrix argument,SVD,11096
12107,What does the PyTorch operation support backward with respect to sparse matrix argument?,All PyTorch operations,10235
12108,What is the only PyTorch operation that supports backward with respect to sparse matrix argument?,torch,4178
12109,"All PyTorch operations, except torch.smm(), support backward with respect to what?",strided matrix arguments,11068
12110,PyTorch does not support matrix multiplication with what layout signature?,M[strided] @ M[sparse_coo],11015
12111,PyTorch applications can still compute this using what?,matrix relation,11015
12112,"If self is a sparse COO tensor that is coalesced, return a coalesced copy of self",uncoalesced,3879
12113,What Return the number of sparse dimensions in a sparse tensor self?,Tensor.sparse_dim,3879
12114,"If self is a sparse COO tensor, what type of tensor is returned?",uncoalesced,6744
12115,What returns the indices of the self tensor when self is a sparse CSR tensor of layout spars,Tensor.col_indices,1900
12116,Convert a tensor to what format?,compressed row storage format,1900
12117,Return the tensor of a sparse COO tensor. Tensor.values Return the values tens,indices,5189
12118,What is the name of the short-time Fourier transform function that will only return complex tensors?,return_complex=True,5887
12119,The interface of the STFT computes the Fourier transform of short overlapping windows of the input is modeled after what?,librosa stft function,2618
12120,What must the input be?,1-D time sequence or a 2-D batch of time sequences,6945
12121,"If window is None (default), it is treated as if having what everywhere in the window?",111,11402
12122,for STFT Window can be a what?,1-D tensor of size win_length,11402
12123,"If what is true, input will be padded on both sides so that the ttt-th frame is centered?",center is True,11402
12124,When does function return the normalized STFT results.,"If normalized is True, the ",11402
12125,"If win_length is None, window will be padded on both sides to length n_fft before being applied.",n_fft,3657
12126,"If win_length is None, window will be padded on both sides to length n_fft before being applied?",n_fft,11375
12127,When does the function return the normalized STFT results?,If normalized is True,3461
12128,"If return_complex is True, the return is a input.dim() + what?",1 dimensional complex tensor,3461
12129,What is the output if return_complex is True?,input.dim() + 2 dimensional real tensor,3461
12130,What type of data does torch.as_tensor() avoid a copy of?,NumPy ndarray,11234
12131,What does torch.tensor() construct when data is a tensor x?,a leaf variable,11234
12132,What do you use if you have a Tensor data and want to avoid a copy?,torch.Tensor.requires_grad_() or torch.Tensor.detach(),11234
12133,Computes the what of a square matrix for an integer n?,n-th power,1729
12134,Returns a bool indicating if CUDA is currently available.,whether PyTorch’s CUDA state has been initialized,1507
12135,Sets the current device. This is a wrapper API to set the stream.,current stream,1579
12136,What does the caching allocator do for a process?,Set memory fraction,5137
12137,What does Computes the exponential of the elements minus 1 of input?,Computes the exponential of the elements minus 1 of input,10335
12138,What is the exponentially scaled zeroth order modified Bessel function?,the first kind,1763
12139,What does the torch._C.Future encapsulate?,asynchronous execution of a callable,8438
12140,What does the torch._C.Future expose?,APIs,8438
12141,What API does add_done_callback use?,callback registration,8438
12142,What is the name of the callback function that will be run when the Future is completed?,Append,5831
12143,What is used to enforce a certain order?,chaining,5831
12144,"If this Future is already completed, the given callback will be run immediately what?",inline,5831
12145,What does a Future not have to be done twice?,a Future cannot be marked completed twice,5831
12146,What is another name for callback?,Future,8284
12147,"If the value contains tensors that reside on GPUs, what will return True even if the asynchronous kernels that are popul",Future.done(),9704
12148,"If the result is already usable, what can be performed?",synchronizations,3515
12149,What happens when the result is set for this Future?,mark this Future as completed and trigger all attached callbacks,10595
12150,What is safe to call this method immediately after launching kernels?,change streams in between,10595
12151,"If the Future is already completed, the given callback will be run immediately what?",inline,3512
12152,"If the Future's value contains tensors that reside on GPUs, the callback might be invoked when?",while the async kernels that are populating those tensors haven’t yet finished executing on the device,3512
12153,What are the dedicated streams set to when the callback is invoked?,current,3512
12154,The non-blocking behavior of what method is similar to the non-blocking behavior of?,wait(),3512
12155,What does the Future's value contain that reside on GPUs?,tensors,3485
12156,What non-blocking behavior is similar to the non-blocking behavior of?,wait(),3485
12157,What are the dedicated streams set as?,current,3486
12158,What is a method that should only be called after a call to wait() has completed?,Obtain the value of an already-completed future,3486
12159,When should the Obtain the value of an already-completed future be called?,after a call to wait() has completed,3486
12160,"If the Future's value contains what that reside on GPUs, the callback might be invoked while the async kernels that are",tensors,3486
12161,The non-blocking behavior of what is similar to the non-blocking behavior of?,wait(),3486
12162,What happens if the callback returns a value that contains tensors that reside on a GPU?,"the
callback might be invoked while the async kernels that are populating
those tensors haven’t yet finished executing on the device",3486
12163,What does a callback do to an already-completed future?,Obtain the value of an already-completed future,3486
12164,What could happen if the callback is called after a call to wait() has completed?,Future may not yet hold a value,3486
12165,What holds the return value of the callback?,A new Future object,9011
12166,What could cause calling value() to fail?,this Future may not yet hold a value,9011
12167,What could happen if the method is called after a call to wait() has completed or inside a callback function passed to then()?,Future may not yet hold a value,4392
12168,How long should the value() method be blocked?,until the value of this Future is ready,4576
12169,When does Block occur?,until the value of this Future is ready,1446
12170,"If the value contains what that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may",tensors,7350
12171,"What is required when accessing and using the values, as long as one doesn't change streams?",No further synchronization,4393
12172,What type of convolution is applied to an input signal composed of several input planes?,1D convolution,1193
12173,What is another name for a 2D transposed convolution operator over an input image composed of several input planes?,deconvolution,1193
12174,Applies what operation in kTkHkWkT times kWkTkHkW regions by step,3D average-pooling,1190
12175,What is the In-place version of threshold()?,In-place version of threshold(),1190
12176,What type of linear unit function does threshold() apply?,rectified linear unit function element-wise,1190
12177,What is the rectified linear unit function element-wise?,In-place,1190
12178,Applies a pooling over an input signal composed of several input planes. Applies a pooling over an input signal composed of several input,3D max,1207
12179,What is the result of a 2D max pooling over an input signal composed of several input planes?,Computes a partial inverse of MaxPool1d,1207
12180,Applies what type of convolution over an input image composed of several input planes?,3D,1222
12181,Applies what over an input image composed of several input planes?,3D convolution,1222
12182,What is the name of the pooling over an input signal?,3D adaptive max,1222
12183,Applies what operation in kHkWkH times kWkHkW regions by step size sHs,2D average-pooling,1235
12184,What does a 3D transposed convolution operator combine an array of sliding local blocks into?,large containing tensor,1235
12185,What is a 2D fractional max pooling over an input signal composed of several input planes?,2D fractional max pooling over,1235
12186,What operation in kTkHkWkT times kWkTkHkW regions by step size,3D average-pooling,1622
12187,What does a 2D max pooling over an input signal consist of?,several input planes,1622
12188,What does an array of sliding local blocks combine into?,a large containing tensor,1622
12189,What is the input signal composed of?,several input planes,1622
12190,What does a 2D adaptive max pooling over an input signal consist of?,several input planes,1622
12191,What does each element of the input Tensor have?,Thresholds,1622
12192,What type of max pooling over an input signal composed of several input planes?,3D,1213
12193,What is the result of the 2D max pooling over an input signal composed of several input planes?,Computes a partial inverse of MaxPool1d,1213
12194,What does a 3D adaptive max pooling over an input signal consist of?,several input planes,1213
12195,"What function element-wise. In-place version of hardtanh(). Applies the hardswish function, element-wise?",HardTanh,1213
12196,In-place version of what?,leaky_relu(),3823
12197,What is the result of two tensors?,Matrix product of two tensors,4213
12198,The non-matrix dimensions are what?,broadcasted,4213
12199,What is a dtype CPU tensor or a GPU tensor?,dtype CPU tensor GPU tensor,9211
12200,What type of tensor is 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.,CPU tensor GPU tensor,1499
12201,How many bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.Hal,16,118
12202,What is another term for binary16?,Sometimes referred,11044
12203,What is torch.float64 or torch.double torch?,64-bit floating point,163
12204,"What is the term used to describe a device that uses 1 sign, 5 exponent, and 10 significand bits?",Brain Floating Point,163
12205,"Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of",Useful,10951
12206,How many bits does a doubletensor have?,16,11040
12207,What is torch.cuda.HalfTensor?,16-bit floating point 2,11046
12208,What is Brain Floating Point stored as?,8-bit signed integer,11046
12209,What is torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor,16-bit floating point 2,120
12210,What is the name of the torch.Tensor?,torch.Tensor,120
12211,What is the same number of exponent bits as float32 quantized 4-bit integer?,8-bit signed integer,11013
12212,What is an alias?,torch.Tensor,11013
12213,What is torch.Tensor an alias for?,default tensor type (torch.FloatTensor),129
12214,What type of integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.,16-bit,129
12215,A tensor can be constructed from a what?,Python list or sequence using the torch,159
12216,What can be constructed from a Python list or sequence using the torch.tensor type?,A tensor,11029
12217,What is torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.By,128-bit complex,114
12218,What can be constructed from a Python list or sequence using the torch.tensor() constructor?,A tensor,11025
12219,"What is the name of the tensor that uses 1 sign, 5 exponent, and 10 significand bits?",binary16,11025
12220,What does a Tensor data need to change?,requires_grad flag,10948
12221,"If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach()",numpy,178
12222,"If you have a Tensor data and want to avoid a copy, use requires_grad_() or detach() to avoid",numpy array,11080
12223,What is the signature of torch.cuda.CharTensor?,16-bit integer,11037
12224,What is True if the Tensor is quantized?,Tensor.is_quantized,3876
12225,"Is True if the Tensor is a meta tensor, False otherwise?",meta,3876
12226,Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing what?,real values of the self tensor,3876
12227,Is Tensor.is_meta True if the Tensor is a meta tensor?,True if the Tensor is a meta tensor,5286
12228,What does torch.abs() Tensor.abs stand for?,Tensor.abs,5286
12229,Where is the Tensor.device?,torch.device,3891
12230,Tensor.imag Returns a new tensor containing what of the self tensor?,imaginary values,6573
12231,What is the name of the feature that adds a scalar or tensor to self tensor?,addbmm,6570
12232,What type of data does a Tensor.new_empty return a Tensor of size size filled with?,uninitialized data,6570
12233,What is another name for addbmm() Tensor?,addbmm,6457
12234,Tensor.is_meta Is what?,True if the Tensor is a meta tensor,6457
12235,What does Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing,real values of the self tensor,6457
12236,The default gain forSELUsacrifices what effect for more stable gradient flow in rectangular layers?,the normalisation effect,5200
12237,Return the what value for the given nonlinearity function?,recommended gain value,5200
12238,What is the recommended gain value for the given nonlinearity function?,nonlinearity gain,5200
12239,What is the Tensor mean?,the mean of the normal distribution std,5200
12240,tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std–,n-dimensionaltorch,5200
12241,"What is the definition of Identity 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac5",Linear,4113
12242,What is needed to induce a stable fixed point in the forward pass?,a variance of1/N,4114
12243,"What is 111 Conv1,2,3D 111 Sigmoid 111 Tanh?",Linear / Identity,4114
12244,"What does Linear / Identity 111 Conv1,2,3,3D 111 Tanh 53frac5335",tensor,4114
12245,What do the initial weights have to have in order to induce a stable fixed point in the forward pass?,variance of1/N,1912
12246,What is the name of the Sigmoid 111 Tanh?,Sigmoid 111 Tanh,5900
12247,What effect does the default gain forSELUsacrifices have for more stable gradient flow in rectangular layers?,the normalisation effect,5900
12248,What is Sigmoid 111 Tanh?,Sigmoid 111 Tanh,5900
12249,What does a tensor fill the input Tensor with?,scalar value0,5900
12250,What is Tanh's number?,111,111
12251,What does usingnonlinearity='selu' give the initial weights?,variance of1/N,4074
12252,What does the default gain forSELUsacrifice for more stable gradient flow in rectangular layers?,the default gain forSELUsacrifices the normalisation effect,4074
12253,What is param?,optional parameter,10397
12254,What is param for the non-linear function?,optional parameter,10397
12255,What value does the input Tensor have to fill the tensor with?,scalar value,10828
12256,What value does a tensor fill the input Tensor with?,scalar value0,10828
12257,What value does an n-dimensionaltorch fill the input Tensor with?,scalar value0,2336
12258,What do examples fill the input Tensor with values drawn from?,the normal distribution,2336
12259,Fills the 2-dimensional inputTensor with what?,identity matrix,2426
12260,What type of torch is a tensor?,2-dimensional,2426
12261,Fills the input Tensor with values drawn from what distribution?,normal distribution,2426
12262,"InLinearlayers, where as many inputs are preserved, what does Fills the 2-dimensional inputTensorwith the identity matrix?",Preserves the identity of the inputs,2426
12263,Preserves the identity of the inputs inLinearlayers where?,as many inputs are preserved as possible,2337
12264,What is the input Tensor filled with values drawn from?,the uniform distribution,2337
12265,What does Fills the 2-dimensional inputTensorwith the identity matrix?,Preserves the identity of the inputs,2419
12266,"Fills the 3, 4, 5-dimensional inputTensorwith what?",Dirac delta function,2419
12267,What is used to fill the inputTensor with values according to the method described inUnderstanding the difficulty of training deep feedforward neural networks,uniform distribution,2419
12268,What is the name of the author of Understanding the difficulty of training deep feedforward neural networks?,X,2419
12269,What dimension is a tensor?,n-dimensional,2419
12270,What is the a-b bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal distribution?,the lower bound of the uniform distribution,8934
12271,"InLinearlayers, where as many inputs are preserved as possible, what does Fills the 2-dimensional inputTensorwith the identity",Preserves the identity of the inputs,2334
12272,What is the name of Glorot's paper Understanding the difficulty of training deep feedforward neural networks?,X,2334
12273,What does Examples Fill the input Tensor with?,scalar value0,2334
12274,Where are as many input channels preserved as possible?,Convolutionallayers,10825
12275,Who is the author of Understanding the difficulty of training deep feedforward neural networks?,Bengio,10825
12276,"In case of groups>1, each group of channels preserves identity <sep>","In case of groups>1, each group of channels preserves identity",10825
12277,"Who were Glorot, X. & Bengio, Y. (2010)?","& Bengio, Y.",10825
12278,"When was Glorot, X. & Bengio, Y. born?",2010,10825
12279,What is another name for Glorot initialization?,Glorot initialization,10825
12280,What is a tensor gain?,an optional scaling factor,10825
12281,"Glorot, X. & Bengio, Y. (2010), using what distribution?",normal distribution,10825
12282,What does tensor fill the 2-dimensional inputTensorwith?,identity matrix,10825
12283,"What function does tensor fill the 3, 4, 5-dimensional inputTensorwith?",Dirac delta function,10825
12284,"What is the resulting tensor's values sampled fromN(0,std2)mathcalN(",Glorot initialization,10825
12285,What is an optional tensor gain?,scaling factor,10825
12286,What is the input Tensor filled with?,scalar value0,10825
12287,What does tensor do?,Fills the input Tensor with the scalar value0,10825
12288,Who wrote Understanding the difficulty of training deep feedforward neural networks?,Bengio,11330
12289,What is an n-dimensionaltorch?,tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor,11330
12290,What is the value to fill the tensor with?,val,11330
12291,What is the resulting tensor known as?,Glorot initialization,10762
12292,What is the standard deviation of the normal distribution?,std,10762
12293,What does Moon-Penrose inverse stand for?,the pseudoinverse,1799
12294,The pseudoinverse is more computationally convenient to understand through what?,SVD,1799
12295,What part of the matrix is used in computations?,lower triangular part of the matrix,1799
12296,"Ifhermitian= True,Ais assumed to be what if complex or symmetric if real?",Hermitian,6129
12297,What is faster and more numerically stable than computing the pseudoinverse explicitly?,usingtorch.linalg.lstsq(),6129
12298,What is a warning about usinglstsq() for multiplying a matrix on the left by the pseudoinverse?,Warning,6129
12299,What does this function synchronize with the CPU?,CUDA inputs,6129
12300,What function synchronizes that device with the CPU?,usestorch.linalg.svd()ifhermitian= False,3614
12301,What does usestorch.linalg.svd() do for CUDA inputs?,synchronizes that device with the CPU,3614
12302,What is the name of the warning that is issued when a matrix is multiplied by the pseudoinverse?,Warning,3614
12303,What is assumed to be Hermitian if complex or symmetric if real?,Ifhermitian= True,3614
12304,The singular values below the specifiedrcondthreshold are treated as what?,zero,3614
12305,What is assumed to be if complex or symmetric if real?,Hermitian,3614
12306,What values that are below the specifiedrcondthreshold are treated as zero and discarded in the computation?,The singular values,3614
12307,What does this function synchronize with the CPU for?,CUDA inputs,3614
12308,What does the function usestorch.linalg.svd() ifhermitian= False?,True,3614
12309,What is a possible way to multiply a matrix on the left by the pseudoinverse?,usingtorch.linalg.lstsq(),7238
12310,The pseudoinverse can be defined algebraically but it is more computationally convenient to understand it through what?,SVD,7238
12311,What does the pseudoinverse support?,batches of matrices,7238
12312,What does the SVD support?,batches of matrices,7238
12313,"What supports input of float, double, cfloat, and cdouble dtypes?",SVD,7238
12314,What is a matrix on the left multiplied by?,the pseudoinverse,7238
12315,How often does a value appear in a given row of the inputtensor?,most often,5325
12316,"If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondimwhere they",1,5325
12317,What is the name of the function that returns whether the output tensor hasdimretained or not?,keepdim,5325
12318,"By default,dimis the what dimension of theinputtensor?",last dimension,5325
12319,Where is this function not defined?,fortorch.cuda.Tensoryet,5325
12320,This function is not defined what?,fortorch.cuda.Tensoryet,5325
12321,What does this class wrap around an arbitraryoptim.Optimizer?,shards,7476
12322,To whom will each rank broadcast its parameters after parameters are updated locally?,all other peers,7476
12323,What does ZeroRedundancyOptimizer use to pack a number of parameters at each rank?,greedy algorithm,7476
12324,How does each parameter belong to a single rank?,not divided among ranks,7476
12325,What is the partition of a zero redundancyoptimizer?,arbitrary,7476
12326,What class does optimizer_class(torch.nn.Optimizer) belong to?,local optimizer,7476
12327,The optimizer instance in each rank is only responsible for what?,updating1/world_sizeparameters,7477
12328,What does this class wrap?,arbitraryoptim.Optimizer,7477
12329,What happens after parameters are updated locally?,each rank will broadcast its parameters to all other peers,7477
12330,What kind of algorithm does ZeroRedundancyOptimizer use?,greedy,7477
12331,What is the partition of each parameter at each rank?,arbitrary,7477
12332,Restore the global parameter groups. to(int) – the rank that receives the global states. (default: 0) <sep>,Restore the global parameter groups,7477
12333,What does state_dict(dict) refer to?,optimizer state,10930
12334,To(int) – the rank that receives the global states. (default: what) Restore the global parameter groups as well as the,0,10930
12335,Should be an object returned from a call from what?,tostate_dict(),10930
12336,What is an object returned from a call tostate_dict() Gets this rank'sstate_dict?,state_dict(dict),5175
12337,What should be an object returned from a call?,tostate_dict(),5175
12338,Restore what as well as the shard?,global parameter groups,5175
12339,What is the globalstate_dict?,last known global optimizer state,5175
12340,What is a closure that reevaluates the model and returns the loss?,closure,5175
12341,What type of loss depends on the underlying optimizer?,Optional,5175
12342,What does optional loss depend on?,underlying optimizer,5175
12343,Inverse short time Fourier Transform is expected to be what?,inverse ofstft(),3862
12344,What is the envelop created by the summation of all the windows at certain point in time?,never zero,3862
12345,What can be trimmed off precisely because they can be calculated?,Left padding,3862
12346,What is expected to be the inverse ofstft()?,Inverse short time Fourier Transform,3862
12347,What should the Inverse short time Fourier Transform return?,least squares estimation,3862
12348,What condition will the algorithm check using?,NOLA condition,3862
12349,What is the envelop created by the summation of all the windows never zero at certain point in time?,0,3684
12350,What is the envelop created by the summation of all windows at certain point in time?,never zero,3684
12351,What can be trimmed off exactly because they can be calculated?,Left padding,3684
12352,"If the signal isn't padded, what can result in a shorter signal than the original signal?",ifcenteris False,3684
12353,"If centeris False, then there will be padding e.g. 'constant','reflect', etc.",IfcenterisTrue,3684
12354,"IfcenterisTrue, then there will be padding e.g.'reflect','reflect', etc.",constant,3684
12355,What is the last window in a window?,last,3684
12356,What are some examples of complex inputs?,"channel,fft_size,n_frame",7132
12357,What does Trueifn_fft! mean in the input size?,fft_size,7132
12358,What is expected to be output ofstft()?,input tensor,7132
12359,Since what version is the input tensor deprecated?,1.8.0,7132
12360,Since what version is input(Tensor) deprecated?,1.8.0,9643
12361,What does trueifn_fft! mean in the input size?,fft_size,9643
12362,Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the,normalized,2116
12363,What is the default signal to trim?,whole,2116
12364,Since what version is real input deprecated?,1.8.0,2116
12365,Whether the STFT was onesided or onesided?,onesided,2116
12366,Whether the STFT was normalized?,normalized,11396
12367,Whether the STFT was onesided or false?,onesided,11396
12368,Whether the output should be complex or if the input should be assumed to derive from a real signal and window?,return_complex,11396
12369,What is window(Optional[torch.Tensor])?,optional window function,11396
12370,What is center(bool)?,Whetherinputwas padded on both sides,11396
12371,What is the amount to trim the signal by?,the original signal length,11396
12372,Is return_complex compatible or incompatible with onesided=True?,incompatible,11396
12373,"What is the value of size (..., signal_length)?",Least squares estimation of the original signal,11396
12374,What is returned by sizeendstartstepleftlceil fractextend - textstart,1-D tensor,5257
12375,What is the default dtype inferred to be?,betorch.int64,5257
12376,Non-integerstepis subject to what when comparing againstend?,floating point rounding errors,5257
12377,"If the data type is not given, infer the data type from the other input arguments.",Ifdtypeis not given,4477
12378,Non-integerstep is subject to what when comparing againstend?,floating point rounding errors,4477
12379,What is non-integerstep subject to when comparing againstend?,floating point rounding errors,4477
12380,What Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked,Python Functions and Modules,2040
12381,What is a wrapper around for atorch.jit.Future[T]asynchronous tasks?,C++torch::jit::Module,2040
12382,In what language is it possible to train models in PyTorch?,Python,2040
12383,What does TorchScript provide?,tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python,2040
12384,What is a script that will be optimized using?,just-in-time compilation,5005
12385,What is a scriptModule that will be optimized using?,just-in-time compilation,5005
12386,Where can a TorchScript program be run independently from Python?,a standalone C++ program,5005
12387,Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix,Python Language Reference Comparison Debugging,5005
12388,What is the introduction to TorchScripttutorial?,introduction to TorchScript,5005
12389,Why is it possible to export a model via TorchScript to a production environment?,Python programs may be disadvantageous for performance and multi-threading reasons,5005
12390,When is it first called during tracing?,Compilesfn,1626
12391,What does tracing create?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,1626
12392,What type of version of a script module can you save for use in a separate process?,offline,1626
12393,What can a scripted function call?,an encoder module generated using tracing,1626
12394,Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tra,Python,7945
12395,TorchScript is a way to create serializable and optimizable models from PyTorch code.,TorchScript,7945
12396,Any TorchScript program can be saved from a Python process and loaded in a process where there is no what?,Python dependency,4245
12397,What will be used to optimize a script?,just-in-time compilation,4245
12398,What version of the module can be saved for use in the TorchScript IR Graph?,offline,4245
12399,"What will clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as",Freezing aScriptModule,4245
12400,In what language can a TorchScript program be run independently from Python?,C++,4245
12401,Python programs may be disadvantageous for what reasons?,performance and multi-threading reasons,2146
12402,What is a Wrapper around C++torch::jit::Module?,represents a single function and does not have any attributes or Parameters,2146
12403,What does JIT do for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Append,Disable JIT,2146
12404,What is used to compile TorchScript code?,TorchScript compiler,5120
12405,When it is first called during tracing?,Compilesfn,5120
12406,What is created by the asynchronous task executingfuncand a reference to the value of the result of this execution?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,5120
12407,Any TorchScript program can be saved from a what process?,Python,5120
12408,Freezing aScriptModulewill clone it and attempt to inline what as constants in the TorchScript IR,"attempt to inline the cloned module’s submodules, parameters, and attributes",5120
12409,What version of the module can be saved for use in a separate process?,offline,5120
12410,What does aScriptModule do?,Load aScriptModuleorScriptFunction,5120
12411,What is a gentle introduction to TorchScript?,theIntroduction to TorchScripttutorial,5120
12412,What is called when it is first called during tracing?,Compilesfn,5120
12413,What is C++torch::jit::Module functionally equivalent to?,aScriptModule,5120
12414,Forces completion of what?,atorch.jit.Future[T]asynchronous task,5120
12415,What do we provide to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python,tools,5120
12416,Trace a module and return an executableScriptModulethat will be optimized using what?,just-in-time compilation,5120
12417,What is used to optimize a script?,just-in-time compilation,1473
12418,"When it is first called during tracing, what does Compilesfn do?",Compilesfn,1473
12419,What is the wrapper around C++torch::jit::Module?,Functionally equivalent to aScriptModule,1473
12420,What is Debugging Disable JIT for Debugging?,Python Language Reference Comparison,1473
12421,What is jit::Module functionally equivalent to?,aScriptModule,1473
12422,What is an executableScriptModule that will be optimized using?,just-in-time compilation,2606
12423,What action does TorchScript perform?,Forces completion of atorch.jit.Future[T]asynchronous task,2606
12424,Freezing aScriptModulewill what?,clone it,2606
12425,What is the Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?,Frequently Asked Questions Known Issues,2606
12426,"What will clone it and attempt to inline the cloned module's submodules, parameters, and attributes as",Freezing aScriptModule,2606
12427,What is a gentle introduction to?,TorchScript,4044
12428,Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create,Known Issues,4044
12429,What is saved an offline version of aScriptModule?,Save an offline version,4044
12430,What does TorchScript do?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,4044
12431,What is the benefit of using TorchScript?,Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency,4939
12432,What Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer?,Python Language Reference Comparison Debugging,4939
12433,What is an example of a TorchScript program that can be run independently from Python?,a standalone C++ program,4941
12434,Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions,Python Functions and Modules,4941
12435,Python Functions and Modules Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently As,Python Language Reference Comparison,5003
12436,"What will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleor",ornn.Module,5003
12437,"A wrapper around what. Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or",C++torch::jit::Module,1163
12438,What can you do to save an offline version of this module?,Save an offline version of this module,1163
12439,What is a way to create serializable and optimizable models from PyTorch code?,Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript,1163
12440,What is TorchScript a way to do?,create serializable and optimizable models from PyTorch code,1163
12441,What does Scripting a function ornn.Module compile as using the TorchScript compiler?,TorchScript code,2498
12442,What is the name of the method that provides for conatiner type refinement in TorchScript?,a pass-through function that returnsvalue,2498
12443,What is an example of converting a PyTorch model to TorchScript and running it in C++?,end-to-end example,2498
12444,What function is called when it is first called during tracing?,Compilesfn,5755
12445,What does asynchronous task executingfuncand a reference to the value of the result of this execution do?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,5755
12446,What is the TorchScript method that indicates to the compiler that the left-hand expression is a class instance attribute with type oftype?,a pass-through function that returnsvalue,5755
12447,What will be used to optimize a script module?,just-in-time compilation,5755
12448,What does atorch.jit.Future[T]asynchronous task do?,Forces completion of atorch.jit.Future[T]asynchronous task,5755
12449,"What will attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the Torch",Freezing aScriptModule,5755
12450,What is the difference between aScriptModule and aScriptModule?,represents a single function and does not have any attributes or Parameters,5755
12451,What does Scripting a function ornn.Module compile as?,TorchScript code,5755
12452,What type of version of a script module can be saved for use in a separate process?,offline,5755
12453,Load aScriptModuleorScriptFunctionpreviously saved what?,withtorch.jit.save,5755
12454,What is a pass-through function that indicates to the TorchScript compiler that the left-hand side expression is a class instance attribute with type,returnsthe_value,2590
12455,What is the difference between aScriptModule and C++torch::jit::Module?,represents a single function and does not have any attributes or Parameters,2590
12456,What can a traced function do?,can call an encoder module generated using tracing,2590
12457,What does Torch.jit.Future[T]asynchronous task do?,Forces completion of atorch.jit.Future[T]asynchronous task,2590
12458,What version of aScriptModule can you save for use in a separate process?,offline,2590
12459,What is used when it is first called during tracing?,Compilesfn,7970
12460,What is the difference between C++torch::jit::Module and aScriptModule?,represents a single function and does not have any attributes or Parameters,7970
12461,What does Trace a module and return an executableScriptModule that will be optimized using just-in-time compilation?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,7974
12462,What type of version of a module can you save for use in a separate process?,offline,7974
12463,The beam search of a sequence to sequence model will typically be written in script but can call what?,an encoder module generated using tracing,7974
12464,When are traced functions particularly useful?,when you need to use control-flow around a simple feed-forward model,2035
12465,What is an example of a scripted function that can call an encoder module generated using tracing?,a traced function in script,2035
12466,What does C++torch::jit::Module have in common with aScriptModule?,represents a single function and does not have any attributes or Parameters,2035
12467,Scripted functions can call what?,traced functions,2035
12468,What is used to generate an encoder module?,tracing,2035
12469,What does executingfuncand a reference to the value of the result of this execution do?,Creates an asynchronous task,2035
12470,Tracing and scripting can be composed to what?,suit the particular requirements of a part of a model,889
12471,What is a pass-through function that indicates to the TorchScript compiler the type of the_value?,returnsthe_value,889
12472,What can a traced function call?,can call an encoder module generated using tracing,889
12473,What is a wrapper around?,C++torch::jit::Module,889
12474,What is a wrapper around C++torch functionally equivalent to?,aScriptModule,889
12475,What type of version of aScriptModule can you save for use in a separate process?,offline,889
12476,Where was aScriptModuleorScriptFunction previously saved?,withtorch.jit.save,889
12477,"What will attempt to inline the cloned module's submodules, parameters, and attributes as constants in the Torch",Freezing aScriptModule,889
12478,What does this function provide in TorchScript?,conatiner type refinement,889
12479,What is an easier approach for converting a model to TorchScript?,tracing or scripting,889
12480,What does returnsthe_value hint TorchScript compiler?,the type ofthe_value,889
12481,What inside of a script function called by a traced function is preserved correctly?,Control-flow,889
12482,What is an example of a script function called by a traced function?,a traced function,889
12483,What can an encoder module be generated using?,tracing,3738
12484,What can be composed to suit the particular requirements of a part of a model?,Tracing and scripting,3738
12485,What can scripted functions call?,traced functions,3738
12486,What can tracing and scripting be composed to?,suit the particular requirements of a part of a model,3738
12487,What can traced functions call?,script functions,3738
12488,What is preserved correctly inside of a script function called by a traced function?,Control-flow,3738
12489,Traced functions can call what?,script functions,3738
12490,When are script functions useful?,when a small part of a model requires some control-flow,3738
12491,What can be called in a traced function?,script function,3738
12492,What can be used to generate a submodule using tracing?,traced module,3738
12493,What can call traced functions?,Scripted functions,3738
12494,What is a traced function particularly useful for?,when you need to use control-flow around a simple feed-forward model,3738
12495,What can be used to generate a submodule using tracing that can be called from the methods of a script module?,fornn.Modules,3738
12496,How can a submodule be called from the methods of a script module?,using a traced module,3738
12497,Debugging withpdbworks except for what?,when we invoke the@torch.jit.scriptfunction,2076
12498,How can we disable JIT?,globally disable JIT,2076
12499,What does TorchScript provide a code pretty-printer for allScriptModuleinstances?,Python syntax,2076
12500,What will you need to access.codeon if theScriptModulehas more than one method?,the module,2076
12501,What is the name of the script that produces the graph?,test.py,2076
12502,What is the name of the above script?,disable_jit_example.py,2076
12503,What is the output produced by the example above?,TorchScript’s compilation of the code for theforwardmethod,8865
12504,We will be able to step into the@torch.jit.scriptfunction as what?,normal Python function,8865
12505,What does the code pretty-printer give an interpretation of the script method’s code as valid?,Python syntax,8865
12506,What document describes the rules for forwardmethod lookup?,theInspecting Codesection,8865
12507,What is the schema for?,built-in functions likeaten,8865
12508,What specifies which values in scope should be passed as inputs?,aten::zerosis the operator (equivalent totorch.zeros) and the input list,8865
12509,"To disable what for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-print",TorchScript compiler,8865
12510,"TorchScript has a representation at a lower level than the code pretty- printer, in the form of what?",IR graphs,7950
12511,What is TorchScript's static single assignment?,SSA,7950
12512,TorchScript follows the same rules described in theInspecting Codesection with regard to what?,forwardmethod lookup,7950
12513,Tracing of in-place operations of tensor views (e.g. what?,indexing,7950
12514,What is a static single assignment?,SSA,7592
12515,What can you use TorchScript’s compilation of the code for theforwardmethod?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7592
12516,What is dependent on the underlying code?,Tracing of control flow,7592
12517,What is TorchScript's compilation of?,the code for theforwardmethod,7592
12518,What is the name of the C++ backend of PyTorch?,ATen,7948
12519,What section describes the rules for forwardmethod lookup?,theInspecting Codesection,7948
12520,What is TorchScript's representation at a lower level than the code pretty- printer?,IR graphs,7948
12521,What is another name for static single assignment?,SSA,7948
12522,What is the name of the instruction that produces the graph?,%rv.1:Tensor,7948
12523,Where can the schema for built-in functions likeaten::zeros be found?,atBuiltin Functions,7948
12524,What can be found atBuiltin Functions?,built-in functions likeaten::zeros,7948
12525,What is the location in the original source file that generated this instruction?,#test.py:9:10,7948
12526,Where is the file named test.py located?,line 9,7948
12527,Operators can also have what?,associatedblocks,7948
12528,Why are operators formatted to reflect their equivalent source code forms?,to facilitate easy debugging,7948
12529,What does TorchScript assign the output to?,%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1,7948
12530,What is the operator (equivalent totorch.zeros)?,aten::zeros,7948
12531,The graph follows the same rules described in what section?,Inspecting Codesection,7958
12532,What language does TorchScript provide a code pretty-printer for allScriptModuleinstances?,Python syntax,7958
12533,What produces this output?,The example above,7958
12534,What does the example script above produce?,the graph,7958
12535,What is the name of the file that generated the instruction?,test.py,7958
12536,What can you use TorchScript's compilation of the code for theforwardmethod?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7958
12537,"If theScriptModulehas more than one method, you will need to what?",access.codeon the method itself and not the module,906
12538,What is the output of the example above?,TorchScript’s compilation of the code for theforwardmethod,906
12539,What can you use this output for?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,906
12540,Where are these associatedblocks found?,In the graph print-out,906
12541,Why are operators formatted in the graph print-out?,to reflect their equivalent source code forms to facilitate easy debugging,6182
12542,Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations,edge cases,6182
12543,What does this message indicate to us that the computation differed between when we first traced it and when we traced it with traced?,diagnostic information,6182
12544,What do we assign the output to a (unique) value namedrv.1?,%rv.1:Tensormeans,6182
12545,What is an example of a control flow dependent on inputs?,tensor shapes,6182
12546,What does check_inputson do?,a list of tuples of inputs that will be used to re-trace the computation and verify the results,6182
12547,"What is the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,",test.py:9:10,6182
12548,What are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code?,Tracing of control flow that is dependent on inputs,6182
12549,What does TorchScript use to assign the output to a (unique) value namedrv.1?,%rv.1:Tensormeans,7964
12550,What are some cases where the trace of a given Python function/module will not be representative of the underlying code?,edge cases,7964
12551,What does SSA stand for?,static single assignment,7964
12552,The graph follows the same rules described in what section with regard toforwardmethod lookup?,theInspecting Codesection,7964
12553,"Tracing of in-place operations of tensor views (e.g., what on the left-hand side of an assignment)",indexing,58
12554,Where is the location in the original source file that generated this instruction?,"on line 9, and at character 10",58
12555,What does this message indicate to us that the computation differed between when we first traced it and when we traced it with thecheck_in,diagnostic information,58
12556,Tracing of control flow that is dependent on inputs (e.g. what?,tensor shapes,58
12557,What is a list of tuples of inputs that will be used to re-trace the computation and verify the results?,check_inputson,58
12558,What does %rv.1:Tensormeans we assign the output to?,%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1,58
12559,What does the tracer produce?,The tracer produces warnings for several problematic patterns in traced computation,7404
12560,Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment),indexing,7404
12561,What is one way to automatically catch many errors in traces?,check_inputson,7404
12562,What does check_inputson thetorch.jit.trace()API.check_inputstakes contain?,a list of tuples of inputs,7404
12563,What can be done to fix the warnings?,build up the result tensor out-of-place withtorch,7404
12564,What does the loop within the body ofloop_in_traced_fndepend on?,the shape of the inputx,7404
12565,Which version of TorchScript does this section detail the changes to TorchScript in?,PyTorch 1.2,7798
12566,If you are new to TorchScript you can do what?,skip this section,7798
12567,What does Torch.jit.script now do?,"attempt to recursively compile functions, methods, and classes",7798
12568,"What is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule?",2.torch.jit.script(nn_module_instance),7798
12569,Methods called fromforwardare what in the order they are used inforward?,lazily compiled,7798
12570,"To stop the compiler from what, add@torch.jit.ignoreor@torch.jit.unused. @ignor",compiling a method,7798
12571,@ignoredcannot be exported;@unusedcan?,@ignoredcannot be exported;@unusedcan,7798
12572,How many main changes to the TorchScript API with PyTorch 1.2?,two,7798
12573,"When you calltorch.jit.script, compilation is what?","“opt-out”, rather than “opt-in”",7798
12574,"What is a simpler, easier-to-use API for?",converting yournn.Modules intoScriptModules,7798
12575,"To compile a method other thanforwardthat is not called fromforward, what is done?",add@torch.jit.export,138
12576,What does @ignoreleaves the method as a call to python?,@ignoreleaves the method as a call to python,138
12577,What is now the preferred way to createScriptModules?,2.torch.jit.script(nn_module_instance),138
12578,Methods called fromforwardare what?,lazily compiled,138
12579,Add@torch.jit.ignoreor@torch.jit.unused. @ignoreleaves the method as a,compiling a method,138
12580,"For empty container types, what should empty container types do?",annotate their types usingPEP 526-styleclass annotations,138
12581,What do these changes combine to provide for converting yournn.Modules intoScriptModules?,"a simpler, easier-to-use API",138
12582,What module is compiled by default?,module’sforward,138
12583,Annotate their types usingPEP 526-styleclass annotations.,empty container types,4268
12584,What is used as an entry point into aScriptModuleand should be compiled?,annn.Module,4268
12585,What is an example of a function that does not need a decorator?,@torch.jit.exporton a method,4268
12586,What can be inferred?,attribute types,4268
12587,What can be marked with aFinalclass annotation instead of adding the name of the member to__constants__?,Constants,4268
12588,Before PyTorch 1.2 what decorator was used to make a function or method callable from code that is exported?,@ignore,8188
12589,What is compiled in the order they are used inforward?,@torch.jit.exportmethods,8188
12590,What is used as an entry point into aScriptModule and should be compiled?,annn.Module,8188
12591,Warning TorchScript class support is experimental. Currently it is best suited for what?,simple record-like types,8188
12592,Warning The@torch.jit.ignoreannotation’s behavior changes in what?,PyTorch 1.2,8188
12593,See@torch.jit.ignoreand@torch.jit.unused for what?,details,8188
12594,Warning TorchScript class support is what?,experimental,8188
12595,If a type cannot be inferred and is what?,not explicitly annotated,8188
12596,Who compiles the module?,TorchScript compiler,8188
12597,"What is assumed to be an entry point, so it does not need this decorator?",forwardimplicitly,8188
12598,Functions can be decorated with what?,@torch.jit.ignoreortorch.jit.unusedif needed,8188
12599,Everything in a user definedTorchScript Classis exported what?,by default,8188
12600,What did PyTorch 1.2 use to get the @ignore decorator back?,use@torch.jit.unused(),7380
12601,TorchScript class support is what?,experimental,7380
12602,Everything in a user definedTorchScript Classis exported what way?,by default,7380
12603,When does the @torch.jit.ignoreannotation’s behavior change?,PyTorch 1.2,7380
12604,"Functions don’t change much, they can be decorated with what?",@torch.jit.ignoreortorch.jit.unusedif needed,7380
12605,What decorator was used to make a function or method callable from code that is exported?,@ignore,7380
12606,What does @torch.jit.ignore and@torch.jit.unused provide?,details,7380
12607,What cannot have their types inferred and must have their types annotated withPEP 526-styleclass?,Empty lists and dicts,7380
12608,"To stop the compiler from compiling a method, add what?",@torch.jit.ignoreor@torch.jit.unused,7922
12609,What eleaves the method as a call to python?,@ignor,7922
12610,What can empty container types do?,annotate their types,7922
12611,What compiler compiles the module?,TorchScript,7922
12612,See@torch.jit.ignoreand@torch.jit.unusedfor what?,details,7374
12613,"To get this functionality back, use what?",@torch.jit.unused(),7374
12614,What is TorchScript class support best suited for?,simple record-like types,7374
12615,What does the TorchScript compiler need to know the types ofmodule attributes?,Most types,7374
12616,"To get this functionality back, what did PyTorch 1.2 use to get it back?",use@torch.jit.unused(),5001
12617,What is an example of a Python 3 type hints?,@torch.jit.exporton a method,5001
12618,What cannot have their types inferred from the value of the member?,Empty lists and dicts,7443
12619,What is used to get the @ignore functionality back?,@torch.jit.unused(),7378
12620,What is now equivalent to @torch.jit.ignore(drop=False)?,@torch.jit.ignoreis,7378
12621,Which module's data is copied to aScriptModule when passed to thetorch.jit.scriptfunction?,atorch.nn.Module,7378
12622,What is an example of a method that does not need the @ignore decorator?,@torch.jit.exporton a method,8186
12623,What is compiled by default?,module’sforward,8186
12624,"Before PyTorch 1.2, what decorator was used to make a function or method callable from code that is exported?",@ignore,8186
12625,What is used to get the @ignore decorator back?,@torch.jit.unused(),8186
12626,What is the name of the decorator used to make a function or method callable from code that is exported?,@torch.jit.ignore,8186
12627,What is used as an entry point into aScriptModule?,annn.Module,8186
12628,How are methods called fromforward lazily compiled?,in the order they are used inforward,8186
12629,In what version of Torch does the @torch.jit.ignoreannotation's behavior change?,PyTorch 1.2,8186
12630,How are functions and methods called fromforward compiled?,as they are seen by the compiler,8186
12631,What specifies the subscripts for each dimension of the inputoperandsin the same order as the dimensions?,Theequationstring,7385
12632,How often must the output subscripts appear for some input operand?,at least once,7385
12633,How can the output subscripts be explicitly defined?,by adding an arrow (‘->’) at the end of the equation,7385
12634,The following equation computes the transpose of what?,matrix multiplication,7385
12635,The dimensions labeled with the same subscript must be what?,broadcastable,7385
12636,What is the exception?,if a subscript is repeated for the same input operand,7385
12637,What specifies the subscripts for each dimension of the inputoperands in the same order as the dimensions?,Theequationstring,7385
12638,"What will be part of the output, sorted in increasing alphabetical order?",The subscripts that appear exactly once in theequation,7385
12639,What is the exception to the Einstein summation convention?,if a subscript is repeated for the same input operand,6089
12640,Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on what?,Einstein,2248
12641,What is the summation subscript in Einsum astorch.einsum?,j,2248
12642,What is the exception to Einsum's broadcastable format?,if a subscript is repeated for the same input operand,2248
12643,What allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summ,Einsum,2248
12644,What is the general idea of Einsum?,label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output,2248
12645,How is the output computed?,by summing the product of the elements of theoperandsalong the dimensions whose subscripts are not part of the output,2248
12646,"What can be computed using einsum astorch.einsum(“ij,jk->ik”, A, B)?",matrix multiplication,2248
12647,What is the summation subscript?,j,2248
12648,What does the PyTorchaudiopackage consist of?,Package Reference PyTorch Libraries,7710
12649,"Thetorchaudiopackage consists of I/O, common audio transformations, and what?",popular datasets,7710
12650,"What consists of I/O, popular datasets and common audio transformations?",Thetorchaudiopackage,7710
12651,What is nonlinearity?,non-linear function,10259
12652,"What is Linear / Identity 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac",nonlinearity gain,10259
12653,What is the default gain forSELUsacrifices?,the normalisation effect,10259
12654,What is the name of the non-linear function?,nonlinearity,8169
12655,What does this give the initial weights?,variance of1/N,8169
12656,What does the default gain forSELUsacrifice?,normalisation effect,8169
12657,What is the value to fill the tensor with Examples Fills the input Tensor with?,scalar value1,10829
12658,What value does tensor Examples Fill the input Tensor with?,scalar value0,10829
12659,Who was the author of Understanding the difficulty of training deep feedforward neural networks?,Y.,10829
12660,How does Glorot describe the difficulty of training deep feedforward neural networks?,using a uniform distribution,10829
12661,What is another name for 111 Sigmoid 111 Tanh?,111 Sigmoid 111 Tanh,109
12662,What is the non-linear function (nn.functionalname) param– optional parameter for the non-linear function Examples Fills the,nonlinearity,4075
12663,What should you do in order to implementSelf-Normalizing Neural Networks?,usenonlinearity='linear'instead ofnonlinearity='selu',4075
12664,What does an n-dimensionaltorch.Tensor Examples Fill?,2-dimensional inputTensorwith the identity matrix,4075
12665,What Preserves the identity of the inputs inLinearlayers?,"Preserves the identity of the inputs inLinearlayers,",4075
12666,What does a Tensor Example Fill the input Tensor with?,scalar value0,112
12667,What do Tensor Examples Fill?,2-dimensional input,112
12668,What is 111 Tanh?,111 Tanh,112
12669,What value does an n-dimensionaltorch.Tensor Examples Fill the input Tensor with?,scalar value0,10831
12670,What does fills the 2-dimensional inputTensorwith?,identity matrix,10831
12671,"What function does the 3, 4, 5-dimensional inputTensor have?",Dirac delta function,10831
12672,"In what year did Glorot, X. & Bengio, Y. begin training deep feedforward neural networks?",2010,10831
12673,What is another name for Glorot?,Glorot initialization,10831
12674,What is a Tensor gain?,an optional scaling factor,10831
12675,What is the result of the resulting tensor?,The resulting tensor,10831
12676,What does std stand for?,the standard deviation of the normal distribution,10831
12677,What value does an n-dimensionaltorch.Tensor Example Fills the input Tensor with?,scalar value1,10260
12678,"What does the 3, 4, 5-dimensional inputTensorwith?",Dirac delta function,10260
12679,Preserves the identity of the inputs in what?,Convolutionallayers,10260
12680,"In case of what, each group of channels preserves identity tensor–a 3, 4, 5-dimensionaltorch.T",groups>1,10260
12681,What is the non-linear function?,nonlinearity,10260
12682,What is the 2-dimensional inputTensor filled with?,identity matrix,10398
12683,What does param stand for?,optional parameter,10398
12684,"What function does the tensor fill the 3, 4, 5-dimensional inputTensorwith?",Dirac delta function,10398
12685,"Glorot, what is Glorot?",X,10398
12686,"Who is Glorot, X. & Ben?",& Ben,10398
12687,What is the optional parameter for the non-linear function?,param,10398
12688,What is the mean of the normal distribution std?,standard deviation of the normal distribution,10398
12689,"Preserves the identity of the inputs inLinearlayers, where where as many inputs are preserved as possible?",as many inputs are preserved as possible,2423
12690,"Fills the 3, 4, 5-dimensional inputTensorwith what function?",Dirac delta function,2423
12691,What is an optional scaling factor in a tensor gain?,an optional scaling factor,2423
12692,Fills the 2-dimensional inputTensorwith what?,identity matrix,2423
12693,Fills the input Tensor with what value?,scalar value1,2423
12694,a– what is the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal distribution?,the lower bound of the uniform distribution,8935
12695,What value does an n-dimensionaltorch.Tensor Examples Fills the input Tensor with?,scalar value0,8935
12696,How does Glorot describe training deep feedforward neural networks?,using a uniform distribution,8999
12697,"What function does a 2-dimensionaltorch.Tensor Examples Fills the 3, 4, 5-dimensional inputTens",Dirac delta function,8999
12698,"What is it called when a tensor has values sampled fromU(a,a)mathcalU(",Glorot initialization,8999
12699,What is batch1(Tensor)?,first batch of matrices to be multiplied batch2(Tensor),4764
12700,What is an example of a batch matrix-matrix product?,Example,4764
12701,What happens in a reduced add step?,all matrix multiplications get accumulated along the first dimension,4764
12702,Each parameter belongs to what?,a single rank,8692
12703,Params(Iterable) – anIterableoftorch.Tensors optimizer_class(tor,local optimizer,8692
12704,"When disabled, each what will be communicated separately?",individual parameter,8692
12705,What uses a greedy algorithm to pack a number of parameters at each rank?,ZeroRedundancyOptimizer,8692
12706,What is the partition of the algorithm?,arbitrary,8692
12707,What is the default behavior of ZeroRedundancyOptimizer?,all trailing arguments will be forwarded to the given optimizer,8692
12708,state_dict(dict) – what state should be an object returned from a call tostate_dict() Gets this rank’sstate,optimizer state,8692
12709,What should state_dict(dict) be?,an object returned from a call tostate_dict() Gets this rank’sstate_dict,8692
12710,What is a list ofparam_groups?,a list ofparam_groups,8692
12711,What corresponds to the param_groups for a rank?,Element 0,8692
12712,"What group(ProcessGroup, optional) –torch.distributedProcessGroup?","group(ProcessGroup, optional) –torch.distributedProcessGroup",10323
12713,"When enabled, parameters will be packed into what?",larger buckets,10323
12714,How many update the consolidated state_dict list?,one per rank,10323
12715,When can adding a param group to theOptimizersparam_groups be useful?,when fine tuning a pre-trained network,10323
12716,How many consolidated state_dict lists are updated per rank?,one per rank,10323
12717,To(int) – the rank that receives the global states. (default: what?,0,10323
12718,"When disabled, what happens to each individual parameter?",each individual parameter will be communicated separately,10323
12719,State_dict(dict) – optimizer state. Should be what?,an object returned from a call tostate_dict() Gets this rank’sstate_dict,10323
12720,Returns what for a given rank?,local_state_dict,10323
12721,What is the class of the local optimizer?,optimizer_class,10323
12722,What should be optimized along with group specific optimization options?,Tensors,10323
12723,"What corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for a",Element 0,10323
12724,globalstate_dict is what?,last known global optimizer state,10323
12725,What performs performs?,Performs,10323
12726,What does optimizer_class(torch.nn.Optimizer) contain?,the class of the local optimizer,10323
12727,What will be forwarded to the given optimizer?,all trailing arguments,10323
12728,What can be useful when fine tuning a pre-trained network?,Add a param group to theOptimizersparam_groups,10323
12729,What can be made trainable and added to theOptimizeras training progresses?,frozen layers,10323
12730,What is to(int)?,the rank that receives the global states,10323
12731,What is the default value for the rank that receives the global states?,0,10323
12732,What is the rank that receives the global states?,to(int),10323
12733,How many entries does the state of the optimizer as adict contain?,two,10323
12734,What is a dict containing all parameter groups Partitions parameters across distributed data parallel ranks?,param_groups,10323
12735,How many entries does the state of the optimizer contain?,two,10323
12736,What is the name of the optimizer state?,state_dict(dict),10323
12737,What is a list of dicts?,a list ofparam_groups,10323
12738,Element 0 corresponds to what rank?,"rank 0,",10323
12739,What function returns the local_state_dict for a given rank?,insidestep(),10323
12740,What is the last known global optimizer state?,globalstate_dict,10323
12741,When can a param group be useful?,when fine tuning a pre-trained network,10392
12742,What is params(Iterable) – anIterableoftorch.Tensors?,optimizer,10392
12743,"Update the consolidated state_dict list, how many per rank?",one per rank,10392
12744,Restore the global parameter groups as well as the what?,shard,10392
12745,state_dict(dict) – what?,optimizer state,10392
12746,"Which element corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for",Element 0,10392
12747,What Specifies what Tensors should be optimized along with group specific optimization options?,param_group(dict),10392
12748,What is the name of the index that returns a new tensor which indexes theinputtensor along dimensiondimusing,aLongTensor,2007
12749,"What function returns a tensor with the same data and number of elements asinput, but with the specified shape?",Alias oftorch.vstack(),2007
12750,What is the boolean maskmask that returns a new 1-D tensor which indexes theinputtensor,aBoolTensor,6063
12751,What is the name of the index which indexes theinputtensor along dimensiondimusing the entries inindex?,aLongTensor,6063
12752,What is returned with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive,a tensor,2213
12753,What parameter is given by the corresponding element ininputi?,rate parameter given by the corresponding element ininputi,2213
12754,Sets the seed for generating random numbers. Returns the random number generator state as atorch.ByteTensor.,random number generator state,5866
12755,Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean what?,0 and variance 1,5451
12756,What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive),a tensor,5451
12757,What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?,a tensor,5577
12758,Returns the indices of what value of all elements in theinputtensor?,the maximum value,5554
12759,Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. Returns the mean,p-norm of (input-other) Returns the log of summed exponentials,5554
12760,Computes what quantiles of each row of theinputtensor along the dimensiondim?,q-th,5554
12761,What is the result of Computes the q-th quantiles of each row of theinputtensor along the dimensiondim,variant oftorch.quantile()that “ignores”NaNvalues,5554
12762,Computes the error function ofinput. The error function is defined as follows: what is the input tensor?,input(Tensor) – the input tensor,7763
12763,Computes the error function ofinput. The error function is defined as follows: input(Tensor) – the input ten,output tensor,7763
12764,Computes the complementary error function ofinput. The complementary error function is defined as follows: what is the input tensor?,input(Tensor) – the input tensor,7763
12765,What is an example of a Computes the exponential of the elements minus 1 ofinput?,Computes the exponential of the elements minus 1 ofinput,7763
12766,What is the name of the function that Computes the error function ofinput?,Computes the complementary error function ofinput,1760
12767,What is the name of the function that Computes the inverse error function ofinput?,Computes the inverse error function ofinput,1760
12768,Computes the elements minus 1 ofinput. Note This function provides greater precision than exp(x) - 1 for small values of,exponential,1760
12769,What is an example of a function that computes the error function ofinput?,Computes the complementary error function ofinput,10350
12770,Computes the first kind of what function for each element ofinput?,exponentially scaled zeroth order modified Bessel function,9669
12771,Computes the first kind for each element ofinput. input(Tensor) – the input tensor. out(,exponentially scaled zeroth order modified Bessel function,9673
12772,What is an example of a function that computes the inverse error function ofinput?,Computes the inverse error function ofinput,9673
12773,What is the complementary error function of input do?,Computes the complementary error function ofinput,9673
12774,What is another name for the median(dim=0) input?,input tensor,8196
12775,What contains the median of each row of input in the dimension dim?,values,5312
12776,What function computes the mean of both medians in input?,torch.quantile(),5312
12777,"By default, what is the last dimension of the input tensor?",dim,1488
12778,What is the last dimension of the input tensor?,dim,1488
12779,What is another name for median(dim=0) input?,input tensor,7551
12780,"If keepdim is True, output tensors are of the same size as input except in the dimension dim where they are of size 1,",keepdim is True,5311
12781,What does torch.squeeze() do to result in the outputs tensor having 1 fewer dimension than input?,Note,5311
12782,"If keepdim is True, the outputs tensor has how many dimensions less than input?",1 fewer dimension,2320
12783,"By default, dim is what dimension of the input tensor?",last dimension,2321
12784,"If keepdim is True, output tensors are of what size?",1 fewer dimension,9621
12785,"If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size",1,3439
12786,"If keepdim is True, the output tensors are of what size?",the same size as input,3439
12787,What is not unique for input tensors with an even number of elements in the dimension dim?,The median is not unique,3439
12788,What is returned when the median is not unique for input tensors with an even number of elements in the dimension dim?,lower of the two medians,3439
12789,What does torch.quantile() use to compute the mean of both medians in input?,q=0.5,3439
12790,What does not necessarily contain the first occurrence of each median value found?,Warning indices,3439
12791,The exact implementation details are what?,device-specific,3439
12792,Do not expect the same result when run on what two devices?,CPU and GPU,3439
12793,What does keepdim do when the output tensors are of the same size as input?,Note,1487
12794,What does keepdim do if keepdim is True?,Note,1487
12795,"If keepdim is True, what happens to the output tensors if dim is the last dimension of the input tensor",squeezed,1487
12796,"When dim is squeezed, the outputs tensor has how many dimensions less than input?",1 fewer dimension,7552
12797,"If keepdim is True, output tensors have how many dimensions less than input?",1 fewer dimension,3438
12798,What is the warning that the output tensors are not unique for input tensors with an even number of elements in the dimension,Warning,3438
12799,"If keepdim is True, output tensors have how much less dimension than input?",1,3438
12800,Which of the two medians is returned if keepdim is True?,lower,3438
12801,What is the name of the warning that the output tensors are not unique for input tensors with an even number of elements,Warning,3438
12802,What does it mean that the median is not unique for input tensors with an even number of elements in the dimension dim?,Note,4405
12803,The median is not unique for input tensors with an what?,even number of elements,4405
12804,Which of the two medians is returned in the case of input tensors with an even number of elements in dimension dim?,lower,7172
12805,What is the warning that the median is not unique for input tensors with even number of elements in dimension dim?,Warning,7172
12806,The median is not unique for what with an even number of elements in the dimension dim?,input tensors,7172
12807,What is used to compute the mean of both medians in input?,torch.quantile(),4406
12808,Which of the two medians is returned?,lower,4406
12809,Do not expect the same result when run on what two devices in general?,CPU and GPU,8210
12810,What are the two main CPUs used in Warning indices?,CPU and GPU,8210
12811,What do not expect to be deterministic?,gradients,9611
12812,Do not expect what to be deterministic?,gradients,9611
12813,indices does not contain the first occurrence of each median value found unless it is what?,unique,9611
12814,On what devices do not expect the same result?,CPU and GPU,9611
12815,"The first tensor will be populated with the median values and the second tensor, with their indices in the dimension dim",out,9624
12816,What is an example of a tensor that must have indices in the dimension dim of input?,Example,9624
12817,What type of data format does ONNX use?,external data format,4671
12818,What is the Operator Export Type?,ONNX,4671
12819,What is an example of an Operator Export Type?,Frequently Asked Questions,2309
12820,What is used to handle Named Arguments as model inputs?,dictionaries,8110
12821,Using dictionaries to handle what as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support,Named Arguments,8110
12822,What field does PyTorch avoid using?,.data field,7982
12823,What do dictionaries handle as model inputs?,Named Arguments,8109
12824,What type of annotations are used to write PyTorch model in Torch way?,Type Annotations,8032
12825,How do you write a PyTorch model?,Write PyTorch model in Torch way,8442
12826,What can be used to handle Named Arguments as model inputs?,dictionaries,8108
12827,What is the external data format?,Training Functions,3843
12828,What does the script save the traced model to?,alexnet.onnx,2781
12829,What keyword causes the exporter to print out a human-readable representation of the network?,verbose=True,2781
12830,What will you need to install to run the exported model?,ONNX Runtime,2781
12831,What model can be exported to ONNX?,SuperResolution model,2781
12832,What is ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTH,Operator Export Type,5871
12833,How does the script export a pretrained AlexNet into ONNX?,runs a single round of inference and then saves the resulting traced model to alexnet.onnx,5871
12834,How does the script save the resulting traced model to alexnet?,runs a single round of inference,952
12835,What type of operators does ONNX support?,Operator Export Type,952
12836,What is the support for Limitations?,TorchVision,7967
12837,What type of operator does ONNX support?,Operator Export Type,6101
12838,What does the script run?,single round of inference,2608
12839,What is a good place to start when using external data format Training Functions?,Frequently Asked Questions,2608
12840,How does the script export a pretrained AlexNet?,runs a single round of inference and then saves the resulting traced model to alexnet.onnx,4109
12841,What does the script export into ONNX?,a pretrained AlexNet,2780
12842,What do you need to install to run the exported script?,caffe2,2780
12843,What can you use for ONNX Runtime?,the backend,2780
12844,The script exports a pretrained AlexNet as defined in what?,torchvision,5081
12845,What is the Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX,Custom operators,2059
12846,What is the name of Custom operators?,Operator Export Type,2058
12847,What does the script run and then saves the resulting traced model to alexnet?,single round of inference,4573
12848,What does _FALLTHROUGH stand for?,ONNX,4573
12849,What is ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH,ONNX,4567
12850,What does _ATEN stand for?,ONNX,4569
12851,What does _ATEN_FALLBACK RAW ONNX_FALLTHROUGH stand for?,ONNX,4571
12852,What is the name of the RAW script that exports a pretrained AlexNet as defined in torchvision into?,ONNX,5080
12853,The script exports a pretrained AlexNet as defined in torchvision into what?,ONNX,2656
12854,What is the name of the external data format?,Training Functions,8081
12855,What does the script run and then saves the resulting traced model to alexnet.onnx?,single round of inference,8081
12856,What does the script export a pretrained AlexNet into?,ONNX,2779
12857,Where does the script save the traced model to?,alexnet.onnx,2779
12858,What is a script that exports a pretrained AlexNet as defined in torchvision into ONNX?,Training Functions,7993
12859,Functions Here exports a pretrained AlexNet as defined in torchvision into what?,ONNX,2655
12860,What is the binary protobuf file that contains both the network structure and parameters of the model you exported?,alexnet.onnx,8082
12861,What can you install ONNX with?,conda,4570
12862,What is the backend for ONNX?,Caffe2,4570
12863,What is the name of the binary protobuf file that contains both the network structure and parameters of the model you exported?,alexnet.onnx,7256
12864,What types of exports can the ONNX exporter be?,trace-based and script-based exporter,7256
12865,What is the name of the program that installs ONNX?,conda,7255
12866,What can you verify using the ONNX library?,the protobuf,8565
12867,How can you install ONNX?,conda,8565
12868,What library can you use to verify the protobuf?,ONNX library,8566
12869,"Once these are installed, you can use the backend for what?",ONNX Runtime,7992
12870,What does the exported script need to run with?,caffe2,7390
12871,What can you use for Caffe2?,the backend,7915
12872,What model can you export to ONNX?,SuperResolution model,8561
12873,What does TorchScript allow?,mixing tracing and scripting,8561
12874,Where can we verify the dynamic control flow?,backends with different loop range,8561
12875,What can you use the backend for once these are installed?,ONNX Runtime,4644
12876,What types of exporter can the ONNX exporter be?,trace-based and script-based exporter,4644
12877,What means that the model you are trying to export is a ScriptModule?,script-based,4644
12878,What is the core data structure in TorchScript?,ScriptModule,4644
12879,What do we allow?,mixing tracing and scripting,4644
12880,What can we verify in backends with?,different loop range,4644
12881,What is needed to run the exported script?,caffe2,7914
12882,What can you use the backend for once caffe2 is installed?,ONNX Runtime,7916
12883,"In the future, there will be what for other frameworks as well?",backends,7916
12884,What is the backend for ONNX Runtime?,ONNX Runtime,7916
12885,What type of exporter can we use to capture the dynamic loop?,script,7916
12886,What backend can you use once these are installed?,Caffe2,4641
12887,What will there be in the future for other frameworks?,backends,4643
12888,What can you use the backend for?,ONNX Runtime,4643
12889,Can the ONNX exporter be both trace-based and script-based?,trace-based and script-based exporter,4640
12890,How do you install ONNX Runtime?,follow these instructions,8560
12891,What is another tutorial of?,exporting the SuperResolution model to ONNX,2786
12892,What is the name of the exporter that can be trace-based and script-based?,ONNX,6930
12893,What language is TorchScript a subset of?,Python,10651
12894,What is an example of an ONNX graph that unrolls the for loop?,trace-based exporter,10651
12895,What do we allow mixing in TorchScript?,tracing and scripting,10651
12896,What can you do to suit the specific requirements of a part of a model?,compose tracing and scripting,10651
12897,What unrolls the for loop with trace-based exporter?,ONNX graph,10651
12898,What can you compose to suit the particular requirements of a part of a model?,tracing and scripting,8232
12899,What happens to the exported ONNX graph?,the exported ONNX graph becomes:,8232
12900,What two types of exports can be combined?,tracing and scripting,8232
12901,What can you compose tracing and scripting to?,suit the particular requirements of a part of a model,8232
12902,What is the result of trace-based exporter?,ONNX graph,8232
12903,Which exporter produces the ONNX graph that unrolls the for loop?,trace-based exporter,8427
12904,TorchScript only supports a subset of what?,Python types,7956
12905,What can you find more details about here?,type annotation,7956
12906,What type of variables does TorchScript only support for script functions?,single static types,7956
12907,"By default, each variable is assumed to be what?",Tensor,7956
12908,"If an argument to a ScriptModule function is not Tensor, its type should be specified using what?",MyPy-style annotations,7956
12909,What does TorchScript compiler fail with if the type annotation is not specified?,runtime error,7956
12910,What does TorchScript only support a subset of Python types?,type annotation,7956
12911,What type of variables does TorchScript only support?,single static types,7956
12912,What should be used if an argument to a ScriptModule function is not Tensor?,MyPy-style annotations,7956
12913,What can PyTorch models be written using?,numpy manipulations,4966
12914,What does tracing treat the numpy values as?,the constant node,4966
12915,What does the PyTorch model need to implement?,torch operators,4966
12916,When do we convert PyTorch models to numpy manipulations?,ONNX model,4966
12917,"For the trace-based exporter, tracing treats the numpy values as what?",the constant node,4966
12918,What need to be defined in init function so that inferencing can handle it properly?,Dropout layer,4966
12919,What does the PyTorch model not convert to?,numpy types,4965
12920,What do not convert to numpy tensors?,numpy types,4965
12921,What do you always use?,torch tensors and torch operators,9189
12922,Do not convert to what?,numpy types,9189
12923,What is another name for torch operators?,torch.concat,9189
12924,What do not convert to numpy types?,torch tensors,9189
12925,What layer needs to be defined in init function so that inferencing can handle it properly?,Dropout layer,9189
12926,What is a torch operator?,torch.concat,1116
12927,What is a torch tensor and torch operator?,torch.concat,1116
12928,What can the use of the.data field cause?,an incorrect trace graph,1116
12929,What can the.data field produce?,an incorrect trace graph,6907
12930,What is a safer alternative to the.data field?,.detach(),6907
12931,Why is the.data field kept?,backward compatibility,6907
12932,What is an old field that is kept for backward compatibility and should be avoided when writing models?,.data field,6907
12933,What is an example of an incorrect trace graph?,ONNX graph,6907
12934,What does the key represent?,the name of the argument in the model signature,7410
12935,What does the value represent?,the value of the argument to be passed,7410
12936,The second method is to represent the keyword arguments as key-value pairs where the key represents the name of the argument in the model signature and what else?,the value represents the value of the argument to be passed,7410
12937,Where does the key represent the name of the argument?,the model signature,7266
12938,What are the keyword arguments represented as?,key-value pairs,7266
12939,The first method is to pass all inputs in the same order as required by what?,the model,7057
12940,How many ways of exporting a model?,two,7057
12941,What is the last argument in the args tuple?,Using a dictionary,8105
12942,The dictionary is always the what in the args tuple?,last argument,8105
12943,What is an example of an empty dictionary as the last argument in the args tuple?,"example,",8105
12944,"For cases in which there are no keyword arguments, models can be exported with what?",empty or no dictionary,7411
12945,What is an exception to this rule?,cases in which the last input is also of a dictionary type,7411
12946,What does the export call assume is intended to represent the optional dictionary consisting of named arguments?,the ‘x’ input,7411
12947,"In cases where the last input is also of a dictionary type, what is the last argument in the args tuple?",empty dictionary,1125
12948,What is an example of an empty dictionary as the last argument in an args tuple?,example,1125
12949,What is an example of a case where an empty dictionary is the last argument in the args tuple?,example,2505
12950,"In order to prevent this from being an issue, a constraint is placed to provide an empty dictionary as the last input in what?",tuple args,8429
12951,The export call assumes that the ‘x’ input is intended to represent the optional dictionary consisting of what?,named arguments,8429
12952,What would the new call look like?,The new call would look like this,9859
12953,What is the value of Model()?,m,9859
12954,What is placed to provide an empty dictionary as the last input in the tuple args?,a constraint,775
12955,What is the last value of a tuple?,a dictionary consisting of named parameters and the corresponding inputs as key-value pairs,775
12956,What is assigned if certain named argument is not present in the dictionary?,default value,775
12957,What would cause a conflict when a dictionary of named parameters is used?,args tuple,775
12958,What provides an example of a conflict when a dictionary of named parameters is used?,model below,775
12959,What is the last input of the args tuple?,dictionary input,775
12960,"In the previous iteration, the call to export API would look like what?",torch.onnx.export,775
12961,What will be written to the file?,binary Protobuf,775
12962,What is the name of the parameter that will be exported if specified?,export_params,775
12963,"If you want to export an untrained model, set export_params to what?",False,775
12964,"What does torch.onnx.export(model, (k, x, ), ‘test.onnx’)",a file-like object,775
12965,What is the ordering of the exported model's parameters specified by?,model.state_dict().values(),775
12966,What would look like this?,The new call,7846
12967,What would assume that the x input is intended to represent the optional dictionary?,the export function,7846
12968,How would the export function work?,as intended,7846
12969,What specifies the ordering of the exported model's parameters?,model.state_dict().values(),7846
12970,What indexing is very flexible and complicated in PyTorch?,Tensor,6192
12971,How many categories of indexing are there in PyTorch?,two,6192
12972,Both categories of indexing are largely supported in what?,exporting,6192
12973,What is below the list of unsupported patterns for LHS indexing?,supported patterns for LHS indexing,6192
12974,What is the latest version of tensor indexing in PyTorch?,opset,6191
12975,Below is the list of what for RHS indexing?,supported patterns,6191
12976,Where does this type of indexing occur?,RHS,7822
12977,What is below the list for RHS indexing?,unsupported patterns,7822
12978,Export is supported for what version >= 9?,ONNX opset,7822
12979,Export is supported for what version of opset?,ONNX opset version >= 11,3710
12980,"In code, this type of indexing occurs on what?",LHS,3710
12981,What is below the list for LHS indexing?,unsupported patterns,3710
12982,"If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that you are exporting with the latest what?",opset,3710
12983,Export is supported for what version of ONNX opset?,ONNX opset version >= 11,7823
12984,Below is the list of what for LHS indexing?,supported patterns,1135
12985,What is the latest opset?,opset_version=12,1135
12986,To what platform are all TorchVision models exportable?,ONNX,1066
12987,What can be found in TorchVision?,More details,1066
12988,All TorchVision models except quantized versions are exportable to what?,ONNX,1066
12989,All TorchVision models except what are exportable to ONNX?,quantized versions,1066
12990,Where can you find more details about TorchVision models?,TorchVision,1066
12991,What are only supported as JIT inputs/outputs?,"tuples, lists and Variables",4668
12992,What are also accepted but their usage is not recommended?,Dictionaries and strings,4668
12993,Why do we allow Caffe2 to call directly to Torch implementations of operators?,to help you smooth over these differences when precision is important,4668
12994,"Depending on model structure, what are the differences between operators implemented by PyTorch and ONNX backends?",negligible,4668
12995,Users need to verify their dict inputs carefully and keep in mind what is not available?,dynamic lookups,4667
12996,Only what are supported as JIT inputs/outputs?,"tuples, lists and Variables",4667
12997,What is not available for dict inputs?,dynamic lookups,4667
12998,What backends often have implementations of operators with some numeric differences?,PyTorch and ONNX backends,4953
12999,"Depending on model structure, the numeric differences in PyTorch and ONNX backends may be what?",negligible,4953
13000,Which backends often have implementations of operators with numeric differences?,PyTorch and ONNX,4953
13001,What operators are supported?,BatchNorm ConstantPadNd Conv Dropout Embedding,7088
13002,What mode is not supported by EmbeddingBag FeatureDropout?,training mode,2254
13003,What is the training mode not supported?,EmbeddingBag FeatureDropout,7089
13004,What is the name of the feature that does not support optional arguments?,Conv Dropout Embedding,1887
13005,What mode is not supported in EmbeddingBag FeatureDropout?,training mode,1888
13006,What type of Embedding does not support optional arguments?,Dropout Embedding,2214
13007,EmbeddingBag FeatureDropout (not supported) EmbeddingBag FeatureDropout (not supported,training mode,2253
13008,What mode is not supported by FeatureDropout?,training mode,2407
13009,What expand expand_as eye flatten floor floor_divide?,eq erf exp,3840
13010,What is the absolute abs of MaxPool1d MaxPool2d MaxPool3d RNN?,acos,4215
13011,What does frobenius_norm stand for?,frobenius_norm,4215
13012,What is the absolute abs of MaxPool2d MaxPool3d RNN?,acos,4217
13013,What is frobenius_norm full?,frobenius_norm full,4217
13014,What abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg,MaxPool3d RNN,4218
13015,What is frobenius_norm full full_like?,frobenius_norm full full_like,4218
13016,What is the abs absolute?,acos,5083
13017,What is frobenius_norm full full?,frobenius_norm full full,8786
13018,What are the names of the two addmm and arange argmax argmin?,addmm and arange argmax argmin,8786
13019,What is the abs absolute acos?,abs absolute acos,8784
13020,What is the name of the adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool,acos,8789
13021,What is adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_,acos,8789
13022,What do you need to add to avg_pool1d?,addmm and arange argmax argmin,8803
13023,What do argmax argmin asin atan support?,addmm and arange,8803
13024,What do you need to add to avg_pool3d?,addmm and arange argmax argmin,8806
13025,What addmm and arange asin atan avg_pool1d avg_pool2d avg,addmm and arange,8806
13026,What is the name of the addmm and arange argmax argmin asin atan avg_pool1d,addmm and arange argmax argmin,8808
13027,What do argmax argmin asin asin atan?,addmm and arange,8808
13028,What type of alpha not supported?,nonzero,8814
13029,What is another name for adaptive_max_pool3d add?,arange argmax argmin,8812
13030,What does nonzero alpha not support?,add,8813
13031,What is another name for argmax argmin?,arange,8835
13032,What is as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos,avg_pool3d,8873
13033,What is avg_pool3d?,avg_pool3d,8932
13034,What is as_strided baddbmm bitshift cat ceil?,avg_pool3d,8932
13035,What is another name for avg_pool1d?,avg_pool2d,8927
13036,What is argmin asin atan?,argmax,8880
13037,What is atan avg_pool1d avg_pool2d avg_pool3d?,argmin,8883
13038,What is atan avg_pool1d avg_pool2d avg_pool3d as_s,argmin,8882
13039,What is the name of the bitshift cat ceil?,baddbmm,8942
13040,What is the name of the avg_pool2d?,avg_pool2d,8929
13041,What is avg_pool3d as_strided?,baddbmm,8929
13042,What is the name of the avg_pool3d as_strided baddbmm bitshift cat?,avg_pool2d,8930
13043,What is the name of the bitshift cat?,as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min,8901
13044,What is baddbmm bitshift cat ceil?,as_strided,8901
13045,What does eq erf exp expand expand expand_as eye flatten floor floor_divide?,div dropout,9028
13046,What expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu,eq erf exp expand,9052
13047,What is elu empty empty like?,eq,9052
13048,What is the benefit of adding export support for operators?,advance usage,947
13049,What can be done to confirm if the operator is standardized or not?,check the ONNX operator list,947
13050,"To add export support, developers need to touch the source code of what?",PyTorch,947
13051,What is required to add export support for operators?,developers need to touch the source code of PyTorch,948
13052,"If the wanted operator is what, it should be easy to add support for exporting such operator?",standardized in ONNX,948
13053,What is an advance usage of PyTorch?,export support,948
13054,How do developers install PyTorch?,follow the instructions for installing PyTorch from source,7858
13055,What do developers need to do to achieve this?,to touch the source code of PyTorch,7858
13056,What should be done to confirm if the operator is standardized or not?,check the ONNX operator list,7858
13057,"To achieve this, developers need to touch the source code of what?",PyTorch,7858
13058,What is the source code of PyTorch?,PyTorch,7857
13059,What can be added to the operator if the operator is standardized in ONNX?,symbolic function,7857
13060,"To confirm whether the operator is standardized, please check what?",ONNX operator list,7857
13061,"To confirm whether the operator is standardized or not, please check what?",ONNX operator list,949
13062,What is always the exported ONNX graph?,first parameter,949
13063,What can you find in torch/csrc/autograd/generated/VariableType.h?,the declaration of the function,3505
13064,"If the operator is an operator of what type, what operator can you find the declaration of the function in torch/csrc/autograd/",ATen,3505
13065,What does torch/onnx/symbolic_opsetversion>.py define?,symbolic function,2093
13066,Make sure the function has what name?,the same name as the ATen operator/function defined in VariableType.h,2093
13067,Why must parameter names match the names in VariableType.h?,dispatch is done with keyword arguments,2093
13068,The first parameter is always the exported what?,ONNX graph,3500
13069,What is the name of the ATen operator/function defined in?,VariableType.h,3507
13070,Why must parameter names EXACTLY match the names in VariableType.h?,dispatch is done with keyword arguments,3507
13071,"In the symbolic function, if the operator is already standardized in what, we only need to create a node to represent the ONNX",ONNX,3507
13072,What is the input argument if ONNX asks for a scalar?,a tensor,3507
13073,"If the input argument is a tensor, but ONNX asks for what?",scalar,3507
13074,What is dispatch done with?,keyword arguments,7058
13075,What is always the first parameter?,the exported ONNX graph,7058
13076,"Inputs are always first, what are non-tensor arguments?",tensors,7058
13077,"Inputs are always first, then what else?",tensors,3506
13078,"In the symbolic function, if the operator is already standardized in ONNX, we only need to do what to represent the ONNX operator",create a node,7059
13079,What does NOT match what is in VariableType.h?,Parameter ordering,7059
13080,Parameter ordering does NOT match what?,VariableType.h,4731
13081,What can _if_scalar_type_as turn into a PyTorch tensor?,Python scalar,3493
13082,What happens if the operator is a non-ATen operator?,the symbolic function has to be added in the corresponding PyTorch Function class,3493
13083,What helper function can convert a scalar tensor into a python scalar?,_scalar,7060
13084,Who asks for a scalar?,ONNX,3508
13085,What parameter is always the exported ONNX graph?,first,3508
13086,What language does the helper function _if_scalar_type_as use?,Python,3492
13087,How do you add a symbolic function if the operator is a non-ATen operator?,Create a symbolic function named symbolic in the corresponding Function class,3499
13088,How do you add a symbolic function to a PyTorch Function class?,Create a symbolic function named symbolic,3499
13089,"If the operator is a non-ATen operator, what has to be added in the corresponding PyTorch Function class?",the symbolic function,3501
13090,What is the name of the symbolic function in the corresponding PyTorch Function class?,Create a symbolic function named symbolic,3501
13091,Where must the symbolic function be added if the operator is a non-ATen operator?,PyTorch Function class,3501
13092,What must EXACTLY match the names in forward?,Parameter names,3501
13093,What must match the outputs of forward?,output tuple size,3501
13094,Symbolic functions should be implemented in what language?,Python,3501
13095,"In the symbolic function, if the operator is already standardized in what, we just need to create a node to represent the ONNX",ONNX,3501
13096,Python methods are implemented via what bindings?,C++-Python,3501
13097,What is an example of handling for elu operator?,missing symbolic function,3501
13098,What type of operator is elu?,ATen operator,3501
13099,What operator does elu have?,ATen,3501
13100,PyTorch is able to export what operator?,elu operator,3501
13101,What is the interface for specifying operator definitions?,experimental,3501
13102,Parameter names except the first must EXACTLY match what?,the names in forward,4732
13103,What type of function is created in the corresponding Function class?,symbolic,1942
13104,What is the first parameter of a symbolic function?,The first parameter is always the exported ONNX graph,1942
13105,"In the symbolic function, if the operator is already standardized in ONNX, we just need to do what to represent the ONNX operator",create a node,7061
13106,What do we try to do?,export the model,7061
13107,What is the first parameter?,always the exported ONNX graph,7062
13108,Where do we add the following lines to?,symbolic_opset9.py,7062
13109,What are Python methods implemented via?,C++-Python bindings,6165
13110,Symbolic functions interact with what?,Python methods,4727
13111,What is an example of missing symbolic function for?,elu operator,4728
13112,How do we handle missing symbolic function for elu operator?,export the model and see the error message as below,4728
13113,What bindings are used to implement Python methods?,C++-Python,6164
13114,Where is the ONNX graph C++ definition located?,torch/csrc/jit/ir/ir.h,6933
13115,Where is virtual Tensor elu found?,VariableType.h,6933
13116,What operator list does PyTorch check?,ONNX,6933
13117,What did we try to do?,export the model,6932
13118,What is handled for elu operator?,missing symbolic function,6932
13119,What do we try to do to the model?,export,6932
13120,What does not support exporting elu operator?,PyTorch,2784
13121,What does PyTorch add the following lines to?,symbolic_opset9.py,2784
13122,"In VariableType.h, what operator does PyTorch not support exporting?",virtual Tensor elu,7054
13123,Where does PyTorch find virtual Tensor elu?,VariableType.h,7054
13124,Where are examples of elu operators found?,"symbolic_opset9.py, symbolic_opset10.py",7054
13125,Who does not support exporting elu operator?,PyTorch,7054
13126,Why did the export fail?,PyTorch does not support exporting elu operator,7055
13127,Where can you export a custom ops implementation?,ONNX,7055
13128,What are examples of PyTorch's elu operator?,"symbolic_opset9.py, symbolic_opset10.py",6934
13129,Where are more examples of ops in PyTorch?,"symbolic_opset9.py, symbolic_opset10.py",7401
13130,What tutorial will help you create and register your own custom ops implementation in PyTorch?,Extending TorchScript with Custom C++ Operators,2479
13131,What is the answer to the question of how to export a custom ops implementation to ONNX?,:,7148
13132,What is the name of the tutorial that allows you to create and register your own custom ops implementation in PyTorch?,Extending TorchScript with Custom C++ Operators,2477
13133,What platform can you export your custom ops implementation to?,ONNX,2477
13134,How do you export a custom ops implementation to ONNX?,:,2477
13135,What is able to export elu operator?,PyTorch,4532
13136,What is another example of PyTorch's export elu operator?,symbolic_opset9.py,4532
13137,Where are more examples of elu operator in PyTorch?,"symbolic_opset9.py, symbolic_opset10.py",4533
13138,Where can operators with unsupported ONNX operators be exported?,export API,2355
13139,What flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX?,operator_export_type flag,2355
13140,What is the operator_export_type flag useful when users try to export?,ATen and non-ATen operators that are not registered and supported in ONNX,2355
13141,What type of operators are exported using the operator_export_type flag?,regular ONNX operators,2355
13142,What is the default mode for exporting ATen and non-ATen operators?,operator_export_type mode,2355
13143,Where is the operator_export_type flag found?,export API,2355
13144,When is the operator_export_type flag useful?,when users try to export ATen and non-ATen operators that are not registered and supported in ONNX,2355
13145,What is the operator_export_type mode used for?,to export all operators as regular ONNX operators,2355
13146,What mode is used to export all operators as ATen ops?,operator_export_type mode,2355
13147,What is the default operator_export_type mode used for?,to export all operators as ATen ops,2355
13148,What is the default operator_export_type mode used to export all operators as?,ATen ops,7743
13149,What mode is used to export all operators as regular ONNX operators?,operator_export_type mode,7743
13150,To fall back on what in ONNX?,unsupported ATen operators,7743
13151,What is the default operator_export_type mode used to export?,raw ir,7743
13152,What is this mode used for?,to export all operators as regular ONNX operators,7743
13153,What is the default operator_export_type mode used to do in ONNX?,fallback on unsupported ATen operators,7743
13154,What is the default operator_export_type mode?,export all operators as regular ONNX operators,7741
13155,What is the default mode for ONNX operators?,operator_export_type mode,7741
13156,This mode is used to export all operators as regular what?,ONNX operators,7741
13157,What are exported to ONNX regularly?,Supported operators,7893
13158,What operator is not supported in ONNX?,aten::triu,7893
13159,Who falls back on aten::triu?,Exporter,7893
13160,What operators are exported to ONNX regularly?,Supported operators,7893
13161,What does the exporter fall back on in ONNX?,unsupported ATen operators,7893
13162,What does the exporter do to ONNX?,fallback on unsupported ATen operators,7893
13163,Who falls back on ATen::triu?,Exporter,7740
13164,To export a raw operator?,ir,7740
13165,What is the mode used to export all operators as?,ATen ops,7740
13166,What does exporter fall back on in ONNX?,unsupported ATen operators,7740
13167,What is this mode used to do?,export all operators as ATen ops,7740
13168,What is the mode used to avoid conversion to ONNX?,fallback on unsupported ATen operators,7740
13169,What type of operator is exported to ONNX?,raw ir,7895
13170,To export what?,raw ir,7895
13171,What are all operators exported as?,ATen ops,7738
13172,What is the purpose of a raw ir?,export,7891
13173,What is the purpose of exporting a raw ir?,export a raw ir,7891
13174,What model did I export but its input size seems to be fixed?,lstm,5039
13175,What can you export models with in it?,loops,5039
13176,What type of inputs can be exported?,primitive type inputs,5039
13177,What type of inputs can you export models with?,primitive type inputs,5039
13178,What records the example inputs shape in the graph?,tracer,7340
13179,What parameter does export api use in case the model should accept inputs of dynamic shape?,dynamic_axes,7340
13180,What is a good way to export models with loops in it?,Tracing vs Scripting,7340
13181,When will primitive type inputs be supported?,PyTorch 1.9 release,7340
13182,What seems to be fixed in my lstm model?,input size,5040
13183,"In case the model should accept inputs of dynamic shape, what parameter can you utilize in export api?",dynamic_axes,5040
13184,What type of inputs can be exported with Tracing vs Scripting?,loops,5040
13185,What are two options for exporting models with loops in it?,Tracing vs Scripting,5040
13186,What does exporter not support conversion of models with String inputs?,primitive type inputs,5040
13187,What type of inputs are supported in PyTorch 1.9 release?,primitive type inputs,5040
13188,When will support for primitive type inputs be added?,PyTorch 1.9,5040
13189,What type of inputs can be exported with PyTorch?,loops,7339
13190,"In case the model should accept inputs of dynamic shape, you can utilize what parameter in export api?",dynamic_axes,7339
13191,What type of inputs can be exported with tracer?,loops,7339
13192,Exporter does not support conversion of models with what inputs?,String inputs,6095
13193,What does not support conversion of models with String inputs?,exporter,6095
13194,When will primitive type inputs be added to ONNX?,PyTorch 1.9,6095
13195,What is tensor in-place indexed assignment like data[index] = new_data supported for?,Indexing,6095
13196,What is tensor list exportable to?,ONNX,6095
13197,What does ONNX export models with in it?,loops,5037
13198,What does ONNX not support implicit scalar datatype casting?,Tracing vs Scripting,5036
13199,How do you export models with loops in it?,export models with loops in it,5036
13200,What does ONNX not support conversion of models with String inputs?,primitive type inputs,5038
13201,What does PyTorch 1.9 support?,primitive type inputs,5038
13202,Exporter does not support what?,conversion of models with String inputs,5038
13203,What does ONNX support?,implicit scalar datatype casting,5033
13204,What is an example of a model where the datatypes are not recorded?,scripted models,5033
13205,What should you check out?,Tracing vs Scripting,4849
13206,What is the name of the two types of scripting?,Tracing vs Scripting,4849
13207,Exporter does not support conversion of models with what input?,String inputs,6094
13208,What type of inputs will PyTorch 1.9 support?,primitive type inputs,6094
13209,What company supports implicit scalar datatype casting?,ONNX,5032
13210,What are converted to constant tensors in ONNX?,Scalars,4318
13211,The exporter will try to figure out the right datatype for what?,scalars,4318
13212,What do you need to do if the exporter fails to figure out the right datatype for scalars?,manually provide the datatype information,4318
13213,What type of models often fail to record datatypes?,scripted models,4318
13214,Who will try to handle that part?,the exporter,4318
13215,What are Scalars converted to in ONNX?,constant tensors,4318
13216,Who will try to figure out the right datatype for scalars?,The exporter,4318
13217,What is supported for ONNX opset version >= 11?,Indexing,4318
13218,What is tensor in-place indexed assignment like?,data[index] = new_data,4318
13219,What is exportable to ONNX?,tensor list,4318
13220,What is not required in the future?,manual changes,4317
13221,What is the goal of ONNX?,improve the datatype propagation in the exporter,4317
13222,What version of ONNX is tensor in-place indexed assignment supported for?,>= 11,5043
13223,Is tensor list exportable to ONNX?,exportable to ONNX,5043
13224,What is tensor in-place indexed assignment supported for?,ONNX opset,5043
13225,What version of ONNX is tensor list supported for?,>= 11,8527
13226,What is tensor list exportable to ONNX?,Indexing,8527
13227,What version of ONNX supports tensor list export?,ONNX opset,8527
13228,What allows export of models in ONNX external data format?,export API,11315
13229,What does the exporter do with the ONNX external data format option enabled?,the exporter stores some model parameters in external binary files,11315
13230,What must be a string specifying the location of the model?,Argument ‘f’,11315
13231,What does the exporter store some model parameters in?,external binary files,11315
13232,Where are external binary files stored?,same location as the ONNX file,11315
13233,Why can't models larger than 2GB be exported in one file?,the protobuf size limit,11315
13234,What should users set use_external_data_format to?,True,11315
13235,"To export models larger than 2GB, users should set use_external_data_format to what?",True,11315
13236,Where are the external binary files stored?,the same location as the ONNX file,11314
13237,The use_external_data_format argument in export API enables export of models in what format?,ONNX external data format,11314
13238,What must argument f be?,a string specifying the location of the model,11314
13239,Where can large models be exported to?,ONNX,7462
13240,Why cannot models larger than 2GB be exported in one file?,protobuf size limit,7462
13241,What allows users to export models in a training-friendly mode?,export API,7995
13242,TrainingMode.TRAINING exports model in what mode?,training-friendly,7995
13243,"If model.training is what, it exports the model in inference mode?",False,7995
13244,What mode does TrainingMode.PRESERVE export the model in if model.training is False?,training-friendly mode,7995
13245,What is the default mode for this argument?,TrainingMode.EVAL,7995
13246,What argument in export API allows users to export models in a training-friendly mode?,Training,7995
13247,What exports model in a training-friendly mode that avoids certain model optimizations?,TrainingMode.TRAINING,7995
13248,TrainingMode.PRESERVE exports the model in inference mode if model.training is what?,False,7995
13249,What does TrainingMode.PRESERVE do if model.training is False?,it exports the model in a training-friendly mode,7995
13250,What format can a model be exported into?,ONNX,2350
13251,What does ONNX support at the moment?,a limited set of dynamic models,2350
13252,What does a TUPLE OF ARGUEMENTS have?,DICTIONARY OF NAMED PARAMETERS,2350
13253,What type of models does torch.nn.Module support?,a limited set of dynamic models,2349
13254,What is a tuple of arguments or torch.Tensor?,args,2349
13255,What format can you export a model into?,ONNX,2349
13256,What is the model to be exported?,model,9923
13257,What is a dictionary to specify the input to the corresponding named parameter?,args,9923
13258,What is another name for args?,torch.Tensor,2351
13259,What is a args?,a dictionary consisting of named arguments,8884
13260,What is a tuple of arguments or torch?,a dictionary consisting of named arguments,8884
13261,What is used to specify the input to the corresponding named parameter?,a dictionary,8758
13262,What is the name of the dictionary to specify the input to the corresponding named parameter?,KEY: str,8758
13263,What is model(*args)?,a valid invocation of the model,4560
13264,A TUPLE OF ARGUEMENTS WITH WHAT?,DICTIONARY OF NAMED PARAMETERS,4560
13265,Any non-Tensor arguments will be what in the exported model?,hard-coded,7139
13266,"If args is a Tensor, this is equivalent to having called it with what of that Tensor?",1-ary tuple,7139
13267,What is a valid invocation of the model?,model(*args),7139
13268,"If args is a Tensor, this is equivalent to having called it with what tuple of that Tensor?",1-ary,7139
13269,Any non-what argument will be hard-coded into the exported model?,Tensor,7139
13270,Any non-what type of arguments will be hard-coded into the exported model?,Tensor,9924
13271,What will be hard-coded into the exported model?,Any non-Tensor arguments,8885
13272,What value is assigned if a certain named argument is not present in the dictionary?,None,7136
13273,What is the last value of the tuple?,a dictionary,7136
13274,"If certain named argument is not present in the dictionary, what value is assigned to it?",default value,7136
13275,What is assigned if the default value is not provided?,None,774
13276,"If a named argument is not present in the dictionary, what is assigned if the default value is not provided?",None,7135
13277,What does the model below return?,x m,1531
13278,What would the call to export API look like in the previous iteration?,torch.onnx.export,3788
13279,What would the new call to export API look like?,The new call would look like this,3788
13280,"What does torch.randn(2, 3) call?",torch.tensor(1,11493
13281,"What is Model() k = torch.randn(2, 3) x = torch.tensor(1)",m,9858
13282,"m = Model() k = torch.randn(2, 3)?",x,9858
13283,The export function assumes that the x input is intended to represent what?,the optional dictionary consisting of named arguments,3787
13284,"In order to prevent this from being an issue, a constraint is placed to provide what type of dictionary as the last input in the tup",empty,3787
13285,"In the previous iteration, what would the call to export API look like?",torch.onnx.export,3787
13286,What does the export function assume is intended to represent the optional dictionary consisting of named arguments?,the x input,1532
13287,"What does torch.onnx.export(model, (k, x), and ‘test.onnx’ mean?",test.onnx,11178
13288,"What does torch.onnx.export(model, (k, x), ‘test.onnx’) stand for?",test.onnx,11178
13289,What type of dictionary is provided as the last input in the tuple args?,empty,7845
13290,"What does torch.onnx.export(model, (k, x, ), call?",test.onnx,7845
13291,What would work as intended?,torch.onnx.export,11179
13292,What is torch.onnx.export f?,file-like object,11182
13293,Set export_params to what if you want to export an untrained model?,False,11182
13294,What will be written to this file?,A binary Protobuf,9322
13295,What does model.state_dict().values() do?,model.state_dict().values(),11181
13296,What does a file-like object have to implement that returns a file descriptor?,fileno,9321
13297,What is a file-like object that has to implement fileno that returns a file descriptor?,f,9321
13298,Names to assign to the input nodes of the graph?,input_names,9321
13299,"What is a bool, default True value?",export_params,9320
13300,What happens if you want to export an untrained model?,the exported model will first take all of its parameters as arguments,9296
13301,What verbose specifies the ordering of the exported model's parameters?,model.state_dict().values(),9296
13302,What is the default TrainingMode.EVAL?,training,9296
13303,What will be exported if specified?,export_params,9295
13304,What is the default value of verbose?,"bool, default False",11336
13305,"If training is False and to a training friendly mode if model, what is it?",True,11336
13306,TrainingMode.TRAINING: export the model in what mode?,training friendly mode,9297
13307,When will we print out a debug description of the trace being exported?,if specified,11337
13308,What does export the internal IR instead of converting it to?,ONNX ops,11337
13309,TrainingMode.PRESERVE: export the model in inference mode if model.training is what?,True,11251
13310,Is model.training true or false?,False,11250
13311,What is the name to assign to the input nodes of the graph?,input_names,11250
13312,"If model.training is what, export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode",False,11250
13313,TrainingMode.EVAL: export the model in what mode?,inference mode,11252
13314,What are the names to assign to the output nodes of the graph?,output_names,11252
13315,Names to assign to the output nodes of the graph?,output_names,10362
13316,What is used to export the model in aten mode?,operator_export_type,10362
13317,"If using aten mode, all the ops original exported by the functions in symbolic_opsetversion>.py are exported as what",ATen ops,10362
13318,OperatorExportTypes.ONNX_ATEN: All ops are exported as what?,ATen ops,10362
13319,What mode does operator_export_type export the model in?,aten mode,8913
13320,What is the name of the function that exports the internal IR directly instead of ONNX ops?,export_raw_ir,8913
13321,What is the name to assign to the output nodes of the graph?,output_names,10361
13322,What is the default False value for the internal IR?,export_raw_ir,10361
13323,What happens if an ATen op is not supported in ONNX or its symbolic is missing?,fall back on ATen op,10361
13324,How is an example graph exported?,as:,10361
13325,What is the name of the function that exports the internal IR directly instead of converting it to ONNX ops?,export_raw_ir,8912
13326,What is the name of the mode that exports the model in aten mode?,aten,8912
13327,What is the default False value for exporting the model in aten mode?,aten,8912
13328,What does operator_export_type convert the internal IR to?,ONNX ops,9677
13329,What is the default OperatorExportTypes.ONNX?,operator_export_type,9677
13330,What ops does export_raw_ir convert the internal IR to?,ONNX,9677
13331,What does export_raw_ir convert the internal IR to instead of converting it to?,ONNX ops,9301
13332,What is the default operator_export_type?,OperatorExportTypes.ONNX,10308
13333,"What does bool, default False mean?",export_raw_ir,9300
13334,OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace).,ONNX,10307
13335,What is exported as: ONNX?,Example graph,10307
13336,What is not supported in ONNX?,aten::triu,3764
13337,What should be passed in tuple format if there is more than one item?,example_outputs,3764
13338,What is the name of the item that should be passed as the example output?,example_outputs,3764
13339,OperatorExportTypes.ONNX: All ops are exported as what?,regular ONNX ops,4674
13340,What type of ops are exported to ONNX regularly?,Registered ops,4674
13341,OperatorExportTypes. What is the namespace for ONNX ops?,ONNX,4674
13342,What is exported as: ONNX ops?,graph,4674
13343,What is the name of the op that can be exported as a custom ONNX op?,OperatorExportTypes.RAW,3762
13344,What is the name of the op that can be exported and implemented by the user for their runtime backend?,OperatorExportTypes.ONNX_FALLTHROUGH,3762
13345,What is the name of the custom ONNX op?,OperatorExportTypes.ONNX_FALLTHROUGH,9699
13346,Who can implement an ONNX op?,the user,10309
13347,What is not supported in the above example?,prim::ListConstruct,3767
13348,What is the default to export the model to the opset version of the onnx submodule?,opset_version,10313
13349,How many stable opset versions does ONNX export to?,one,10313
13350,What is the current supported stable opset version?,9.,10313
13351,What must the opset_version be?,_onnx_main_opset,10313
13352,"By default, how many stable opset versions do we export to?",one stable opset version,10313
13353,What is the name of the opset version of the onnx submodule?,opset_version,10313
13354,What will replace some of the ops that have all constant inputs with pre-computed constant nodes?,Constant-folding optimization,10313
13355,What are examples of outputs required when exporting?,ScriptModule or TorchScript Function,10313
13356,What must be provided when exporting a ScriptModule or TorchScript Function?,‘example_outputs’,10313
13357,What is the name of the submodule we export the model to?,opset_version,9701
13358,What must be _onnx_main_opset or _onnx_stable_opsets?,opset_version,3763
13359,What must be _onnx_main_opset or in _onnx_stable_opsets?,opset_version,10312
13360,What is the name of the constant-folding optimization that is applied to the model during export?,do_constant_folding,9194
13361,What will the constant-folding optimization replace some of the ops that have all constant inputs with?,pre-computed constant nodes,9194
13362,What happens if do_constant_folding is True?,the constant-folding optimization is applied to the model during export,9194
13363,What does example_outputs represent?,Model’s example outputs being exported,9283
13364,"If there is more than one item, it should be passed in what format?",tuple format,9283
13365,What is the value of ample_outputs?,x,9283
13366,How many items should be passed as the example output?,only one item should be passed as the example output,9283
13367,What should be passed as the example output if there is more than one item?,only one item,9195
13368,"If there is more than one item, what should be passed in tuple format?",example_outputs,9195
13369,Model's what is being exported?,example outputs,9281
13370,"What strip the field ""doc_string"" from the exported model?",strip_doc_string,9282
13371,"What does dictstring, dictpython:int, string> or dictstring, list(int)",dynamic_axes,10771
13372,"When does strip_doc_string (bool, default True) strip the field “doc_string” from the exported model?",if True,10771
13373,"What does dictstring, dictpython:int, string>> or dictstring, list(int",dynamic_axes,10771
13374,"What is the term for dictstring, dictpython:int, string or dictstring, list(in",dynamic_axes,9230
13375,What is the default value of dynamic_axes?,empty,9230
13376,"What are dictstring, dictpython:int, string>> or dictstring, list(int",dynamic_axes,9230
13377,What can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE,dynamic axes,7387
13378,What can dynamic axes be defined as?,keep_initializers_as_inputs,7387
13379,"If initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs, what is",False,4167
13380,"If True, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs?",If False,4167
13381,What is the MIXED MODE of (1) and (2)?,keep_initializers_as_inputs,4167
13382,When are all the initializers in the exported graph added as inputs to the graph?,If True,4562
13383,What happens if keep_initializers_as_input is true?,all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph,3368
13384,When are all initializers in the exported graph added as inputs to the graph?,If True,3368
13385,When are initializers not added as inputs to the graph?,If False,4563
13386,What is the lower triangular part of a matrix defined as?,the elements on and below the diagonal,5546
13387,Returns what of the lower triangular part of a row-by-col matrix in a 2-by-N Tensor?,the indices,5546
13388,What is the lower triangular part of the matrix defined as?,the elements on and below the diagonal,7163
13389,What scale is used to subtract other from input?,alpha,6084
13390,Supports what?,broadcasting,6084
13391,What is an example of a scalar multiplier?,Example,6084
13392,For what type of tensor does it compute the logical AND?,bool tensors,1697
13393,What is the output tensor of a bool tensor?,output tensor,1697
13394,What will flow back from the result of this operation to input?,gradients,5299
13395,Detach() creates a tensor without what relationship to input?,autograd relationship,5299
13396,What is the default memory format of the returned tensor?,torch.preserve_format,5299
13397,To create a tensor without an autograd relationship to input see what?,detach(),5299
13398,What does memory_format return?,the desired memory format of returned tensor,5299
13399,What is obj a PyTorch tensor?,PyTorch storage object,5213
13400,"If obj is a PyTorch tensor, Returns True if obj is what?",PyTorch storage object,5214
13401,"If obj is a PyTorch tensor, what is it?",PyTorch storage object,3917
13402,What is the name of the GPU that enables you to run your tensor computations?,CUDA,3917
13403,What type of data type does obj return True if it is a PyTorch tensor?,complex data type,3917
13404,What is the data type of input?,floating point data type,5228
13405,Returns True if the input is a single element tensor which is not equal to what after type conversions?,zero,5228
13406,What does torch.empty() construct with data?,a tensor,5228
13407,"If the data type of input is a torch.float64, torch.float32, torch.float16, and torch.bfloat16,",floating point data type,5212
13408,"If obj is a PyTorch storage object, what is it?",PyTorch tensor,5212
13409,What is the data type of input if it is a complex data type?,"one of torch.complex64, and torch.complex128",5212
13410,Sets the efault floating point dtype to what?,d,5240
13411,What is returned by setting the default torch.Tensor type to floating point tensor type t?,the total number of elements in the input tensor,5240
13412,Returns what if the input is a single element tensor?,True,5240
13413,Returns what if the input is a single element tensor which is not equal to zero after type conversions?,True,5240
13414,What type of tensor is constructed with data?,tensor,5240
13415,What type of input is not equal to zero after type conversions?,a single element tensor,7328
13416,Returns True if the data type of input is what?,a floating point data type,7328
13417,What type of object is obj?,PyTorch storage object,5209
13418,What does Sets the default floating point torch.dtype do?,Get the current default floating point torch.dtype,5220
13419,What is the efault floating point dtype?,d,5838
13420,What is returned if the input is a single element tensor?,the total number of elements in the input tensor,5227
13421,Returns True what if the data type of input is a floating point data type?,if the data type of input is a floating point data type,5227
13422,What does Returns True if the input is a single element tensor?,the total number of elements in the input tensor,5226
13423,What does the torch.Tensor type return?,the total number of elements in the input tensor,2693
13424,What does this do on the CPU?,Disables denormal floating numbers,5239
13425,What does the Disable denormal floating numbers on CPU do?,Note,5239
13426,What is returned by setting the default torch.Tensor type to t?,the total number of elements in the input tensor,5837
13427,What is the default torch.Tensor type?,floating point tensor type t,5843
13428,Returns what in the input tensor?,the total number of elements,5842
13429,What are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.rand,Random sampling creation ops,5221
13430,What does denormal floating numbers do on the CPU?,Disables denormal floating numbers,2148
13431,What type of data does torch.empty() construct?,tensor,2148
13432,What type of tensor is constructed in COO(rdinate) format with specified values at the given indices?,sparse,2694
13433,What does torch.empty() construct a tensor with?,data,5086
13434,What type of data does a torch.empty() construct?,tensor,5086
13435,"What is created by creating a Tensor input with specified size, stride and storage_offset?",a view of an existing torch,1944
13436,"What returns a tensor filled with the scalar value 0, with the same size as input?","a tensor filled with the scalar value 0, with the same size as input",1944
13437,What returns a tensor filled with the scalar value 1?,"a tensor filled with the scalar value 1, with the shape defined by the variable argument size",1944
13438,Creates a Tensor from what?,numpy.ndarray,4394
13439,What is the shape of the scalar value?,the shape defined by the variable argument size,5431
13440,What is the scalar value 1?,the same size as input,5431
13441,"What is returned when the scalar value is 0. Returns a tensor filled with the scalar value 0, with the",a tensor,5431
13442,"Returns a tensor filled with the scalar value 0, with what size as input?",same size,5431
13443,"Returns a tensor filled with the scalar value 1, with what size as input?",same size,5431
13444,What is the size of the scalar value 0?,the same size as input,5431
13445,What does the scalar value 0 return?,the shape defined by the variable argument size,5431
13446,What is returned when the scalar value 0 is returned?,a tensor,5431
13447,"Returns a tensor filled with the scalar value 0, with what as input?",same size,5431
13448,What is the scalar value of the tensor filled with?,the same size as input,5434
13449,Returns what type of scalar value filled with the scalar value 1?,a tensor,5434
13450,Returns what tensor of size?,a 1-D tensor,5434
13451,What is the return value of a tensor filled with the scalar value?,"a tensor filled with the scalar value 0, with the same size as input",5428
13452,What defines the shape of the scalar value?,variable argument size,5429
13453,What does return a tensor of size endstartstepleftlceil fractextend,a 1-D tensor,5249
13454,How many tensors does endstartstep+1leftlfloor fractextend return?,1,5249
13455,How large is the tensor of size?,1-D,5250
13456,What happens when a tensor is divided into chunks?,Splits,1817
13457,What is used to split a tensor into multiple tensors horizontally?,indices_or_sections,6031
13458,Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is what?,LongTensor,6031
13459,What does move the dimension(s) of input at the position(s) in source to the position(s) in destination?,Moves the dimension(s) of input at the position(s) in source to the position(s) in destination,6031
13460,"Returns what with the same data and number of elements as input, but with the specified shape?",a tensor,6031
13461,Returns a tensor with all the dimensions of input of size what?,1,6031
13462,What does Splits a tensor into multiple tensors depthwise according to?,indices_or_sections,6022
13463,What happens to a tensor with three or more dimensions?,Splits input,6037
13464,What does Splits input into multiple tensors depthwise according to?,indices_or_sections,6037
13465,What does Splits input into multiple tensors horizontally according to?,indices_or_sections,6037
13466,Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which,BoolTensor,6037
13467,How are tensors stacked in sequence?,horizontally,6065
13468,What axis are the values of a tensor gathered along?,an axis,6036
13469,What is the index that indexes the input tensor along dimension dim?,LongTensor,6057
13470,Stack tensors in sequence in what direction?,depthwise,6057
13471,Stack tensors horizontally according to what?,indices,6057
13472,What does it do to split tensors in sequence horizontally?,Stack tensors,2669
13473,What dimension does a BoolTensor index?,1-D,2669
13474,What is specified by dim?,axis,2669
13475,What does it do when a tensor is split into multiple tensors horizontally?,Stack tensors,2669
13476,What is a new tensor that indexes the input tensor along dimension dim?,LongTensor,5350
13477,What is the name of the Alias for torch.vstack()?,Alias of torch.vstack(),5350
13478,What does it do to split input into multiple tensors horizontally?,Stack tensors,6030
13479,Stack tensors in sequence how?,horizontally,6064
13480,What type of tensor indexes the input tensor?,1-D,6064
13481,What kind of version of the input tensor is the new tensor?,narrowed,6064
13482,What is used to move the dimension(s) of input at the position(s) in source to the position(s) in destination?,Alias,5349
13483,Returns a new tensor that indexes the input tensor according to what dimension?,1-D,5349
13484,What does torch.movedim() return?,Alias,5330
13485,What is the alias for torch.movedim()?,torch.vstack(),5330
13486,What is the out-of-place version of torch.vstack()?,torch.Tensor.scatter_(),5330
13487,What does torch.movedim() return for a new tensor that is a narrowed version of input tens,Alias,1031
13488,What is the name of the Alias for torch.movedim()?,torch.vstack(),1031
13489,What is the name of the Alias that returns a new tensor that is a narrowed version of input tens,torch.movedim(),1031
13490,What does torch split the tensor into?,chunks,5468
13491,What does the tensor return with?,the specified shape,5468
13492,What is the size of the input tensor?,size 1,5468
13493,What version of torch.Tensor.scatter_() splits the tensor into chunks?,Out-of-place,5344
13494,Sets the seed for generating random numbers to what type of random number?,non-deterministic,5859
13495,What does the torch.ByteTensor set?,the random number generator state,5859
13496,What does the torch.ByteTensor do?,Draws binary random numbers (0 or 1) from a Bernoulli distribution,5573
13497,What does each row of a tensor contain?,num_samples indices,5663
13498,What indices does each row of a tensor contain?,num_samples,5662
13499,What does it do to a Bernoulli distribution?,Draws binary random numbers,5854
13500,Returns a tensor where each row contains what indices sampled from the multinomial probability distribution located in the corresponding,num_samples,5461
13501,The tensor of the same size as input has each element sampled from what distribution?,Poisson,5461
13502,The tensor of the same size as input is sampled from what distribution with rate parameter given by the corresponding element in input?,Poisson,5444
13503,What is returned with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input?,a tensor of the same size as input,5452
13504,What kind of distribution is the tensor filled with random numbers from?,uniform,5425
13505,Random integers are generated uniformly between what two levels?,low (inclusive) and high (exclusive),5425
13506,What is returned when a tensor is filled with random numbers from a uniform distribution on the interval?,a tensor,5425
13507,Random integers are generated uniformly between what two intervals?,low (inclusive) and high (exclusive),5426
13508,What is filled with random numbers from a uniform distribution on the interval?,a tensor,5426
13509,What is returned when a tensor is filled with random integers from a uniform distribution on the interval?,a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive),5483
13510,What is returned with the same size as input that is filled with random numbers from a uniform distribution on the interval?,a tensor,5482
13511,"What is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.lay",torch.rand_like,5482
13512,What is the range of the random permutation of integers in a tensor?,0 to n,5474
13513,What returns a random permutation of integers from 0 to n - 1?,a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.,5421
13514,What is another name for a tensor filled with random numbers from a normal distribution with mean 0 and variance 1?,standard normal distribution,5421
13515,What is the size of a tensor that is filled with random numbers from a normal distribution with mean 0 and variance 1?,same size,5421
13516,What is the range of integers in a tensor?,0 to n,5422
13517,Returns a random permutation of integers from what range?,0 to n - 1,5422
13518,Where are the random numbers sampled from?,the discrete uniform distribution,10958
13519,What is the name of the distribution torch?,exponential,10961
13520,To what does torch.save() save an object?,a disk file,5744
13521,To what type of file does torch.save() save an object?,disk,5744
13522,Loads an object saved with what function from a file?,torch.save(),5744
13523,What context managers are helpful for locally disabling and enabling gradient computation?,"torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled()",7009
13524,"What is a more detailed description of the context managers torch.no_grad(), torch.enable_grad(), and torch.set",Locally disabling gradient computation,7009
13525,"What type of context managers are torch.no_grad(), torch.enable_grad(), and torch.set_grad_enable",thread local,7009
13526,What context manager enables gradient calculation?,Context-manager,7009
13527,What context manager disables gradient calculation?,Context-manager,7009
13528,Which context manager disables gradient calculation?,Context-manager,7008
13529,"What is another name for the context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_",Locally disabling gradient computation,7008
13530,What is the name of the tensor returned by Alias for torch.acos()?,inverse hyperbolic cosine,1008
13531,What is the element-wise division of tensor1 by?,tensor2,1005
13532,What is the inverse cosine of the elements of input?,inverse hyperbolic cosine,1005
13533,What does Alias for torch.acosh do?,Adds the scalar other to each element of the input input,5386
13534,What does Alias for torch.asin() return of the elements of input?,inverse hyperbolic sine,5386
13535,Computes what of each element in input?,inverse cosine,1771
13536,What does the Alias for torch.acosh() do?,Adds the scalar other to each element of the input input,1770
13537,What type of cosine is the inverse of the elements of input?,hyperbolic,5385
13538,What does torch.acosh do?,Adds the scalar other to each element of the input input,5385
13539,What is the name of the inverse hyperbolic cosine of the elements of input?,Alias,5385
13540,What value does the element-wise multiplication of tensor1 by tensor2 multiply the result by?,scalar,4813
13541,What does the element-wise multiplication of tensor1 by tensor2 multiply the result by?,scalar value,4813
13542,Computes the element-wise angle of the given input tensor in what units?,radians,4813
13543,What is the name for torch.asin()?,Alias,4813
13544,What does torch.acosh() do?,Adds the scalar other to each element of the input input,1011
13545,A new tensor is returned with what of the elements of input?,arcsine,1011
13546,What element of the elements of input returns a new tensor?,cosine,1022
13547,What is the element-wise division of tensor1 by tensor2?,multiplication,968
13548,What element of the elements of input does Alias for torch.asinh() return a new tensor with?,arctangent,1017
13549,What value does the multiplication of tensor1 by tensor2 multiply the result by?,scalar,4825
13550,What element of input does Alias for torch.asinh() return a new tensor with?,arctangent,1732
13551,What tangent does Alias for torch.atan return?,inverse hyperbolic tangent,5369
13552,What type of sine does Alias for torch.asin() return?,inverse hyperbolic,5369
13553,What element of input does Alias for torch.asinh() return?,arctangent,5369
13554,What element-wise of inputi/otheritextinput_i / textother_i,arctangent,1016
13555,What Alias for returns a new tensor with the inverse hyperbolic tangent of the elements of input?,torch.atan(),1016
13556,What is the name of the Alias that returns a new tensor with the inverse hyperbolic sine of the elements of input,torch.atanh(),5394
13557,What does it do to the given input tensor?,Computes the bitwise NOT,5394
13558,What does the Alias for torch.atan() compute?,bitwise OR of input and other,1020
13559,Tests if all elements in input evaluate to what?,True,5640
13560,What does the p-norm of (input - other) return?,the minimum value of all elements in the input tensor,5640
13561,Returns what value of each slice of the input tensor in the given dimension(s) dim?,minimum value,5640
13562,Returns what value of the input tensor in the given dimension(s) dim?,the maximum value of each slice,5611
13563,What does the test return?,the maximum value of all elements in the input tensor,5556
13564,What returns the maximum value of each slice of the input tensor in the given dimension(s) dim?,the indices of the minimum value(s) of the flattened tensor or along a dimension,5556
13565,Where is the maximum value of each slice of the input tensor?,the given dimension(s) dim,5610
13566,What does return the maximum value of each slice of the input tensor in the given dimension(s) dim?,the minimum value of each slice of the input tensor in the given dimension(s) dim,5610
13567,What is returned when the input tensor evaluates to True?,the maximum value of all elements in the input tensor,5610
13568,Returns the maximum value of all elements in the input tensor. Returns the minimum value of all elements in what tensor?,input tensor,5610
13569,Returns the what of (input - other)?,p-norm,5610
13570,What is the value of (input - other)?,p-norm,10868
13571,What is the value of the log of summed exponentials of each row of the input tensor in the given dimension dim?,p-norm,5604
13572,What is the value of all elements in the input tensor?,mean value,5604
13573,Returns what of (input - other)?,the p-norm,5649
13574,"Returns the median of the values in input, ignoring what values?",,5557
13575,What does the sum of all elements return?,the product of all elements in the input tensor,5625
13576,What does torch.gt() do?,Computes,1664
13577,Returns a new tensor with boolean elements representing if each element is what?,finite,5363
13578,Tests if each element of input is what or not?,positive infinity,5363
13579,What does torch.le() do?,Computes,5363
13580,Tests if each element of input is what?,negative infinity,6772
13581,What happens if each element of input is negative infinity or not?,Tests,5362
13582,Returns a new tensor with what representation if each element of input is close to the corresponding element of another?,boolean elements,5362
13583,Returns a new tensor with what representation if each element of input is close to the corresponding element of other?,boolean elements,5362
13584,What does torch.gt() use?,Alias,1029
13585,What does Alias for torch.gt() do if each element of input is negative infinity or not?,Tests,1029
13586,What element of input is returned if the new tensor is finite or not?,,5358
13587,What is the infinity of each element of input?,negative,5358
13588,What type of infinity does each element of input have?,negative,6774
13589,Returns a new tensor with what representation if each element of input is NaN or not?,boolean elements,5360
13590,What is the name of the window function that computes the Kaiser window?,Hann,5884
13591,What two parameters does the Kaiser window have?,window_length and shape parameter beta,5884
13592,Computes what with window length and shape parameter beta?,the Kaiser window,5884
13593,What is the function that returns the frequency of each value in an array of non-negative ints?,Count the frequency of each value in an array of non-negative ints,5262
13594,What is the function that returns a 3-dimensional view of each input tensor with zero dimensions?,Count the frequency of each value in an array of non-negative ints,5276
13595,Returns what type of input?,copy,5276
13596,Returns what of each input tensor with zero dimensions?,a 3-dimensional view,5275
13597,What does broadcast_tensors() use instead of broadcast_tensors()?,shapes,5275
13598,Count the value in an array of non-negative ints?,frequency,1928
13599,What type of input is broadcast_tensors() used for?,shapes,1928
13600,What is the name of the sequence of tensors?,Do cartesian product,1460
13601,What does batched the p-norm distance between each pair of the two collections of row vectors do?,Computes,5538
13602,What is the function that returns a copy of input?,Compute combinations of length rrr of the given tensor,5538
13603,What type of object is broadcast_tensors() used for?,shapes,5905
13604,Compute combinations of what of the given tensor?,length rrr,5905
13605,Returns the cross product of vectors in what ension dim of input?,dim,5905
13606,What is returned when calculating the cross product of vectors in dimension dim of input and other?,the cross product of vectors in dimension dim of input and other,5537
13607,What does the given sequence of tensors do?,Do cartesian product,2170
13608,Computes batched what between each pair of the two collections of row vectors?,p-norm distance,2170
13609,What does Compute of the given tensor do?,Compute combinations of length rrr,2170
13610,What is the cumulative maximum of elements of input in the dimension dim?,a namedtuple,1657
13611,What is the cumulative minimum of elements of input in the dimension dim?,a namedtuple,1657
13612,What is the product of vectors in dimension dim of input and other?,cross product,1657
13613,Computes batched the distance between each pair of the two collections of row vectors?,p-norm,1656
13614,Computes what combinations of length rrr of the given tensor?,combinations of length rrr of the given tensor,1656
13615,What is returned when calculating combinations of length rrr of the given tensor?,the cross product of vectors in dimension dim of input and other,1634
13616,Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim.,values,5514
13617,What does Returns a namedtuple return?,the cumulative product of elements of input in the dimension dim,5514
13618,Returns what in the dimension dim?,the cumulative sum of elements of input,5514
13619,What reduces the number of steps required to perform a batch matrix-matrix product of matrices stored in batch1 and batch2,add step,4767
13620,Performs a matrix multiplication of which matrices?,mat1 and mat2,4767
13621,What product of the matrix mat and the vector vec is performed?,matrix-vector product,4790
13622,Performs what of the matrices mat1 and mat2?,matrix multiplication,4790
13623,What does the matrix-vector product of the matrix mat and the vector vec perform?,outer-product of vectors vec1 and vec2,4766
13624,What happens when a reduced add step is performed?,all matrix multiplications get accumulated along the first dimension,4766
13625,What product of vectors vec1 and vec2 is added to the matrix input?,outer-product,4834
13626,What is the name of the low-level function that computes the dot product for 1D tensors?,Alias of torch.outer(),4834
13627,What does the batch matrix-matrix product consist of?,matrices stored in input and mat2,4835
13628,Performs what of vectors vec1 and vec2 and adds it to the matrix input?,outer-product,4835
13629,What product is added to the matrix input?,outer-product of vectors vec1 and vec2,4789
13630,What performs a batch matrix-matrix product of matrices in batch1 and batch2?,batch matrix-matrix product of matrices,4788
13631,Performs a matrix-vector product of what?,matrix mat and the vector vec,4798
13632,What is the Alias of?,torch.outer,4798
13633,What product of matrices in batch1 and batch2 performs?,batch matrix-matrix,4757
13634,Who computes the decomposition of a symmetric positive-definite matrix AAA?,Cholesky,4833
13635,Performs the outer-product of which vectors and adds it to the matrix input?,vec1 and vec2,4833
13636,What product of matrices in batch1 and batch2 is performed?,batch matrix-matrix,4833
13637,Computes the decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices?,Cholesky,4773
13638,In what two batches is the batch matrix-matrix product of matrices performed?,batch1 and batch2,4756
13639,Computes what decomposition of a symmetric positive-definite matrix AAA?,Cholesky,4756
13640,What is used to compute the inverse of a symmetric positive-definite matrix AAA?,LAPACK routines,1776
13641,What does the Cholesky factor uuu do?,returns matrix inv,1778
13642,What does a linear system of equations with a positive semidefinite matrix need to be inverted given?,Cholesky factor matrix uuu,1778
13643,What does torch.outer compute the dot product for?,1D tensors,1778
13644,What does the Cholesky factor matrix uuu compute?,dot product of two 1D tensors,5595
13645,Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu?,a linear system of equations,1671
13646,What does the Cholesky factor uuuuuuuuuuuuuuuuuuuuuuu,the dot product of two 1D tensors,1670
13647,What does the Cholesky factor uuu compute?,dot product of two 1D tensors,1670
13648,What type of matrix does it compute the eigenvalues and eigenvectors of?,real square matrix,1670
13649,What does torch.outer() compute the dot product for?,1D tensors,1717
13650,What does torch.linalg.det() calculate?,log determinant,1717
13651,What does linalg calculate of a square matrix or batches of square matrices?,log determinant,1704
13652,Computes the solution to what problems for a full rank matrix AAA of size (mn)(m times n),least squares and least norm,7602
13653,Computes what of a matrix or batches of matrices A?,LU factorization,1681
13654,Where are PyTorch casting rules described?,type promotion documentation,5696
13655,Sets whether PyTorch operations must use what algorithms?,deterministic,5696
13656,What does PyTorch return with the smallest size and scalar kind?,the torch.dtype,5696
13657,What kind of type does the torch.dtype return?,scalar,5695
13658,Sets whether PyTorch operations must use what kind of algorithms?,deterministic,5695
13659,What is the name of the type that would result from performing an arithmetic operation on the provided input tensors?,torch.dtype,5675
13660,What kind of type returns the torch.dtype with the smallest size?,scalar,5675
13661,"If the global deterministic flag is turned on, what is returned?",True,2120
13662,What does torch.dtype return?,smallest size and scalar kind,5676
13663,What does the torch.dtype do?,Sets whether PyTorch operations must use “deterministic” algorithms,5676
13664,A wrapper around what program's assert that is symbolically traceable?,Python,5676
13665,What is returned with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2?,the torch.dtype,5676
13666,What is set by the torch.dtype with the smallest size and scalar kind?,whether PyTorch operations must use “deterministic” algorithms,5676
13667,What is the current status of FX?,Beta release,7506
13668,FX is a toolkit for developers to use to transform what?,nn.Module instances,7507
13669,How many main components does FX consist of?,three,7507
13670,What release is FX currently in?,Beta,7507
13671,What are the three main components of FX?,"symbolic tracer, an intermediate representation, and Python code generation",7507
13672,What performs symbolic execution of the Python code?,symbolic tracer,7507
13673,What does the symbolic tracer feed through the code?,fake values,7507
13674,Where can more information about symbolic tracing be found?,symbolic_trace() and Tracer documentation,7507
13675,What is a toolkit for developers to use to transform nn.Module instances?,FX,7507
13676,What is recorded on Proxies?,Operations,7507
13677,What is the container for the operations that were recorded during symbolic tracing?,intermediate representation,7507
13678,What is the format on which transformations are applied?,The IR,7507
13679,What makes FX a Python-to-Python transformation toolkit?,Python code generation,7507
13680,What is what makes FX a Python-to-Python transformation toolkit?,Python code generation,7507
13681,What is a demonstration of the components in action?,demonstration,7505
13682,FX is a toolkit for developers to use to do what?,transform nn.Module instances,7505
13683,What are the operations recorded on?,Proxies,7315
13684,What does the symbolic tracer perform?,symbolic execution,7315
13685,The intermediate representation consists of a list of what?,Nodes,7315
13686,Where can you find more information about symbolic tracing?,symbolic_trace() and Tracer documentation,7316
13687,What does the symbolic tracer feed through the Python code?,fake values,7314
13688,What is recorded on the Proxies?,Operations,7314
13689,Where can you find more information about the IR?,documentation for Graph,7149
13690,"For each Graph IR, we can create valid Python code matching what?",Graph’s semantics,5007
13691,What is GraphModule?,nn.Module,2739
13692,What does Graphmodule have?,"a graph attribute, as well as code and forward attributes generated from that graph",2739
13693,What is the name of the GraphModule?,root’s original name or a name that makes sense within the context of your transform,2739
13694,What is a torch.nn.Module instance that holds a Graph and a forward method generated from the Graph?,GraphModule,5008
13695,Where can you find examples of transformations?,examples repository,5876
13696,What is a function that looks like this?,FX transform,5877
13697,What is an FX transform?,function that looks like this,5877
13698,What will your transform acquire from the torch.nn.Module?,a Graph,8686
13699,Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for what?,composability,8686
13700,What must you call to bring the generated forward() method on the GraphModule in sync with the modified Graph?,GraphModule.recompile(),8686
13701,How many primary approaches can you take to building a new Graph?,two,8686
13702,What can you pass your transform to?,TorchScript,8685
13703,What should you do to ensure that the inputs and outputs of your FX transform are a torch?,Note,8685
13704,What does your transform acquire from the torch.nn.Module?,Graph,8685
13705,What can you pass your FX transform to?,TorchScript,8685
13706,What has been traced into a Graph?,torch.nn.Module,2722
13707,What are we going to cover here?,the basics,2722
13708,A Graph represents a method on what?,GraphModule,2722
13709,What are the inputs to a Graph?,operations that run inside the method,2722
13710,What runs inside a GraphModule?,operations,2722
13711,What.nn.Module has been traced into a Graph?,torch,3925
13712,What information does a Graph need to represent a method on a GraphModule?,What are the inputs to the method,2625
13713,Where can you find full treatment of the semantics of graphs?,the Graph documentation,2625
13714,What is a Graph?,a data structure that represents a method on a GraphModule,2723
13715,Where can the full treatment of the semantics of graphs be found?,the Graph documentation,2624
13716,What does a Graph represent a method on?,GraphModule,2624
13717,What is the information that a Graph requires?,What are the inputs to the method,2624
13718,What is the output value from a method?,return,2624
13719,What is an example of a graph?,a short example,2624
13720,What are the operations that run inside a GraphModule?,operations,3926
13721,What is the return value from a method?,output,3926
13722,What are all three concepts represented with?,Node instances,1093
13723,What are the operations that run inside the method?,inputs,8326
13724,What are the inputs to the method?,inputs to the method,10305
13725,"In FX, method inputs are specified via what?",special placeholder nodes,10305
13726,What is an example of a Graph rewrite?,deleting or appending nodes,10305
13727,What is the return value from the method?,output,8328
13728,What is the output value from the method?,return,8328
13729,What method is used to print out a table showing the nodes of the module?,Graph.print_tabular(),8328
13730,What are the operations that run inside a method?,operations,8327
13731,What method is used to print out a table showing the nodes of the module MyModule?,Graph.print_tabular(),2792
13732,What is the placeholder x x?,args kwargs,8888
13733,What are kwargs placeholders?,args,8888
13734,What is target args kwargs placeholder x x?,opcode name,10304
13735,What is the placeholder for the target args kwargs?,x x,9961
13736,Name target args kwargs placeholder what?,x x,9962
13737,What is the target args kwargs placeholder?,x x,10800
13738,What is the kwargs placeholder?,x x,9763
13739,What is the name of the placeholder x x?,kwargs,9763
13740,What is linear_1 linear?,call_module,70
13741,What type of _weight is x?,linear,70
13742,What is the placeholder?,x x,10425
13743,What is get_attr?,linear,11424
13744,What does get_attr linear_weight stand for?,linear.weight,11474
13745,What does linear.weight stand for?,linear_weight,9800
13746,"What type of.weight (x, linear_weight)  call_module linear_1 linear (add_1)  call_",linear,9795
13747,What is linear_1 linear (add_1)?,call_module,9009
13748,What is the name of the built-in function add?,add_1,8815
13749,What is the name of the call_function?,topk_1,69
13750,What is the name of the add_1 function?,call_module linear_1 linear,189
13751,What is the name of the call_module linear_1 linear?,add_1,189
13752,What is the name of the add_1 module?,call_module linear_1 linear,11473
13753,What is another name for linear_1 linear?,add_1,9797
13754,What is another name for add_1?,call_module linear_1 linear,9008
13755,What is another name for call_module linear_1 linear?,add_1,9008
13756,What is the name of the built-in method sum...> (relu_1)?,call_function sum_1,67
13757,What is the term for add_1?,linear,9791
13758,What are the inputs to a method?,inputs,10426
13759,How are method inputs specified in FX?,special placeholder nodes,10571
13760,What does relu stand for?,call_function sum_1,10571
13761,"In this case, we have a single placeholder node with a target of what?",x,10577
13762,What is a built-in method sum...> (relu_1)?,call_function sum_1,9793
13763,What is the name of the call_method?,call_method relu_1 relu,9005
13764,What is the name of the function that is built-in method sum...> (relu_1)?,call_function sum_1,9792
13765,What is the name of the built-in method?,call_function topk_1,11471
13766,What does 'dim' mean?,call_function topk_1,9002
13767,What is built-in method sum...> (relu_1) ‘dim’: -1 call_function topk,sum_1,10784
13768,What is the ‘dim’ of a built-in method?,-1,191
13769,What is one way to build a new Graph?,symbolic tracing,4649
13770,What calls do we want to replace torch.add() calls with?,torch.mul(),4649
13771,How can we help build a new Graph?,simply take the Graph we obtain from symbolic tracing and modify it,4651
13772,What do we want to replace torch.add() calls with?,torch.mul() calls,4651
13773,What does FX have for transforming the graph?,utility functions,4651
13774,Who has utility functions for transforming the graph that can be found in the Graph documentation?,FX,4650
13775,"For simple transformations that only consist of substitutions, what can you make use of?",subgraph rewriter,4650
13776,What are some more involved Graph rewrites?,deleting or appending nodes,4650
13777,What is an example of using the APIs to append a call to a graph?,torch.relu(),8242
13778,What API can be used to append a call to a graph?,torch.relu(),8243
13779,What can you use for simple transformations that only consist of substitutions?,subgraph rewriter,2571
13780,Subgraph rewriter can be used for simple transformations that only consist of what?,substitutions,2571
13781,How many op Conv/Batch Norm fusions are there?,one,5155
13782,What is the basic usage of the Quantization Invert Transformation?,Basic usage,5155
13783,What is the usage of Quantization Invert Transformation?,Basic,5155
13784,How many op Conv/Batch Norm fusion replace_pattern?,one,5155
13785,What will capture the operations that are performed on them and append them to the Graph?,Proxy objects,1144
13786,What is an example of using for Graph manipulation?,Proxys,1144
13787,What can using Proxys do for transformations that require a large amount of rewrite rules?,improve readability and maintainability of the rules,1144
13788,What will the Proxy objects append the operations to?,Graph,7926
13789,What are the Proxy objects called?,arugments,7926
13790,Proxy objects will capture the operations performed on them and append them to what?,the Graph,7926
13791,Proxys allows you to specify your rewrite rules as native Python code?,avoiding explicit graph manipulation,3695
13792,What are two examples of a large amount of rewrite rules?,vmap or grad,3695
13793,Proxys allows you to specify your rewrite rules as native what?,Python code,3695
13794,What is a working example of using for Graph manipulation?,Proxys,3695
13795,What is a useful code organizational pattern in FX?,loop over all the Nodes in a Graph and execute them,880
13796,What can a loop over all the Nodes in a Graph be used for?,runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys,880
13797,How can we generate a new Graph?,feeding Proxy values through an interpreter,880
13798,What class is provided to encompass this pattern?,Transformer,880
13799,What class behaves similarly to Interpreter?,Transformer,880
13800,What would we run and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime?,GraphModule,878
13801,What might a GraphModule record the torch shape and dtype properties on the nodes as we see them at runtime look like,look like:,878
13802,What is a full interpreter for?,FX,1320
13803,What is the name of the class that can be overridden via method overrides?,Interpreter class,1320
13804,How can certain aspects of the interpreter's execution be overridden?,method overrides,879
13805,What is not that complicated but can be very useful?,a full interpreter,879
13806,How can certain aspects of the interpreter’s execution be overridden?,method overrides,1321
13807,What do we feed through an interpreter to generate a new Graph?,Proxy values,1321
13808,What is provided to encompass the pattern of generating a new Graph by feeding Proxy values through an interpreter?,Transformer class,3696
13809,How does Transformer compare to Interpreter?,Transformer behaves similarly to Interpreter,3696
13810,What is the name of the class that can be used to generate a new Graph?,Shape Propagation,3696
13811,What does Transformer.transform() do?,Shape Propagation,3696
13812,What is the name of the tool that is used to measure propagation?,Shape Propagation Performance Profiler,5879
13813,What does Shape Propagation Performance Profiler stand for?,Shape Propagation Performance Profiler,5879
13814,What is a good way to make sure your code is correct?,authoring transformations,4582
13815,What might we need to do in the course of authoring transformations when our code is not quite right?,some debugging,4582
13816,What is the first step in debugging?,inspect and debug the generated code,4582
13817,What is the next step in debugging?,debug the process of transformations that led to the generated code,4582
13818,"If you're not familiar with debuggers, please see what section?",auxiliary section Available Debuggers,4582
13819,"In the course of authoring transformations, what will often not be quite right?",our code,4582
13820,"In this case, we may need to do what?",debugging,4582
13821,What is the key to debugging?,work backwards,4582
13822,What is the first step in working backwards?,inspect and debug,4582
13823,What is the process of transformations that led to the generated code?,debug,4582
13824,"If you’re not familiar with debuggers, please see what auxiliary section?",Available Debuggers,4582
13825,What is the auxiliary section for debuggers?,Available Debuggers,4584
13826,What does the output of most deep learning modules consist of?,floating point torch,1424
13827,"To motivate this, let’s use what?",an example,4583
13828,What is the first tool in our toolbox to check?,if transformed modules are behaving as we expect compared to a reference implementation,7687
13829,FX generates the forward() function on what?,GraphModules,7686
13830,What is the first tool in our toolbox to do?,check if transformed modules are behaving as we expect,7686
13831,What can we do with the generated code?,debugging,7686
13832,"If you want to run the same code multiple times, it can be a bit tedious to step to the right code with pdb.",multiple times,7686
13833,What can we use for debugging the generated code?,several techniques,2799
13834,What traditional debugging techniques are not as straightforward?,print statements or pdb,7685
13835,Where does FX generate the forward() function?,GraphModules,1416
13836,How many techniques can we use for debugging the generated code?,several,1416
13837,"The code that represents the Graph is not in any source file, but we can step into it manually using what?",pdb,1416
13838,Invoke what to step into the running program?,pdb,1416
13839,What is one way to step into the right code with pdb?,copy-paste,1416
13840,Invoke pdb to do what?,step into the running program,3863
13841,"The code that represents the graph is not in any source file, but we can step into it manually using what when the forward pass is invoked?",pdb,3863
13842,What is invoked to step into the running program?,pdb,3863
13843,"When the forward pass is invoked, what is used to step into the running program?",pdb,3863
13844,What generates the forward() function on GraphModules?,FX,1414
13845,What is used to step into the Graph when the forward pass is invoked?,pdb,1415
13846,What is the easiest way to step into the right code with pdb?,copy-paste,3864
13847,What is used to step into the running program?,pdb,3864
13848,What can be a bit tedious to step to the right code with if you want to run the same code multiple times?,pdb,3591
13849,What is one way to examine the generated forward pass into your code?,copy-paste,3591
13850,"If you want to run the same code multiple times, it can be a bit tedious to do what with pdb?",step to the right code,3591
13851,What is one way to step to the right code with pdb?,copy-paste the generated forward pass into your code and examine it from there,3594
13852,What method can be used to print different attributes of the Nodes in the Graph?,print_tabular,3594
13853,What is a method in GraphModule that allows you to dump out the generated FX code to a folder?,GraphModule.to_folder(),2740
13854,What is the name of the method that allows you to examine modules and parameters?,to_folder,2740
13855,Where can we look at the code within the generated code?,foo/module.py,2740
13856,What can be used to debug the generated code?,pdb,2740
13857,GraphModule.to_folder() can be used to examine modules and parameters using what method?,to_folder,2740
13858,Where can we modify the code to debug the generated code?,foo/module.py,987
13859,What can we use to debug the generated code?,pdb,2741
13860,What is the next step when a transformation is creating incorrect code?,debug,2741
13861,What can a simple visual comparison trace down?,a bug,2741
13862,What do we break on a pdb session to see what goes wrong?,transform_graph(traced),2741
13863,Where can we look at the code to debug the generated code?,foo/module.py,985
13864,Where can we look at the code?,foo/module.py,985
13865,What type of statements can be added to foo/module.py?,print statements,985
13866,What can be added to the code to debug the generated code?,print statements,3593
13867,What is the first step in identifying a transformation that is creating incorrect code?,debug,4540
13868,What section of the documentation will we check first?,Limitations of Symbolic Tracing,4540
13869,What transformation is the goal of debugging?,GraphModule,4540
13870,What is enough to trace down a bug?,a simple visual comparison,4540
13871,What do we use to find what goes wrong?,a debugger,4540
13872,How can we see what's happening during the transform?,"breaking on transform_graph(traced), then pressing s to “step into” the call to transform_graph(traced).",4540
13873,What method can be edited to print different attributes of the Nodes in the Graph?,print_tabular,4540
13874,Where can we find a quick answer to this question?,Writing Transformations,4538
13875,"Once we verify what, the goal becomes figuring out what went wrong during our GraphModule transformation?",tracing is working as expected,4538
13876,What can we compare before and after we've applied our transformations?,traced Module,8118
13877,What is a good debugger for when it's not clear what's going wrong?,pdb,8118
13878,What debugger can be a good next step if it’s not clear what’s going wrong?,pdb,8118
13879,What is the code that can be used to trace down a bug?,consider the following code,8118
13880,What direction does the code in the example above go?,off,986
13881,What can we compare our traced Module before and after we’ve applied our transformations?,utility functions,8119
13882,What code showed us that there was an error in our transforms?,print(traced),8119
13883,What showed us that there was an error in our transforms?,call to print,8114
13884,What is the name of the call to print(traced)?,transform_graph(traced),8114
13885,What session does a debugger start?,pdb,8114
13886,What did the call to print(traced) show in our transforms?,an error,8114
13887,What session does the debugger start?,pdb,2727
13888,What did the call to traced show us that there was an error in our transforms?,print,8113
13889,What might we want to see in the print_tabular method?,input_nodes and users,8279
13890,What does the print_tabular method want to see?,input_nodes,8279
13891,What IDEs usually have a debugger built in?,PyCharm or VSCode,7186
13892,What is a built-in debugger?,graphical wrapper,7186
13893,What can you use in your IDE by pulling up a terminal window in your IDE?,pdb,3363
13894,What is the built-in debugger usually called?,graphical wrapper,3363
13895,What is a terminal window in VSCode called?,pdb,3363
13896,What does FX use to capture the semantics of programs in a transformable/analyzable form?,symbolic tracing,2391
13897,What is the real name of the program that is executed by symbolic tracing?,torch.nn.Module or function,2391
13898,"The data flowing through the program during this execution is not real data, but what?",symbols,2391
13899,What does symbolic tracing have?,some limitations,2391
13900,What type of flow does symbolic tracing not support?,dynamic control flow,2391
13901,What are two examples of dynamic control flow?,loops or if statements,2391
13902,What is symbolic tracing also known as?,symbolic execution,2392
13903,What is the data flowing through the program during the execution of a torch.nn.Module or function?,symbols,2392
13904,What type of control flow does symbolic tracing not support?,loops or if statements,2392
13905,What type of dynamic control flow does symbolic tracing not support?,loops,1112
13906,What is an example of a program that does not support dynamic control flow?,examine the following program,7164
13907,What is an example of a program that does not currently support dynamic control flow?,the following program,7164
13908,What does the condition depend on?,input values,7164
13909,What is the main limitation of symbolic tracing?,it does not currently support dynamic control flow,1113
13910,What does not currently support dynamic control flow?,symbolic tracing,7165
13911,What is the name of the program that walks back through your code to show you where this situation happens?,The traceback,7165
13912,What is static control flow?,loops,7165
13913,What is an example of a static control flow?,concrete example,7165
13914,The condition to the if statement relies on the value of what?,x.sum(),6998
13915,What can change if you pass a new input tensor to the traced function?,x,6998
13916,What walks back through your code to show you where this situation happens?,traceback,6998
13917,What is a dynamic control flow?,if you pass a new input tensor to the traced function,6998
13918,What is it called when a new input tensor is passed to the traced function?,dynamic control flow,6997
13919,What shows you where a dynamic control flow happens?,traceback,6997
13920,What is the name of the function that shows you where a situation happens?,traceback,6997
13921,What happens if you pass a new input tensor to the traced function?,x can change,2521
13922,How does the value of x change?,if you pass a new input tensor to the traced function,2521
13923,What is supported on the other hand?,static control flow,4631
13924,What is a static control flow?,if statements whose value cannot change across invocations,4631
13925,In what program does static control flow arise for code making decisions about a model's architecture based on hyper-parameters?,PyTorch programs,4631
13926,What does static control flow arise for code making decisions about a model's architecture based on?,hyper-parameters,4631
13927,What is the if-statement if self.do_activation?,static,4631
13928,What can do_activation be considered to be?,hyper-parameter,4631
13929,What is a valid pattern supported by?,symbolic tracing,4631
13930,"What does not depend on any function inputs, thus it is static?",if-statement if self.do_activation,4631
13931,What are many instances of dynamic control flow?,semantically static control flow,4631
13932,How can dynamic control flow be made to support symbolic tracing?,removing the data dependencies on input values,4631
13933,What type of example is a static control flow?,concrete,4630
13934,What is supported in PyTorch?,static control flow,4630
13935,What is supported by the if-statement if self.do_activation?,symbolic tracing,7109
13936,"What are torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_co",Tensor constructors,7109
13937,"In dynamic control flow, the sections of the program that contain code can be traced as calls to what?",the Method,3771
13938,What function can be used to trace sections of a dynamic control flow?,wrap(),3771
13939,What is a function that can be traced instead of tracing through them?,wrap(),3771
13940,What is the name of the method that can be traced in a dynamic control flow?,Customizing Tracing with the Tracer class,3772
13941,How can the behavior of tracing be customized?,subclassing Tracer,3772
13942,What is the class that underlies the implementation of symbolic_trace?,The Tracer class,3772
13943,What does FX use to intercept calls?,__torch_function__,2387
13944,What would we still like to capture builtin Python functions in?,symbolic tracing,2387
13945,What error tells us about the built-in function len?,the built-in function len is not supported,2387
13946,How can we make it so that functions like len are recorded in the trace as direct calls?,wrap() API,2387
13947,What is the mechanism by which FX intercepts calls?,__torch_function__,2387
13948,"Some functions are not covered by __torch_function__, but we would still like to capture them in symbolic tracing.",builtin Python functions or those in the math module,2387
13949,What built-in function is not supported by __torch_function__?,len,2387
13950,What API can we use to make it so that functions like len are recorded in the trace as direct calls?,wrap() API,2387
13951,What built-in function is not supported?,len,7048
13952,How can we make it so that functions like this are recorded in the trace as direct calls?,wrap() API,7048
13953,What API can be used to make it so that functions like len are recorded in the trace as direct calls?,wrap(),7048
13954,The Tracer class is the class that underlies the implementation of what?,symbolic_trace,6952
13955,The set of leaf modules can be customized by overriding what?,Tracer.is_leaf_module(),6952
13956,How can the set of leaf modules be customized?,overriding Tracer.is_leaf_module(),6952
13957,The error tells us that what is not supported?,built-in function len,7049
13958,What are the modules that appear as calls in the symbolic trace rather than being traced through?,Leaf Modules,4069
13959,What is the default set of leaf modules?,standard torch.nn module instances,4069
13960,What can be overridden to customize the set of leaf modules?,Tracer.is_leaf_module(),4069
13961,What is currently not traceable?,Tensor constructors,6189
13962,What can be wrapped in a torch.fx.wrap function?,torch.randn,6189
13963,When will the behavior of nondeterministic constructors be fixed?,in a future release,6189
13964,How are Python 3-style type annotations preserved?,symbolic tracing,6189
13965,What type annotations are not currently supported?,Python 2-style comment type annotations,6189
13966,Annotations on what are not currently supported?,local names,6189
13967,What can be used and the value they produce will be embedded in the trace as a constant?,deterministic constructors,7028
13968,What do the arguments to deterministic constructors refer to?,dynamic input sizes,7028
13969,What may be a viable substitute for deterministic constructors?,ones_like or zeros_like,7028
13970,When will this behavior be fixed?,future release,7028
13971,What type annotations are supported and will be preserved by symbolic tracing?,Python 3-style type annotations,7028
13972,What is not currently supported?,Annotations on local names within a function are not currently supported,7028
13973,"What are torch.zeros, torch.ones, torch.rand, torch.randn, and torch.sparse_",traceable,6188
13974,What do the arguments to constructors refer to?,dynamic input sizes,7027
13975,What will have a single random value embedded in the trace?,Nondeterministic constructors,4336
13976,Is this the intended behavior of nondeterministic constructors?,not,4336
13977,What is a workaround?,wrap torch.randn in a torch.fx.wrap function and call that instead,4336
13978,What type annotations will be preserved by symbolic tracing?,Python 3-style type annotations,4336
13979,What is used to preserve type annotations?,symbolic tracing,4336
13980,"Nondeterministic constructors (rand, randn) will have a single what value embedded in the trace?",random,4336
13981,What is the likely result of a single random value embedded in the trace?,not the intended behavior,4336
13982,What type annotations are supported?,Python 3-style type annotations,7274
13983,What will this function return if given an nn.Module or function instance root?,GraphModule,6168
13984,FX can't typically trace through data structures due to the presence of what?,control flow,6168
13985,What type of function can typically not trace through concrete_args because of the presence of control flow?,FX,6168
13986,What allows you to partially specialize your function?,concrete_args,9108
13987,What value can we use concrete_args to specialize on?,b,9108
13988,What allows you to specialize on the value of b to trace through?,concrete_args,2716
13989,What will a function return given an nn.Module or function instance root?,GraphModule,2716
13990,What can we use to specialize on the value of b to trace through?,concrete_args,2382
13991,FX can typically not trace through this because of the presence of what?,control flow,2382
13992,What can we use to specialize on the value of b to trace through this?,concrete_args,2382
13993,FX can't typically trace through a GraphModule because of the presence of what?,control flow,2715
13994,What type of function can typically not trace through a GraphModule because of the presence of control flow?,FX,2715
13995,What does concrete_args allow you to do with your function?,partially specialize,9107
13996,How can we use concrete_args to trace through control flow?,to specialize on the value of b,9107
13997,What type of function can't trace through control flow?,FX,9107
13998,"What can be passed in concrete_args, but they will be ignored?",different values of b,9107
13999,What does concrete_args do to specialize on the value of b?,f = fx.symbolic_trace,2381
14000,FX can't typically trace through this because of the presence of what?,control flow,2381
14001,FX can't trace through this because of the presence of what?,control flow,2381
14002,Why can't FX trace through this?,control flow,2539
14003,What does f =?,fx.symbolic_trace,9318
14004,What is the name of the function to be traced?,f = fx.symbolic_trace,9316
14005,What can we use to eliminate data-structure handling from our function?,concrete_args,4461
14006,"To avoid overspecializing, pass in what for values that shouldn't be specialized?",fx.PH,4461
14007,What does concrete_args use to flatten your input?,pytrees,9317
14008,What is the root of a module or function to be traced and converted into?,Graph representation,8245
14009,What eliminates data-structure handling from our function?,concrete_args,8245
14010,What should be passed in for values that shouldn’t be specialized?,fx.PH,8245
14011,Inputs to be what type of ly specialized?,partial,8245
14012,"What value can still be passed in, but they will be ignored?",b,4460
14013,Inputs to be partially specialized are called what?,concrete_args,4460
14014,What will use pytrees to flatten your input?,concrete_args,8246
14015,What does concrete_args use to flatten input?,pytrees,8246
14016,What should be passed in for values that shouldn't be specialized?,fx.PH,8246
14017,What does root convert into?,Graph representation,8246
14018,What will a “leaf function” be preserved as in the FX trace instead of being traced through?,CallFunction,8246
14019,What can a “leaf function” be used as?,a decorator,8246
14020,What happens when you pass in different values of b?,they will be ignored,4462
14021,What is the representation of a Module or function to be traced and converted into?,Graph,4462
14022,"To avoid overspecializing, pass in what for values that shouldn’t be specialized?",fx.PH,2717
14023,What is the representation of a module or function to be traced and converted into?,Graph,8247
14024,What is the name of the module to be partially specialized enable_cpatching?,concrete_args,10615
14025,What is the name of the module to be traced and converted into a Graph representation?,GraphModule,10615
14026,What is the name of the Module created from the recorded operations from root?,GraphModule,10615
14027,What enables C-level patching of functions?,enable_cpatching,9254
14028,What is a module created from the recorded operations from root?,a Module created from the recorded operations from root,9104
14029,Where can GraphModule be called?,module-level scope,9105
14030,What allows C-level patching of functions?,enable_cpatching,9105
14031,"What has a graph attribute, as well as code and forward attributes generated from that graph?",Graphmodule,9105
14032,"Concrete_args (Optional[Dict[str, any]]) – Inputs to be partially specialized what",enable_cpatching,10616
14033,What function can be called at module-level scope to register fn_or_name as a “leaf function”?,GraphModule,8755
14034,What is created from the recorded operations from root?,a Module,8755
14035,What can this function be used as?,a decorator,8754
14036,What is a Module created from?,a Module created from the recorded operations from root,8754
14037,"GraphModule This function can be called at what level to register fn_or_name as a ""leaf function""",module-level scope,8754
14038,A “leaf function” will be preserved as what node in the FX trace?,CallFunction,8754
14039,Where can a function be called to register fn_or_name as a “leaf function”?,module-level scope,7517
14040,What can a wrapped function be used as?,decorator,7513
14041,What is the function or name of the global function to insert into the graph when it’s called?,fn_or_name,7512
14042,What is the nn.Module generated from?,fx.Graph,9366
14043,What is the function or name of the global function to insert into the graph when it's called GraphModule?,nn.Module,9366
14044,What does a Graphmodule contain?,Warning,9365
14045,"When a graph is reassigned, code and forward will be what?",automatically regenerated,8207
14046,What does recompile() do?,Construct a GraphModule,8207
14047,"If you edit the contents of the graph without reassigning the graph attribute itself, you must call what to update the generated code?",recompile(),8207
14048,What is the name of the function that will update the generated code if you edit the contents of the graph without reassigning the graph attribute,recompile(),2738
14049,What is an nn.Module generated from an fx.Graph?,GraphModule,2738
14050,What attributes will be automatically regenerated when a graph is reassigned?,code and forward,2738
14051,What is another name for a GraphModule?,Construct a GraphModule,2738
14052,What do you need to construct to update the generated code?,GraphModule,8361
14053,What will be automatically regenerated when a graph is reassigned?,code and forward,8206
14054,"When a graph is regenerated, code and forward will be automatically regenerated?",reassigned,8360
14055,What type of module can you construct?,GraphModule,1828
14056,What purpose does class_name denote the name of this GraphModule for?,debugging,9485
14057,What does it do to the given submodule?,Adds the given submodule to self,9485
14058,"If unset, all error messages will report as originating from where?",GraphModule,9080
14059,What denotes the name of this GraphModule for debugging purposes?,class_name,9080
14060,"If class_name is unset, all error messages will report as originating from what?",GraphModule,9080
14061,What should the class_name be set to?,root’s original name or a name that makes sense within the context of your transform,9080
14062,What does self do to unused submodules?,Deletes all unused submodules,9080
14063,What is the original name of the GraphModule?,root,9367
14064,What does m stand for?,submodule,9367
14065,What does this install if they are subpaths of target?,empty Modules,7587
14066,What is the target of the new submodule?,fully-qualified string name,7587
14067,What does a Module have that are used?,children,7587
14068,What can this method be called to do?,clean up an nn.Module without manually calling delete_submodule on each unused submodule,7587
14069,How is a Module's forward called?,call_module node 3,7587
14070,When will the module not be deleted?,if target is not a valid target,7587
14071,What do we want to delete?,submodule,7587
14072,What method can be called to clean up an nn.Module without manually calling on each unused submodule?,delete_submodule,7587
14073,The generated code of this GraphModule will be what?,out of date,7587
14074,What return value means that the target was not a valid reference to a submodule?,False,7587
14075,What will report as originating from GraphModule if it's unset?,all error messages,10618
14076,"If the class_name is unset, all error messages will report as originating from where?",GraphModule,9079
14077,What is the full-qualified string name of the new submodule?,target,963
14078,What is the submodule itself?,the actual object we want to install in the current Module,963
14079,What is the name of the nn.Module?,bool,7585
14080,What does target stand for?,The fully-qualified string name of the new submodule,10804
14081,What is the name of the submodule itself?,m,1829
14082,What is the name of the fully-qualified string of the new submodule?,target,10805
14083,What does this method return?,True,9863
14084,What returns the Python code generated from the Graph underlying this GraphModule?,Return the Python code generated from the Graph underlying this GraphModule,5185
14085,How is the forward of a Module called?,call_module node 3,5185
14086,What is returned from the Graph underlying this GraphModule?,Python code,5185
14087,What returns the Graph underlying this GraphModule?,bool,5185
14088,What happens to the unused submodules?,Deletes all unused submodules from self,9862
14089,What does the Python code do to all unused submodules from self?,Deletes all unused submodules,9862
14090,What does bool return?,Graph,768
14091,"What is considered ""used"" if any of the following is true?",A Module,768
14092,What method can be called to clean up an nn.Module without manually calling?,delete_submodule,768
14093,What node is used to call a Module's forward?,call_module,768
14094,When will a module not be deleted?,if target is not a valid target,768
14095,What is the return value for each object in the chain denoted by target?,True,10908
14096,Python code generated from what underlying GraphModule?,Graph,10908
14097,A return value of what means that the target was not a valid reference to a submodule?,False,10908
14098,"To return True, each object in the chain must either a) not exist yet, or b) reference an what?",nn.Module,10907
14099,What does self do?,Deletes all unused submodules,10907
14100,What does delete_submodule do?,Deletes the given submodule from self,7586
14101,How is a Module considered to be used?,A Module is considered “used” if any one of the following is true,964
14102,Deletes the given submodule from self. Deletes the given submodule from self.,delete_submodule,8977
14103,What can this method be called to do without manually calling delete_submodule on each unused submodule?,clean up an nn.Module,10806
14104,Deletes the given submodule from self without manually calling what?,delete_submodule,2099
14105,What does delete all unused submodules from self?,Deletes all unused submodules from self,2099
14106,When should bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?,after editing the contained graph,2099
14107,How is the forwarding of a module called?,call_module node 3,2098
14108,What happens when a submodule is not a valid target?,Deletes the given submodule from self,2098
14109,What does delete_submodule do to a given submodule from self?,Deletes,2098
14110,When is the module not deleted?,if target is not a valid target,2098
14111,What method can be used to clean up an nn.Module without manually calling on each unused submodule?,delete_submodule,767
14112,What does delete_submodule do to a given submodule?,Deletes,7728
14113,A return value of what indicates that the target was not a valid reference to a submodule?,False,10774
14114,When should bool Return the Graph underlying this GraphModule be called?,after editing the contained graph,10774
14115,What does dump out module to folder with?,module_name,10774
14116,Dumps out module to folder with what so that it can be imported with from folder> import module_name>?,module_name,10774
14117,What is the name of the method that can be called to clean up an nn.Module without manually calling delete_submodule?,bool,7727
14118,What is the name of the submodule that is not a valid reference to a submodule?,bool,7727
14119,What will the module not be deleted?,if target is not a valid target,2102
14120,What happens to the given submodule from self?,Deletes,2102
14121,What is target?,The fully-qualified string name of the new submodule,7181
14122,Where is the given submodule deleted from?,self,5186
14123,What causes the module to not be deleted?,if target is not a valid target,7180
14124,What do we want to do with a submodule?,delete,10775
14125,What is the fully-qualified string name of the new submodule?,target,10803
14126,What Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?,bool,10802
14127,What is each chunk of the tensor?,a view of the original tensor,6038
14128,"If what is an integer type, tensor will be split into equally sized chunks?",split_size_or_sections,6038
14129,What will be smaller if the tensor size along the given dimension dim is not divisible by split_size?,Last chunk,3484
14130,"If split_size_or_sections is a list, tensor will be split into what chunks?",len,3484
14131,"If tensor is an integer type, then tensor will be split into equally sized chunks?",split_size_or_sections,3484
14132,What is another name for tensor to split?,Tensor,3484
14133,What does split the tensor into?,chunks,6039
14134,What is each chunk of a tensor?,a view of the original tensor,6039
14135,"If split_size_or_sections is an integer type, tensor will be split into what?",equally sized chunks,6039
14136,"If the tensor size along the given dimension dim is not divisible by split_size, what will make the last chunk smaller?",if the tensor size along the given dimension dim is not divisible by split_size,6039
14137,"If split_size_or_sections is what, then tensor will be split into len(split_size_or",a list,6039
14138,What is split_size_or_sections?,int,6039
14139,What will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_,tensor,3483
14140,What is an example of a tensor to split?,Example,3483
14141,Returns what with the signs of the elements of input?,a new tensor,5406
14142,What is the name of the extension created for C++?,setuptools,2020
14143,What is included in the setuptools.Extension for CUDA/C++?,"path, library path and runtime library",2020
14144,How many arguments does setuptools.Extension have to build a C++ extension?,bare minimum,1891
14145,What language does setuptools.Extension build for?,CUDA/C++,1891
14146,How many arguments does setuptools.Extension have to build a CUDA/C++ extension?,bare minimum,1891
14147,What creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension?,Convenience method,1891
14148,Where are all arguments forwarded to?,setuptools.Extension constructor,1894
14149,What are the CUDA include?,"path, library path and runtime library",1894
14150,"If a new card is installed, the extension may need to be what?",recompiled,1894
14151,What is the Convenience method that creates a setuptools.Extension for C++?,setuptools.Extension for C++,2021
14152,What creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C,Convenience method,2021
14153,Where are all arguments for a setuptools.Extension for CUDA/C++?,setuptools.Extension constructor,2021
14154,All arguments are forwarded to what?,setuptools.Extension constructor,1072
14155,What creates a setuptools.Extension with the bare minimum arguments to build a CUDA/C++ extension?,Convenience method,1072
14156,What is included in the setuptools.Extension?,"CUDA include path, library path and runtime library",1890
14157,What is included in the setuptools.Extension to build a CUDA/C++ extension?,"path, library path and runtime library",1890
14158,What language does the example create a setuptools.Extension for?,CUDA/C++,1889
14159,What extension is created by the convenience method that creates a setuptools.Extension?,setuptools.Extension for CUDA/C++,1889
14160,What is the name of the extension created for CUDA/C++?,setuptools,2284
14161,What does the example create for CUDA/C++?,setuptools.Extension,2284
14162,What is an example of a setuptools.Extension?,Compute capabilities,1073
14163,What are the bare minimum arguments to build a CUDA/C++ extension?,"CUDA include path, library path and runtime library",1073
14164,What is the name of the extension for CUDA/C++?,setuptools,1073
14165,What does the example create?,setuptools.Extension for CUDA/C++,1071
14166,What is an example of a setuptools.Extension constructor?,Compute capabilities,1069
14167,What is the name of the constructor used by setuptools?,Extension constructor,1069
14168,What is the name of the method that creates a setuptools.Extension?,Convenience,1892
14169,What is an example of a CUDA/C++ extension?,Compute capabilities,1892
14170,What is included in the Convenience method to build a CUDA/C++ extension?,"CUDA include path, library path and runtime library",1892
14171,What is the name of the extension constructor?,setuptools,2283
14172,What is the name of the method that creates a setuptools.Extension with the bare minimum arguments to build a CUDA,Convenience,2024
14173,Convenience method that creates a setuptools.Extension with the bare minimum (usually sufficient) arguments to build a CU,Creates a setuptools.Extension for CUDA/C++,2024
14174,What creates a setuptools.Extension?,Convenience method,1893
14175,What is an example of what?,Compute capabilities,2282
14176,What is one of the functions of a computer?,Compute capabilities,1632
14177,What are the capabilities of what?,Compute capabilities,1632
14178,"By default, the extension will be what to run on all archs of the cards visible during the building process of the extension?",compiled,1481
14179,What compute capability does a visible card have?,CC,1481
14180,What is a compute capability that is newer than the newest version for which nvcc can build fully-compiled binaries?,CC,1481
14181,"The extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus what else?",PTX,1070
14182,What can you use to override the default behavior?,TORCH_CUDA_ARCH_LIST,8585
14183,What is the default behavior of TORCH_CUDA_ARCH_LIST?,5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX,8585
14184,What does TORCH_CUDA_ARCH_LIST mean?,python build_my_extension.py,8585
14185,What do you want the extension to support?,CCs,8585
14186,What is the TORCH_CUDA_ARCH_LIST?,python build_my_extension.py,6175
14187,What does python build_my_extension.py contain?,python build_my_extension.py,6175
14188,What will happen if more archs are included?,the slower the building process,4500
14189,The setuptools.build_ext subclass takes care of passing the minimum required compiler flags as well as what?,mixed C++/CUDA compilation,4500
14190,What does the setuptools.build_ext subclass take care of passing?,mixed C++/CUDA compilation,7805
14191,What subclass takes care of passing the minimum required compiler flags?,setuptools.build_ext,7805
14192,"When using BuildExtension, it is allowed to supply a dictionary for what?",extra_compile_args,7805
14193,When is it possible to supply different flags to the C++ and CUDA compiler?,mixed compilation,7805
14194,What does setuptools.build_ext take care of passing?,minimum required compiler flags,783
14195,What does the setuptools.build_ext subclass take care of passing the minimum required compiler flags?,custom setuptools build extension,782
14196,What does the setuptools.build_ext subclass handle?,mixed C++/CUDA compilation,7804
14197,"When using BuildExtension, it is allowed to supply what for extra_compile_args?",a dictionary,8383
14198,What is the default name for the Ninja backend?,use_ninja,11321
14199,What greatly speeds up compilation compared to the standard setuptools.build_ext?,Ninja,11321
14200,How many workers does the Ninja backend use to build the extension?,#CPUS + 2 workers,11321
14201,What does the standard setuptools.build_ext use if Ninja is not available?,Fallbacks,11320
14202,What does Ninja greatly speed up compared to setuptools.build_ext?,compilation,11320
14203,How much resources does the Ninja backend use on some systems?,too many resources,1491
14204,What environment variable can be set to a non-negative number?,MAX_JOBS,1491
14205,What does the Ninja backend use to build the extension?,#CPUS + 2 workers,1491
14206,What does JIT stand for?,just-in-time,1491
14207,What is emitted to load an extension?,a Ninja build file,1491
14208,In what process is the library loaded?,Python,1491
14209,What does the Ninja backend do?,Loads a PyTorch C++ extension just-in-time (JIT),8384
14210,What does this use up on some systems?,too many resources,1490
14211,What is another name for just-in-time?,JIT,4130
14212,What extension is loaded just-in-time?,PyTorch C++,4130
14213,In what program is the library loaded?,Python,7905
14214,What is used to compile the sources?,the default system compiler,7871
14215,What can be provided to pass additional arguments to the compilation process?,extra_cflags or extra_ldflags,7871
14216,What can you use to pass further include directories?,extra_cflags,7871
14217,What can be set to override the default system compiler?,CXX environment variable,7871
14218,"To compile your extension with optimizations, pass what?",extra_cflags=['-O3'].,7871
14219,"What would you use extra_cflags=['-O3'""?",to compile your extension with optimizations,7870
14220,What is the name of the extension to build?,name,9964
14221,What must the name of the extension to build be the same as?,pybind11 module,9964
14222,What is sources?,A list of relative or absolute paths to C++ source files,9964
14223,Extra_cflags - optional list of what to forward to the build?,compiler flags,9964
14224,Name – The name of the extension to build must be the same as the name of what module?,pybind11,9964
14225,What is a list of relative or absolute paths to C++ source files?,sources,9964
14226,What does extra_cuda_cflags forward to when building CUDA sources?,nvcc,9964
14227,What do extra_cuda_cflags forward to when building CUDA sources?,nvcc,9308
14228,What does extra_ldflags list to forward to the build?,linker flags,9308
14229,What can extra_include_paths forward to the build?,include directories,9308
14230,What is an optional list of compiler flags to forward to the build?,extra_cflags,9308
14231,What does extra_cflags forward to the build?,compiler flags,9308
14232,What is an optional list of linker flags to forward to the build?,extra_ldflags,9312
14233,extra_include_paths – optional list of what to forward to the build?,include directories,9312
14234,What turns on verbose logging of load steps?,If True,9312
14235,"If True, turns on verbose logging of load steps.",verbose,9312
14236,Extra_ldflags - optional list of what to forward to the build?,linker flags,9312
14237,Extra_include_paths - optional list of what to forward to the build?,include directories,9312
14238,What is build_directory used for?,build workspace,9312
14239,What is sources a list of?,relative or absolute paths to C++ source files,10730
14240,What does extra_ldflags include to forward to the build?,linker flags,10730
14241,Sources – A list of what to C++ source files?,relative or absolute paths,10730
14242,What is an optional path to use as build workspace?,build_directory,10730
14243,What is the optional path to use as build workspace?,build_directory,9310
14244,What is an optional list of compiler flags to forward to nvcc when building CUDA sources?,extra_cuda_cflags,9310
14245,What does extra_cuda_cflags forward to nvcc when building CUDA sources?,compiler flags,9309
14246,What does extra_ldflags forward to the build?,linker flags,9309
14247,What does extra_include_paths provide to forward to the build?,include directories,9309
14248,Extra_include_paths – optional list of what to forward to the build?,include directories,9309
14249,"Is_python_module – If what, imports the produced shared library as a Python module?",If True,9716
14250,What happens if False?,build a standalone executable,9716
14251,The loaded PyTorch extension is returned as what?,Python module,9716
14252,What does the PyTorch extension return?,nothing,9716
14253,What is the shared library loaded into the process as?,side effect,9716
14254,"If False, behavior depends on what?",is_standalone,9717
14255,"If False, what load the constructed extension into the process as a plain dynamic library?",is_standalone,9717
14256,What is the name of the extension that is loaded into the process as a plain dynamic library?,is_standalone,9719
14257,What is the loaded PyTorch extension as?,Python module,9719
14258,What does is_python_module return?,nothing,9719
14259,What happens if is_standalone is True?,build a standalone executable,9720
14260,What is loaded into the process as a side effect?,The shared library,9720
14261,What is added to the PATH environment variable on Windows?,TORCH_LIB_PATH,9720
14262,If is_python_module is what?,True Example,9720
14263,What does the loaded PyTorch extension return as?,Python module,9720
14264,What does it return if is_python_module is True Example Loads a PyTorch extension just-,nothing,5583
14265,What is returned when a PyTorch extension is loaded as a Python module?,path to the executable,5583
14266,Is_python_module is what?,True Example,5583
14267,What is loaded as a Python module?,PyTorch extension,5583
14268,What does Returns the loaded PyTorch extension as a Python module?,nothing,5583
14269,On what platform is TORCH_LIB_PATH added as a side effect?,Windows,5583
14270,What does TORCH_LIB_PATH do?,Return the path to the executable,5199
14271,What does is_python_module load from string sources?,PyTorch C++ extension just-in-time (JIT),5199
14272,What does is_python_module behave exactly like?,load(),5199
14273,What is the name of the function that loads a PyTorch C++ extension just-in-time?,load_inline(),5199
14274,What is a good example of how to use load_inline()?,tests,5199
14275,What does is_python_module do?,True Example Loads a PyTorch C++ extension just-in-time,5199
14276,What is the difference between load() and load()?,takes its sources as strings rather than filenames,5199
14277,Where are the strings stored?,build directory,5199
14278,What are the tests for using this function?,good examples,5199
14279,What is different from torch.Tensor.repeat() but similar to numpy.repeat?,Repeat elements,5153
14280,What is torch.Tensor.repeat() similar to?,numpy.repeat,5153
14281,Why is repeats broadcasted?,to fit the shape of the given axis,5153
14282,What is the dimension along which to repeat values?,dim,5153
14283,What is the default way to return a flat output array?,flattened input array,5153
14284,What does repeats refer to?,The number of repetitions for each element,5153
14285,"By default, use the flattened input array, and return a what?",flat output array,5153
14286,What is similar to torch.Tensor.repeat()?,numpy.repeat,5153
14287,What is different from numpy.repeat?,torch.Tensor.repeat(),5153
14288,"If the repeats is tensor([0, 0,..., 1, 1,..., 2, 2,...,...]),",n1,5153
14289,What is broadcasted to fit the shape of the given axis?,repeats,9630
14290,"The repeated tensor has the same shape as input, except along what axis?",axis,9630
14291,Where is a repeated tensor different from the input tensor?,the given axis,9630
14292,What is an example of an input tensor?,Tensor,9630
14293,Where is a repeated tensor different from an input tensor?,the given axis,10584
14294,How are repeats broadcasted?,to fit the shape of the given axis,10584
14295,"By default, what is used to return a flat output array?",flattened input array,10584
14296,What is an example of a repeatable tensor?,Tensor,10584
14297,What does repeats stand for?,The number of repetitions for each element,10584
14298,What is another name for repeats?,Tensor or int,10584
14299,Where is the repeated tensor different from the input tensor?,axis,10584
14300,What is the name of the tensor that has the same shape as input?,Tensor,5152
14301,What is the name of the tensor that is different from numpy.repeat?,torch.Tensor.repeat(),5152
14302,Where does the repeated tensor differ from the input tensor?,axis,5152
14303,What is this different from?,torch.Tensor.repeat(),8198
14304,"By default, use what to return a flat output array?",flattened input array,9183
14305,What is different about a repeated tensor than an input tensor?,the given axis,9183
14306,How many times does 0 appear in a tensor?,n1 times,9183
14307,Along what axis does the tensor have the same shape as input?,axis,9183
14308,What happens when the scalar other is added to each element of the input input and returns a new resulting tensor?,Adds the scalar other to each element of the input input and returns a new resulting tensor,966
14309,If input is of type what must other be a real number?,FloatTensor or DoubleTensor,966
14310,"If input is of type what, other must be a real number, otherwise it should be an integer?",FloatTensor or DoubleTensor,966
14311,How is each element of the tensor other multiplied?,multiplied by the scalar alpha and added to each element of the tensor input,10332
14312,What is returned after each element of input is multiplied by the scalar alpha?,The resulting tensor,10332
14313,What is the first input tensor?,input,10332
14314,"What is the number to be added to each element of input out (Tensor, optional) – the output tensor?",other,10332
14315,What is the number to be added to each element of input out?,output tensor,10332
14316,What type of tensor must the alpha be a real number?,FloatTensor or DoubleTensor,10332
14317,Each element of the tensor other is multiplied by what?,scalar alpha,967
14318,What must alpha be if other is of type FloatTensor or DoubleTensor?,real number,967
14319,What is returned after each element of the tensor input is multiplied by the scalar alpha?,The resulting tensor,2308
14320,What is the second input tensor alpha?,the second input tensor alpha,2308
14321,What multiplier is added to each element of the tensor input?,scalar alpha,2308
14322,What type of input must be a real number?,FloatTensor or DoubleTensor,3426
14323,What must the shapes of input and other be?,broadcastable,3426
14324,The shapes of input and other must be what?,broadcastable,2249
14325,What must be a real number if other is of type FloatTensor or DoubleTensor?,alpha,10339
14326,"What is out (Tensor, optional)?",output tensor,10339
14327,What is multiplied by each element of the tensor other?,scalar alpha,10339
14328,What is the scalar multiplier for other?,the second input tensor alpha,10339
14329,What is returned after each element of the input tensor is multiplied by the scalar alpha?,The resulting tensor,9625
14330,"Out (Tensor, optional) – what is the output tensor?",output tensor,10341
14331,What returns the random number generator state as a torch?,ByteTensor,5661
14332,What is the Kronecker product denoted by?,otimes,1680
14333,What are the elements of a complex tensor?,Cartesian coordinates,1837
14334,What is the absolute value of the complex tensor?,abs,1837
14335,What must be the absolute value of the complex tensor?,float or double,1837
14336,What is the angle of the complex tensor?,angle,1837
14337,What must the angle of the complex tensor be?,same dtype,1837
14338,"Out (Tensor) – If the inputs are torch.float32, must be what?",torch.complex64,1837
14339,What must the inputs be if the inputs are torch.float64?,torch.complex128,1837
14340,What is an example of a complex tensor?,Example,1837
14341,What is the sum of exponentiations of the inputs in base-2?,Logarithm,4136
14342,What is the name of the function that calculates the logarithm of the sum of exponentiations of the inputs in base-2?,torch.logaddexp(),4136
14343,What is the sum of exponentiations of the inputs in base-2 called?,Logarithm,4136
14344,What is the name of the function that calculates pointwise logarithm?,torch.logaddexp(),4136
14345,What does it do to determine if PyTorch operations must use deterministic algorithms?,Sets whether PyTorch operations must use “deterministic” algorithms,5868
14346,What is a deterministic algorithm?,always produce the same output,5868
14347,"If only nondeterministic algorithms are available, what will PyTorch operations throw when called?",RuntimeError,5868
14348,What is torch.nn.Conv1d called on?,CUDA tensor,5868
14349,What operation will act deterministically when mode=True?,torch.nn.Conv1d,5868
14350,Sets whether PyTorch operations must use what?,PyTorch operations must use “deterministic” algorithms,5869
14351,"When nondeterministic algorithms are available, what will they throw when called?",RuntimeError,5869
14352,When will normally-nondeterministic operations act deterministically?,mode=True,5869
14353,What operations will act deterministically when mode=True?,normally-nondeterministic operations,7082
14354,What does index_put have to do with?,accumulate=False,11123
14355,What is torch.nn.Conv2d called on?,CUDA tensor torch,11123
14356,What is torch.nn.Conv1d when called on?,CUDA tensor torch,11122
14357,What is the index a list of tensors torch?,a CPU tensor,11130
14358,What is the torch.nn.ConvTranspose2d called on?,CUDA tensor torch,11130
14359,What is torch.nn.Conv3d called on?,CUDA tensor torch,11125
14360,What is the torch.nn.ConvTranspose1d called on?,CUDA tensor torch,11127
14361,What does a CUDA tensor require when the input dimension is one and called on a CUDA tensor?,grad,11132
14362,When is torch.bmm() called on sparse-dense?,CUDA tensors torch,11017
14363,What is a list of tensors torch?,index,10955
14364,What is the index of a CPU tensor?,a list of tensors,10956
14365,When is index_put() with accumulate=True called on?,a CPU tensor torch,10966
14366,When is index_add() called?,CUDA tensor torch,10966
14367,What tensor requires grad torch?,CUDA,10967
14368,When will normally-nondeterministic operations throw a RuntimeError?,mode=True,11126
14369,What will normally-nondeterministic operations throw when mode=True?,RuntimeError,7085
14370,What requires grad torch?,CUDA tensor,11065
14371,What is used when trying to differentiate a CUDA tensor torch?,torch.repeat_interleave,11212
14372,What is the torch.index_add() called on?,CUDA tensor torch,11071
14373,What does torch.index_select() attempt to differentiate?,CUDA tensor,11072
14374,What is index_copy() called on?,a CPU or CUDA tensor,11210
14375,What does torch.repeat_interleave() attempt to differentiate?,CUDA tensor torch,11211
14376,When is torch.Tensor.index_copy() called on?,a CPU or CUDA tensor,10964
14377,What will throw a RuntimeError when mode=True?,normally-nondeterministic operations,7084
14378,What type of torch does AvgPool3d attempt to differentiate?,a CUDA tensor torch,11118
14379,AvgPool3d when attempting to differentiate what?,CUDA tensor torch,11119
14380,What is AdaptiveAvgPool2d used to differentiate?,a CUDA tensor torch,11109
14381,What is AdaptiveAvgPool2d when attempting to differentiate?,CUDA tensor torch,11110
14382,What mode is used when trying to differentiate a CUDA tensor?,linear bilinear bicubic trilinear,11073
14383,What mode is used when attempting to differentiate a CUDA tensor?,mode='max',11142
14384,What type of torch is AdaptiveMaxPool2d when attempting to differentiate?,CUDA tensor,11114
14385,What is a fractionalmaxPool2d when trying to differentiate?,a CUDA tensor torch,11136
14386,What type of torch does FunctionalMaxPool2d attempt to differentiate?,CUDA tensor,11136
14387,What is used when attempting to differentiate a CUDA tensor torch?,ReplicationPad1d,11139
14388,"When attempting to differentiate a CUDA tensor torch, what does ReflectPad1d do?",CUDA tensor torch,11152
14389,What type of torch is used when attempting to differentiate a CUDA tensor torch?,trilinear torch,11262
14390,What is the ReflectionPad1d when trying to differentiate?,a CUDA tensor torch,11151
14391,When is EmbeddingBag used to differentiate a CUDA tensor?,mode='max',11154
14392,"When attempting to differentiate a CUDA tensor torch, what is ReflectionPad2d?",CUDA tensor torch,11154
14393,What is used when trying to differentiate a CUDA tensor when mode='max' torch?,EmbeddingBag,11115
14394,"When attempting to differentiate a CUDA tensor torch, what does ReplicationPad1d do?",CUDA tensor torch,11157
14395,What does ReplicationPad2d attempt to differentiate?,a CUDA tensor torch,11159
14396,What does ReplicationPad3d attempt to differentiate?,a CUDA tensor torch,11162
14397,ReplicationPad3d when attempting to differentiate a what?,CUDA tensor torch,11162
14398,When is torch.nn.NLLLoss called on?,a CUDA tensor torch,11149
14399,What is torch.nn.NLLLoss when called on?,CUDA tensor torch,11148
14400,What does torch.nn.CTCLoss attempt to differentiate?,a CUDA tensor torch,11121
14401,What is CTCLoss when attempting to differentiate?,CUDA tensor torch,11121
14402,What does median() output when called on a CUDA tensor?,indices,11134
14403,"When mode='max' torch, what does torch.nn.EmbeddingBag attempt to differentiate?",CUDA tensor,11134
14404,What is required when a CUDA tensor is called on a CUDA tensor?,grad,10979
14405,What does a CUDA tensor require when the input dimension is larger than one?,grad,10979
14406,What is the result of the element-wise multiplication of tensor1 by tensor2?,scalar value,4824
14407,"The shapes of tensor, tensor1, and tensor2 must be what?",broadcastable,4824
14408,What types of inputs must be a real number?,FloatTensor or DoubleTensor,4824
14409,What is the tensor to be multiplied?,tensor to be multiplied tensor2,4824
14410,What value is multiplied by the result of the element-wise multiplication of tensor1 by tensor2?,scalar,4824
14411,What is the tensor to be added to input?,tensor1,4824
14412,What is the same as this function?,torch.maximum,7656
14413,What language is std::fmax a wrapper around?,C++,7656
14414,"Supports broadcasting to a common shape, type promotion, and what other inputs?",integer and floating-point inputs,7656
14415,What is another (Tensor)?,second input tensor,7656
14416,If both elements are what is NaN propagated?,,1747
14417,What is the same as the input-wise maximum of input and other?,torch.maximum(),1748
14418,What C++ function is this function a wrapper around?,std::fmax,1748
14419,"What does this function support to a common shape, type promotion, and integer and floating-point inputs?",broadcasting,1748
14420,What type promotion does std::fmax support?,broadcasting,7534
14421,What program's fmax function is similar to std::fmax?,NumPy,7534
14422,What are the sub-tensors all of which split a tensor?,views of input,6026
14423,Input (Tensor) – the tensor to split what?,indices_or_sections,6026
14424,What is used to split indices or sections?,tensor,9634
14425,What does input split?,indices_or_sections,9634
14426,"If indices_or_sections is an integer n or a zero dimensional long tensor with value n, input",n sections,9612
14427,What is an integer n or a zero dimensional long tensor with value n?,indices_or_sections,9612
14428,"If indices_or_sections is divisible by n along dimension dim, each section will be of equal size?",n,9612
14429,"If input is divisible by n along dimension dim, each section will be of equal size, what is the result?",input.size(dim) / n,6027
14430,What is input split into if indices_or_sections is an integer n or a zero dimensional long tensor,n sections,9613
14431,How many dices_or_sections is an integer n or a zero dimensional long tensor with value n?,n,3418
14432,"If indices_or_sections is a one-dimensional long tensor, what ensional long tensor",dim,3417
14433,"What would indices_or_sections=[2, 3] and dim=0 result in?",tensors,3417
14434,"If indices_or_sections is a tensor, what must it be on the CPU?",zero-dimensional or one-dimensional long tensor,3417
14435,What is the default value of a tensor?,0,3417
14436,What is the default value for indices_or_sections?,0,3417
14437,What does torch.rand_like(input) return?,a tensor,5481
14438,"dtype (torch.dtype, optional) – what is returned Tensor?",the desired data type,5481
14439,"If None, what does torch.rand_like(input.size()) do?",defaults to the dtype of input,5481
14440,What is the size of input that determines size of output tensor?,input,9632
14441,What is calculated if Bessel's correction is True?,the sample deviation,9182
14442,What is the function that determines the standard deviation of all elements in the input tensor?,Calculates the standard deviation of all elements in the input tensor,9182
14443,What is calculated if unbiased is True?,the sample deviation,9182
14444,What is unbiased?,whether to use Bessel’s correction,9182
14445,"Out (Tensor, optional) - what is used to calculate the standard deviation of all elements in the input tensor?",output tensor,3537
14446,"If unbiased is True, Bessel's correction will be used.",If unbiased is True,3537
14447,Int or tuple of python:ints) – the dimension or dimensions to reduce.,dim,3538
14448,Whether to use Bessel's correction (N=1delta N = 1N=1)?,unbiased,3538
14449,"If Bessel's correction is used, what will be used?",If unbiased is True,9623
14450,What happens if unbiased is True?,"the sample deviation is calculated, without any correction",11277
14451,What is an example of a Bessel's correction?,Example,11277
14452,Returns what with the cosine of the elements of input?,a new tensor,5378
14453,What is in beta and subject to change?,Warning Quantization,8176
14454,Quantization is in what state and subject to change?,beta,5057
14455,What does PyTorch support backends for?,running quantized operators efficiently,1345
14456,What are provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss?,Higher-level APIs,1345
14457,What does PyTorch support for running quantized operators efficiently?,backends,4993
14458,What kind of CPUs are typically found in mobile/embedded devices?,ARM CPUs,7935
14459,What does FakeQuantize do to support both CPU and CUDA?,Note,7935
14460,What backends does PyTorch support to run quantized operators efficiently?,x86 CPUs with AVX2 support or higher,7934
14461,What is the name of the backend that PyTorch supports?,Note,7934
14462,What is the difference between ARM and x86 CPUs?,Note,7934
14463,What CPUs have AVX2 support or higher?,x86,11429
14464,What CPUs are typically found in mobile/embedded devices?,ARM CPUs,11430
14465,What CPUs have inefficient implementations without AVX2?,x86 CPUs with AVX2 support or higher,11428
14466,Why do you move the model to CPU?,to test the quantized functionality,11428
14467,What does PyTorch not provide on CUDA?,quantized operator implementations,1351
14468,Where should the model be moved to to test the quantized functionality?,CPU,1351
14469,Where should the model be moved in order to test quantized functionality?,CPU,1351
14470,What does FakeQuantize support both CPU and CUDA?,Note,1351
14471,Move the model to what in order to test the quantized functionality?,CPU,900
14472,What is typically found in mobile/embedded devices?,ARM CPUs,900
14473,What does PyTorch not provide?,quantized operator implementations on CUDA,900
14474,What is a way to test the quantized functionality?,Move the model to CPU,901
14475,What is the corresponding implementation chosen automatically based on?,PyTorch build mode,7014
14476,Move the model to what to test the quantized functionality?,CPU,7012
14477,Which program doesn't provide quantized operator implementations on CUDA?,PyTorch,4357
14478,What company doesn't provide quantized operator implementations on CUDA?,PyTorch,1352
14479,What is the name of Quantization-aware training?,FakeQuantize,5065
14480,What program supports both CPU and CUDA?,FakeQuantize,5064
14481,What platform does fbgemm work on?,x86,4447
14482,Quantization currently supports two backends: fbgemm and what?,qnnpack,8371
14483,What are the two modes of quantization provided by PyTorch?,Eager Mode Quantization and FX Graph Mode Quantization,8371
14484,What is the name of the backend that quantizes a model to run on ARM?,qconfig,8370
14485,On what platform is fbgemm currently supported?,x86,7013
14486,What is qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for?,quantization aware training,10530
14487,What should the torch.backends.quantized.engine parameter match?,the backend,10529
14488,What is the backend set to qnnpack for inference?,torch.backends.quantized.engine = 'qnnpack',10529
14489,What is the backend set to for inference?,qnnpack,3699
14490,What is qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') used for?,quantization aware training,9376
14491,PyTorch provides Eager Mode Quantization and what other mode of quantization?,FX Graph Mode Quantization,10527
14492,What is the use of qconfig = torch.quantization.get_default_qat_qconfig('qnnpack'),quantization aware training,10527
14493,What provides two different modes of quantization?,PyTorch,8372
14494,What is the beta version of Eager Mode Quantization?,beta,8372
14495,What is qconfig used for?,quantization aware training,10528
14496,What type of training does PyTorch provide?,quantization aware training,9379
14497,PyTorch provides Eager Mode Quantization and what other mode?,FX Graph Mode Quantization,9379
14498,What is PyTorch used for?,quantization aware training,9380
14499,"Along with Eager Mode Quantization, what other mode does PyTorch provide?",FX Graph Mode Quantization,11004
14500,What mode of quantization does PyTorch provide?,Eager Mode Quantization,11004
14501,What provides Eager Mode Quantization and FX Graph Mode Quantization?,PyTorch,4976
14502,What is the first mode of quantization that new users are encouraged to try?,FX Graph Mode Quantization,4308
14503,What is the name of FX Graph Mode Quantization?,Eager Mode Quantization,4308
14504,What are the three types of Quantization modes?,"Static, Dynamic, Weight Only",7090
14505,What is the beta prototype of Eager Mode Quantization FX Graph Mode?,Quantization Release Status,2243
14506,What is the name of the beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Supported,FX Graph Mode Quantization,2376
14507,What Quantization Release Status beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Support for Customization Limited Support Fully Supported Quant,FX Graph Mode,2374
14508,What is the status of the Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Supported Quantizing,FX Graph Mode Quantization Release Status beta prototype,2374
14509,What is the status of the beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Quantizing Functional,Release Status,5134
14510,What type of Post Training Quantization is supported?,Static,6097
14511,What is the status of the Operator Fusion Manual?,beta prototype,8955
14512,What type of modules are Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Support,Manual Automatic Quantizing Modules,4197
14513,What is Supported Supported Supported Quantizing Functionals/Torch Ops Manual?,Automatic Quantizing Modules,1378
14514,What type of support does the Quant/DeQuant Placement Manual Automatic Support for?,Customization,5048
14515,What type of Quantization Mode Support Post Training Quantization?,Dynamic,5073
14516,What Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quantization,Automatic Quantizing Modules,1380
14517,What supports Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quant,Manual Automatic Quantizing Modules,4199
14518,What is the type of Quantiztion Aware Training?,Static Input/Output Model Type torch,4199
14519,What is Supported Supported Quantizing Functionals/Torch Ops Manual?,Automatic Quantizing Modules,1379
14520,What Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post,Quantizing Modules,5074
14521,"Quantization Mode Support Post Training Quantization: Dynamic, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization",Static,5074
14522,Quantizing Functionals/Torch Ops Manual Automatic Support for what?,Customization,5072
14523,What type of support is limited to Fully Supported Quantization Mode Support?,Manual Automatic Support for Customization,4200
14524,What is Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support?,Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support,4200
14525,What mode supports three types of quantization?,Eager Mode Quantization,8956
14526,What type of Quantization is supported in Eager Mode Quantization?,Dynamic,4877
14527,What type of Quantiztion Aware Training is Static Post Training Quantization?,Dynamic,5077
14528,What is required post training in static quantization?,calibration,2375
14529,What type of quantization is supported with activations read/stored in floating point and quantized for compute?,dynamic quantization,11147
14530,What type of training does Eager Mode Quantization support?,quantization aware training,11147
14531,What is the name of the type of quantization supported in Eager Mode Quantization?,quantization aware training,5049
14532,In what format are activations read/stored?,floating point,7406
14533,Where are activations read/stored in Eager Mode Quantization?,floating point,11145
14534,What type of training is supported in Eager Mode Quantization?,quantization aware training,3851
14535,What is a type of quantization that is quantized with activations read/stored in floating point and quantized for compute?,dynamic quantization,9227
14536,What is another type of quantization?,static quantization,9227
14537,What blog post provides a more comprehensive overview of the tradeoffs between static quantization and quantization aware training?,Pytorch,10756
14538,"What is the term for weights quantized, activations quantized, calibration required post training?",static quantization,10756
14539,Where can you find a more comprehensive overview of the tradeoffs between quantization types?,Pytorch,10536
14540,What blog post provides a more comprehensive overview of the tradeoffs between quantization types?,Pytorch,4856
14541,What is a more comprehensive overview of the tradeoffs between quantization types?,Introduction to Quantization,4855
14542,What is the name of the blog post that provides a more comprehensive overview of the tradeoffs between quantization types?,Pytorch,4855
14543,What is the name of the quantization type?,Pytorch,4855
14544,What happens to the activations during inference?,the activations are dynamically quantized during inference,7697
14545,What dominates the model execution time?,loading weights from memory,7697
14546,What types of models can use dynamic quantization?,LSTM and Transformer type models with small batch size,7697
14547,What is an example of a diagram used for dynamic quantization?,API,7697
14548,When are the activations dynamically quantized?,inference,7697
14549,What type of model has a small batch size?,Transformer,7697
14550,What is the term for quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference?,dynamic quantization,7697
14551,What can you learn more about in our dynamic quantization tutorial?,dynamic quantization,10537
14552,What is the term for dynamic quantization?,dynamic quantization,7902
14553,What is the name of the API that you can use to learn more about?,dynamic quantization,899
14554,What can you learn more about in the static quantization tutorial?,static quantization,2127
14555,What is the name of the technique used in the API?,static quantization,2127
14556,What does the static quantization tutorial cover?,static quantization,7904
14557,What is the name of the tutorial that teaches quantization aware training?,QAT,5053
14558,What is an example of a diagram?,API,2126
14559,"To learn more about quantization aware training, please see what tutorial?",QAT,2126
14560,"What type of quantization is applied after training, quantization parameters are calculated based on sample calibration data?",Post Training Quantization,5061
14561,What type of quantization is supported by FX Graph Mode?,Post Training Quantization,5061
14562,What type of training simulates quantization during training?,Quantization Aware Training,5061
14563,What type of Quantization is included in Post Training Quantization?,Weight Only Quantization,4880
14564,What is the name of the type of Quantization that only weight is statically quantized?,Weight Only Quantization,4880
14565,What is the name of the type of quantization in which weight is statically quantized and activation is dynamically quantized?,Dynamic Quantization,8310
14566,What type of Quantization includes both weight and activations?,Static Quantization,5051
14567,What is the name of the type of Quantization where only weight is statically quantized?,Dynamic Quantization,5051
14568,How many different types of quantization can we theoretically have?,6,1138
14569,What is an argument of the prepare_fx function?,qconfig_dict,1138
14570,What is the name of the tutorial that provides more information about FX Graph Mode Quantization?,User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization,1138
14571,What is one of the supported quantization types?,FX Graph Mode Quantization,1137
14572,What are the supported quantization types in FX Graph Mode Quantization?,Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization,4881
14573,What are both weight and activations in Static Quantization?,statically quantized,6070
14574,Are the two ways of classification independent or independent?,independent,7440
14575,How is the configuration done in post training quantization?,qconfig_dict,7313
14576,What type of quantization is supported by qconfig_dict?,post training quantization,7313
14577,What type of quantization is supported by FX Graph Mode Quantization?,post training quantization,7313
14578,What is the most common type of post training quantization?,weight only,8312
14579,What function does qconfig_dict belong to?,prepare_fx,8312
14580,What is the name of the document that provides more information about FX Graph Mode Quantization?,User Guide on Using FX Graph Mode Quantization,4882
14581,What is the FX Graph Mode Post Training Static Quantization?,User Guide on Using FX Graph Mode Quantization,8311
14582,What is the name of the document that describes FX Graph Mode Post Training Static Quantization?,User Guide on Using FX Graph Mode Quantization,6071
14583,What program supports both per tensor and per channel asymmetric linear quantization?,PyTorch,4989
14584,What happens when the values in a tensor are scaled and offset by a different value?,the scale and offset become vectors,4989
14585,What is the benefit of using per channel quantization?,lesser error,4989
14586,What is the mapping performed by?,converting the floating point tensors,7169
14587,What is converted by mapping?,floating point tensors,7169
14588,What operations do not cause additional quantization error?,padding,4507
14589,Operations like what do not cause additional quantization error?,padding,4507
14590,What is represented with no error after quantization?,zero,4507
14591,"Along with quantization parameters, what parameters can be stored in a Quantized Tensor?",scale and zero_point,3745
14592,What does a Quantized Tensor allow for?,serialization of data in a quantized format,3745
14593,Quantized Tensors allow for what type of data in a quantized format?,serialization,3745
14594,What supports a limited subset of data manipulation methods of the regular full-precision tensor?,Quantized Tensors,5070
14595,What do Quantized Tensors restrict support to?,8 bit weights,5070
14596,"For NN operators included in PyTorch, we restrict support to what?",8 bit weights,5070
14597,What does data_type = qint8 support?,8 bit weights,174
14598,Operator implementations currently only support what for weights of the conv and linear operators?,per channel quantization,4478
14599,What type of activations do operators currently only support per channel quantization for weights of the conv and linear operators?,8 bit activations,172
14600,Many operations for quantized tensors are available under the same API as what?,full float version,4208
14601,What performs re-quantization?,NN modules,4208
14602,What do quantized NN modules explicitly take in the operation signature?,output quantization parameters,4208
14603,What is the name of the fusion pattern that affects quantization?,torch.nn.intrinsic.quantized,4208
14604,What are quantized versions of that perform re-quantization?,NN modules,955
14605,What do quantized versions of NN modules explicitly take in the operation signature?,output quantization parameters,955
14606,In what language are quantized tensor operations available?,torch,955
14607,What does torch.nn.intrinsic.quantized support?,fused versions,956
14608,Where are operations for quantized tensors available under the same API as full float version?,torch.nn,956
14609,What are fused versions corresponding to?,common fusion patterns,3701
14610,The list of supported operations is sufficient to cover what types of models?,CNN and RNN models,3701
14611,What is a common fusion pattern that impacts quantization?,torch.nn.intrinsic.quantized,3701
14612,What are the default implementations of to select the scale factor and bias based on observed tensor data?,observers,8404
14613,How can quantization be applied to different parts of the model?,selectively,8404
14614,"What is supported for conv2d(), conv3d() and linear()?",per channel quantization,8404
14615,"What do conv2d(), conv3d() and linear() support?",per channel quantization,8403
14616,What can developers provide?,quantization functions,8403
14617,What can be applied selectively to different parts of the model or configured differently for different parts of the model?,Quantization,8403
14618,"What do we provide support for for conv2d(), conv3d(), and linear()?",per channel quantization,8235
14619,Quantization workflows work by adding or what?,replacing,8235
14620,What is nn.Conv2d converted to?,nn.quantized.Conv2d,8235
14621,The model stays a regular what-based instance throughout the process?,nn.Module,8235
14622,How do quantization workflows work?,adding,8235
14623,What is added as a.observer submodule?,observers,8235
14624,What does nn.Conv2d convert to?,nn.quantized.Conv2d,8235
14625,What type of instance does the model stay in throughout the process?,nn.Module,8235
14626,What is it necessary to make some modifications to prior to quantization?,model definition,3942
14627,What does the user need to do to convert operations that require output requantization to module form?,output requantization,3942
14628,How does quantization work?,module by module,3942
14629,What is another name for torch.nn.functional.relu?,torch.nn.ReLU,3943
14630,What do some operations require to be converted from functionals to module form?,output requantization,1902
14631,To what form can any operations that require output requantization be converted?,module form,1902
14632,What is an example of a conversion from functionals to module form?,torch.nn.ReLU,1902
14633,Convert operations that require output requantization from functionals to what?,module form,1902
14634,What is done by assigning.qconfig attributes on submodules or by specifying qconfig_dict?,Specify which parts of the model need to be quantized,6014
14635,What layer will not be quantized if setting model.conv1.qconfig = None?,the model.conv layer,6014
14636,What will the quantization settings for model.linear1 be using instead of the global qconfig?,custom_qconfig,6014
14637,What can be done by assigning.qconfig attributes on submodules or by specifying qconfig_dict?,Specify which parts of the model need to be quantized,6015
14638,What tensor operations require special handling to determine output quantization parameters?,add and cat,6015
14639,What are used to specify where activations are quantized and de-quantized?,QuantStub and DeQuantStub modules,6015
14640,What does the user need to do in addition to Specify where activations are quantized and de-quantized?,static quantization techniques,2573
14641,What does the user need to do in addition to quantizing activations?,Specify where activations are quantized and de-quantized,2574
14642,What fusions do we currently support?,"Linear, Relu",2574
14643,What can be quantized and de-quantized using QuantStub and DeQuantStub modules?,activations,6013
14644,What do QuantStub and DeQuantStub modules specify?,where activations are quantized and de-quantized,6013
14645,What are two examples of tensor operations that require special handling to determine output quantization parameters?,add and cat,8092
14646,What are two examples of operations that require special handling to determine output quantization parameters?,add and cat,8092
14647,What API is used to fuse modules?,torch.quantization.fuse_modules(),2663
14648,Set the reduce_range argument on observers to what if you are using the fbgemm backend?,True,5828
14649,By how much is the range of quantized data type reduced?,1 bit,5828
14650,The reduce_range argument prevents what on some int8 instructions?,overflow,5828
14651,What do you want to pass a non-quantized Tensor to?,a quantized kernel,3579
14652,What is an example of a non-quantized tensor?,e2e,3579
14653,What is a common workaround for a non-quantized tensor?,torch.quantization.QuantStub,3578
14654,How is torch.quantization.QuantStub done?,manually,3578
14655,If you see an error similar to: This means that you are trying to pass what to a quantized kernel?,a non-quantized Tensor,3578
14656,What is used to quantize the tensor?,torch.quantization.QuantStub,3578
14657,What is an example of a non-quantized tensor to a quantized kernel?,e2e,3578
14658,What does the error indicate you are trying to pass a non-quantized Tensor to?,a quantized kernel,3578
14659,How is torch.quantization.QuantStub used?,manually,3578
14660,In what mode is torch.quantization.QuantStub used?,Eager mode quantization,3580
14661,What is a common workaround for passing a non-quantized Tensor to a quantized kernel?,torch.quantization.QuantStub,3580
14662,What is a non-quantized Tensor trying to pass to?,a quantized kernel,7722
14663,What is an example of an e2e error?,an error similar to:,7722
14664,What are you trying to pass to a quantized kernel?,a non-quantized Tensor,7722
14665,What is a common workaround to pass a non-quantized tensor to a quantized kernel?,torch.quantization.QuantStub,7722
14666,What is an example of a non-quantized tensor being passed to a quantized kernel?,e2e,7722
14667,What does the error indicate you are trying to pass a quantized Tensor to?,a non-quantized kernel,3582
14668,What is a common workaround to dequantize the tensor?,torch.quantization.DeQuantStub,3582
14669,How is torch.quantization.DeQuantStub done?,manually,3582
14670,What is an example of a quantized tensor?,e2e,3582
14671,If you see an error similar to: This means that you are trying to pass a quantized Tensor to what?,a non-quantized kernel,3582
14672,What is used to dequantize the tensor?,torch.quantization.DeQuantStub,3582
14673,What model does torch.quantization convert to quantized form?,FP32,11205
14674,What module implements the functions you call to convert your model from FP32 to quantized form?,torch.nn.intrinsic,11205
14675,What critical fusion is performed by torch.quantization?,conv+relu,11205
14676,What model does torch.nn.intrinsic convert to quantized form?,FP32,7750
14677,What is a helper function for performing critical fusions?,torch.nn.intrinsic,7750
14678,What format does the module convert your model from?,FP32,7750
14679,What converts the weights to int8?,convert(),7750
14680,What is the name of the helper function used to convert your model from FP32 to quantized form?,torch.nn.intrinsic,7750
14681,What are the combined modules of torch.nn.intrinsic?,conv + relu,11163
14682,What module implements the quantized implementations of fused operations?,torch.nn.qat,7749
14683,What are the fused modules implemented by this module?,conv + relu,7749
14684,What is removed from current unpruned units selected at random?,specifiedamountof,4918
14685,What are currently unpruned channels along the specifieddimselected at random?,specifiedamountof,4918
14686,Prune (currently unpruned) units in a tensor at what?,random,4918
14687,Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest what?,L1-norm,4918
14688,Prune entire (currently unpruned) channels in a tensor based on what?,Ln-norm,4918
14689,What is used to prune tensors corresponding to all parameters inparameters?,specifiedpruning_method,4918
14690,What check whethermoduleis pruned by looking forforward_pre_hooksin its modules?,prune.is_pruned,4918
14691,Prune entire (currently unpruned) channels in a tensor at what?,random,4924
14692,What is the lowest Ln-norm?,L1-norm,4924
14693,Removes what from a module?,spectral normalization reparameterization,4924
14694,What method is used to prune tensors corresponding to all parameters inparameters?,specifiedpruning_method,10498
14695,What type of normalization reparameterization is removed from a module?,spectral,10498
14696,What is implemented using the new parametrization functionality intorch.nn.utils.parameterize.register_parametrization(),Parametrizations,10498
14697,What is used to parametrize Tensors on existing Modules?,Utility functions,10498
14698,Utility functions to parametrize Tensors on existing Modules can be used to what?,parametrize a given Parameter or Buffer,10498
14699,What would transform an object into a parameter?,parameterizations,10498
14700,"For more information on how to implement your own parametrizations, see what?",Parametrizationstutorial,10498
14701,What is the name of the Context manager that enables the caching system within parametrizations registered withregister_parametrization()?,parametrize.cached,10498
14702,What does parametrize.is_parametrized return?,ReturnsTrueif module has an active parametrization,10498
14703,What are Utility functions in other modules?,Utility functions in other modules,10498
14704,What type of reparametrization does prune.customFromMask apply?,pruning,10470
14705,Parametrizations implemented using the new parametrization functionality?,intorch.nn.utils.parameterize.register_parametrization(),10470
14706,What does.spectral_norm apply spectral normalization to a parameter in the given module?,parametrizations,10470
14707,What does prune.is_pruned Applies to a parameter in the given module?,weight normalization,10501
14708,What reparameterization does prune.remove remove from a module?,spectral normalization,10501
14709,What does prune.identity apply to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?,pruning reparametrization,10501
14710,What is currently unpruned channels along the specifieddimselected at random?,specifiedamountof,10504
14711,Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Utility,parametrizations.spectral_norm,10504
14712,Prunes tensor removing the specifiedamountof (currently unpruned) units with the lowest what?,L1-norm,4936
14713,Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. What,parametrizations.spectral_norm,4936
14714,What parametrize.register_paramet?,parametrize.register_paramet,4936
14715,Pytorch Hub is a pre-trained model repository designed to facilitate what?,research reproducibility,5020
14716,In most cases importing the right function what is sufficient?,inhubconf.py,5020
14717,What is highly recommended to add to Pytorch Hub?,a few examples,5020
14718,Pytorch Hub can't be a what?,random commit,5020
14719,What provides convenient APIs to explore all available models in hub throughtt?,Pytorch Hub,5020
14720,What are the allowed arguments in the model?,positional/keyword arguments,5025
14721,Where can pretrained weights be stored?,github repo,5025
14722,Who supports publishing pre-trained models to a github repository?,Pytorch Hub,5025
14723,What does the code snippet specifies if we expand the implementation inpytorch/vision/hubconf.py?,an entrypoint forresnet18model,5025
14724,Why do we use the expanded version as an example?,to show how it works,5025
14725,What is the minimum size of a pretrained model?,2GB,5025
14726,Pytorch Hub supports publishing pre-trained models to a github repository. It can't be a what?,random commit,5025
14727,What is the default value for a github repo or a local directory?,is False,9469
14728,repo_or_dir(string) – what is repo_owner/repo_name[:tag_name]?,repo name,9469
14729,What default is 'github'?,Default,9469
14730,"What does verbose(bool,optional) do?",mute messages about hitting local caches,9469
14731,Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(boo,"source(string,optional) –'github'|'local'",9271
14732,The docstring of what entrypoint Example Show the docstring of. github(string) – a string with format,entrypointmodel,9271
14733,"What does the verbose(bool,optional) do?",mute messages about hitting local caches,9271
14734,What is the message about hitting local caches?,first,9271
14735,"What does source(string,optional) -'github'|'local' specify?",how repo_or_dir is to be interpreted,8763
14736,url(string) – URL of the object to download dst(string) – what?,Full path where object will be saved,11303
14737,"If the object is already present, it’s deserialized and returned. The default value ofmodel_dirishub_dir>/",inmodel_dir,11303
14738,What is the name of the document that specifies how to remap storage locations?,a function or a dict,11303
14739,"What is the default value for file_name(string,optional) – name for the downloaded file?",False,11303
14740,What arguments does model.footakes have to run?,model.footakes to run,11303
14741,What is the name of the directory in which to save the object map_location(optional)?,"url(string) – URL of the object to download model_dir(string,optional)",7218
14742,What does themodelcallable have when called with the given*argsand**kwargs?,output,7218
14743,What is instantiated by*argsand**kwargsintorch.hub.load()?,model,7218
14744,What is a suggested workflow to see all available methods?,dir(model),7218
14745,The locations are used in the order of what?,Callinghub.set_dir,9338
14746,What is set if environment variableXDG_CACHE_HOMEis set?,$XDG_CACHE_HOME/torch/hub,9338
14747,"Ifset_dir()is not called, what is the default path?",$TORCH_HOME/hub,9338
14748,Optionally set the what directory used to save downloaded models & weights?,Torch Hub directory,9338
14749,d(string) – what is used to save downloaded models & weights?,path to a local folder,9338
14750,"By default, we don’t do what after loading it?",clean up files,9338
14751,Hub uses the cache what if it already exists in the directory returned byget_dir()?,by default,9338
14752,Users can force a reload by what?,callinghub.load,9338
14753,"What does callinghub.load(...,force_reload=True) do?",delete the existing github folder and downloaded weights,9338
14754,What works by a github branch?,Torch hub,9338
14755,What methods are used to get a new tensor with the data ininputfake quantized per channel?,"scale,zero_point,quant_minandquant_max",5380
14756,What is the input value(s) of intorch.float32?,input(Tensor),5380
14757,What is the lower bound of the quantized domain?,quantized domain quant_max(int64),5380
14758,"What is returned with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max",a new tensor,5380
14759,What is the input value(s) in intorch.float32?,input(Tensor),5380
14760,What is a new tensor with the data ininputfake quantized per channel?,fake_quantized per channel tensor Tensor Example:,5380
14761,Returns a 1-dimensional view of each input tensor with what dimensions?,zero,5261
14762,How are input tensors with one or more dimensions returned?,as-is,5261
14763,What is computed for each element ininput?,Heaviside step function,1675
14764,What is the Heaviside step function defined as?,input(Tensor) – the input tensor,1675
14765,values(Tensor) – The values to use what?,whereinputis zero,1675
14766,What is an example of a Heaviside step function?,Example:,1675
14767,What type of storage object is ifobj a storage object?,PyTorch,5246
14768,Returns what ifobjis a PyTorch storage object?,True,5246
14769,What type of storage object does ifobjis return True?,PyTorch,5246
14770,obj(what) returns true ifobjis a PyTorch storage object?,Object,5246
14771,What is the name of the object to test?,obj(Object),5246
14772,What algorithm is this function a front-end to?,LOBPCG,2443
14773,A less robust method may fail when what is applied to singular input?,Cholesky,7532
14774,What is the front-end to?,LOBPCG,7532
14775,Who introduced the LOBPCG method?,Andrew Knyazev,9910
14776,What types of inputs are supported by the LOBPCG method?,"dense, sparse, and batches of dense matrices",9910
14777,What type of method is the LOBPCG method with orthogonal basis selection?,robust,9910
14778,What does the LOBPCG method have?,orthogonal basis selection,9910
14779,When does a less robust method fail?,when Cholesky is applied to singular input,7533
14780,What are the advantages of the robust methods?,converge much faster and are more stable,7533
14781,What is performed only when theArequires gradients?,symmetrization map,7533
14782,What is the LOBPCG method with orthogonal basis selection?,robust method,7533
14783,What is the usage of the basic method?,generally not recommended,7533
14784,What is the name of the algorithm that may fail when applied to a singular input?,Cholesky,7531
14785,What type of method is the orthogonal basis selection method?,robust,7531
14786,What is the LOBPCG method with?,orthogonal basis selection,9909
14787,What is applied to singular input?,Cholesky,9909
14788,"What is the name of the method that supports dense, sparse, and batches of dense matrices?",robust method,9911
14789,"What type of method is supported by dense, sparse, and batches of dense matrices?",robust,9911
14790,What is the advantage of using the basic method over the robust method?,the robust methods converge much faster and are more stable,9911
14791,What is generally not recommended but there are cases where the use of the basic method may be preferred?,the usage of the basic method,9911
14792,What types of inputs are supported by the robust method?,"dense, sparse, and batches of dense matrices",9912
14793,What are the inputs supported by the LOBPCG method?,"dense, sparse, and batches of dense matrices",9912
14794,"What type of method supports dense, sparse, and batches of dense matrices?",robust,9912
14795,How much time does the basic method spend per iteration?,least time,4382
14796,Which method spends least time per iteration?,the basic method,4382
14797,Which method converges much faster and is more stable?,robust methods,3723
14798,How much time does the basic method spend?,least time per iteration,3723
14799,What is generally not recommended?,usage of the basic method,3723
14800,What must X be?,dense tensor,3723
14801,What is the default number of requested eigenpairs?,number ofXXXcolumns,3723
14802,What is generally not recommended but there are cases where the basic method may be preferred?,the usage of the basic method,2444
14803,What is used to find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of,matrix-free LOBPCG methods,2444
14804,What is the name of the method used to find the k largest eigenvalues and the corresponding eigenvectors?,robust method,2444
14805,What are supported inputs?,"dense, sparse, and batches of dense matrices",2444
14806,What are the supported inputs?,"dense, sparse, and batches of dense matrices",6099
14807,Which method does not support sparse and complex inputs?,The backward method,6099
14808,What converges much faster and is more stable?,robust methods,6099
14809,Which methods converge much faster and are more stable?,robust methods,6100
14810,"iK(tensor,optional) – the input tensor of size(,m,m)(*,",preconditioner,6100
14811,What is generally not recommended but there are cases where the usage of the basic method may be preferred?,usage of the basic method,6100
14812,Supported inputs are what?,"dense, sparse, and batches of dense matrices",6100
14813,What is it assumed thatAis symmetric is?,A.gradis not,6100
14814,Where is A - t * A.gradis symmetric?,first-order optimization routines,6100
14815,X must be a what?,dense tensor,6100
14816,What does the backward method not support?,sparse and complex inputs,8183
14817,What is the default value for the number of requested eigenpairs?,number ofXXXcolumns,8183
14818,When does the backward method work?,whenBis not provided,6968
14819,What are we actively working on?,extensions,6968
14820,The backward method does not support what?,sparse and complex inputs,6968
14821,"When not specified,Bis interpereted as what?",identity matrix,6968
14822,"What is the input tensor of size(,m,m)(*, m, m)(,m,","iK(tensor,optional)",6968
14823,What type of tensor must X be?,dense,6968
14824,"If XXXis specified, the value ofn(when specified) must be what?",the number ofXXXcolumns,6968
14825,When is the map performed only?,when theArequires gradients,6968
14826,What type of inputs does the backward method not support?,sparse and complex inputs,6966
14827,What is assumed about A.gradis?,symmetric,8209
14828,What should be done in first-order optimization routines before runninglobpcgwe do the following symmetrization map?,A - t * A.gradis symmetric,8408
14829,When is the map performed?,when theArequires gradients,8408
14830,What is assumed to be symmetric?,A.gradis,8408
14831,"When specified, the input tensor of size(,m,m)(*, m, m)(,m",preconditioner,8408
14832,What is the default number of XXXcolumns?,Default,8408
14833,What do we do to make sure thatA - t * A.gradis symmetric?,first-order optimization routines,8408
14834,"What does k(integer,optional) default to when specified?",the number ofXXXcolumns,8408
14835,In what routines do we make sure thatA - t * A.gradis symmetric?,first-order optimization routines,3722
14836,Is A.gradis symmetric or symmetric?,symmetric,8407
14837,"When specified, the input tensor of size(,m,m)(*,m, m)(,m,",preconditioner,8450
14838,What happens to the size of the generated random approximation of eigenvectors?,ifXXXis not specified,8450
14839,What is the number of requested eigenpairs?,"k(integer,optional)",9570
14840,"What is the default value of k(integer,optional)?",number ofXXXcolumns,9570
14841,"When specified, the input tensor of size will be used as what?",preconditioner,9570
14842,What is the residual tolerance for stopping criterion?,"tol(float,optional)",9570
14843,"What does largest(bool,optional) solve the eigenproblem for?",smallest eigenvalues,9570
14844,What is the default value to solve the eigenproblem for the smallest eigenvalues?,True,9570
14845,What is the default value for the number ofXXXcolumns?,Default,9569
14846,What must be the number ofXXXcolumns if XXXis specified?,the value ofn,9569
14847,What is the default value of the generated random approximation of eigenvectors?,fornisk,1393
14848,What specifies the size of the generated random approximation of eigenvectors?,"n(integer,optional)",9955
14849,What must the value ofn(when specified) be?,the number ofXXXcolumns,9955
14850,"What does method(str,optional) do?",select LOBPCG method,9955
14851,Where can you find the LOBPCG method?,the description of the function above,9955
14852,What is the default value of the LOBPCG method?,ortho,9955
14853,What is a function for tracing the iteration process called?,tracker,9955
14854,"A,B,iK- input what?",Tensor arguments,9955
14855,"What is the default value for largest(bool,optional)?",True,9955
14856,What is the default value of isfeps?,0.5,10933
14857,What is the smallest non-zero floating-point number of the given input tensorAdata type?,largest,10933
14858,What is the default floating-point number of the given input tensorAdata type?,Default isfeps,10933
14859,Where is the LOBPCG method described?,the description of the function above,10933
14860,What holds the full state of the iteration process in the following attributes?,LOBPCG instance,10933
14861,"Tol(float,optional) – what is the stopping criterion?",residual tolerance,10933
14862,What does the LOBPCG instance hold the full state of the iteration process in?,The LOBPCG instance holds the full state of the iteration process in the following attributes,10933
14863,What is the default to solve the eigenproblem for the smallest eigenvalues?,True,10932
14864,"Tol(float,optional) – what for stopping criterion?",residual tolerance,10932
14865,"What is the default value of largest(bool,optional)?",Default is True,10932
14866,What is the default name of the LOBPCG method?,ortho,9769
14867,What is the eigenproblem for the largest eigenvalues?,largest,9769
14868,What is the maximum number of iterations?,niter,9769
14869,"For infinite iteration but until convergence criteria is met, what is the default?",use-1,9769
14870,What happens when the maximum number of iterations is reached?,the iteration process is hard-stopped and the current approximation of eigenpairs is returned,9769
14871,"When the maximum number of iterations is reached, what happens to the iteration process?",hard-stopped,9769
14872,What is a function for tracing the iteration process called at each iteration step with LOBPCG instance as an argument?,tracker,9769
14873,"What is the default value of the largest(bool,optional) function?",Default,9769
14874,What does tracker call at each iteration step with as an argument?,LOBPCG instance,9769
14875,"What method does method(str,optional) select?",LOBPCG method,9770
14876,"For infinite iteration but until convergence criteria is met, what is used?",use-1,9770
14877,"When True, solve the eigenproblem for the largest eigenvalues. Otherwise, solve the eigenproblem for smallest e","largest(bool,optional)",9770
14878,"Largest(bool,optional) – when True, solve the eigenproblem for what?",smallest eigenvalues,9770
14879,What is required to select LOBPCG method?,description of the function above,9770
14880,"When reached, the iteration process is hard-stopped and what is returned?",the current approximation of eigenpairs is returned,9770
14881,"Tracker(callable,optional) – a function for what?",tracing the iteration process,9770
14882,"Iftrackersetsbvars[“force_stop”] = True, the iteration process will be hard-stopped.",force_stop,9770
14883,What are the various parameters to LOBP?,"ortho_fparams, ortho_bparams(ortho_iparams,)",9770
14884,What is the name of the eigenproblem for the largest eigenvalues?,largest,9768
14885,What is the smallest eigenvalue?,smallest,9768
14886,"When the maximum number of iterations is reached, the iteration process is what?",hard-stopped,9768
14887,"What is the default value of the largest(bool,optional)?",Default,9768
14888,When does use-1 for infinite iteration?,until convergence criteria is met,9768
14889,What is niter?,maximum number of iterations,9977
14890,What instance holds the full state of the iteration process in the following attributes?,LOBPCG,9977
14891,What is an argument for tracker?,LOBPCG instance,9977
14892,What are iteration Tensor variables?,"E,X,S,R",9977
14893,"When niter(int,optional) is reached, what happens to the iteration process?",the iteration process is hard-stopped,9977
14894,What is the name of the iteration variable that holds the full state of the iteration process?,instance,9977
14895,What is the name of the function that can be called?,tracker,9954
14896,"What does method(str,optional) select?",LOBPCG method,9908
14897,LOBPCG method – select LOBPCG method. See what?,description of the function above,9908
14898,"When the maximum number of iterations is reached, what happens?",the iteration process is hard-stopped,9908
14899,"Iftrackersetsbvars[“force_stop”] = True, the iteration process will be hard-stopped. ortho",force_stop,9908
14900,What is a tensor of eigenvalues?,tensor of eigenvalues,9908
14901,Where can you find out more about the LOBPCG method?,the description of the function above,9907
14902,When is the iteration process hard-stopped and the current approximation of eigenpairs returned?,until convergence criteria is met,9907
14903,What is the name of the function that can be used for infinite iteration?,tracker,9907
14904,What is a function for tracing the iteration process?,tracker,11248
14905,What is used as an argument for tracker?,LOBPCG instance,11248
14906,What is the current residualivars[“converged_count”]- the current number of converged eigenpairst,current state of convergence criteria,11248
14907,When is use-1 used for infinite iteration?,until convergence criteria is met,9976
14908,What is a function for?,tracing the iteration process,8761
14909,What is an argument for a function for tracing the iteration process?,LOBPCG instance,8761
14910,The LOBPCG instance holds the full state of the iteration process in what attributes?,"iparams,fparams,bparams",8761
14911,"Iftrackersetsbvars = True, the iteration process will be hard-stopped.",Iftrackersetsbvars,8761
14912,ivars[“istep”] is what?,current iteration step,8761
14913,What is LOBPCG a function for?,tracing the iteration process,8760
14914,"What does A,B,iK stand for?",input Tensor arguments,11249
14915,"What is the tensor of eigenvalues of size(,k)(*, k)(,k)",X,11249
14916,"What are the dictionaries of integer, float, and boolean valued input parameters?","iparams,fparams,bparams",11249
14917,"Iftrackersetsbvars[“force_stop?”] = True, the iteration process will be hard-stopped. ortho",force_stop,11249
14918,When was Andrew V. Knyazev published?,2001,11249
14919,Who published the Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method?,SIAM J. Sci.,11249
14920,What is the name of the computer that tracks the iteration process?,Comput,11249
14921,"What are dictionaries of integer, float, and boolean valued input parameters?","iparams,fparams,bparams",9694
14922,What is the current approximation of eigenvectorsE?,current iteration stepX,9694
14923,ivars[“istep”]- the current approximation of eigenvectorsE- the current approxim,current iteration stepX,9694
14924,What is an example of a Tensor valued iteration variables?,For instance,9693
14925,"ivars,fvars,bvars,tvars are dictionaries of what types of variables?","integer, float, boolean, and Tensor valued iteration variables",9751
14926,What are the input Tensor arguments?,"A,B,iK",9751
14927,What are the iteration Tensor variables?,"E,X,S,R",9751
14928,"What are the dictionaries of integer, float, boolean, and Tensor valued iteration variables?","ivars,fvars,bvars,tvars",9751
14929,ivars[“istep”]- what?,current iteration stepX,9751
14930,What does rerr stand for?,current state of convergence criteria,9750
14931,"What are dictionaries of integer, float, boolean, and Tensor valued iteration variables?","ivars,fvars,bvars,tvars",9750
14932,What is the current number of converged eigenpairstvars?,current state of convergence criteria,9752
14933,"Iftrackersetsbvars[“force_stop”] = True, what will be hard-stopped?",the iteration process,9752
14934,What is the current approximation of eigenvaluesR?,the current residualivars,9752
14935,What is the current approximation of?,eigenvalues,9752
14936,"E,X,S,R- iteration of what?",Tensor variables,892
14937,"Iftrackersetsbvars[“force_stop”] = True, the iteration process will be what?",hard-stopped,892
14938,What are the various parameters to LOBPCG algorithm when usingmethod=”ortho”?,"ortho_fparams, ortho_bparams(ortho_iparams,)",892
14939,"Iftrackersetsbvars = True, the iteration process will be hard-stopped?",Iftrackersetsbvars,892
14940,What are ortho_fparams and ortho_bparams?,various parameters to LOBPCG algorithm when usingmethod=”ortho”,892
14941,"E,X,S,R- what?",iteration Tensor variables,892
14942,The current residualivars[“converged_count”]- the current number of converged eigenpairstvars,current state of convergence criteria,892
14943,What is the current iteration stepX- the current approximation of eigenvectorsE- the current approximation,ivars,892
14944,What is the current iteration stepX?,ivars,2551
14945,When does tracker make copies of Tensor objects?,whentrackerstores Tensor objects from the LOBPCG instance,2551
14946,"What does E,X,S,R refer to?",iteration Tensor variables,2230
14947,"What is the iteration of E,X,S,R?",Tensor variables,2230
14948,When must tracker make copies of Tensor objects?,whentrackerstores Tensor objects from the LOBPCG instance,2230
14949,When must it make copies of Tensor objects?,whentrackerstores Tensor objects from the LOBPCG instance,4499
14950,"Whentrackerstores Tensor objects from the LOBPCG instance, it must what?",make copies,4499
14951,"What does ortho_fparams, ortho_bparams(ortho_iparams) represent?",various parameters,4499
14952,"Iftrackersetsbvars[“force_stop”] = what, the iteration process will be hard-stopped?",True,3642
14953,What is the name of the parameter to the LOBPCG algorithm when usingmethod=”ortho”?,ortho_fparams,3642
14954,What is the name of the iteration process that will be hard-stopped?,Iftrackersetsbvars,3642
14955,"What does ortho_fparams, ortho_bparams(ortho_iparams) mean?",various parameters to LOBPCG algorithm,3642
14956,Returns a 2-dimensional view of each input tensor with what dimensions?,zero,5269
14957,How are input tensors with two or more dimensions returned?,as-is,5269
14958,What is assumed thatAis symmetric?,A.gradis not,6967
14959,In what routines is A - t * A.gradis symmetric?,first-order optimization routines,6967
14960,What does nSpecify if XXXis not specified?,nspecifies the size of the generated random approximation of eigenvectors,890
14961,What is the default value for the size of the generated random approximation of eigenvectors?,Default value fornisk,890
14962,"What is tol(float,optional) for stopping criterion?",residual tolerance,890
14963,What is the default isfeps?,smallest non-zero floating-point number of the given input tensorAdata type,890
14964,"When does use-1. tracker(callable,optional) – a function for tracing the iteration process?",until convergence criteria is met,9978
14965,"niter(int,optional) – what?",maximum number of iterations,9978
14966,Tracker is called at each iteration step with what as an argument?,LOBPCG instance,9978
14967,What is returned when the iteration process is hard-stopped?,the current approximation of eigenpairs,9978
14968,Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive,matrix-free LOBPCG methods,2445
14969,What type of method does the LOBPCG method with orthogonal basis selection use?,robust method,2445
14970,Is the use of the basic method recommended or recommended?,generally not recommended,2445
14971,Is Ais symmetric or not?,A.gradis not,2445
14972,What is the input tensor of size(optional)?,"X(tensor,optional)",2445
14973,What does torch.isfinite do?,Returns a new tensor with boolean elements representing if each element is finite or not,5359
14974,What are finite when both their real and imaginary parts are finite?,Complex values,5359
14975,What is requirement on tensor when using fliplr?,2-D,2473
14976,Why is copying a tensor's data slower than viewing that data?,more work,2473
14977,In what direction do the entries in each row of tensor appear in a different order than before?,left/right,2473
14978,What is the name of'snp.fliplr'?,NumPy,2473
14979,What is torch.fliplris expected to be?,slower thannp.fliplr,2473
14980,What does flip tensor in the left/right direction return?,a new tensor,2473
14981,What does flip tensor do?,Flip the entries in each row in the left/right direction,2473
14982,What must the tensor be at least?,2-D,2473
14983,Does torch.flip make a copy of input's data?,Yes torch.flip makes a copy ofinput’s data,5699
14984,What is the value to fill the output tensor with torch.full?,fill_value(Scalar),9341
14985,What  type of tensor is created with torch.full?,a tensor of sizesizefilled withfill_value,2029
14986,How is the desired layout of returned Tensor with torch.ones?,with torch.layout,9221
14987,What is the name of the layout of returned Tensor with torch.ones?,Default:torch.strided,9221
14988,What is the desired data type of returned Tensor with torch.zeros_like?,"dtype, if specified",5436
14989,What is the default layout of returned Tensor with torch.zeros_like?,Default:torch.strided,10357
14990,What is the current state of the generator?,a torch.ByteTensor,2702
14991,"What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?",Avoid having many 0 bits in the seed,2689
14992,What is the Generator state returned as?,a torch.ByteTensor,2689
14993,Generator.device -> device Gets what of the generator?,current device,2689
14994,What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?,Negative inputs,2689
14995,What does a generator get a non-deterministic random number from?,std::random_device or the current time,2689
14996,What does the generator do?,Sets the Generator state,2689
14997,What object is used to seed a Generator?,torch.Generator,2689
14998,What is the desired state?,new_state,2689
14999,What should you do when setting a large seed?,Avoid having many 0 bits in the seed,5867
15000,What is the object that sets the seed for generating random numbers?,torch.Generator,5867
15001,What is an example of a torch.Generator object?,Generator,5867
15002,What is reduced with torch.var_mean?,dimension,9185
15003,How to specify that output tensor should retain the dimension or not?,by specifying keepdim(bool),5667
15004,"Ifdimis a list of dimensions, what does it do?",reduce over all of them,5667
15005,"If specified, the input tensor is casted what before the operation is performed?",todtype,5667
15006,What is the default for the input tensor?,None,5667
15007,What does the tuple contain when using torch.var_mean?,variance and mean,10347
15008,"If funbiasedisTrue, what will be used?",Bessel's correction is used,10347
15009,What type of storage does Everytorch.Tensor cast?,bfloat16,1358
15010,What does filename(str) – file name to map shared(bool)?,filename(str) – file name to map shared(bool),1358
15011,What type of storage does Everytorch.Tensor return a copy of?,char,2270
15012,What happens if the object is already in CUDA memory and on the correct device?,no copy is performed and the original object is returned,10254
15013,"If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?",the argument has no effect,10254
15014,"For compatibility, may contain what place of thenon_blockingargument?",keyasyncin,10254
15015,What happens when memory is shared between all processes?,All changes are written to the file,10254
15016,What type of storage does the keyasyncin cast this storage to?,bfloat16 type,10254
15017,"If the object is already in CUDA memory and on the correct device, what happens?",no copy is performed,3532
15018,Returns a CPU copy of this storage if it’s not already on the CPU Returns a copy of what type of storage?,float type,5303
15019,Where is a copy of this object stored?,CUDA memory,5281
15020,"What if true and the source is in pinned memory, the copy will be asynchronous with respect to the host?",non_blocking(bool),3530
15021,If the object is already in what?,CUDA memory,3530
15022,What type of storage does the keyasyncin cast?,double type,3530
15023,What does bool stand for?,non_blocking,1535
15024,What type of storage is casts this storage to?,float type,1535
15025,Casts this storage to what type Casts this storage to bool type Casts this storage to bool type Casts this storage to,bfloat16,1535
15026,What happens when a storage is cast to float type IfsharedisTrue?,memory is shared between all processes,1551
15027,What does this no-op do for storages already in shared memory and for CUDA storages?,Moves the storage to shared memory,1536
15028,Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what storage?,CUDA,1536
15029,"IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",non_blocking(bool),1536
15030,"IfsharedisTrue, what happens?",memory is shared between all processes,1536
15031,"IfsharedisTrue, then memory is shared between all processes.",All changes are written to the file,1536
15032,What is the name of the number of elements in a storage?,size(int),10696
15033,If true how copy is performed what with respect to the host?,asynchronously,9224
15034,"If no copy is performed and the original object is returned, what happens?",If this is already of the correct type,1561
15035,What type of storage does Theasyncarg cast this storage to?,bfloat16 type,5682
15036,What type of storage does the keyasyncarg cast this storage to?,bfloat16,79
15037,What part of a complex tensor is equal to real?,real,1838
15038,What is the real part of the complex tensor?,real(Tensor),1838
15039,Real(Tensor) – The real part of the complex tensor. Must be what?,float or double,1838
15040,What is the imaginary part of the complex tensor?,imag,1838
15041,What must the imag(Tensor) be?,same dtype asreal,1838
15042,"If the inputs are torch.float64, ouput tensor must be what?",be torch.complex128,1838
15043,What does solve solve AX = b  assume A is?,upper-triangular,5969
15044,"What is the default value of transpose(bool,optional)?",False,5969
15045,"What is the name for multiple right-hand sides of size(,m,k)(*, m, k)(,",b(Tensor),6121
15046,"Supports input of float, double, cfloat and what other data type?",cdouble,6121
15047,What is the default value for the upper-triangular system of equations?,True,6121
15048,"What is the name of the input triangular coefficient matrix of size(,m,k)(*, m, k)(",b(Tensor),11237
15049,What is the default matrix system of equations?,upper triangular,11296
15050,"What is the default value of unitriangular(bool,optional)?",Default:False,11296
15051,What is the default value for transpose?,Default:False,11256
15052,"If True, the diagonal elements of A are assumed to be 1 and not referenced from A..",Default:False,11256
15053,"What is a solution, cloned_coefficient?",namedtuple,11256
15054,What is the error function of input do?,Computes the complementary error function ofinput,1762
15055,What is computed for each element of input with torch.special.expit?,exponentially scaled zeroth order modified Bessel function,1790
15056,What does input return with each of the elements of inputrounded to the closest integer?,Returns a new tensor,5368
15057,What is the nearly optimal approximation of a singular value decomposition of a centered matrix?,"namedtuple(U,S,V)",4809
15058,What do theVVVcolumns represent?,the principal directions,4810
15059,"matmul(A,V[:,:k])projects data to what?",first k principal components,4411
15060,What does S2/(m1)S ** 2 / (m - 1)S2/(m1)contains?,eigenvalues,4411
15061,What does the pseudorandom number generator do to obtain repeatable results?,reset the seed,6171
15062,What kind of (sub)gradients does this function produce?,deterministic,5638
15063,"Returns a namedtuple(values,indices) wherevaluesis the minimum value of each row of the inputtensor",wherevaluesis the minimum value of each row of theinputtensor in the given dimensiondim,5638
15064,What is the value of each row of the inputtensor in the given dimensiondim?,the minimum value of each row of theinputtensor in the given dimensiondim,8197
15065,Indices are ordered from left to right according to when each was sampled.,first samples are placed in first column,3845
15066,"Ifinputis a matrix,outis a matrix of shape(mnum_samples)(m times",matrix withmrows,3845
15067,"When a sample index is drawn without replacement, it cannot be drawn again for what row?",a row,3845
15068,num_samples must be lower than the min number of non-zero elements in each row of input if it is a,matrix,4446
15069,What is an example of a pseudorandom number generator?,Example,4446
15070,What does PyTorch use to represent neural networks?,modules,4997
15071,What are the building blocks of stateful computation?,Modules,4997
15072,How does PyTorch integrate with its autogradsystem?,Tightly integrated,4997
15073,What type of networks can PyTorch modules allow for?,multi-layer neural networks,1471
15074,How does PyTorch'sautogradsystem integrate?,Tightly integrated,1471
15075,What are some of PyTorch's modules able to do?,"prune, quantize",1471
15076,What is PyTorch's Optimizers tightly integrated with?,PyTorch’sautogradsystem,7851
15077,What are modules easy to work with and do?,transform,7851
15078,What module uses an affine transformation to its input?,PyTorch’sLinearmodule,4253
15079,"What are not required to have state, and can also be stateless?",modules,4253
15080,What should subclassModule for composability with other modules?,module,3922
15081,What do random-initializedweightandbiastensors define?,affine transformation,7814
15082,The input is matrix-multiplied with what?,theweightparameter,3906
15083,"What can contain other modules, making them useful building blocks for developing more elaborate functionality?",Modules,4259
15084,What allows us to chain together multiple modules?,theSequentialmodule,4259
15085,What does Sequentialautomatically feed the output of the first MyLinearmodule as input into?,theReLU,4509
15086,What is the limit of Sequentialautomatically feeding the output of the firstMyLinearmodule as input into the secondMyLinearmodule?,in-order chaining of modules,4509
15087,Why is it recommended to define a custom module for anything beyond the simplest use cases?,full flexibility on how submodules are used for a module’s computation,3720
15088,What can be iterated through via a call tochildren() ornamed_children()?,children,3720
15089,What do calls toparameters() andnamed_parameters() do?,recursively include child parameters,3720
15090,What does defining a custom module give you?,full flexibility,3719
15091,What are the two submodules of a neural network called?,children,2514
15092,What are the parameters of a module?,its direct parameters as well as the parameters of all submodules,2502
15093,What can be used to optimize a neural net?,PyTorch’s Optimizers,4103
15094,What does loss.backward() call to apply the gradients to the parameters?,optimizer.step(),9019
15095,"Who computes a loss, zeros the network's parameters' gradients, calls loss.backward() to update the parameters' gradients",runs the network,10630
15096,What can sometimes be tricky?,Training neural networks,11469
15097,How many layers does _layer_net_optim.html have?,two,7996
15098,What is called to apply gradients to the parameters?,optimizer.step(),9020
15099,How many layers of a neural network can be optimized?,two,9020
15100,What can we save if we want to save a model to disk?,itsstate_dict,3792
15101,What is itsstate_dict?,state dictionary,3792
15102,Where are learnable aspects of computation contained?,thestate_dict,4749
15103,What are non-learnable aspects of computation?,Persistentbuffers,4749
15104,Non-persistentbuffers are left out of what?,serialization,4749
15105,What are non-persistentbuffers?,not contained within thestate_dict,1469
15106,Where are Persistentbuffers contained?,thestate_dict,1469
15107,Why are non-persistentbuffers not contained within thestate_dict?,left out of serialization,1469
15108,What is the purpose of the current value of the running mean?,it will be restored when loading a serialized form of the module,1284
15109,What is considered part of the module’sstate_dictand?,the current value of the running mean,4547
15110,For what do you need to check out:,more information,4547
15111,What can be iterated over usingbuffers() ornamed_buffers()?,Buffers of a module,1459
15112,"For more information, check out what?",Saving and loading,1459
15113,What allows the user to return an updated value that will be used throughout the remaining computation?,hooks,1084
15114,What is Atorch.Tensoris?,multi-dimensional matrix,1359
15115,What type of tensoris float64?,double,11064
15116,What are some examples of atorch.Tensor attributes?,"thetorch.dtype,torch.device, andtorch.layoutattributes",2238
15117,What does current implementation oftorch.Tensor introduce?,memory overhead,2238
15118,What Returns a new Tensor withdataas the tensor data?,new_tensor,7887
15119,Computes what of either a matrix or batch of matricesinput?,singular value decomposition,1804
15120,What is the singular value decomposition represented as?,"namedtuple(U, S, V)",1804
15121,What is the transpose ofVfor?,real inputs,1803
15122,"Ifinputis a batch of what, thenU,S, andVare also batched with the same batch dimensions asinput?",matrices,1803
15123,"If someisTrue(default), the returnedUandVmatrices will contain what?","onlymin(n, m)orthonormal columns",3640
15124,What effect does argumentsome have whencompute_uvisFalse?,argumentsomehas no effect whencompute_uvisFalse,3640
15125,"What data type is supported by float, double, cfloat and cfloat?",cdouble,3640
15126,What should be replaced with withtorch.linalg.svd()?,Note Differences,3640
15127,"Ifcompute_uvisFalse, the returnedUandVwill be what?",zero-filled matrices,3608
15128,The dtypes ofUandVare the same asinput’s.Swill always be what?,real-valued,3608
15129,What is deprecated in favor of oftorch.linalg.svd()?,torch.svd(),3607
15130,What does the implementation oftorch.linalg.svd() on CPU use instead of?gesvdfor speed?,LAPACK’s routine?gesdd,7117
15131,What version of CUDA does cuSOLVER's routinesgesvdjandgesvdjBatchedon?,10.1.243,7299
15132,What happens when a matrix is represented as a column-major matrix?,returnedUwill not be contiguous,4401
15133,The matrix (or batch of matrices) will be represented as what?,column-major matrix,4415
15134,The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. what?,Fortran-contiguous,4415
15135,What does returnedUnot be contiguous?,returnedUwill not be contiguous,4415
15136,What is the name of the warning that the matrix will not be contiguous?,Warning,7116
15137,When will the gradients with respect toUandVonly be finite?,when the input does not have zero nor repeated singular values,7101
15138,When do gradients with respect toUandV depend onS1?,when the matrix has small singular values,7101
15139,What can be multiplied byUandV?,arbitrary phase factor,8150
15140,"Different platforms, like NumPy, or inputs on different device types, may produce what?",differentUandVtensors,8150
15141,"What is the input tensor of size(*, m, n)where*is zero or more batch dimensions consisting of(",input(Tensor),8150
15142,What controls whether to compute the reduced or full decomposition?,"some(bool,optional)",8150
15143,What is the output tuple of tensors?,"out(tuple,optional)",8150
15144,When is the singular value decomposition not unique?,wheninputhas repeated singular values,8150
15145,"What setting controls whether to compute the reduced or full decomposition, and consequently, the shape of returnedUandV?",Default:True,8150
15146,What controls whether to computeUandV?,compute_uv,8150
15147,What is the default value for compute_uv?,Default:True,8150
15148,What can be used to multiply the columns of the spanning subspace inUandV?,a rotation matrix,8149
15149,"What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?",Default:True,9652
15150,What is the name of the function that controls whether to computeUandV?,compute_uv,9652
15151,"What is the default value of compute_uv(bool,optional)?",Default:True,9652
15152,What does SciPy do with the following cases?,Computesinput*log(other),1810
15153,Computesinput*log(other)with the following cases. Similar to what?,SciPy’sscipy.special.xlogy,1810
15154,At least one of inputorothermust be what?,a tensor,1810
15155,Converts a float tensor to what?,a per-channel quantized tensor,1909
15156,What must be used to convert a float tensor to a per-channel quantized tensor?,one of the quantized dtypes,1909
15157,What does the context-manager do if it has been disabled viano_gradorset_grad_enabled?,Enables gradient calculation,1876
15158,Why is this context manager thread local?,it will not affect computation in other threads,1876
15159,What does this context manager function as?,decorator,1876
15160,Make sure to instantiate with what?,parenthesis,1876
15161,What might be complex?,eigenvalues and eigenvectors,1726
15162,When input is on CUDA torch.eig()causes what?,host-device synchronization,1727
15163,What does it mean to split a tensor into a specific number of chunks?,Concatenates the given sequence ofseqtensors in the given dimension,1822
15164,What axis does Splitsinput gather values along?,bydim,1820
15165,How many chunks does a tensor split into?,chunks,6024
15166,How many dimensions does Splitsinput have?,one or more dimensions,6024
15167,What is the name of the tensor that Stacks tensors in sequence depthwise?,Gathers values along an axis specified bydim,6024
15168,How do tensors stack in sequence?,horizontally,6066
15169,What is the new tensor that indexes theinputtensor according to the boolean maskmask?,1-D,6066
15170,What is acolumn wise?,Stack tensors in sequence horizontally,2005
15171,What is the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?,aLongTensor,2005
15172,What is the length of Stack tensors in sequence depthwise?,third axis,6061
15173,What does Stack tensors in sequence depthwise gather?,Gathers values along an axis specified bydim,6061
15174,What is the new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?,aLongTensor,2674
15175,What type of tensor is in sequence horizontally?,Stack,6045
15176,What is dimensiondimusing the entries inindex?,aLongTensor,5355
15177,What version ofinputtensor is Alias fortorch.movedim()?,narrowed,5355
15178,What is the name of the tensor that is a narrowed version ofinputtensor?,Alias oftorch.vstack(),5347
15179,What type of version ofinputtensor is a new tensor?,narrowed,5347
15180,What is the name of the new tensor that indexes theinputtensor according to the boolean maskmask,1-D,5332
15181,What happens to a sequence of tensors?,Concatenates a sequence of tensors along a new dimension,1055
15182,What does a tensor concatenate?,a sequence of tensors along a new dimension,5472
15183,What does the generator object generate?,pseudo random numbers,2037
15184,What does atorch.ByteTensor mean?,Sets the seed for generating random numbers,5865
15185,What is atorch.ByteTensor?,random number generator state,5861
15186,What are given to a tensor of random numbers drawn from separate normal distributions?,mean and standard deviation,5450
15187,What does a Bernoulli distribution draw?,Draws binary random numbers,2209
15188,What does a tensor return with each element sampled from a Poisson distribution with rate parameter given by the corresponding element inin,a tensor of the same size asinput,5456
15189,Where is a tensor filled with random numbers?,uniform distribution,5427
15190,Where is a tensor filled with random integers generated?,betweenlow(inclusive) andhigh(exclusive),5427
15191,What is a tensor with the same size asinput filled with random numbers?,uniform distribution,5493
15192,What is the name of the tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh?,exclusive,5493
15193,What is a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?,tensor,5419
15194,What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?,same shape,5417
15195,What is the same shape as Tensorinputfilled with random integers generated?,uniformly,5477
15196,What is a tensor filled with random numbers from a normal distribution with mean0and variance1?,random permutation of integers from0ton-1,5424
15197,What does a random permutation of integers from0ton-1 mean?,mean 0 and variance 1.,5424
15198,What are some more in-place random sampling functions defined on?,Tensors,5424
15199,Where are there more in-place random sampling functions defined?,Tensors,7395
15200,What type of distribution does tensor.log_normal_() originate from?,log-normal distribution,7394
15201,What type of distribution did the numbers sample from?,discrete uniform distribution,10957
15202,What is saved to a disk file?,an object,5746
15203,Loads an object saved from a file?,withtorch.save(),5746
15204,What is the number of threads used for inter-op parallelism on CPU?,the number of threads used for interop parallelism,5647
15205,what Returns the number of threads used for what parallelism on CPU?,inter-op,5647
15206,What is a Context-manager?,disabled gradient calculation,2342
15207,What does Context-manager enable?,gradient calculation,2342
15208,What does Context-manager set gradient calculation to?,on or off,2342
15209,What does inference mode return if inference mode is currently enabled?,True,2342
15210,What does Alias fortorch.abs() return a new tensor?,inverse hyperbolic cosine,1040
15211,What does the scalarotherto each element of inputinput return a new resulting tensor?,Alias fortorch.acosh(),1040
15212,What does Alias fortorch.abs() compute the inverse cosine of each element ininput?,Alias fortorch.acos(),1694
15213,Who returns a new tensor with the arcsine of the elements ofinput?,Alias fortorch.asin(),1694
15214,What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinput,Alias fortorch.atan,1694
15215,What type of cosine does Alias fortorch.acosh() return a new tensor?,hyperbolic,1773
15216,What is the cosine of elements ofinput?,hyperbolic,1042
15217,What does Alias fortorch.acosh() perform?,the element-wise multiplication,1044
15218,What does add the scalarotherto each element of the inputinputand returns a new resulting tensor?,Alias fortorch.acosh(),1044
15219,What is the inverse of the elements ofinput?,hyperbolic sine,1046
15220,What is the name of the new tensor with the arctangent of the elements ofinput?,Alias fortorch.asinh(),1046
15221,What does Alias fortorch.atanh() have?,inverse hyperbolic tangent,1046
15222,What does the scalarotherto multiply the result by the scalarvalueand add it toinput?,element-wise division,973
15223,What does the element-wise division oftensor1bytensor2 perform?,the element-wise multiplication,973
15224,What is the name of the elements ofinput?,arcsine,973
15225,What does each element of inputinput return a new resulting tensor?,scalarotherto,973
15226,Performs what division of oftensor1bytensor2?,element-wise division,4822
15227,Who returns a new tensor with the arcsine of elements ofinput?,Alias fortorch.asin,5374
15228,What is the name of the element-wise arctangent ofinputi/otheritextinput_i,Alias fortorch.atanh,1045
15229,What does Alias fortorch.asinh() do?,Computes the bitwise OR ofinputandother,1047
15230,What is the indices of the maximum value of all elements in?,theinputtensor,5552
15231,What is the minimum value of a flattened tensor?,the flattened tensor or along a dimension,5552
15232,What returns the indices of the minimum value of each slice of theinputtensor in the given dimension(s)dim?,the minimum value of each slice of theinputtensor in the given dimension(s)dim,5562
15233,Where is the minimum value of all elements in theinputtensor?,theinputtensor,5562
15234,What is the return of (input-other) elements in theinputtensor?,p-norm,5562
15235,What does the input tensor test if all elements ininputevaluate to?,Tests if all elements ininputevaluate toTrue,5615
15236,What is the minimum value of all elements in?,theinputtensor,5645
15237,How do all elements ininputevaluate toTrue?,Tests,6766
15238,What does the input tensor ignore?,the median of the values ininput,10870
15239,What is the mean value of all elements in?,theinputtensor,5619
15240,What does ignoringNaNvalues mean value in theinputtensor?,the median of the values ininput,5619
15241,What does Computes element-wise equality?,Trueif two tensors have the same size and elements,5569
15242,What is the name of the name of the tensor?,Alias fortorch.gt(),5569
15243,How do two tensors have the same size and elements?,Trueif two tensors have the same size and elements,8009
15244,What does Alias fortorch.ge() use to return a new tensor with boolean elements representing if,Computesinput,8009
15245,Who returns a new tensor with boolean elements representing if each element isfiniteor not?,Alias fortorch.gt(),8009
15246,What is the name of Alias fortorch.ge()?,Computesinput,1050
15247,What is a new tensor with boolean elements representing if each element ofinputis close to the corresponding element of,Alias fortorch.gt(),1050
15248,What is the test if each element ofinputis infinite?,positive infinity,1812
15249,What type of input does Alias fortorch.gt() use?,Computesinput,1812
15250,What is a new tensor with boolean elements representing if each element isfiniteor not?,Alias fortorch.gt(),1812
15251,What is a new tensor with boolean elements representing if each element ofinputis “close” to the corresponding,Alias fortorch.gt(),1052
15252,What represents if each element ofinputis “close” to the corresponding element ofother?,boolean elements,1052
15253,What test does each element ofinputis negative infinity or not?,Tests,5367
15254,What is a positive or negative infinity test?,infinite,6775
15255,What does a new tensor with boolean elements represent if each element ofinputis real-valued or not?,Tests,6775
15256,Tests if each element ofinputis is what?,positive infinity,6777
15257,What is the test if each element ofinputis positive infinity or not?,negative infinity,6777
15258,What happens if each element ofinputis negative infinity or not?,Tests,6776
15259,What is the name of the short time Fourier Transform?,Inverse,5886
15260,What type of window function is Hamming window function?,Blackman,5886
15261,What is the name of the blackman window function?,Hamming,5886
15262,What is the name of the window function that computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?,Hann,5886
15263,What does the Kaiser window have?,window lengthwindow_lengthand shape parameterbeta,5886
15264,What is a view of each input tensor with zero dimensions?,3-dimensional,5272
15265,What is a block diagonal matrix from provided tensors?,Create a block diagonal matrix,5272
15266,Broadcastsinputto the shapeshape is similar tobroadcast_tensors() but for what?,shapes,5272
15267,How many dimensions does a 3-dimensional view of each input tensor have?,zero,5279
15268,What does Count each value in an array of non-negative ints?,frequency,5279
15269,What is the name of a block diagonal matrix from provided tensors?,Create a block diagonal matrix,5279
15270,What is the same asbroadcast_tensors()?,shapes,5279
15271,What are the indices of the buckets to which each value in theinputbelongs?,the boundaries of the buckets are set byboundaries,1465
15272,What doesbroadcast_tensors() look for?,shapes,1465
15273,What distance does a cartesian product batched between each pair of two collections of row vectors?,p-norm distance,1465
15274,What does a cartesian product of the given sequence of tensors return?,copy ofinput,1465
15275,What is the cumulative minimum of elements ofinputin the dimensiondim?,a namedtuple,5520
15276,What is the cumulative sum of elements ofinputin the dimensiondim?,cumulative product,5520
15277,What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?,add step,4779
15278,What does the batch matrix-matrix product of matrices stored inbatch1andbatch2 perform?,matrix multiplication,4779
15279,What is the name of the matrices inbatch1andbatch2?,batch matrix-matrix product,4779
15280,What does the vectorvec perform?,matrix-vector product,4796
15281,What does the matricesmat1andmat2 perform?,matrix multiplication,4796
15282,What does the matrix-vector product perform?,batch matrix-matrix product of matrices inbatch1andbatch2,4806
15283,What is the matrix-vector product of vectorsvec1andvec2?,outer-product,4804
15284,What decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?,Cholesky,4804
15285,What is the name of the batch matrix-matrix product of matrices stored ininputandmat2?,Alias,4786
15286,What is a batch matrix-matrix product of matrices stored?,ininputandmat2,4785
15287,What are the numbers of a real square matrix?,eigenvalues and eigenvectors,4785
15288,Who calculates log determinant of a square matrix or batches of square matrices?,Alias fortorch.linalg.slogdet,4785
15289,What is the outer-product of?,vectorsvec1andvec2,4841
15290,What is a low-level function for calling directly?,LAPACK’s geqrf,4841
15291,What is the name of Alias fortorch.linalg.det()?,Alias fortorch.linalg.inv,4841
15292,What does a batch matrix-matrix product of matrices inbatch1andbatch2 perform?,batch matrix-matrix product of matrices inbatch1andbatch2,4839
15293,What is the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?,Cholesky,5602
15294,What is a real square matrix?,eigenvalues and eigenvectors,5602
15295,What is the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?,Cholesky factor matrixuuu,1782
15296,What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuuu,Solves,4784
15297,What matrix product does the Cholesky decomposition of a symmetric positive-definite matrixAAAor return?,NNN2-D tensors,5601
15298,What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?,dot product of two 1D tensors,5601
15299,What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?,Solves,1674
15300,What does the Cholesky decomposition of for batches of symmetric positive-definite matrices?,symmetric positive-definite matrixAAAor,1674
15301,What is the inverse of a symmetric positive-definite matrixAAAor?,returns matrixinv,1674
15302,What is the Cholesky factor matrixuuu?,dot product of two 1D tensors,4782
15303,What does AAAusing's Cholesky factor matrixuuu compute?,dot product of two 1D tensors,1781
15304,What does a real square matrix have?,eigenvalues and eigenvectors,1723
15305,What is a low-level function for calling LAPACK's geqrf?,Alias oftorch.outer(),1723
15306,What is the eigenvalues and eigenvectors of a real square matrix?,low-level function,1723
15307,What is a positive semidefinite matrix to be inverted?,Cholesky factor matrixuuu,5967
15308,Computes what dot product of two 1D tensors?,dot product of two 1D tensors,5967
15309,What does this function do for calling LAPACK's geqrf directly?,low-level function,1710
15310,"As of 0.4, this function does not support what?",anoutkeyword,5438
15311,What is equivalent totorch.zeros?,oldtorch.zeros_like,5438
15312,What is the size ofinput?,input(Tensor),5438
15313,What is the default to the dtype ofinput?,ifNone,5438
15314,What is the default to the layout ofinput?,ifNone,5438
15315,"What is equivalent totorch.zeros(input.size(),out=output)?",oldtorch.zeros_like,5438
15316,What is the default to the device ofinput?,ifNone,5438
15317,What is the default for a tensor?,False,5438
15318,What is the size ofinput that determines the size of the output tensor?,input(Tensor),5437
15319,"What is equivalent to totorch.zeros(input.size(),out=output)?",oldtorch.zeros_like,5437
15320,"Warning As of 0.4, this function does not support what?",anoutkeyword,8138
15321,What is the oldtorch.zeros_like equivalent?,totorch.zeros,1309
15322,As of what date does this function not support anoutkeyword?,0.4,1309
15323,What is the desired layout of tensor?,layout,9215
15324,What is the name of the object that gets the current device of a Generator?,torch.Generator object,1132
15325,What does atorch.Generator object do?,Sets the seed for generating random numbers,1132
15326,What is an example of a generator that gets a non-deterministic random number from std::random_device?,torch.Generator object,1132
15327,What does it do when a generator is seeded?,Sets the Generator state,2688
15328,What does generator.device -> device get?,current device,2312
15329,What object is used to get a non-deterministic random number from std::random_device or the current time?,torch.Generator,2312
15330,What does a Generator get a non-deterministic random number from?,std::random_device or the current time,2312
15331,Everytorch.Tensorcasts this storage to what type of type?,bfloat16,2271
15332,What type of storage does everytorch.Tensorcast?,float type,2271
15333,What type of storage does Casts this storage to bool type Casts this storage to char type Returns?,a copy,1538
15334,What does Casts this storage to char type Return?,a copy,1540
15335,Where is a copy of this storage stored?,CUDA memory,1547
15336,What type of storage does the changes on the storage not affect the file?,IfsharedisFalse,1550
15337,What type of argument has no effect?,non_blocking,10256
15338,What does filename(str) mean?,filename(str) – file name to map shared,10256
15339,"If the changes on the storage do not affect the file, what is the name of the file that must contain at leastsize * sizeof(Type)",IfsharedisFalse,78
15340,What does it do to the storage if it's not already pinned?,Moves the storage to shared memory,78
15341,What is the name of the file that must contain at leastsize * sizeof(Type)bytes?,IfsharedisFalse,10700
15342,What Casts this storage to short type Returns a list containing the elements of this storage?,self,4279
15343,What may be multiplied by for complex-valuedinput?,arbitrary phase factor,8162
15344,What happens when inputhas multiple singular values?,repeated singular values,8162
15345,"When inputis on CUDA,torch.eig()causes what?",host-device synchronization,5924
15346,"Ifeigenvectors=False, what is it?",empty tensor,9236
15347,If the correspondingeigenvalues[j]is a what?,real number,9236
15348,If the correspondingeigenvalues[j]andeigenvalues[j + 1]form what?,a complex conjugate pair,9236
15349,What can shape(nn)(n times n)(nn) be used to compute?,normalized (unit length) eigenvectors,4397
15350,What could a map-style dataset read from a folder on the disk?,theidx-th image and its corresponding label,1350
15351,Who configures the options for map-style and iterable-style datasets?,the constructor arguments of aDataLoader,9870
15352,What is the name of a dataset that could read theidx-th image and its corresponding label from a folder on the disk?,SeeDataset,2526
15353,When using anIterableDatasetwith what?,multi-process data loading,2526
15354,"Automatic batching, single- and multi-process data loading, and what other option are configured by the constructor arguments of aDataLoa",automatic memory pinning,8924
15355,How are the options configured?,the constructor arguments of aDataLoader,10692
15356,What is an instance of a subclass ofIterableDataset that implements the__iter__()protocol?,iterable-style dataset,7189
15357,What document describes how to avoid duplicated data when using multi-process data loading?,SeeIterableDatasetdocumentations,7189
15358,What are iterable-style datasets particularly suitable for?,cases where random reads are expensive or even improbable,1130
15359,What is the name of a map-style dataset that could read theidx-th image and its corresponding label from a folder on,SeeDataset,7188
15360,What is the name of the dataset that implements the__iter__()protocol?,SeeDataset,5796
15361,What is anIterableDatasetwith?,multi-process data loading,7273
15362,How is the data loading order controlled by the user-defined iterable?,yielding a batched sample at each time,7273
15363,What are sampleclasses used to do?,represent iterable objects over the indices to datasets,7253
15364,What can aSampler do in the common case with stochastic gradient decent?,randomly permute a list of indices and yield each one at a time,7253
15365,How will a sequential or shuffled sampler be constructed?,based on theshuffleargument to aDataLoader,7253
15366,Who supports automatically collating individual fetched data samples into batches?,DataLoader,785
15367,What are batched samples?,containing Tensors,4303
15368,What yields batched samples instead of individual samples whenbatch_size(default1) is notNone?,data loader,4303
15369,What type of datasets can users alternatively specifybatch_sampler?,map-style datasets,4303
15370,"In some cases, users may want to handle batching what?",manually in dataset code,8860
15371,What type of data would it be cheaper to load manually?,batched data,8860
15372,Who returns each member of thedatasetobject?,the data loader,979
15373,"In certain cases, users may want to handle batching what?",manually,787
15374,Each sample obtained from thedatasetis processed with the function passed as what?,thecollate_fnargument,787
15375,What could be cheaper to handle batching manually in dataset code?,directly load batched data,787
15376,What is roughly equivalent to loading from when automatic batching is disabled?,map-style dataset,787
15377,What is used to collate samples?,automatic batching,786
15378,What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareN,automatic batching,3709
15379,What is the function passed to each sample obtained from thedataset?,thecollate_fnargument,8359
15380,What does the defaultcollate_fnsimply do when automatic batching is disabled?,defaultcollate_fnsimply converts NumPy arrays into PyTorch Tensors,8348
15381,"When automatic batching is disabled,collate_fnis called with what?",each individual data sample,8348
15382,What is expected when automatic batching is enabled?,to collate the input samples into a batch,3798
15383,How many subprocesses to use for data loading?,how many subprocesses to use for data loading,10279
15384,What is the name of the main process that will be loaded in the main process?,default:0,10641
15385,What can be used to define the strategy to draw samples from the dataset?,anyIterablewith__len__implemented,10641
15386,What must not be specified if specified?,shuffle,10641
15387,What is the name of how many subprocesses to use for data loading?,num_workers,10641
15388,What is the default value for data loading?,default:0,10641
15389,"What is the name of the sampler that likesampler, but returns a batch of indices at a time?",batch_sampler,10642
15390,What is the mutually exclusive feature of batch_sampler?,"withbatch_size,shuffle,sampler, anddrop_last",8950
15391,What does collate_fn merge a list of samples to form?,a mini-batch of Tensor,10277
15392,What is used when using a map-style dataset?,batched loading,10278
15393,What is used when using batched loading from a map-style dataset?,pin_memory,10278
15394,If your data elements are a what type?,custom type,10278
15395,What is the default timeout value for collecting a batch from workers?,0,10278
15396,What is num_workers?,total number of workers,2278
15397,What merges a list of samples to form a mini-batch of Tensor(s)?,collate_fn,9094
15398,What is set toTrue to drop the last incomplete batch if the dataset size is not divisible by the batch size?,drop_last,9094
15399,What is set toTrueto drop the last incomplete batch?,drop_last,9094
15400,What type of dataset is collate_fn used when using batched loading?,map-style dataset,8949
15401,When using batched loading from what type of dataset?,map-style dataset,9093
15402,What is the default value for worker_init_fn?,default:None,9093
15403,What merges a list of samples to form a mini-batch of Tensors?,collate_fn,9092
15404,What is collate_fn used when using?,batched loading,9092
15405,If your data elements are what?,a custom type,9092
15406,What pinned memory does the data loader copy Tensors into?,CUDA,10413
15407,What type of data elements does yourcollate_fn return a batch that is a custom type?,a custom type,10413
15408,What is the name of the data loader that will copy Tensors into CUDA pinned memory before returning them?,pin_memory,10412
15409,What is the name of the name of a batch that is not divisible by the batch size?,default:False,10412
15410,What type of data elements does yourcollate_fnreturns a batch that is a custom type?,a custom type,10412
15411,What is the default setting for the size of a dataset?,default:False,10412
15412,"What is bool,optional?",pin_memory,10414
15413,What is the timeout value for collecting a batch from workers?,timeout,10414
15414,What should the timeout value for collecting a batch from workers always be?,non-negative,10414
15415,"What is int,optional,keyword-only arg?",prefetch_factor,10414
15416,What does the data loader not shutdown the worker processes after a dataset has been consumed once?,IfTrue,10414
15417,What is a worker_init_fncannot be an unpicklable object?,lambda function,10414
15418,Where is Multiprocessing best practiceson more details related to multiprocessing?,PyTorch,10414
15419,What is the size of the dataset not divisible by the batch size?,IfFalseand the size of dataset is not divisible by the batch size,9199
15420,How many num_workers samples will be prefetched across all workers?,2,9199
15421,"If the size of dataset is not divisible by the batch size, what happens?",the last batch will be smaller,9200
15422,What will be smaller if the dataset size is not divisible by the batch size?,the last batch,9198
15423,"What will be called on each worker subprocess with the worker id (an int in[0,num_workers-1]) as",worker_init_fn,11421
15424,Where can you find more details about multiprocessing?,PyTorch,11421
15425,"If notNone, worker_init_fn(callable,optional) will be called on each worker subprocess with what input?",worker id,11420
15426,"What is the default value of generator(torch.Generator,optional)?",default:None,11420
15427,Who will use this RNG to generate random indexes?,RandomSampler,10919
15428,Who will use generator to generate random indexes and multiprocessing to generatebase_seedfor workers?,RandomSampler,9447
15429,How many prefetch_factors are there?,2,10438
15430,"What is the name of the word ""False""?",Warning,10438
15431,What does the data loader allow to keep alive after a dataset has been consumed once?,Datasetinstances,10439
15432,What does PyTorch have to do with multiprocessing best practices?,Warning,10439
15433,What is the term for persistent_workers?,IfTrue,10405
15434,What does persistent_workers allow to keep alive?,workersDatasetinstances,10405
15435,What does PyTorch do when it comes to multiprocessing?,Warning,10405
15436,"Worker_init_fncannot be an unpicklable object, e.g., a lambda function.",thespawnstart method,8164
15437,What type of numbers do My data loader workers return?,random,8164
15438,What does a custom sampler need to be provided to make it work with a map-style dataset with non-integral indices/,iterable Dataset,8164
15439,What is an example of a function that is not unpicklable?,lambda function,8163
15440,Where is the Multiprocessing best practiceson more details related to multiprocessing?,PyTorch,8163
15441,What is an example of a spawnstart method?,Warning,8163
15442,Where can you find more information about multiprocessing?,PyTorch,8163
15443,"Worker_init_fncannot be an unpicklable object, e.g., a lambda function?",thespawnstart method,3527
15444,What does worker_init_fncannot be an unpicklable object?,lambda function,3527
15445,What is the name of the multiprocessing best practices?,PyTorch,3527
15446,"Worker_init_fncannot be an unpicklable object, e.g., what?",a lambda function,3528
15447,What is the warning len(dataloader)heuristic based on?,the length of the sampler used,3528
15448,When are iterable datasets particularly useful?,when data come from a stream,3528
15449,"Whendatasetis what, it instead returns an estimate based on onlen(dataset)/batch_size?",anIterableDataset,9781
15450,What does PyTorch trust in handling multi-process loading?,userdatasetcode,8211
15451,What interacts with Multi-process data loading?,howIterableDataset,8211
15452,What is len(dataloader)heuristic based on?,the length of the sampler used,9782
15453,When can more than one batch worth of samples be dropped?,whendrop_lastis set,2965
15454,What program can not detect sharding cases in general?,PyTorch,2965
15455,What does howIterableDataset interact with?,Multi-process data loading,2965
15456,Which type of dataset can not detect such cases in general?,PyTorch,2966
15457,What can happen to an otherwise complete batch?,broken into multiple ones,2966
15458,What interacts withMulti-process data loading?,howIterableDataset,5795
15459,What are subclasses expected to overwrite__len__()?,manySamplerimplementations and the default options ofDataLoader,5795
15460,What could subclasses optionally do?,overwrite__len__(),5795
15461,What class represents aDataset?,abstract class,1119
15462,What is the name of the list of datasets to be concatenated?,datasets(sequence) – List of datasets to be concatenated,1119
15463,What does My data loader workers return identical random numbers?,Warning SeeReproducibility,8179
15464,What do multi-process data loadingnotes return for random seed related questions?,Randomness,8179
15465,What does an abstract class represent?,aDataset,5799
15466,What does SeeReproducibility andMy data loader workers return?,random numbers,5799
15467,Randomness in multi-process data loadingnotes for what?,random seed related questions,8180
15468,All datasets that represent what should subclass it?,a map from keys to data samples,1079
15469,What should all subclasses do?,overwrite__iter__(),1079
15470,What should all subclasses overwrite__getitem__() support?,fetching a data sample for a given key,5800
15471,What type of class represents aDataset?,abstract class,1118
15472,What should all subclasses overwrite to overwrite__len__()?,Note,1118
15473,All datasets that represent a map from keys to data samples should what?,subclass it,1080
15474,What should all subclasses do to support fetching a data sample for a given key?,overwrite__getitem__(),1080
15475,What should all datasets that represent a map from keys to data samples do?,subclass it,1080
15476,What must be provided to make it work with a map-style dataset with non-integral indices/keys?,a custom sampler must be provided,1080
15477,What is the example of splitting workload across all workers using worker_init_fn?,splitting workload across all workers,1080
15478,What will each sample be retrieved by?,indexing tensors along the first dimension,1080
15479,What do all datasets that represent from keys to data samples should subclass it?,a map,1078
15480,What is the name of a subclass that should overwrite__getitem__()?,Note,1078
15481,What does DataLoaderby default construct?,index sampler,4368
15482,What should all datasets that represent an iterable of data samples should do?,subclass it,1081
15483,What type of stream does a subclass of data come from?,a stream,1081
15484,Where does data come from?,a stream,2066
15485,What does DataLoaderby use to create a custom sampler?,An iterable Dataset,2066
15486,What default constructs a index sampler that yields integral indices?,DataLoaderby,2068
15487,What is a custom sampler required to work with a map-style dataset with non-integral indices/keys?,iterable Dataset,2067
15488,What should all datasets that represent an iterable of data samples do?,subclass it,1082
15489,When are subclasses particularly useful?,when data come from a stream,1082
15490,What is the name of the dataset that represents an iterable of data samples?,An iterable Dataset,1127
15491,What is an iterable dataset particularly useful when data come from?,a stream,1127
15492,What would return an iterator of samples in an iterable dataset?,overwrite__iter__(),1127
15493,What is a dataset that represents an iterable of data samples called?,iterable Dataset,1128
15494,What would overwrite__iter__() do?,return an iterator of samples,1092
15495,What does *tensors (Tensor) mean?,tensors that have the same size of the first dimension,2276
15496,What is this class useful to?,assemble different existing datasets,2276
15497,What is the definition of datasets to be concatenated?,List of datasets to be concatenated,2276
15498,Each sample will be retrieved by what along the first dimension?,indexing tensors,2276
15499,What is a concatenation of multiple datasets?,Dataset,2280
15500,What are tensors that have the same size of the first dimension?,*tensors(Tensor),2280
15501,What is the example 1 of?,splitting workload across all workers,2277
15502,What is this class useful for?,assemble different existing dataset streams,2277
15503,How is each sample retrieved?,indexing tensors along the first dimension,2277
15504,What class is useful to assemble different existing datasets?,Dataset,8346
15505,What is used to split workload across all workers?,worker_init_fn,2279
15506,What is a concatenation of multiple datasets useful for?,to assemble different existing datasets,2279
15507,What is the name of the list of datasets to be concatenated Dataset for chainning multipleIterableDatasets?,List of datasets to be concatenated Dataset for chainning multipleIterableDatasets,1129
15508,What is this useful for preventing?,data type overflows,9663
15509,What is a return of the input tensor in the given dimensiondim?,the sum of each row of theinputtensor in the given dimensiondim,9663
15510,What is returned when a row of the inputtensor is set to a given dimensiondim?,Returns the sum of each row of theinputtensor in the given dimensiondim,2327
15511,"Ifdimis a list of dimensions, reduce over all of them?",IfkeepdimisTrue,2327
15512,What is the name of the output tensor that has fewer dimension(s)?,orlen(dim),2327
15513,What is the sum of each row of theinputtensor in the given dimensiondim?,Ifdimis a list of dimensions,5670
15514,What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?,IfkeepdimisTrue,5670
15515,What is the name of the output tensor that has fewer dimension(s) than input?,orlen(dim),9664
15516,"If the output tensor is of the same size as input, what is it?",IfkeepdimisTrue,3623
15517,What does the input tensor compute?,bitwise NOT,1698
15518,What type of type does the input tensor have?,Boolean types,1698
15519,"For what type of tensor, it computes the logical NOT of the input tensor?",bool tensors,1698
15520,What is out of the input tensor?,output tensor,1698
15521,What does compute the bitwise NOT of the given input tensor do?,Computes the bitwise NOT,1698
15522,For what type of tensor does it compute the logical NOT?,bool tensors,1698
15523,What is a tensor with two or more dimensions?,Splitsinput,6050
15524,What is the equivalent of calling Splitsinput?,torch,6050
15525,What is tensor to split?,input(Tensor),6042
15526,What is a tensor with one or more dimensions?,Splitsinput,6042
15527,What is used to split a tensor with two or more dimensions into multiple tensors vertically?,indices_or_sections,6049
15528,"What is the difference between splitsinput and torch.tensor_split(input, indices_or_sections",ifindices_or_sectionsis an integer it must evenly divide the split dimension,6049
15529,"What is the equivalent of calling input, indices_or_sections, dim=0?",torch.tensor_split,7635
15530,What is the split dimension of torch.tensor_split?,dim=0,7635
15531,What is input to split?,tensor,3620
15532,What is the split dimension of tensor_split?,zero,3620
15533,What is equivalent to calling torch?,ifinputhas two or more dimensions,3620
15534,What is a split in a tensor?,view ofinput,6041
15535,What are splitsinput according to?,indices,6041
15536,What is input derived from NumPy'snumpy.hsplit()?,tensor,7538
15537,What is the name of the elements ofinput that returns a new tensor?,hyperbolic cosine,5381
15538,What library does torch.cosh use?,Sleef library,5381
15539,What does the Sleef library use?,Seeherefor details,5381
15540,Returns what with the hyperbolic cosine of the elements of input?,a new tensor,5381
15541,"Wheninputis on the CPU, the implementation of torch.cosh may use what library?",Sleef library,5381
15542,Fillsselftensor with numbers samples from what distribution parameterized by the given meanmuand standard deviationsigma?,log-normal distribution,2433
15543,What is not represented by the mean and standard deviation of the underlying normal distribution?,returned distribution,2433
15544,What is similar tobroadcast_tensors()but for?,shapes,5917
15545,What is the equivalent tobroadcast_tensors()but for shapes?,totorch.broadcast_tensors,5917
15546,What is this useful for?,broadcasting tensors of common batch shape,5917
15547,What is used for broadcasting tensors of common batch shape but different rightmost shape?,mean vectors with covariance matrices,5917
15548,What is an example of a shape compatible with all input shapes?,*shapes(torch.Size) – Shapes of tensors,5917
15549,What is a shape compatible with all input shapes?,RuntimeError,5917
15550,What is similar to tobroadcast_tensors() but for?,shapes,5917
15551,What does shapebut avoid?,create to intermediate tensors,5917
15552,What shape is used to broadcast tensors of common batch shape but different?,rightmost shape,5917
15553,What is useful for broadcasting tensors of common batch shape but different rightmost shape?,mean vectors with covariance matrices,5917
15554,A shape compatible with what?,all input shapes,5917
15555,What happens if shapes are incompatible?,RuntimeError,5917
15556,What is the behavior similar to python’sitertools.product?,Do cartesian product,2175
15557,What does the torch package have?,CUDA counterpart,7331
15558,What does the PyTorch tensor have?,CUDA counterpart,3920
15559,What is True ifobjis?,PyTorch storage object,3920
15560,What is the data type ofinputis a complex data type?,"one oftorch.complex64, andtorch.complex128",3920
15561,What is the data type ofinputis?,floating point data type,3920
15562,What does the defaulttorch.Tensortype set to floating point tensor typet?,current default floating pointtorch.dtype,2697
15563,What is the default option for?,printing,2697
15564,What does the default floating pointtorch.dtype mean?,Note,2697
15565,What is a floating point tensor typet?,defaulttorch.Tensortype,5846
15566,What is the defaulttorch.Tensortype to floating point tensor typet?,Note,5846
15567,What does the defaulttorch.Tensortype do?,Disables denormal floating numbers on CPU,5846
15568,What is the total number of elements in?,theinputtensor,5679
15569,What is a Disables denormal floating numbers on CPU?,Set options for printing,5679
15570,What does a denormal floating number on a CPU do?,Disables denormal floating numbers on CPU,5679
15571,What are listed underRandom sampling?,Random sampling creation ops,5092
15572,What format does asparse tensor in?,COO(rdinate) format,5092
15573,What does a view of an existingtorch.Tensorinput have?,"specifiedsize,strideandstorage_offset",5092
15574,What is a tensor filled with the scalar value1?,the shape defined by the variable argumentsize,5092
15575,What does a tensor fill with the scalar value1 have?,the same size asinput,5092
15576,What are Disables denormal floating numbers on CPU?,Set options for printing,5823
15577,What does the CPU do?,Disables denormal floating numbers,5823
15578,What type of withdata does torch.empty() construct?,tensor,5823
15579,What happens on CPU?,Disables denormal floating numbers,2151
15580,What are listed underRandom samplingand include:torch.rand()torch.rand_like()torch.,Random sampling creation ops,4396
15581,What is the name of aTensor created?,anumpy.ndarray,2032
15582,What does the tensor fill with the scalar value0 have?,the same size asinput,2032
15583,What does a tensor fill with the scalar value0 have?,the same size asinput,5439
15584,What is the shape of a tensor filled with the scalar value0?,the shape defined by the variable argumentsize,5439
15585,What does an existingtorch.Tensorinput have?,"specifiedsize,strideandstorage_offset",1949
15586,What defines the shape of a tensor filled with the scalar value?,variable argumentsize,5443
15587,What does a tensor fill with the scalar value have?,the same size asinput,5443
15588,What is the tensor of sizeendstartstepleftlceil?,1-D,5443
15589,How many -D tensor of sizeendstartstep+1leftlceil fractextend,1,5252
15590,What is the tensor of sizesteps created?,one-dimensional,5258
15591,What is the tensor of sizeendstartstep+1leftlfloor?,1-D,5258
15592,"What is a tensor of sizesteps that values are evenly spaced fromstarttoend, inclusive?",one-dimensional,5440
15593,What is the logarithmic scale of a one-dimensional tensor of sizesteps?,basebase,5440
15594,What is a tensor filled with uninitialized data?,uninitialized,5440
15595,What is a tensor on the diagonal and zeros elsewhere?,2-D,2017
15596,What does a complex tensor return?,a view ofinputas,5497
15597,The last dimension of the input tensor is expected to represent what?,real and imaginary components of complex numbers,5497
15598,"What does the function return for an input complex tensor ofsizem1,m2,...,mim1, m2, do",torch.view_as_complex,5497
15599,What does compute the element-wise logical XOR of the given input tensors do?,Computes the element-wise logical XOR,1746
15600,What is the name of the function that generates a question?,Alias fortorch.abs(),1039
15601,What is an object that represents the data type of atorch.Tensor?,Atorch.dtype,1365
15602,How many different data types does PyTorch have?,twelve,1365
15603,How many -bit floating point1 torch does PyTorch have?,16,2242
15604,What is the 16-bit floating point1 torch?,double,2242
15605,What type of torch does atorch.Tensor have?,double torch,1364
15606,What is the name of the 16-bit floating point2 torch?,HalfTensor,116
15607,What is the name of the torch that is int16ortorch.short torch?,16-bit integer,116
15608,How many -bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch?,128,116
15609,What is the number of unsigned torch in the BFloat16Tensor?,8-bit,116
15610,What is the name of the torch that is intTensor?,64-bit,116
15611,What is the name of the torch that float64ortorch.double torch?,64-bit floating point torch,165
15612,What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?,8-bit,165
15613,What is the name of the unsigned torch.uint8 torch?,8-bit integer,182
15614,What is the name of the 8-bit integer (signed) torch.int8 torch?,ByteTensor,182
15615,What is the number of integers in the torch.int8 torch?,8-bit,10940
15616,What is the name of the name of the torch.int16ortorch.short torch?,16-bit integer,11061
15617,What is the name of the integer (signed) torch.int8 torch?,8-bit,161
15618,What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?,64-bit,161
15619,What type of integer is intTensor?,64-bit,161
15620,What is the number of integers in the torch.int32ortorch.int torch?,32-bit,10941
15621,What is the number of integers in torch.int32ortorch.int torch?,32-bit,10944
15622,IntTensor is what type of integer (signed) torch.int64ortorch.long?,64-bit,11031
15623,What is a 8-bit integer?,unsigned,181
15624,"What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?",precision,5976
15625,What is the sign of a ByteTensor?,8-bit integer,10939
15626,What is important when it has the same number of exponent bits asfloat32?,range,11077
15627,What is the sign for torch.int32ortorch.int torch?,32-bit integer,153
15628,What is the name of the 64-bit integer (signed) torch.int64ortorch.long torch?,Boolean torch,169
15629,What is the BoolTensor sometimes referred to as?,binary16,10942
15630,How do we promote when the dtypes of inputs to an arithmetic operation differ?,by finding the minimum dtype that satisfies the following rules,10942
15631,What is the name of a torch.int32ortorch.long torch?,64-bit integer,11075
15632,What is another name for a 64-bit integer?,long torch,168
15633,What is important when a brain floating point is used?,precision,11018
15634,"What is used to use 1 sign, 8 exponent and 7 significand bits?",Brain Floating Point,11078
15635,What can be used to find out if atorch.dtypeis a floating point data type?,propertyis_floating_pointcan be used,131
15636,What is BooleanTensor sometimes referred to as?,binary16,1453
15637,"To find out if atorch.dtypeis a complex data type, what property can be used?",propertyis_complex,11019
15638,What types are not yet supported?,Quantized and complex types,11019
15639,We do not inspect values when determining the minimumdtypesof an operand. Quantized and complex types are not yet supported. Unlike what,numpy,11019
15640,Promotion Examples:,An integral output tensor cannot accept a floating point tensor,11019
15641,What is an object representing the device on which atorch.Tensoris or will be allocated?,Atorch.device,1363
15642,"If the device ordinal is not present, this object will always represent what for the device type?",current device,1363
15643,What does get_device() do?,returns an ordinal for cuda tensors,1363
15644,What is optional for the device type?,device ordinal,1363
15645,What is a device type called?,cpu'or'cuda,1362
15646,What is called if the device ordinal is not present?,aftertorch.cuda.set_device(),1362
15647,What does the torch.device contains?,device type,7442
15648,What does Thetorch.device contain?,device type,7442
15649,How can Atorch.Tensor's device be accessed?,theTensor.deviceproperty,1360
15650,What can be done by using a string or a device ordinal?,fast prototyping of code,1360
15651,What returns an ordinal for cuda tensors?,matchesTensor.get_device(),1361
15652,What is the name of the device that is not supported for cpu tensors?,Note,7441
15653,What does the device ordinal allow for?,fast prototyping of code,8131
15654,What is an object that represents the memory layout of atorch.Tensor?,A torch.layoutis,8190
15655,What do strided tensors provide?,"multi-dimensional,stridedview of a storage",8190
15656,What represents the jump in the memory necessary to go from one element to the next in the k-th dimension of the Tensor?,k-th stride,8190
15657,What does the k-th stride make it possible to do?,perform many tensor operations efficiently,8190
15658,What is an object representing the memory format on which atorch.Tensoris or will be allocated?,A torch.memory_format is,1367
15659,What are the possible values atorch.memory_format?,torch.contiguous_format,1367
15660,Where is atorch.Tensor allocated?,dense non-overlapping memory,1367
15661,What order are atorch.tensoris strides represented by?,decreasing,1367
15662,What is used in functions likecloneto preserve the memory format of the input tensor?,torch.preserve_format,1367
15663,What type of order are values represented by?,decreasing order,4876
15664,What are the possible values of torch.contiguous_format?,dense non-overlapping memory,4876
15665,What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?,NHWC,4876
15666,What is the name of the tensor that is or will be allocated in dense non-overlapping memory?,torch.channels_last,11032
15667,Where is Tensor allocated?,dense non-overlapping memory,11032
15668,What order are atorch.Tensoris represented by?,decreasing,1366
15669,What property returnsTrue if the data type is a floating point data type?,propertyis_floating_pointcan be used,11074
15670,What property returnsTrue if the data type is a complex data type?,property is_complex can be used,11074
15671,What is important when Brain Floating Point has the same number of exponent bits asfloat32?,range,10938
15672,"When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum d",arithmetic,170
15673,64-bit integer torch.int64ortorch is what?,long torch,170
15674,What is the name of the program that does not inspect values when determining the minimumdtypesof an operand?,numpy,170
15675,"What is the term used to describe the use of 1 sign, 5 exponent, and 10 significand bits?",binary16,11076
15676,"When is 1 sign, 5 exponent, and 10 significand bits used?",when precision is important,2064
15677,What are some inputs to an arithmetic operation?,"add,sub,div,mul",2064
15678,How many bits does a CharTensor have?,16-bit,11056
15679,What is the size of a ShortTensor?,32-bit,147
15680,IntTensor is what type of integer?,64-bit,11011
15681,"If there are no higher-category zero-dim operands, we promote to a type with sufficient what to hold all dimensione",size and category,10943
15682,What operand has a higher category than dimensioned operands?,zero-dimension tensor,10943
15683,"What values are specified to replaceNaN, positive infinity, and negative infinity values ininput?","bynan,posinf, andneginf",5159
15684,"By default,NaN is replaced with what value?",zero,5159
15685,What values are used to replace negative infinity values in input?,"bynan,posinf, andneginf",5159
15686,What is the default value for negative infinity?,the least finite value,5159
15687,What is the value to replaceNaNs with?,,9965
15688,What is the default value to replaceNaNs with?,zero,9965
15689,What is the value to replace positive infinity values with?,posinf,9965
15690,What is the default value to replace positive infinity values with?,None,9965
15691,When are positive infinity values replaced with the greatest finite value representable byinput's dtype?,If None,9965
15692,What is the value to replace negative infinity values with?,neginf,9965
15693,"Neginf(Number,optional) – if a Number, the value to replace negative infinity values with?",None,9965
15694,What is the greatest finite value represented by?,byinput’s dtype,9665
15695,When are positive infinity values replaced with the greatest finite value representable by input's dtype?,If None,9665
15696,What does each element of the inputinput by the corresponding element ofother do?,Divides,2168
15697,What is the divisor of input(Tensor)?,rounding,2168
15698,"By default, this performs a “what” division like Python 3?",true,1492
15699,What is another name for floor division?,therounding_modeargument,1492
15700,Always promotes integer types to the default what?,scalar type,1492
15701,What does input(Tensor) – the dividend other(TensororNumber) – the divisor?,rounding,1492
15702,What is the name of the floor division function?,therounding_modeargument,4358
15703,What is the divisor of the input(Tensor)?,rounding,4358
15704,Always promotes integer types to what type?,default scalar type,6140
15705,Performs what type of rounding if both input andotherare integer types?,no rounding,6140
15706,What is equivalent to NumPy'snp.true_divide?,true division,6140
15707,What is the default behavior?,None,8036
15708,What is the equivalent to in Python and NumPy'snp.true_divide?,true division,8036
15709,"""trunc"" rounds the results of the division towards what?",zero,8036
15710,What type of integer division is trunc equivalent to?,C-style,8036
15711,What is equivalent to the/operator in Python and NumPy'snp.true_divide?,true division,8036
15712,What rounds the results of the division towards zero?,trunc,8036
15713,What type of rounding rounds the results of the division down?,floor,8036
15714,What does the//operator and NumPy'snp.floor_divide equivalent to?,floor division,8036
15715,What type of rounding does None perform?,no rounding,8036
15716,What type of division is equivalent to trunc?,C-style integer division,8036
15717,What is the name of the rounding that rounds the results of a division down?,floor,8036
15718,What is equivalent to floor division in Python?,NumPy’snp.floor_divide,8036
15719,What type of rounding is performed if both input and otherare integer types?,no,9651
15720,What is the case when the inputs are promoted to the default scalar type?,if bothinputandotherare integer types,9651
15721,What is equivalent to true division in Python?,C-style integer division,10623
15722,What is the equivalent to in Python?,true division,10623
15723,What is the default behavior of rounding?,None,10623
15724,"Does rounding_mode(str,optional) perform any rounding?",no,10623
15725,"Rounding_mode(str,optional) - default behavior. Performs no rounding and, if bothinputandother",None,10623
15726,"When does rounding_mode(str,optional) perform no rounding?",if bothinputandotherare integer types,10623
15727,What is equivalent to the/operator in Python?,true division,10623
15728,What happens if both input and otherare integer types?,Performs no rounding,10334
15729,What is the name of Alias fortorch.trunc?,Alias fortorch.trunc(),1060
15730,What is another name for fortorch.trunc()?,Alias fortorch.trunc(),1060
15731,What does the Appendix Migrate to?,PyTorch 1.2 Recursive Scripting API References,2038
15732,What Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer?,Python Functions and Modules,7944
15733,What is the difference between Python and PyTorch?,Python Language Reference Comparison,4250
15734,Appendix Migrating to what?,PyTorch 1.2 Recursive Scripting API References,7943
15735,What did Appendix Migrate to?,PyTorch 1.2 Recursive Scripting API References,1472
15736,What language can a TorchScript program be saved from?,Python,2039
15737,What Python Language Reference Comparison Debugging Disable JIT for Debugging?,Python Functions and Modules,5002
15738,What Python Functions and Modules?,Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer,5002
15739,What is TorchScript?,a way to create serializable and optimizable models from PyTorch code,3858
15740,What program can a TorchScript program be saved from?,Python,7955
15741,What is an example of a standalone TorchScript program that can be run independently from Python?,C++ program,7955
15742,Why are Python programs disadvantageous?,performance and multi-threading reasons,7955
15743,What is the introduction to TorchScript?,introduction to TorchScript,7955
15744,In what language is it possible to train models in PyTorch using familiar tools?,Python,7955
15745,What is a benefit of using TorchScript?,Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency,7955
15746,What does TorchScript do for Debugging?,Disable JIT,2144
15747,What type of process can a TorchScript program be saved from?,Python,7977
15748,What is the name of the Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API Reference,Frequently Asked Questions,2604
15749,Any TorchScript program can be saved from what process?,Python,7954
15750,What is the name of the Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?,Known Issues,4043
15751,What is TorchScript a way to create serializable and optimizable models from?,PyTorch code,4251
15752,How can a TorchScript program be saved from a Python process and loaded in a process where there is no Python dependency?,Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency,4251
15753,What is References TorchScript?,a way to create serializable and optimizable models,5118
15754,What program can be run independently from Python?,TorchScript,8281
15755,What language is used to run a PyTorch model?,C++,2497
15756,"What will inspect the source code, compile it as TorchScript code using the TorchScript compiler?",Scripting a function ornn.Module,2497
15757,What is the end-to-end example of converting a PyTorch model to TorchScript and running it in C++?,theLoading a PyTorch Model in C++tutorial,2497
15758,A wrapper around C++torch::jit::Module. Functionally equivalent to what?,aScriptModule,2497
15759,What is an end-to-end example of converting a PyTorch model to TorchScript and running it in C++?,Loading a PyTorch Model in C++tutorial,2493
15760,What is C++torch::jit::Module?,wrapper,2493
15761,How is a scriptFunction optimized?,just-in-time compilation,2496
15762,When does Compilesfn occur?,Compilesfnwhen it is first called during tracing,2496
15763,What is an example of a module that will inspect the source code and compile it as TorchScript code using the TorchScript compiler?,Scripting a function ornn,5754
15764,When is Compilesfn called?,Compilesfnwhen it is first called during tracing,5754
15765,What does Scripting a function ornn.Module compile it as?,TorchScript code,5754
15766,What will be used to optimize a function?,just-in-time compilation,7969
15767,What will be used to optimize a module?,just-in-time compilation,7969
15768,What type of task is created?,asynchronous task executingfunc,1624
15769,When is Compilesfn first called?,tracing,1625
15770,What does Compilesfn do?,Forces completion of atorch.jit.Future[T]asynchronous task,1625
15771,What does this create?,asynchronous task executingfunc,7973
15772,What is used to optimize aScriptModule?,just-in-time compilation,7973
15773,What does it do to complete the atorch.jit.Future[T]asynchronous task?,Forces completion of atorch.jit.Future[T]asynchronous task,7972
15774,What type of task does atorch.jit.Future[T]ask create?,asynchronous task,2033
15775,What is the function that returns the result of atorch.jit.Future[T]asynchronous task?,Forces completion of atorch.jit.Future[T]asynchronous task,2588
15776,What does TorchScriptModule do?,Forces completion of atorch.jit.Future[T]asynchronous task,2588
15777,What type of version of atorch.jit.Future[T]asynchronous task can be saved for use in a separate process?,offline,2588
15778,What is atorch.jit::Module functionally equivalent to?,aScriptModule,2589
15779,Where can you save a version of this module for use in a separate process?,offline,2589
15780,What does Freezing aScriptModule do?,Save an offline version of this module for use in a separate process,2603
15781,What does this function provide for in TorchScript?,conatiner type refinement,2603
15782,What is the left-hand expression used to indicate to the TorchScript compiler?,a class instance attribute with type oftype,2603
15783,What can you do with the cloned module?,Save an offline version of this module for use in a separate process,887
15784,What did aScriptModuleorScriptFunction previously save?,withtorch.jit.save,2653
15785,What is one way to save a cloned aScriptModule?,Save an offline version of this module for use in a separate process,2653
15786,A wrapper around C++torch::jit::Module is functionally equivalent to what?,aScriptModule,888
15787,What is one way to save a cloned module?,Save an offline version of this module for use in a separate process,888
15788,Where can you save a version of aScriptModule for use in a separate process?,offline,888
15789,What is the TorchScript function that provides for conatiner type refinement?,a pass-through function that returnsvalue,888
15790,What is functionally equivalent to?,aScriptModule,2654
15791,What is the left-hand side expression used to indicate to the TorchScript compiler?,a class instance attribute with type oftype,2654
15792,What is the TorchScript method that provides for conatiner type refinement?,a pass-through function that returnsvalue,2654
15793,What is one way to save a module?,Save an offline version of this module for use in a separate process,5741
15794,Where does this function provide for conatiner type refinement?,TorchScript,7491
15795,What language should a function in TorchScript be left as?,Python,7491
15796,What does this decorator indicate to the compiler that a function or method should be ignored and left as?,Python function,7491
15797,What was previously saved withtorch.jit.save?,Load aScriptModuleorScriptFunction,4127
15798,What is the TorchScript conatiner type refinement?,a pass-through function that returnsvalue,7492
15799,What does returnthe_value hint TorchScript compiler about?,the type ofthe_value,7554
15800,What does the returnthe_value hint TorchScript compiler?,the type ofthe_value,7554
15801,Tracing or scripting can be composed to suit the specific requirements of a part of a model?,TorchScript,3736
15802,What is a traced function particularly useful when you need to use around a simple feed-forward model?,control-flow,3736
15803,What can the beam search of a sequence to sequence model call?,an encoder module generated using tracing,3736
15804,What can a beam search of a sequence to sequence model call?,encoder module,5752
15805,What is a traced function useful when you need to use around a feed-forward model?,control-flow,5752
15806,In what language can a traced function be called?,script,5752
15807,When are scripted functions useful?,when you need to use control-flow around a simple feed-forward model,3737
15808,What type of model is most of the model?,feed-forward network,2274
15809,What is an example of a function that can call a traced function?,script function in a traced function,2274
15810,What is a script function called in?,a traced function,2274
15811,What type of network is most of the model?,feed-forward network,7976
15812,Tracing can be called from the methods of what?,script module,7976
15813,What can be used to generate a submodule using fornn.Modules?,tracing,2273
15814,What can be used to call a submodule using tracing?,script module,2273
15815,How can a submodule be generated using fornn.Modules?,using a traced module,2273
15816,TorchScript is a statically typed subset of what programming language?,Python,7953
15817,What is the name of the reference for TorchScript?,fullTorchScript Language Reference,7953
15818,What supports the use of most PyTorch functions and many Python built-ins?,TorchScript,7961
15819,What does TorchScript Builtins provide?,a full reference of supported functions,7961
15820,What is a list of unsupported PyTorch functions and modules?,SeeTorchScript Unsupported Pytorch Constructs,7961
15821,What does TorchScript support?,PyTorch functions and many Python built-ins,7961
15822,Where can you find a full reference of supported functions?,SeeTorchScript Builtins,7961
15823,TorchScript supports a subset of what PyTorch functions?,tensor and neural network functions,7961
15824,What is supported by TorchScript?,most modules,7961
15825,TorchScript supports a subset of what PyTorch provides?,tensor and neural network functions,7960
15826,Where are most modules fromtorch.nn supported?,TorchScript,7960
15827,Where can you find a list of unsupported PyTorch functions and modules?,SeeTorchScript,7960
15828,What is a list of unsupported PyTorch Constructs?,SeeTorchScript,5801
15829,"What is also supported, but no other Python modules are supported?",Themathmodule,5801
15830,"For a full listing of supported features, seePython Language Reference Coverage.",Python,5801
15831,Where are many of Python's built-in functions supported?,TorchScript,4207
15832,What is also supported in TorchScript?,Themathmodule,4207
15833,Many of Python'sbuilt-in functions are supported in what?,TorchScript,4207
15834,What language is supported by the Python Language Reference Coverage?,Python,2490
15835,What is the full list of supported Python features?,Python Language Reference Coverage,2490
15836,What does Debugging this script use?,withpdbworks,2074
15837,How can we disable TorchScript?,globally disable JIT,2074
15838,"If the above script is calleddisable_what, we can invoke it like so?",jit_example.py,2074
15839,"To disable what for a specific function, see@torch.jit.ignore.",TorchScript compiler,2074
15840,What will we be able to step into the@torch.jit.scriptfunction as?,normal Python function,8863
15841,What is an example of a code pretty-printer for allScriptModuleinstances?,example,8863
15842,What compiler does TorchScript provide a code pretty-printer for allScriptModuleinstances?,TorchScript,2075
15843,What does TorchScript provide a code pretty-printer for?,allScriptModuleinstances,2075
15844,What does the example above produce?,output,2075
15845,Debugging what script works except for when we invoke the@torch.jit.scriptfunction?,withpdb,2075
15846,What can we do to make the @torch.jit.scriptfunction a normal Python function and not compile it?,globally disable JIT,2075
15847,What does the code pretty-printer do?,gives an interpretation of the script method’s code as valid Python syntax,7957
15848,AScriptModulewith a singleforwardmethod will have what?,attributecode,7957
15849,What do you need to do if theScriptModulehas more than one method?,access.codeon the method itself and not the module,7957
15850,How can we inspect the code of a method namedfooon aScriptModule?,accessing.foo.code,7957
15851,"If theScriptModulehas more than one method, you will need to do what?",access.codeon the method itself and not the module,7957
15852,What does TorchScript provide for allScriptModuleinstances?,a code pretty-printer,7957
15853,The example above produces this output: This is TorchScript's compilation of the code for what?,theforwardmethod,7957
15854,What can you use the code pretty-printer for allScriptModuleinstances?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7957
15855,What is the code for?,theforwardmethod,7589
15856,What can you use this to ensure TorchScript has captured correctly?,model code,7589
15857,How can you use the forwardmethod?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7589
15858,What is another way to verify that TorchScript has captured your model code correctly?,tracing or scripting,7590
15859,What is the purpose of theforwardmethod?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7590
15860,What is TorchScript's compilation of the code for?,theforwardmethod,7591
15861,What is the C++ backend of PyTorch?,ATen,7591
15862,What is an example of TorchScript's SSA intermediate representation?,example,7946
15863,What produces the graph?,The example script above,7962
15864,What is the example script that produces the graph?,instruction%rv.1:Tensor=aten::zeros,9491
15865,What does graph follow the same rules described in theInspecting Codesection with regard to?,forwardmethod lookup,9491
15866,What does the script assign the output to a (unique) value namedrv.1?,%rv.1:Tensormeans,9491
15867,The graph follows the same rules described in theInspecting Codesection with regard to what?,forwardmethod lookup,9490
15868,What is the value that we assign the output to?,ofTensortype,9490
15869,What type of value is assigned to the output?,ofTensortype,6180
15870,What is an example of an instruction that assigns the output to a (unique) value namedrv.1?,instruction%rv.1:Tensor=aten::zeros,6180
15871,What is the name of the instruction%rv.1:Tensor=aten?,test.py,6180
15872,What is the unique value assigned to rv.1?,ofTensortype,6180
15873,What does %rv.1:Tensormeans assign the output to?,a (unique) value namedrv.1,7052
15874,What is the operator (equivalent totorch.zeros) that specifies which values in scope should be passed as inputs?,aten,7052
15875,Which operator specifies which values in scope should be passed as inputs?,aten,8916
15876,What operator specifies which values in scope should be passed as inputs?,aten,8916
15877,What can be inspected to confirm that the computation described by aScriptModuleis correct?,Graphs,8916
15878,What does check_inputson thetorch.jit.trace()API.check_inputstakes give us?,diagnostic information,8916
15879,How can data-dependent control flow be captured?,usingtorch.jit.script(),8916
15880,Tracing of in-place operations of tensor views (e.g. in-place operations) on the left-hand side of,indexing,8916
15881,What is the name of the unique value assigned to the output?,%rv.1,56
15882,What is the equivalent totorch.zeros?,aten,6181
15883,What is the name of the file that generated the instruction%rv.1:Tensor=aten?,test.py,6181
15884,What type of functions can be found atBuiltin Functions?,built-in functions,8915
15885,What is the operator?,aten,8915
15886,Graphs can be inspected as shown to confirm that the computation described by what is correct?,aScriptModuleis,54
15887,Tracing of in-place operations of tensor views (e.g. what is on the left-hand side of an assignment),indexing,54
15888,What can be inspected to confirm that the computation described by aScriptModule is correct?,Graphs,4529
15889,What type of cases exist where the trace of a given Python function/module will not be representative of the underlying code?,edge cases,7403
15890,What is dependent on inputs?,control flow,7403
15891,What is an example of in-place operations of tensor views?,indexing,7403
15892,What is an example of a control flow that is dependent on inputs?,tensor shapes,7403
15893,What is an example of a tensor view that may not be traceable in the future?,indexing,7403
15894,Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) is,indexing,2745
15895,Tracing of control flow that is dependent on what?,inputs,7979
15896,Tracing of control flow that is dependent on inputs (e.g. what) Tracing of in-place operations of tens,tensor shapes,7979
15897,What does this message indicate to us that the computation differed between when we first traced it and when we traced it?,thecheck_inputs,7979
15898,What is an example of a list of tuples of inputs that will be used to re-trace the computation and verify the,example,7978
15899,What is the term for in-place operations of tensor views?,Tracing,7980
15900,In-place operations of tensor views (e.g. what on the left-hand side of an assignment) may be traceable in,indexing,7980
15901,"In the future, these cases may be what?",traceable,4491
15902,In what time frame may these cases be traceable?,future,4491
15903,What is an example of a check_inputson that will be used to re-trace the computation and verify the results?,Gives us the following diagnostic information,4490
15904,What does check_inputson give us?,diagnostic information,4663
15905,What does the loop within the body ofloop_in_traced_fn depend on?,the shape of the inputx,4664
15906,How can a data-dependent control flow be captured?,usingtorch.jit.script(),4664
15907,The loop within the body ofloop_in_traced_fndepends on what?,the shape of the inputx,4664
15908,What does the message indicate that the computation differed between when we first traced it and when we traced it?,thecheck_inputs,4664
15909,What is a list of inputs that will be used to retrace the computation and verify the results?,tuples,4664
15910,What does the list of tuples of inputs that will be used to re-trace the computation give us?,diagnostic information,4664
15911,A trace of a function that contains an in-place assignment on a slice of a Tensor produces several warnings and what else?,a graph,3797
15912,A trace of a function that contains an in-place assignment on a slice (a view) of a Tensor produces several warnings,a graph,3797
15913,What produces warnings for several problematic patterns in traced computation?,tracer,7338
15914,What is an in-place assignment on a Tensor?,a slice,7338
15915,What does the tracer produce for several problematic patterns in traced computation?,warnings,7338
15916,How can we fix this problem?,build up the result tensor out-of-place withtorch,7338
15917,What does the model train on GPU and do inference on?,CPU,5042
15918,What do I want to train a model on?,GPU,5042
15919,What are the best practices for training a model on GPU and doing inference on CPU?,best practices,5042
15920,What do I store attributes on?,aScriptModule,5042
15921,The tracer may witness what type of creation on a specific device?,tensor,7679
15922,What may the tracer witness on a specific device?,tensor creation,7679
15923,Casting the model before saving ensures that the tracer has what?,the correct device information,7679
15924,What is one way to inform the compiler of attributes onScriptModule?,How do I store attributes on aScriptModule,7679
15925,What model will result in a compilation error?,IfModelis instantiated,7679
15926,How many ways to inform the compiler of attributes onScriptModule?,4,7679
15927,What does casting the model before saving it do?,ensures that the tracer has the correct device information,2449
15928,What attribute is equivalent to register_buffer?,typeTensor,2449
15929,What will result in a compilation error since the compiler doesn't know aboutx?,IfModelis instantiated,2449
15930,What is the first step in converting a model from GPU to CPU?,convert your model from GPU to CPU and then save it,2449
15931,What do you store on aScriptModule?,attributes,2449
15932,What is equivalent to an attribute (see 4) of typeTensor?,register_buffer,2449
15933,What is an example of aScriptModule?,model like:,2448
15934,What does casting a model before saving ensure the tracer has?,the correct device information,2448
15935,Where do I store attributes?,aScriptModule,2448
15936,What does aScriptModule store attributes on?,a model,2448
15937,What do I store on aScriptModule?,attributes,5035
15938,What will happen if a model is instantiated?,a compilation error,5035
15939,What is one way to inform the compiler of attributes on aScriptModule?,How do I store attributes on aScriptModule,5035
15940,What model would result in a compilation error if the compiler didn't know aboutx?,IfModelis instantiated,5035
15941,What type of attribute does register_buffer correspond to?,typeTensor,5035
15942,What is equivalent to an attribute of typeTensor?,register_buffer,3599
15943,How many ways are there to inform the compiler of attributes onScriptModule?,4,3599
15944,"IfModelis is instantiated, what will happen?",a compilation error,3599
15945,"IfModelis instantiated, what will happen to a model?",a compilation error,5748
15946,What is register_buffer equivalent to?,attribute,134
15947,How many Constants are saved directly in the code of the model?,3.,134
15948,Annotating a class member asFinal or adding it to a list called what will mark the contained names as constants?,constants,134
15949,Where are constants saved?,the code of the model,134
15950,How are constants saved in the code of the model?,Seebuiltin-constantsfor details,134
15951,Values wrapped inregister_buffer will work as they do what?,onnn.Modules,134
15952,Register_buffer is equivalent to what type of typeTensor?,attribute,103
15953,What do I want to trace?,module’s method,5041
15954,Invokingtracewith a module's method captures what asconstants?,module parameters,5723
15955,What are you passing instead of the module instance?,module’s method,5723
15956,What does the method you are tracing use?,module’s parameters,7501
15957,What is passed instead of the module instance?,module’s method,7501
15958,Invokingtracewith a module's method captures what?,module parameters,3867
15959,What creates a new module and copies parameters into the new module?,invokingtracewith module’s instance,3867
15960,Invokingtracewith a module's method captures what as constants?,module parameters,3867
15961,What does invokingtrace with a module's instance do?,creates a new module and correctly copies parameters into the new module,3867
15962,What does seetorch.jit.trace_module do?,trace a specific method on a module,3867
15963,What is used to trace a specific method on a module?,seetorch.jit.trace_module,3867
15964,"If you're usingSequentialwith TorchScript, the inputs of some of theSequentialsubmodules may be",falsely inferred to beTensor,3597
15965,What is the canonical solution to this problem?,subclassnn,3597
15966,The inputs of some of theSequentialsubmodules may be falsely inferred to what?,beTensor,3597
15967,What is the canonical solution?,redeclareforward,3597
15968,What version of TorchScript does this section detail?,PyTorch 1.2,7796
15969,What can you do if you are new to TorchScript?,skip this section,7796
15970,"What will now attempt to recursively compile functions, methods, and classes that it encounters?",1.torch.jit.script,7796
15971,In what order are methods called fromforward compiled?,inforward,7796
15972,"To compile a method other thanforward that is not called fromforward, what is done?",add@torch.jit.export,7796
15973,How many main changes are there to the TorchScript API with PyTorch 1.2?,two,7795
15974,"What is the name of the command that makes compilation of TorchScript ""opt-out"" instead of ""opt-in""?",calltorch.jit.script,7795
15975,"When you compile functions, methods, and classes, compilation is ""opt-out"", rather than ""opt-in""?",calltorch.jit.script,104
15976,What does Torch.jit.script now try to do?,"recursively compile functions, methods, and classes",104
15977,In what environment are yournn.Modules ready to be optimized and executed?,a non-Python environment,104
15978,What is the preferred way to createScriptModules?,2.torch.jit.script(nn_module_instance),136
15979,What does @ignoredcannot be exported do?,@ignoredcannot be exported,136
15980,"To stop the compiler from compiling a method, what is done?",add@torch.jit.ignoreor@torch.jit.unused,136
15981,What does @ignoreor do to a method?,@ignoreleaves the method as a call to python,136
15982,What styleclass annotations are used for empty container types?,PEP 526,136
15983,ScriptModules are ready to be optimized and executed in a non-what environment?,Python,135
15984,What is the module'sforward compiled by?,default,8369
15985,What method is lazily compiled in the order they are used inforward?,fromforwardare,8368
15986,By what method is the module'sforward compiled?,default,8368
15987,What is a method on annn.Module used for?,an entry point into aScriptModuleand,8368
15988,What is assumed to be an entry point?,forwardimplicitly,8368
15989,Functions and methods called what are compiled as they are seen by the compiler?,fromforwardare,8368
15990,What is an example of a method that does not need a decorator?,@torch.jit.exporton a method,8368
15991,What module's data is copied to aScriptModule when passed to thetorch.jit.scriptfunction?,atorch.nn.Module,8368
15992,What is a way to compile a method that is not called fromforward?,add@torch.jit.export,7185
15993,"To compile a method that is not called fromforward that is not called fromforward, what is done?",add@torch.jit.export,7185
15994,What are the two exceptions that prevent the compiler from compiling a method that is not called fromforward?,@ignoredcannot be exported;@unusedcan,7185
15995,What can be used in place oftorch.jit.annotate?,Python 3 type hints,7185
15996,What is a way to stop the compiler from compiling a method?,add@torch.jit.ignoreor@torch.jit.unused,7921
15997,What does @ignoredcannot be exported?,@ignoredcannot be exported;@unusedcan,7921
15998,Most attribute types can be what?,inferred,7921
15999,What type of type can be annotated using PEP 526-styleclass annotations?,empty container types,7921
16000,What does @ignoreleaves the method as a call to?,python,7921
16001,What can be used to mark constants instead of adding the name of the member to__constants__?,aFinalclass annotation,7921
16002,What does add@torch.jit.ignoreor@torch.jit.unused do?,stop the compiler from compiling a method,7921
16003,What type of hints can be used in place oftorch.jit.annotate?,Python 3,7921
16004,What does @ignoreor do to a method as a call to python?,@ignoreleaves,7921
16005,How can empty container types be marked with aFinalclass annotation instead of adding the name of the member to__constants__?,annotate their types usingPEP 526-styleclass annotations,7921
16006,To what does the method need to be called?,python,7184
16007,What does add@torch.jit.export do to stop the compiler from compiling a method?,add@torch.jit.ignoreor,7184
16008,What does @ignoreor do to a call to python?,@ignoreleaves the method,7184
16009,What does the new usage look like?,@ignoredcannot be exported;@unusedcan,7198
16010,What does a compiler do to compile a method that is not called fromforward?,add@torch.jit.export,7869
16011,What type of types can be annotated using PEP 526-styleclass annotations?,empty container types,7869
16012,What cannot be exported?,@ignored,9906
16013,What type of attributes can be annotated using PEP 526-styleclass annotations?,empty container types,9906
16014,What does @unusedreplace a method with?,an exception,9906
16015,What does @torch.jit.ignore and @torch.jit.unused provide?,details,9906
16016,To what does the method have to be called to stop the compiler from compiling it?,python,7868
16017,What is the difference between an exported method and an unused one?,@ignoredcannot be exported;@unusedcan,7868
16018,What does add@torch.jit.ignoreor do?,@ignoreleaves the method,7868
16019,What type of container types can be annotated using PEP 526-styleclass annotations?,empty container types,7868
16020,What does @ignoreor do to stop the compiler from compiling a method as a call to python?,@ignoreleaves,7919
16021,What do empty container types do?,annotate their types usingPEP 526-styleclass annotations,7919
16022,What is the method a call to?,python,9905
16023,How many styleclass annotations are needed for empty container types?,526,9905
16024,What happens when a method is called to python?,and@unusedreplaces it with an exception,9905
16025,What type of containers can be annotated using PEP 526-styleclass annotations?,empty container types,9905
16026,What type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecor,Python 3,4266
16027,What type of hints can be used in place oftorch.jit.annotate The@torch.jit.script_methodde,Python 3,5000
16028,What classes inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class?,The@torch.jit.script_methoddecorator Classes,7373
16029,What class inherits fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class,The__constants__array,7373
16030,What class does The__constants__array inherit from?,Attributewrapper,7373
16031,What is the name of the function that changes behavior in PyTorch 1.2?,Thetorch.jit.annotatefunction,7445
16032,What is the name of the decorator used to make a function or method callable from code that was exported before PyTorch 1.2?,@torch.jit.ignore,7445
16033,When did The@torch.jit.ignoreannotation's behavior change?,PyTorch 1.2,8185
16034,In what version of Python did the @torch.jit.ignoreannotation's behavior change?,PyTorch 1.2,7371
16035,What is another name for the @ignore decorator in PyTorch 1.2?,@torch.jit.ignore,7371
16036,What is now equivalent to @torch.jit.ignore?,@torch.jit.ignore,4267
16037,What is required for empty container types?,annotate their types usingPEP 526-styleclass annotations,4267
16038,What does @torch.jit.ignore and @torch.jit.unused contain?,details,4267
16039,"In addition to fromforward, what is compiled in the order they are used inforward?",any@torch.jit.exportmethods,8367
16040,What is used as an entry point into aScriptModuleand?,annn.Moduleis,7488
16041,What is an example of a method that does not need this decorator?,@torch.jit.exporton a method,7488
16042,What does this decorator indicate about a method on annn.Module?,Warning,7489
16043,What does this decorator indicate that a method on annn.Module should be compiled?,Warning,7489
16044,What are compiled as they are seen by the compiler?,Functions and methods called fromforward,9390
16045,Forwardimplicitly is assumed to be what?,entry point,9390
16046,What is the main benefit of using @torch.jit.exporton a method?,Functions don’t change much,9390
16047,"If a type cannot be inferred and is not explicitly annotated, it will not be added as what to the resultingScriptModul",an attribute,9390
16048,Functions can be decorated with what if needed?,@torch.jit.ignoreortorch.jit,2658
16049,"What doesn't change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif",Functions,2658
16050,What does forwardimplicitly do?,Warning,9389
16051,What is the name of the warning that can be added to Functions and methods called fromforwardimplicitly?,Warning,9389
16052,What is TorchScript class support?,experimental,8204
16053,What is an example of a simple record-like type?,aNamedTuple,8204
16054,What can functions be decorated with if needed?,@torch.jit.ignore,8204
16055,What is TorchScript best suited for?,simple record-like types,8204
16056,What can be decorated with@torch.jit.ignore if needed?,functions,8204
16057,What type of type is TorchScript best suited for?,aNamedTuple,7490
16058,What is exported by default?,Everything in a user definedTorchScript Classis exported by default,2660
16059,How is everything in a user definedTorchScript Class exported?,by default,2269
16060,What cannot have their types inferred and must have their types annotated withPEP 526-styleclass annotations?,Empty lists and dicts,2269
16061,Why can functions be decorated with@torch.jit.ignoreortorch.jit.unusedif needed?,Functions don’t change much,2275
16062,What doesn't change much?,Functions,2659
16063,Who needs to know the types ofmodule attributes?,The TorchScript compiler,6951
16064,What can most types be inferred from?,the value of the member,6951
16065,What cannot have their types inferred and must have their types annotated with PEP 526-styleclass annotations?,Empty lists and dicts,6951
16066,What can be used to mark members as constant?,TheFinaltype constructor,6951
16067,What happens if members are not marked constant?,"If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute",6951
16068,What was Torch.jit.annotate used to tell?,TorchScript compiler,6951
16069,What are now supported?,Python 3 style type hints,6951
16070,What was Torch.jit.annotate used for?,to tell the TorchScript compiler what the type should be,6951
16071,How is everything exported in a TorchScript Class?,by default,2268
16072,What happens if a type is not explicitly annotated?,it will not be added as an attribute,2268
16073,What does the TorchScript compiler need to know?,types ofmodule attributes,6950
16074,Most types can be inferred from what?,value of the member,6950
16075,What happens if a type cannot be inferred and is not explicitly annotated?,it will not be added as an attribute,6950
16076,What opens opportunities for optimization if the value is known to be fixed?,UsingFinal,7376
16077,What is the name of the new API?,New API,7376
16078,What is assumed to have typeTensor?,Containers,1861
16079,Containers are assumed to have what?,typeTensor,1861
16080,What is now supported by TorchScript?,Python 3 style type hints,1861
16081,What compiler was Torch.jit.annotate used to tell?,TorchScript,1861
16082,What is now supported?,Python 3 style type hints,1861
16083,What compiler used Torch.jit.annotate to tell what type should be?,TorchScript,1861
16084,What compiler used Torch.jit.annotate?,TorchScript,4619
16085,What does Alias fortorch.le stand for?,Alias fortorch.le,1053
16086,What is each zero or one dimensional tensortintensors first reshaped into?,"a(t.numel(),1)column",2008
16087,What type of tensor does ifobj return True?,PyTorch tensor,5248
16088,What does ifobjis a PyTorch tensor return?,True,5248
16089,What is the function that returns true ifobjis a PyTorch tensor?,doingisinstance,5248
16090,What is better for typechecking with mypy?,thatisinstancecheck,5248
16091,What is the return value for ifobjis a PyTorch tensor?,True,5248
16092,What is the function that returns True ifobjis a PyTorch tensor?,"doingisinstance(obj,Tensor)",5248
16093,What is thatisinstancecheck better for?,typechecking with mypy,5248
16094,"obj(obj,Tensor) – Object to test Example:",Object,5248
16095,What does set_flush_denormal() do?,Disables denormal floating numbers on CPU,2153
16096,If your system supports flushing denormal numbers and it successfully configures what?,flush denormal mode,2153
16097,On what architectures is set_flush_denormal() only supported?,x86 architectures,2153
16098,What controls whether to enable flush denormal mode or not?,mode(bool),2153
16099,What does set_flush_denormal() do on CPU?,Disables denormal floating numbers,2153
16100,ReturnsTrueif your system supports what?,flushing denormal numbers,2153
16101,Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on what convention?,Einstein summation convention,6086
16102,Sums the product of the elements of what dimension specified using a notation based on the Einstein summation convention?,inputoperandsalong,6086
16103,What summation convention is used to sum the product of the elements of inputoperandsalong dimensions specified?,Einstein,6086
16104,What is the name of the equation that is used to describe it?,Equation,2259
16105,What is the name of the Equation?,Equation,2259
16106,What can be added at the end of the equation to define the output subscripts?,an arrow,2261
16107,What does Theequationstring separate subcripts for each operand by?,a comma,2261
16108,What will be part of the output?,The subscripts that appear exactly once in theequation,2261
16109,What is an example of an equation that can be explicitly defined by adding an arrow at the end of the equation followed by the subscripts for the,the following equation computes the transpose of a matrix multiplication,4691
16110,What does torch.einsumhandle ellipsis differ from?,NumPy,4517
16111,What does torch.einsum handle differently from NumPy?,ellipsis,4517
16112,Why does torch.einsum not optimize the given expression?,a different formula for the same computation may run faster or consume less memory,11053
16113,What does torch.einsum handle differently from NumPy in that it allows dimensions covered by the ellipsis to be summed,ellipsis,11053
16114,What project can optimize the formula for you?,opt_einsum,4418
16115,What are the subscripts for the Einstein summation?,equation(string),4418
16116,Why does this function not optimize the given expression?,a different formula for the same computation may run faster or consume less memory,4418
16117,What are the operands to compute the Einstein sum of?,operands,4418
16118,What are operands(Tensor) – The operands to compute the Einstein sum of?,Examples,4418
16119,"If one of the elements being compared is what, then that element is returned.maximum()is not supported for tensors with",a NaN,1749
16120,What is the Bartlett window function?,full window size,1401
16121,Where is the full window size?,whereNNNis the full window size,11380
16122,"What is the same as totorch.bartlett_window(L+1,periodic=False)?",havetorch.bartlett_window,7142
16123,What is the name of the window that contains a single value?,window_length,3654
16124,What type of tensor type does ifNone use?,Default,3654
16125,What type of tensors are supported?,floating point types,3654
16126,"If False, return a window to be used as periodic function.",symmetric window,7144
16127,What is inputwindow_length?,a positive integer,7144
16128,"What is the size of the returned window periodic(bool,optional)?",window_length(int),7144
16129,What type of tensor types are supported?,floating point types,7144
16130,What is true when the returned window is ready to be used as a periodic window with functions liketorch.stft()?,ifperiodic,7144
16131,"What is a window Tensor of size(window_length,)(textwindow_length,)(wind",1-D tensor,9170
16132,What is deprecated and may be removed in a future PyTorch release?,torch.norm,11177
16133,What is the default name for the order of norm?,Default:'fro,11177
16134,What is used when computing vector norms?,ortorch.linalg.vector_norm(),8102
16135,What is the signature for these functions?,slightly different,8102
16136,"If the input is complex and neitherdtypenoroutis specified, what will be the corresponding floating point type?",the result’s data type,8102
16137,What is the default value for the order of norm?,Default:'fro,8102
16138,The input tensor's data type must be either a what?,floating point or complex type,8102
16139,What function is used when computing vector norms?,ortorch.linalg.vector_norm(),8215
16140,What does ortorch.linalg.matrix_norm() do?,matrix norms,11174
16141,What function does usetorch.linalg.norm() instead of torch.norm?,ortorch.linalg.vector_norm(),11174
16142,How different is the signature for these functions from the signature for torch.norm?,slightly different,5594
16143,"If the input is complex and neitherdtypenoroutis specified, the result’s data type will be the corresponding what?",floating point type,5594
16144,What produces the same result asp=2 in all cases except whendimis a list of three or more dims?,Frobenius norm,5594
16145,Nuclear norm can only be calculated across exactly how many dimensions?,two,5594
16146,"For complex inputs, the norm is calculated using what?",absolute value of each element,5594
16147,"What does p(int,float,inf,-inf,'fro','nuc',optional) return",the order of norm,5594
16148,What type of data type must the input tensor have?,floating point or complex type,9646
16149,How is the norm calculated for complex inputs?,the absolute value of each element,9646
16150,"If input is complex and neitherdtypenoroutis specified, the result's data type will be the corresponding floating point type (e.",ifinputis complexfloat,9646
16151,The corresponding dimensions of inputare what?,flattened,9646
16152,Nuclear norm can only be calculated across what?,exactly two dimensions,9646
16153,"p(int,float,inf,-inf,'fro','nuc',optional) – what",the order of norm,9646
16154,"If input is complex and neitherdtypenoroutis specified, the result's data type will be what?",corresponding floating point type,9646
16155,What is the name of the nuclear norm?,Frobenius norm,9646
16156,"What is p(int,float,inf,-inf,'fro','nuc',optional)?",the order of norm,10366
16157,"What does p(int,float,inf,-inf,'fro','nuc',optional',",the order of norm,10366
16158,What is the name of the norm that can be calculated?,Frobenius norm,10366
16159,What are the following norms that can be calculated?,ord matrix norm vector norm ’fro’,9645
16160,What is the default order of norms?,Default:'fro',10875
16161,What is the name of the order in which a norm can be calculated?,order of norm,10875
16162,What is the order of norms?,order of norm,10875
16163,The corresponding dimensions of input are what?,flattened,9882
16164,What is the vector norm 'fro'?,matrix norm,9882
16165,What is the ord matrix norm vector norm?,fro,10328
16166,What is the vector norm fro?,ord matrix norm,10328
16167,When does Frobenius norm throw an error?,whendimis a list of three or more dims,2609
16168,What can only be calculated across exactly two dimensions?,Nuclear norm,2609
16169,What can be calculated across any number of dimensions?,vector norm,7361
16170,What happens to the corresponding dimensions of input?,flattened,10876
16171,The vector norm can be calculated across what?,any number of dimensions,10783
16172,What is the vector norm calculated across any number of dimensions?,sum(abs(x)**ord)**(1./ord),10783
16173,What is the Frobenius norm?,fro,11491
16174,What does nuclear norm stand for?,Number,11488
16175,What case does Frobenius norm throw an error?,whendimis a list of three or more dims,11488
16176,What is nuclear norm?,Number,10274
16177,What is Number – sum(abs(x)**ord)**(1./ord))?,nuclear norm,10274
16178,What is the value of the vector norm?,Number,11483
16179,What is the sum of the vector norm?,sum(abs(x)**ord)**(1./ord),11485
16180,What is the output tensor ignored?,ifdim=None,10353
16181,What is the input tensor casted to when performing the operation?,:attr:’dtype’,10353
16182,"If specified, the input tensor is casted to what?",:attr:’dtype’,9213
16183,What applies only to tensors with exactly two dimensions?,Frobenius norm,9213
16184,What is an example of a tensor that can only be applied across exactly two dimensions?,Example,9213
16185,What does Alias fortorch.atanh do?,Alias fortorch.atanh(),1049
16186,What is the name of the website that provides information about Alias fortorch.atanh?,Alias fortorch.atanh,1049
16187,What happens if an element ininputevaluates toTrue?,Tests,6770
16188,This function matches the behavior of what function?,NumPy,6770
16189,What is the dtype of output foruint8?,Foruint8the dtype of output isuint8itself,9659
16190,What is returned for each row of inputin the given dimensiondim?,returnsTrueif any element in the row evaluate toTrueandFalseotherwise,9659
16191,What does input(Tensor) do?,Tests if any element ininputevaluates toTrue,9659
16192,What does the function return if any element in the row evaluate toTrueandFalseotherwise?,returnsTrue,9659
16193,This function matches the behaviour of what function in returning output of dtypebool for all supported dtypes exceptuint8?,NumPy,7550
16194,What is the dtype of output?,isuint8itself,7550
16195,What does the function return for each row ofinputin the given dimensiondim?,Trueif any element in the row evaluate toTrueandFalseotherwise,7550
16196,Which function returns output of dtypebool for all supported dtypes exceptuint8?,NumPy,7550
16197,What does this function match the behaviour of NumPy in returning output of dtypeboolfor all supported dtypes exceptuin,Foruint8the dtype of output isuint8itself,7550
16198,What function returns the output tensor of the same size asinput?,IfkeepdimisTrue,7550
16199,"If keepdimisTrue, the output tensor has what?",1 fewer dimension thaninput,7550
16200,What function ensures that the output tensor is of the same size as input except in the dimensiondim where it is of size 1?,IfkeepdimisTrue,7550
16201,What is the dtype of foruint8?,output isuint8itself,4424
16202,What function returns output of dtypebool for all supported dtypes exceptuint8?,NumPy,4424
16203,What does the function return for each row of inputin the given dimensiondim?,Trueif any element in the row evaluate toTrueandFalseotherwise,4424
16204,"For each row of inputin the given dimensiondim, returns what?",Trueif any element in the row evaluate toTrueandFalseotherwise,4424
16205,"For each row ofinputin the given dimensiondim, returns what?",Trueif any element in the row evaluate toTrueandFalseotherwise,2512
16206,Where is the output tensor of the same size asinput except in the dimensiondim?,size 1,2512
16207,What are features sometimes hidden behind?,run-time flags,4912
16208,What does Thetorchaudiopackage consist of?,Package Reference PyTorch Libraries,4912
16209,What libraries are included in Thetorchaudiopackage?,Package Reference PyTorch Libraries,2411
16210,What types of types does this function support?,"float,double,cfloatandcdoubledtypes forinput",5512
16211,What is the pivoted LU factorization of A fromtorch.lu()?,LU_data(Tensor),5512
16212,"This function supports float,double, and what other type of input?",cfloat,5512
16213,"What is the RHS tensor of size(,m,k)(*, m, k)(,m",b(Tensor),8938
16214,"What is the pivoted LU factorization of A fromtorch.lu()of size(,m,m)(*,",LU_data,8938
16215,"What are the pivots of the LU factorization fromtorch.lu()of size(,m)(*, m",LU_pivots,8938
16216,What must the batch dimensions ofLU_pivots be equal to?,the batch dimensions ofLU_data,8938
16217,What must be the batch dimensions of LU_pivots be to the batch dimensions of LU_data?,equal,8938
16218,What types of input does this function support?,"float,double,cfloatandcdoubledtypes",7567
16219,What does LU_data(Tensor) do?,LU factorization,7567
16220,What type of input does this function support?,forinput,7567
16221,What does the documentation of bytorch.sort() provide?,exact semantics,5566
16222,What is the dimension to sort along descending?,"dim(int,optional)",5566
16223,What is returned bytorch.sort()?,second value,5566
16224,What do you need to know about the second value returned bytorch.sort()?,semantics,5566
16225,"What is the dimension to sort along descending(bool,optional)?","dim(int,optional)",5566
16226,What returns the cumulative maximum of elements of input in the dimensiondim?,a namedtuple,5315
16227,What is the index location of each maximum value found in the dimensiondim?,Andindices,5318
16228,"What is the dimension to do the operation over out(tuple,optional)?",dim(int),5318
16229,What returns the cumulative minimum of elements of input in the dimensiondim?,a namedtuple,5318
16230,What is the output specified by for a 3-D tensor?,inputandindexmust have the same number of dimensions,2676
16231,Do inputandindexdo broadcast against each other?,not broadcast against each other,2676
16232,What does index(LongTensor) contain?,"indices of elements to gather sparse_grad(bool,optional)",2676
16233,What is the destination tensor?,out,2676
16234,What must have the same number of dimensions?,inputandindex,9679
16235,What type of elements represent if each element of input is real-valued or not?,boolean elements,5364
16236,What is the default value of a boolean tensor?,True,5364
16237,What type of values are considered real?,real,5364
16238,When are complex values considered real?,when their imaginary part is 0. input(Tensor) – the input tensor,5364
16239,What is a boolean tensor that is whereinputis real and False elsewhere?,True,5364
16240,What is nonlinearity gain?,nonlinearity gain Linear / Identity,10257
16241,"What gain Linear / Identity 111 Conv1,2,3D 111 Tanh 53frac5335 Re",nonlinearity,10257
16242,What is the name of the 111 Sigmoid?,111 Tanh,9440
16243,"What is the definition of Identity 111 Conv1,2,3D 111 Tanh 53frac5335 ReLU",Linear,4112
16244,What is the name of the 111 Tanh?,111 Sigmoid,108
16245,What is the number of the Leaky Relu 2sqrt22 Leaky Relu?,111 Tanh,110
16246,How many Tanhs are there?,111,110
16247,What should you use in order to implementSelf-Normalizing Neural Networks?,'linear',156
16248,What is the name of the 2sqrt22?,Leaky Relu,141
16249,What does the default gain forSELU sacrifice for more stable gradient flow in rectangular layers?,normalisation effect,139
16250,What is 21+negative_slope2sqrtfrac21 + textnegative_slope,Leaky Relu,4073
16251,What is the name of the relu that is 21+negative_slope2sqrtfrac21 +,Leaky Relu,4073
16252,What is the default gain forSELUsacrifices the normalisation effect for more stable gradient flow in rectangular layers?,linear,8167
16253,What is the initial weights' variance?,variance of1/N,3746
16254,What is the optional parameter for the non-linear function Examples?,nonlinearity,3746
16255,What is the default gain for forSELUsacrifices the normalisation effect for more stable gradient flow in rectangular layers?,selu,8166
16256,What is the variance of the initial weights?,variance of1/N,8166
16257,What is the non-linear function param?,optional parameter,8166
16258,"In order to implementSelf-Normalizing Neural Networks, you should use what?",linear,3747
16259,Param is what type of parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform distribution?,optional,10396
16260,What is the uniform distributionU?,"a,b",2427
16261,Fills the input Tensor with values drawn from what?,the uniform distribution,2427
16262,What does a tensor represent?,a– the lower bound of the uniform distribution,10826
16263,What is the upper bound of the uniform distribution?,b,8998
16264,b– what is the upper bound of the uniform distribution?,the upper bound,8998
16265,What is the value to fill the tensor with Examples?,an n-dimensionaltorch.Tensor val,8995
16266,Fills the input Tensor with what value1?,scalar,2424
16267,What is the mean of the normal distribution?,mean,9895
16268,What value does the input Tensor fill with?,scalar,10760
16269,What does a tensor do when as many inputs are preserved as possible?,Preserves the identity of the inputs inLinearlayers,10833
16270,What value does Tensor val fill the input Tensor with?,scalar value,10833
16271,What does a tensor do inLinearlayers?,Preserves the identity of the inputs,10833
16272,What is the function that preserves the identity of the inputs inLinearlayers?,Preserves the identity of the inputs inLinearlayers,2418
16273,What does Linearlayers do?,Preserves the identity of the inputs,11329
16274,What does a tensor do that allows as many inputs to be preserved as possible?,Preserves the identity of the inputs inLinearlayers,2422
16275,What does filling the 2-dimensional inputTensor with the identity matrix do?,Preserves the identity of the inputs,2422
16276,What does filling the 2-dimensional inputTensor do?,Preserves the identity of the inputs,2335
16277,What does tensor fill the 2-dimensional inputTensor with?,the identity matrix,10824
16278,What does tensor do inLinearlayers?,Preserves the identity of the inputs,10824
16279,What does Examples fill the input Tensor with?,scalar value0,2333
16280,What does Filling the 2-dimensional inputTensor with the identity matrix do?,Preserves the identity of the inputs,2333
16281,What reduces the amount of matrix multiplications in a batch matrix-matrix product?,add step,4762
16282,Batch1andbatch2must be what?,3-D tensors,4762
16283,What must batch1andbatch2 be?,3-D tensors,4763
16284,"If input is ignored, andnanandinfin it will not be propagated?","Ifbetais 0,",4763
16285,What is a(bnm)(b times n times m)(bn,Ifbatch1,3600
16286,Which inputs will not be propagated ifbetais 0?,andnanandinfin,3600
16287,"For inputs of typeFloatTensororDoubleTensor, what must be?",argumentsbetaandalphamust be real numbers,3600
16288,"If input is ignored, andnanandinfin it will not be propagated, what is it?","Ifbetais 0,",8946
16289,"Argumentsbetaandalphamust be real numbers, otherwise they should be what?",integers,8946
16290,What must argumentsbeta andalpha be for inputs of typeFloatTensororDoubleTensor?,argumentsbetaandalphamust be real numbers,3604
16291,What is the name of the Moore-Penrose inverse?,pseudoinverse,1797
16292,What is the most computationally convenient way to understand the pseudoinverse?,SVD,1797
16293,What does the Moon-Penrose inverse stand for?,the pseudoinverse,1797
16294,Ifhermitian= what is assumed to be Hermitian if complex or symmetric if real?,True,7236
16295,Supports input of what types of input?,"float, double, cfloat and cdouble dtypes",6127
16296,What type of matrices does Ais support?,batches of matrices,6127
16297,"Ifhermitian= what, is Ais assumed to be Hermitian if complex or symmetric if real?",True,6127
16298,What part of the matrix is used instead of Hermitian?,lower triangular part of the matrix,3612
16299,What should be done to the singular values that are below the specifiedrcondthreshold?,Note,3612
16300,What is the difference between the singular values and the norm of the eigenvalues whenhermitian= True?,Note,3612
16301,What are the singular values that are below the specifiedrcondthreshold treated as?,zero,7297
16302,What does the function usestorch.linalg.svd() if?,True,7297
16303,What function synchronizes a CUDA input with the CPU?,usestorch.linalg.svd()ifhermitian= False,7297
16304,What function is used for multiplying a matrix on the left by the pseudoinverse?,usingtorch.linalg.lstsq()if possible,7297
16305,Why is it always preferable to uselstsq()?,faster and more numerically stable,7568
16306,What is the name of the function used to synchronize a CUDA input with the CPU?,Warning,7568
16307,What is a warning about usinglstsq()?,Warning,4426
16308,What does torch.linalg.inv() compute?,the inverse of a square matrix,11082
16309,What does torch.linalg.inv() compute of a square matrix?,inverse,11082
16310,What is the default value of the atorch.Tensor?,1e-15,11082
16311,What does torch.linalg.lstsq()computesA.pinv() @Bwith?,numerically stable algorithm,11084
16312,What is the tolerance value to determine when is a singular value zero?,rcond,11084
16313,"What is the tensor of shape(*, m, n)where*is zero or more batch dimensions?",A(Tensor),11084
16314,What is the default value for atorch.Tensor?,Default:1e-15,11084
16315,What does hermitian indicate if complex or symmetric if real?,Hermitian,11084
16316,What is the default value of Hermitian?,False,11084
16317,What is the default value of atorch.Tensor?,1e-15,5768
16318,What computes the inverse of a square matrix?,torch.linalg.inv(),5768
16319,Indicesis what of each mode value found?,index location,5324
16320,"By default,dimis is what?",the last dimension of theinputtensor,5324
16321,How often does a value appear in a given row of theinputtensor?,most often,5324
16322,"By default,dimis the last dimension of what?",theinputtensor,1495
16323,When are the output tensors of the same size asinput except in the dimensiondimwhere they are of size 1?,IfkeepdimisTrue,1495
16324,"By default,dimis what dimension of the inputtensor?",last dimension,1495
16325,In what case are output tensors of the same size as input except in the dimensiondimwhere they are of size 1?,IfkeepdimisTrue,1495
16326,What is another name for squeezed output tensors?,seetorch.squeeze(),1495
16327,What is the result of the output tensors being squeezed?,1 fewer dimension thaninput,3625
16328,Note This function is not defined what?,fortorch.cuda.Tensoryet,3625
16329,What makes the output tensors of the same size as input except in the dimensiondimwhere they are of size 1?,IfkeepdimisTrue,3625
16330,What is the name of the function that results in the output tensors having 1 fewer dimension than input?,seetorch.squeeze(),3625
16331,What is this function not defined?,fortorch.cuda.Tensoryet,4422
16332,"Out(tuple,optional) is the result tuple of two what?",output tensors,4422
16333,What website does Alias fortorch.ne belong to?,Alias fortorch.ne,1057
16334,What is the name of the local optimizer?,group,10321
16335,What is a bool for when parameters are packed into larger buckets?,parameters_as_bucket_views,10321
16336,What will remain intact when disabled?,butparams.data,10381
16337,What is an example of how to add a param group to theOptimizersparam_groups?,Add a param group to theOptimizersparam_groups,10381
16338,Param.datafields will point to what at different offsets?,bucket views,10382
16339,What does state_dict(dict) represent?,optimizer state,10382
16340,What does param_group(dict) specify?,Tensors,10376
16341,How many consolidated state_dicts are there per rank?,one,10376
16342,What is the default value for the rank that receives global states?,0,10376
16343,What does state_dict(dict) contain?,optimizer state,10376
16344,What is the name of the call that returns the state of the optimizer?,tostate_dict(),10376
16345,What is a list of dict?,a list ofparam_groups,10376
16346,"Which element corresponds to rank 0, etc.?",Element 0,10376
16347,We need all the ranks for the broadcast what?,insidestep(),10376
16348,What does insidestep() return for a given rank?,local_state_dict,10376
16349,What does param_groups contain across distributed data parallel ranks?,Partitions parameters,10376
16350,What does state_dict(dict) return?,globalstate_dict,10376
16351,What can be made trainable by adding a param group to theOptimizersparam_groups?,frozen layers,75
16352,How many consolidated state_dicts are updated per rank?,one per rank,7470
16353,What is the default value for global parameter groups?,0,7470
16354,How many states does the consolidated state_dict list update?,one per rank,2299
16355,How many consolidated state_dicts does theOptimizersparam_groups update?,one per rank,944
16356,What does param_group(dict) do?,Specifies what Tensors should be optimized along with group specific optimization options,10322
16357,What is a dict containing all parameter groups?,param_groups,8063
16358,What is state_dict(dict)?,optimizer state,10927
16359,Restore the global parameter groups as well as what else?,shard,5172
16360,What dict contains all parameter groups Partitions parameters across distributed data parallel ranks?,param_groups,2706
16361,What is the state of the optimizer as adict?,Gets this rank’sstate_dict,2706
16362,What does insidestep return for a given rank?,local_state_dict,2706
16363,Should be an object returned from a call to what?,state_dict(),10754
16364,What is the name of the broadcast we need all the ranks for?,insidestep(),10378
16365,What is the name of the function that returns the local_state_dict for a given rank?,insidestep(),10378
16366,What does getlocal_state_dict for state_dict(dict) return?,globalstate_dict,10378
16367,Param_groups contains all parameter groups across distributed data parallel ranks.,Partitions parameters,10378
16368,"Which element of the list corresponds to rank 0, etc.?",Element 0,10378
16369,What does getlocal_state_dictfor state_dict(dict) return?,globalstate_dict,10378
16370,We need all the ranks for the broadcast for what?,insidestep(),7310
16371,What does rank(int) return?,getlocal_state_dictfor,7311
16372,What is the name of the rank to getlocal_state_dictfor?,rank,7311
16373,What differs between?,optimizer classes,9180
16374,What class differs between param_groups and param_groups?,optimizer classes,9180
16375,A list ofparam_groups is a list of what?,dict,4752
16376,What is a part of distributed data parallel ranks?,Partitions parameters,4752
16377,Globalstate_dict consist of a list of what?,shards,4752
16378,Partitions parameters across distributed data what?,parallel ranks,4752
16379,"What corresponds to rank 0, etc.?",Element 0,7473
16380,What is a single optimization step called?,parameter update,5585
16381,What is returned for a given rank?,local_state_dict,5585
16382,What is the name of the rank to getlocal_state_dict for state_dict(dict)?,rank,5585
16383,Inverse short time Fourier Transform is expected to be the inverse of what?,ofstft(),3861
16384,What is the envelop created by?,the summation of all the windows,3683
16385,What is the envelop created by the summation of all the windows at a certain point in time?,0,3683
16386,Why does stft return a shorter signal than the original signal?,Sincestft()discards elements at the end of the signal if they do not fit in a frame,3683
16387,What is the envelop created by the summation of all windows never zero at certain point in time?,0,3683
16388,"If the signal isn’t padded, what can result in a shorter signal than the original signal?",ifcenteris False,3683
16389,What discards elements at the end of the signal if they do not fit in a frame?,Sincestft(),5935
16390,"If the signal isn't padded, what happens?",IfcenterisTrue,5935
16391,"IfcenterisTrue, there will be padding e.g. what?",constant,5935
16392,What is the last window of a signal?,last window,5935
16393,Why can left padding be trimmed off exactly?,because they can be calculated,3606
16394,"If there is padding, what is it called?",IfcenterisTrue,3606
16395,"If centerisTrue, then there will be padding e.g.'reflect','reflect', etc.",constant,3606
16396,What is the default setting for padding?,IfcenterisTrue,3606
16397,"Who wrote ""Signal estimation from modified short-time Fourier transform""?",D. W. Griffin and J. S. Lim,8694
16398,What was the title of the IEEE Trans. ASSP?,"vol.32, no.2, pp.236-243",8694
16399,What is the input tensor expected to be?,output ofstft(),8694
16400,When was real input deprecated?,1.8.0,8694
16401,What is the channeldimension?,optional,7131
16402,What is the default value of hop_length(Optional[int])?,Default:n_fft//4),9642
16403,What is Torch.ones(Optional[torch.Tensor]) called?,win_length,2113
16404,What is the default value for center(bool)?,True,9524
16405,What is the distance between neighboring window frames?,hop_length,9524
16406,What is the default value of center(bool)?,True,9524
16407,What is normalized(bool)?,Whether the STFT was normalized,11404
16408,Whether the STFT was normalized. (Default:False) onesided(Optional) – Whether the STFT was one,normalized,9055
16409,What is the amount to trim the signal by (i.e. the original signal length)?,length,9055
16410,What is the default signal length?,whole signal,9055
16411,What is onesided(Optional[bool])?,Whether the STFT was onesided,10302
16412,What is the default value for the output of a STFT?,return_complex,10302
16413,What is the value to trim the signal by?,length,9783
16414,What is incompatible withonesided=True?,return_complex,9783
16415,What is the Least squares estimation of the original signal of size?,Tensor,9783
16416,What is Optional[bool]) – Whether the output should be complex or if the input should be assumed to derive from,return_complex,9783
16417,What is incompatible with return_complex?,withonesided=True,9783
16418,What is the default value for the starting value for the set of points?,Default:0,5254
16419,What is returned by sizeendstartstepleftlceil fractextend?,1-D tensor,5254
16420,What is step(Number)?,the gap between each pair of adjacent points,9267
16421,What is the default value for the ending value for the set of points step(Number)?,Default:1,9264
16422,Warning More than one element of a created tensor may refer to what?,a single memory location,8170
16423,Why are the functions liketorch.Tensor.expand() more advisable to use?,easier to read,8170
16424,What can in-place operations result in?,incorrect behavior,4263
16425,What should you do if you need to write to the tensors?,clone them first,4263
16426,More than one element of a created tensor may refer to what?,single memory location,4263
16427,What may result in incorrect behavior?,in-place operations,4263
16428,What should you do if you need to write to the tensors first?,clone,4263
16429,Why is liketorch.Tensor.expand() more advisable to use?,easier to read,4263
16430,More than one element of a created tensor may refer to a single what?,memory location,4263
16431,What returns a view of a tensor?,PyTorch functions,4203
16432,What are PyTorch functions that return a view of a tensor called?,liketorch.Tensor.expand(),4203
16433,Why are PyTorch functions liketorch.Tensor.expand() more advisable to use?,easier to read,4203
16434,What is the shape of the output tensor stride?,size(tupleorints),4203
16435,What is size(tupleorints)?,the shape of the output tensor stride(tupleorints),4203
16436,What is a transposed version of input?,tensor,5458
16437,The resultingouttensor shares its underlying storage with what?,theinputtensor,5458
16438,What is the second dimension to be transposed?,dim1(int),5458
16439,Returns the cross product of vectors in what?,dimensiondimofinputandother,5522
16440,The size of theirdimdimension should be what?,3,5522
16441,"Ifdimis is not given, what happens?",defaults to the first dimension found with the size 3,5522
16442,What might happen if the size of inputandother is not given?,unexpected,5522
16443,"What is the second input tensor dim(int,optional)?",other(Tensor),5522
16444,How far can a n-D tensor be rotated?,90 degrees,5713
16445,What is the number of times to rotate dims?,k(int),5713
16446,What does a listortuple represent?,axis,5713
16447,What does asscatter_() do?,Adds all values from the tensorotherintoself,961
16448,"For each value insrc, it is added to what?",index inself,961
16449,What happens to a 3-D tensor for a 3-D tensor?,selfis updated as:,961
16450,Where can this operation behave nondeterministically when given tensors?,CUDA device,4427
16451,What does the backward pass implement only forsrc.shape==index.shape?,SeeReproducibility,4427
16452,"When empty, what does the operation return?",returnsselfunchanged,4427
16453,When given tensors on what device may this operation behave nondeterministically?,CUDA,4427
16454,What is the name of the operation that may behave nondeterministically when given tensors on a CUDA device?,Reproducibility,4427
16455,"When empty, the operation returns what?",selfunchanged,4427
16456,What is the axis along which to index index(LongTensor)?,dim(int),962
16457,What is the source element to scatter and add?,src(Tensor),962
16458,The upper triangular part of a matrix returns how many tensors?,2,5685
16459,What part of a matrix is defined as the elements on and above the diagonal?,the upper triangular part,5685
16460,What is n0n geq 0n0 called?,the order of the polygamma function,1809
16461,The order of the polygamma function is implemented only for what?,nonnegative integers,1809
16462,What is the input tensor of the polygamma function?,input(Tensor),1809
16463,What is an example of the order of the polygamma function?,Example,1809
16464,Calculates the variance of all elements in what?,theinputtensor,3644
16465,What is calculated if Bessel's correction is used?,the sample variance,3644
16466,"What does out(Tensor,optional) calculate?",the variance of all elements in theinputtensor,11279
16467,Whether the output tensor hasdimretained or not?,keepdim,11279
16468,What is calculated in the inputtensor?,the variance of all elements,9661
16469,What is the default value of Bessel's correction?,IfunbiasedisTrue,9186
16470,How does this function check if allinputandothersatisfy the condition?,elementwise,7526
16471,"Atol(float,optional) is used to compare atol(float,optional) to what?",absolute tolerance,7526
16472,"What is the default value of equal_nan(bool,optional)?",False Example,7526
16473,What is Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API,Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer,2072
16474,What is one way to disable JIT for Debugging?,Disable JIT for Debugging,2145
16475,What is the name of the Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive,Inspecting Code Interpreting Graphs Tracer,3853
16476,What is TorchScript a way to do from PyTorch code?,create serializable and optimizable models,1162
16477,What Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?,Frequently Asked Questions,2605
16478,References TorchScript is a way to do what from PyTorch code?,create serializable and optimizable models,5119
16479,What type of version of aScriptModule can be saved for use in a separate process?,offline,2034
16480,What can a scripted function call an encoder module generated using?,tracing,5753
16481,What can we step into the@torch.jit.scriptfunction as?,normal Python function,8864
16482,What produces the output of TorchScript's compilation of the code for theforwardmethod?,The example above,905
16483,What can you use this output to do?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,905
16484,What is an example of a static single assignment (SSA) intermediate representation?,example,905
16485,What instruction produces the graph?,test.py:9:10,7947
16486,What operator is equivalent to totorch.zeros?,aten::zerosis the operator,7963
16487,What is the unique value that we assign the output to?,ofTensortype,7053
16488,What does assign the output to a (unique) value namedrv.1?,%rv.1:Tensormeans,57
16489,What is thetorch.jit.trace()API.check_inputstakes?,check_inputson,4530
16490,What is an example of an edge case where the trace of a given Python function/module will not be representative of the underlying code?,example,4530
16491,Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) Note,indexing,7981
16492,Tracing of in-place operations of what?,tensor views,7981
16493,What is a list of inputs that will be used to re-trace the computation and verify the results?,tuples,7981
16494,What does a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor produce,a graph,4665
16495,The message indicates that the computation differed between when we first traced it and when we traced it with what?,check_inputs,2726
16496,The loop within the body ofloop_in_traced_fndepends on what of the inputx?,shape,2726
16497,How are methods called fromforward compiled?,in the order they are used inforward,7372
16498,When does the @torch.jit.ignoreannotation's behavior change?,PyTorch 1.2,7372
16499,What is the name of the @ignore decorator used to make a function or method callable from code that is exported?,@torch.jit.ignore,7372
16500,What does Einsum stand for?,Sums the product of the elements of the inputoperandsalong dimensions,6088
16501,What is the name of the Equation that allows multi-dimensional linear algebraic array operations?,Equation,6088
16502,What function returns the full window size?,Bartlett window function,1402
16503,What is the value of the Bartlett window function?,full window size,1402
16504,What is true when the returned window is used as a periodic window?,ifperiodic,1402
16505,What is the inputwindow_length a positive integer controlling the returned window size?,whereNNNis the full window size,11381
16506,What is true when the returned window is used as a periodic window with functions liketorch.stft()?,ifperiodic,11381
16507,"What is the size of returned window periodic(bool,optional)?",window_length(int),11381
16508,The input tensor's data type must be what?,floating point or complex type,5593
16509,"What is 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac5335",gain Linear / Identity,9441
16510,Returns a new tensor containing imaginary values of the self tensor. What Returns a new tensor,Tensor.imag,6428
16511,What is added to the tensor?,a scalar or tensor to self tensor,6428
16512,What is another name for Tensor.argmax?,Tensor.argmax,6209
16513,What is the In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos,Tensor.absolute_,6209
16514,What is added to the self tensor?,a scalar or tensor to self tensor,5342
16515,What is the name of the tensor that returns a scalar or tensor to self tensor?,Tensor.addbmm,5342
16516,What is In-place version of abs() Tensor.absolute,Tensor.abs_,6201
16517,What does add add?,a scalar or tensor to self tensor,3820
16518,What is added to the Tensor.absolute,a scalar or tensor to self tensor,6205
16519,What is Tensor.add_  In-place version of?,add() Tensor.addbmm,999
16520,What type of elements can be stored in a sparse array?,non-zero,4450
16521,What is the consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers?,"at least
(2 * 8 + 4) * 100 000 = 2 000 000 bytes",2531
16522,What is the function that provides the size of a sparse COO tensor?,torch.sparse_coo_tensor(),2531
16523,How much memory saving does a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers have with COO format?,200 fold,2530
16524,How is a  sparse tensor constructed?,"by providing the two tensors of
indices and values, as well as the size of the sparse tensor",848
16525,What is extension of sparse tensors with scalar values to sparse tensor called?,hybrid tensors,848
16526,What are hybrid tensors?,"an extension of sparse tensors with scalar values
to sparse tensors with (contiguous) tensor values",1123
16527,What is the type of the indices of specified elements?,torch.int64,10861
16528,What  permits uncoalesced sparse tensors?,PyTorch sparse COO tensor format,6142
16529,"Is mixing
of dense and sparse dimensions supported?","Dense dimensions always follow sparse dimensions, that is, mixing
of dense and sparse dimensions is not supported",6142
16530,How do we denote a N-dimensional hybrid tensor?,"We use (M + K)-dimensional tensor where M and K are the numbers of sparse and dense
dimensions, respectively, such that M + K == N holds",8295
16531,What does the coalescing process do?,"It will accumulate the multi-valued elements
into a single value using summation",8295
16532,What format permits uncoalesced sparse tensors?,PyTorch sparse COO tensor,2107
16533,What supports values stored as strided tensors?,s.values().layout == torch.strided,10632
16534,When is the lexicographical ordering of indices advantageous?,"for implementing algorithms that involve many element
selection operations",10636
16535,What is the output of torch.Tensor.coalesce() method?,a sparse tensor with indices of specified tensor elements unique and sorted in lexicographical order,3725
16536,What should be done if an operation can produce repeated entries?,you should coalesce your sparse tensors to prevent them from growing too large,3725
16537,how is addition of sparse COO tensors implemented?, by simply concatenating the indices and values tensors:,11388
16538,What of specified tensor elements are unique?,indices,10866
16539,The indices are sorted in what order?,lexicographical order,10855
16540,What is True if the Tensor uses sparse storage layout?,Tensor.is_sparse,7072
16541,What is returned by the indices of the sparse tensor mask?,a new sparse tensor with values from a strided tensor self,7072
16542,If self is a sparse COO tensor that is coalesced Returns what?,True if self is a sparse COO tensor that is coalesced,6465
16543,Tensor.is_sparse returns what?,True if the Tensor uses sparse storage layout,6465
16544,What function will only return complex tensors?,return_complex=True,2619
16545,From what version did return_complex have to always be given explicitly for real inputs?,1.8.0,2619
16546,"If hop_length is None (default), what is it treated as","equal to
floor(n_fft / 4)",2619
16547,"Ignoring what, this method computes the following expression: where mmm is the index of the sliding window, and omega",optional batch dimension,5888
16548,"If what is true, input will be padded on both sides to length n_fft before being applied?",center is True,5888
16549,Which version required return_complex required to be given explicitly for real inputs?,1.8.0,8151
16550,What should be set so function stft will only return complex tensors?,return_complex=True,8151
16551,"If what is true, input will be padded on both sides so that the n_fft is padded before being applied?",center is True,8151
16552,"If win_length is None (default), what is it equal to?",n_fft,6946
16553,What begins at time thop_lengtht times texthop_length?,ttt-th frame,4497
16554,"If win_length is None (default), what value is taken?",n_fft,9639
16555,What is the default setting for input when center is True?,"input will be padded on
both sides so that the ttt-th frame is centered",9639
16556,"If the input or window tensors are complex, what is not possible?",onesided output is not possible,3401
16557,"If return_complex is True, the output is what?",input.dim() + 2 dimensional real tensor,3401
16558,Returns either a complex tensor or what?,"a real tensor of size (∗×N×T×2)(* \times N
\times T \times 2)(∗×N×T×2).",3401
16559,Who selects a given stream?,Context-manager,1880
16560,What does comm.reduce_add do?,Sums tensors from multiple GPUs.,1580
16561,Returns the index of a currently selected device. Returns what for a given device?,currently selected Stream,5533
16562,What Gathers tensors?,comm.gather,5533
16563,What releases unoccupied cached memory currently held by the caching allocator?,Wrapper around a CUDA stream,5508
16564,what computes the entropy on input  elementwise?,torch.special.entr,7759
16565,What does torch.special.expm1 do?,Computes the exponential of the elements minus 1 of input,7759
16566,What computes the natural logarithm of the absolute value of the gamma function on input.,torch.special.gammaln,7759
16567,Computes the natural logarithm of the absolute value of the gamma function on input?,torch.special.gammaln,10336
16568,"If the Future is already completed, the given callback will be run what way?",inline,1158
16569,GPU support is a what?,beta feature,2666
16570,What is the reference to this Future?,"The callback must take one argument, which
is the reference to this Future",9705
16571,What does fut.wait() do?,Obtain the value of an already-completed future,4469
16572,What method will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the,wait(),4469
16573,What is returned from  the inverse hyperbolic tangent of the elements of input?,a new tensor,5401
16574,What is the domain of the inverse hyperbolic tangent?,"(-1, 1)",5401
16575,What is Roll the tensor along the given dimension?,"Elements that are shifted beyond the
last position are re-introduced at the first position.",5712
16576,What is the sum of exponentiations of inputs called?,Logarithm,4137
16577,What is the sum of exponentiations of the inputs?,Logarithm,4137
16578,what function is useful in statistics?,torch.logaddexp. ,1514
16579,when torch.logaddexp function is useful in statistics?,When the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers,1514
16580,How is log(ex+ey)logleft(ex + eyright)log(ex+e,pointwise,1514
16581,What part of the calculated probability is stored?,logarithm,1514
16582,What performs a reduction on a single tensor?,with torch.logsumexp(),1514
16583,What does the function (ex+ey)logleft(ex + eyright)log(ex,pointwiselog,1514
16584,What is the name of the function that performs a reduction on a single tensor?,with torch.logsumexp(),1514
16585,What does this function allow?,adding probabilities,4138
16586,What is calculated by the function (ex+ey)logleft(ex + eyright)log,pointwiselog,4138
16587,Where is this function torch.logaddexp useful?,statistics,4138
16588,What is stored in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers?,the logarithm of the calculated probability,4138
16589,What is the function that calculates pointwiselog(ex+ey)logleft(ex + ey,Logarithm of the sum of exponentiations of the inputs,4138
16590,What function performs a reduction on a single tensor?,with torch.logsumexp(),4138
16591,What is an example of a logarithm of the sum of exponentiations of the inputs?,Example,4138
16592,What should this op be disambiguated?,with torch.logsumexp(),7773
16593,what is Warning  about testing.html?,This module is in a PROTOTYPE state,8202
16594,What are we actively looking for feedback for?,UI/UX improvements or missing functionalities,7765
16595,What is the only way NaN's are considered equal to each other?,ifequal_nanisTrue,7765
16596,"What types of values are considered close if they have the same device(if check_device is True), same dtype(ifcheck",real-valued and finite,7765
16597,What does  the  module Assert that actual and expected are closed based on what ?,"close if they have same device , same stride , non finite",7178
16598,"If actual and expected are  close, they are considered close if they are what?",real-valued and finite,7764
16599,What state is testing.html module in?,PROTOTYPE state,7766
16600,What is considered close if and only if they are equal?,Non-finite values,7766
16601,What does expected(Any) mean?,Expected input,7766
16602,What does rtol(Optional[float]) represent?,Relative tolerance,7766
16603,Atol(Optional[float]) is what?,Absolute tolerance,7766
16604,What does actual(Any) mean?,Actual input,7766
16605,What must also be specified if atol is specified?,"if specified atol, rtol must also be specified",7766
16606,What are we actively looking for for UI/UX improvements or missing functionalities?,feedback,7766
16607,Asserts  that actual and expected are close?,that actual and expected are close,7766
16608,NaN’s are only considered equal to each other what?,ifequal_nanisTrue,7766
16609,When may the available functions change in this module?,PyTorch releases,7766
16610,How can be Tensor’s constructed?,with torch.as_tensor(),7766
16611,"If what must also be specified, default values based on the dtype are selected with the below table. atol(Optional[float]",specifiedatol,7766
16612,What is the name of the flag that indicates that two NaN values will be considered equal?,equal_nan,7766
16613,What type of values are considered close if both their real and imaginary components are considered close according to the definition above?,complex-valued,7766
16614,What are still being added to this module?,New functions,8201
16615,What is close when actual and expected are real-valued and finite?,Asserts that actual and expected are close,1337
16616,"If actual and expected are  real-valued and finite, they are considered close if and they have the same device(what?",if check_device is True,3394
16617,What type is expected input?,actual and expected can  be Tensor’s or any array-or-scalar-like of the same type,3394
16618,"If actual and expected are  what type of valued, they are considered close if both their real and imaginary components are considered close according to the definition above",complex,3394
16619,"What is considered close if they have the same device(if check_device is True), same dtype(If Check_dtype",Asserts  that actual and expected are close,1335
16620,How are non-finite values considered close?,if and only if they are equal,8862
16621,What is the name of the same device?,if check_device is True,8862
16622,What does if check_device is True stand for?,same device,8862
16623,"If actual and expected are  complex-valued, they are considered close if both their real and imaginary components are considered close according to the definition above.",If actual and expected are  complex-valued,8862
16624,What can be Sequence's or Mapping's?,be Sequence’s or Mapping’s,8862
16625,"If what must also be specified, default values based on the dtype are selected with the below table?",specifiedatol,8862
16626,What are selected with the below table if specifiedatol is omitted?,default values based on the dtype,8862
16627,"If actual and expected are  what, they are considered close if both their real and imaginary components are considered close according to the definition above?",complex-valued,8862
16628,"Which values are considered close if they have the same device, same dtype",If actual and expected,3393
16629,What are real-valued and finite?,actual and expected,3393
16630,What are considered close if and only if they are equal?,Non-finite values,8861
16631,"If actual and expected are what, they are considered close if both their real and imaginary components are considered close according to the definition above?",complex-valued,8861
16632,What does if check_device is True mean?,same device,8861
16633,What are complex-valued?,actual and expected,8861
16634,What is the definition of close if both real and imaginary components are considered close?,complex-valued,3390
16635,"If actual and expected are  complex-valued, they are considered close what?",if both their real and imaginary components are considered close according to the definition above,3391
16636,What are actual and expected considered close if their structure matches and all their elements are considered close according to the above definition?,be Sequence’s or Mapping’s,3391
16637,What must also be specified if actual and expected can  be Tensor's are considered close if both their real and imaginary components are considered,specifiedatol,3391
16638,What are selected with the below table if specified atol must also be specified?,default values based on the dtype,3391
16639,What can be constructed with torch.as_tensor()?,whichtorch.Tensor’s,8795
16640,What can be Tensor's or any array-or-scalar-like of the same type?,actual and expected,8795
16641,What else can actual and expected be?,be Sequence’s or Mapping’s,8795
16642,What must also be specified if rtol(Optional[float]) is omitted?,specifiedatol,8795
16643,"If ""relaxed"" complex values are considered as NaN if what component is NaN?",real or imaginary component,8795
16644,When are be Sequence's or Mapping's considered close?,when their structure matches,8794
16645,What else can actual and expected can  be Tensor’s or scalar-like of the same type?,be Sequence’s or Mapping’s,8794
16646,Actual(Any) – what type of input?,actual and expected can  be Tensor’s or any array-or-scalar-like of the same type,8796
16647,Atol(Optional[float]) – what?,Absolute tolerance,8796
16648,What are selected with the below table?,default values based on the dtype,8796
16649,What can be considered close if their structure matches and all their elements are considered close according to the above definition?,be Sequence’s or Mapping’s,8796
16650,rtol(Optional[float]) – what?,Relative tolerance,8796
16651,Actual input. expected(Any) – what?,Expected input,8796
16652,"_nan(Union[bool,str]) – If True, two NaN values will be considered equal",equal,8796
16653,rtol(Optional[float]) – Relative tolerance. If what?,specified atol must also be specified,8796
16654,Check_dtype(bool) – If True(default) – asserts that corresponding tensors have what?,same dtype,8796
16655,"If check_dtype(bool) is disabled, tensors with different type's are what?",promoted to a common dtype,8796
16656,Check_stride(bool) – If True(default) asserts that what?,corresponding tensors have the same stride,8796
16657,How can actual and expected be Tensor’s or any array-or-scalar-like of the same type be constructed?,with torch.as_tensor(),8796
16658,Absolute tolerance. If what?,specified rtol must also be specified,8796
16659,"Check_device(bool) – If True(default) – If True(default), asserts that what?",corresponding tensors are on the same device,8796
16660,What must be specified if rtol(Optional[float]) is omitted?,"If omitted,
default values based on the dtype are selected",8797
16661,What is optional if atol(Optional[float]) is also specified?,specified rtol must,8797
16662,What does rtol stand for?,Relative tolerance,8797
16663,What must be specified if atol(Optional[float]) – Absolute tolerance?,specified rtol must also be specified,8797
16664,What is omitted if specified rtol must also be specified?,default values based on the dtype are selected with the below table,8797
16665,What happens if default values are omitted?,default values based on the dtype are selected with given input,8921
16666,What is atol?,Absolute tolerance,8921
16667,Atol(Optional[float]) – What is atol?,Absolute tolerance,8921
16668,If True(default) asserts that what is on the same device?,corresponding tensors,8921
16669,If True(default) asserts that which tensors have the same stride?,check_stride asserts corresponding tensors have the same stride,8921
16670,"What does union[bool,str] stand for?",equal_nan,8921
16671,"If check_device(bool) is disabled, what happens to tensors on differentdevice's before being compared?",tensors on differentdevice’s are moved to the CPU,8921
16672,What are selected with the below table if specified rtol must also be specified?,default values based on the dtype,8921
16673,When are complex values considered as NaN ?,if either the real or imaginary component is NaN,8921
16674,If True(default) asserts that what?,corresponding tensors have the same dtype,8921
16675,What are promoted to a common dtype if check_dtype(bool) is disabled?,tensors with different type’s,8921
16676,What happens if atol is omitted?,default values based on the dtype are selected with the below table,10629
16677,What happens if rtol is omitted?,default values based on the dtype are selected with the below table,10629
16678,What happens if rtol(Optional[float]) is omitted?,default values based on the dtype are selected with the below table,10629
16679,"If atol(Optional[float]) is specified, what must also be specified?",specified rtol must,10629
16680,What is rtol?,Relative tolerance,10629
16681,"If rtol(Optional[float]) – Relative tolerance, what must be specified?",specified atol must also be specified,10629
16682,Atol(Optional[float]) – What?,Absolute tolerance,10629
16683,What are two NaN values considered equal?,equal,10629
16684,What is selected with the below table?,default values based on the dtype,10629
16685,"If check_device(bool) is disabled, what are moved to the CPU before being compared?",tensors on differentdevice’s,10629
16686,"If check_dtype(bool) is disabled, which tensors are promoted to a common dtype?",tensors with different type’s,10629
16687,Check_dtype(bool) – If True(default) asserts that corresponding tensors have what?,same dtype,10629
16688,What must be specified if rtol(Optional[float]) – Relative tolerance?,specified atol must also be specified,8798
16689,"If atol(Optional[float])must also be specified, what must also be specified?",specified rtol,8798
16690,What happens if specified atol must also be specified?,default values based on the dtype are selected with the below table,8798
16691,What is selected with the below table if specified rtol must also be specified?,default values based on the dtype,8798
16692,If True(default) asserts that corresponding tensors have what?,same dtype,8798
16693,"If omitted, what is selected with the below table?",default values based on the dtype are selected ,3395
16694,"If atol(Optional[float]) – Absolute tolerance, what must also be specified?",rtol,3395
16695,Actual(Any) – what?,Actual input,3395
16696,How can be Tensor’s be constructed?,with torch.as_tensor(),3395
16697,What does check_device(bool) stand for?,If True(default),3395
16698,What values are only considered close if and only if they are equal?,Non-finite values,3395
16699,"rtol(Optional[float]) – Relative tolerance. If what is omitted, default values based on the",specified atol must also be specified,3395
16700,"If omitted, what are selected with the below table?",default values based on the dtype are selected with the below table,3395
16701,"If check_device(bool) is disabled, what happens to tensors on differentdevice’s?",tensors on differentdevice’s are moved to the CPU before being compared,3395
16702,What does rtol(Optional[float]) mean?,Relative tolerance,10628
16703,What must be specified if rtol is a relative tolerance?,specified atol must,10628
16704,"If true, two NaN values will be considered what?",equal,10628
16705,"If ""relaxed"", complex values are considered as NaN if either the real or imaginary component is NaN or what?",real or imaginary component is NaN,10628
16706,What is the requirement for Absolute tolerance?,if specified rtol must also be specified,10628
16707,What happens if atol(Optional[float]) is omitted?,default values based on the dtype are selected,10628
16708,"If ""relaxed"" complex values are considered as NaN if either the what component is NaN?",real or imaginary component,10628
16709,What are two NaN values considered to be if true?,equal,8920
16710,What are selected  if atol is omitted?,default values based on the dtype,8920
16711,Complex values are considered as NaN if what component is NaN?,real or imaginary component,8920
16712,What are considered as NaN if either the real or imaginary component is NaN?,complex values,9277
16713,"What does equal_nan(Union[bool,str]) mean?","If True, two NaN values will be considered equal",9276
16714,If what is true that two NaN values will be considered equal.,"equal_nan(Union[bool,str])",9276
16715,What is considered as NaN if either the real or imaginary component is NaN?,complex values,9276
16716,What asserts that corresponding tensors are on the same device?,check_device(bool),9062
16717,"If check_device is disabled, tensors on differentdevice's are moved to what?",CPU,9062
16718,"If the check is disabled, tensors with different type's are promoted to what?",common dtype,9062
16719,Where are tensors on different devices moved before being compared?,CPU,9063
16720,What does check_stride assert?,corresponding tensors have the same stride,9063
16721,If any tensor is what?,quantized or sparse,9063
16722,Check_device(bool) – If True(default) – asserts that what are on the same device?,corresponding tensors,9063
16723,"Check_dtype(bool) – If True(default) – If True(default), asserts that corresponding ten",same dtype,9063
16724,"If check_dtype is disabled, tensors with different type’s are promoted to what?",common dtype,9063
16725,What is the name of the error message that can be used if the values of corresponding tensors mismatch?,See below for details,9063
16726,"If corresponding tensors are not on the same device, what is the cause of the error?",If Check_device,9063
16727,"Assertion Error– If Check_device, but corresponding tensors are not on the same device. As",If Check_dtype,9063
16728,What does check_dtype assert?,corresponding tensors have the same dtype,9061
16729,What does check_device(bool) assert is on the same device?,corresponding tensors,9061
16730,"If True(default), asserts that corresponding tensors have what?",same dtype,9061
16731,What are tensors with different type's promoted to?,a common dtype,9061
16732,What asserts that corresponding tensors have the same dtype?,check_dtype(bool),9065
16733,"If this check is disabled, tensors with different type's are promoted to what?",a common dtype,9064
16734,What does check_dtype(bool) do?,"If True(default), asserts that corresponding tensors have the same dtype",9064
16735,"If this check is disabled, tensors with different type's are promoted  to a common dtype?",check_dtype(bool),9064
16736,What asserts that corresponding tensors have the same stride?,check_stride,9070
16737,"What does msg(Optional[Union[str,Callable[[Tensor,Tensor,Tensor,DiagnosticInfo],str]]])","Optional error message to use if
the values of corresponding tensors mismatch",9940
16738,What is the UsageError if any tensor is quantized or sparse?,temporary restriction,9940
16739,Can be passed as what in which case it will be called with the mismatching tensors and a namespace of diagnostic info?,callable,9939
16740,"msg(Optional[Union[str,Callable[[Tensor,Tensor,Diagno",Optional error message,9939
16741,What do you need to know about the mismatching tensors?,details,9939
16742,What can't be constructed from an array-or-scalar-like?,a torch.Tensor,9939
16743,"If corresponding tensors are not on the same device, what is an example of an Assertion Error?",If Check_device,9939
16744,What should you do if a torch.Tensorcan't be constructed from an array-or-scalar-like?,use check_stride(bool),9069
16745,What if a torch.Tensorcan't be constructed from an array-or-scalar-like?,have to  refer UsageError,9069
16746,What does not match if the inputs are Mapping's?,their set of keys,1333
16747,What do not have the same shape?,corresponding tensors,1333
16748,What is an example of an Assertion Error if corresponding tensors are not on the same device?,If Check_device,1333
16749,What does the following table display for different type’s?,default rtol and atol,1333
16750,What type of a torch.Tensor can't be constructed from?,array-or-scalar-like,8074
16751,What is the name of the corresponding tensor that does not have the same dtype?,If Check_dtype,8074
16752,Which tensor does not have the same stride?,If Check_stride,8074
16753,What is a temporary restriction and will be relaxed in the future?,a torch.Tensorcan’t be constructed from an array-or-scalar-like,8074
16754,What type of tensor is a torch.Tensor?,quantized or sparse,8073
16755,Assertion Error– If corresponding tensors do not have what?,same shape,8075
16756,What is UsageError?,UsageError– If only rtol or atol is specified,8078
16757,"What happens if the inputs are Sequence’s, but their length does not match?","If the inputs are Sequence’s, but their length does not match",8078
16758,What type of tensors have different types?,array-likes,8076
16759,What do corresponding tensors not have?,same shape,8076
16760,What is the default rtol and atol for different type's?,dtype rtol atol float16,8076
16761,What causes Assertion Error if corresponding tensors are not on the same device?,If Check_device,1329
16762,What is an Assertion Error if corresponding tensors do not have the same dtype?,If Check_dtype,8077
16763,What happens if the inputs are Mapping’s but their set of keys do not match?,corresponding tensors do not have the same shape,1332
16764,What error occurs when corresponding tensors do not have the same stride?,If Check_stride,1327
16765,What does max_abs_diff_idx represent?,Index of greatest absolute difference,1327
16766,What does not match when the inputs are Mapping's?,their set of keys,1331
16767,"If the inputs are Mapping's, but what do they do not match?",their set of keys do not match,1331
16768,What happens if corresponding tensors do not have the same shape?,Assertion Error– If corresponding tensors do not have the same shape,1330
16769,What error occurs if corresponding tensors are not on the same device?,If Check_device,1330
16770,What does the dtype refer to in case actual and expected do not have the same dtype?,the dtype refersto the promoted type in case actual and expected do not have the same dtype,1330
16771,What is the namespace of diagnostic information that will be passed to msg if?,total_mismatches,1330
16772,the dtype refers to what type in case actual and expected do not have the same dtype?,promoted type,1325
16773,What is the dtype?,rtol,1325
16774,What type does the dtype refer to in case actual and expected do not have the same dtype?,promoted type,1334
16775,What if the values of corresponding tensors are not close?,Assertion Error,1334
16776,"What does max_abs_diff_idx(Union[int, Tuple[int,...]) represent?",Index of greatest absolute difference,1334
16777,"What does max_rel_diff(Union[int, float]) represent?",Greatest relative difference of the inputs,1334
16778,What is the name of the total mismatches divided by number of elements?,mismatch_ratio,7092
16779,"What does max_abs_diff(Union[int, float]) represent?",Greatest absolute difference of the inputs,7092
16780,In what case does the dtype refer to the promoted type?,case actual and expected do not have the same dtype,7092
16781,"What does max_abs_diff_idx(Union[int, Tuple[int,...]]) represent?",Index of greatest absolute difference,7092
16782,"What does max_rel_diff(Union[int, float]) represent of the inputs?",Greatest relative difference,7092
16783,"What does max_rel_diff_idx(Union[int, Tuple[int,...]]) represent",Index of greatest relative difference,7092
16784,What returns a 2-D square tensor with the elements of inputas the diagonal?,If input is  a vector,3421
16785,What is the argument diagonal controls?,"If diagonal = 0, it is the main diagonal",3421
16786,What always returns the diagonal of its input?,torch.diagonal(),3421
16787,What always constructs a tensor with diagonal elements specified by the input?,torch.diagflat(),3421
16788,What is the diagonal of a given matrix?,k-th,3421
16789,"If input is  a matrix (2-D tensor), then returns a what tensor with the elements of input?",1-D,3421
16790,What is the main diagonal?,"If diagonal > 0, it is above the main diagonal",3421
16791,"If diagonal 0, it is what?",below the main diagonal,3421
16792,"If input is  a matrix, then returns what tensor with the elements of input?",1-D,3420
16793,What is the diagonal to consider?,"diagonal(int,optional)",3420
16794,What is above the main diagonal?,"data, If diagonal > 0,",3407
16795,Where is the main diagonal If diagonal >0?,above,3407
16796,Where is the main diagonal If diagonal 0?,below,3407
16797,"What does out(Tensor,optional) refer to?",output tensor,3407
16798,What does torch.diagflat()always return where the input vector is the diagonal?,the square matrix,3407
16799,"If diagonal  0, it is what?",below the main diagonal,3419
16800,"If input is  a matrix (2-D tensor), then returns what with the diagonal elements of input?",1-D tensor,3419
16801,What  return  if obj is a PyTorch tensor?,PyTorch storage object,3918
16802,What does Returns True if the data type of input is a single element tensor which is not equal to zero after type conversion,Sets the default floating point dtype tod,5215
16803,What is returned if obj is a PyTorch storage object?,PyTorch storage object,5210
16804,Returns True what if the inputs a single element tensor?,if the inputs a single element tensor,5230
16805,What does it do if the data type of input is a single element tensor which is not equal to zero after type conversions,Sets the default floating point dtype tod,5222
16806,Returns what if the data type of input is a floating point data type?,True,5231
16807,Returns True if the inputs a single element tensor which is not equal to zero after type conversions?,if the inputs a single element tensor,5231
16808,Constructs a sparse tensor in COO(rdinate) formatwith what?,specified values at the given indices,5231
16809,What is returned when the default torch.Tensortype is set to floating point tensor typet?,the total number of elements in the input tensor,5216
16810,Returns what if obj is a PyTorch tensor?,True,5216
16811,Returns what if the inputs a single element tensor which is not equal to zero after type conversions?,True,5242
16812,What is returned by setting the default torch.Tensortype to floating point tensor typet?,the total number of elements in the input tensor,5839
16813,What is the default torch.Tensortype set to?,floating point tensor typet,5224
16814,What does Releases all unoccupied cached memory ?,Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device,5141
16815,What does seememory_reserved do?,Set memory fraction for a process,5141
16816,What function is deprecated?,seemax_memory_reserved(),5141
16817,What is seememory_reserved?,Deprecated,5141
16818,What is the name of the function that sets memory fraction for a process?,seememory_reserved(),5141
16819,What is the name of the function that releases all unoccupied cached memory currently held by the caching allocator?,seemax_memory_reserved(),5141
16820,What type of tensor does it construct from Random sampling?,tensor with data,5087
16821,What format is the a sparse tensor in?,COO(rdinate) format,5088
16822,What does the a sparse tensor in COO(rdinate) format contain?,specified values at the given indices,5088
16823,What does a sparse tensor in COO(rdinate) format contain?,specified values at the given indices,2149
16824,What does the a sparse tensor contain?,specified values at the given indices,5678
16825,Returns what tensor of size end start step left l ceil fractextend -,1-D,5678
16826,With what a  view of an existing torch.Tensor input is created?,"specified size,stride and storage_offset",1843
16827,What is the shape defined by the variable argument size?,scalar value 0,1843
16828,what numpy can be used  to create a tensor?,a numpy.ndarray,1967
16829,What is the shape of the scalar value 1?,the shape defined by the variable argument size,5433
16830,What is the shape of the scalar value 0?,the shape defined by the variable argument size,5433
16831,Returns a what size tensor of size end start step left l ceil fractextend,1-D,5433
16832,What is the size of the scalar value 1?,the same size as input,5435
16833,"Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size. Returns a",same size,5432
16834,What is filled with the scalar value 0?,a tensor,5432
16835,Returns a tensor filled with the scalar value 1 with what?,the shape defined by the variable argument size,5430
16836,What is the tensor of size end start step left l ceil fractextend?,1-D,5251
16837,What is the tensor of size end start step+1 left l floor fractext end?,1-D,5251
16838,What does the Splits input do?,Concatenates the given sequence of seq tensor in the given dimension,1814
16839,What splits a tensor with three or more dimensions into multiple tensors depth wise according toindices_or_section,Splits input,1814
16840,What does the given sequence of seq tensor in the given dimension do?,Concatenates,1814
16841,What does Splits input use to split a tensor into multiple tensors depth wise?,indices_or_sections,1814
16842,How does one create a new tensor horizontally?,horizontally stacking,6021
16843,How many tensors does Splits input split into?,multiple tensors depth wise,6021
16844,Stack tensors in sequence depth wise (along what axis)?,third axis,2002
16845,What creates a new tensor horizontally?,horizontally stacking the tensors intensors,2001
16846,How many dimensions does a Splits input have?,three or more,6020
16847,What does Splits input do?,Gathers values along an axis specified by dim,6035
16848,What does a tensor do in sequence depth wise?,Stack tensors,6035
16849,What is Splits input?,a tensor with three or more dimensions,6035
16850,"Splits input, a tensor with one or more dimensions, into what according toindices_or_sections?",multiple tensors horizontally,6035
16851,What is the axis specified by Splits input?,by dim,6034
16852,Splits input is a tensor with how many dimensions?,one or more dimensions,6034
16853,What does Splits input use to split a tensor?,indices_or_sections,6034
16854,What does Splits input do to create a new tensor?,Stacks tensors,6034
16855,What do tensors with one or more dimensions Splits input into?,multiple tensors horizontally,6034
16856,Where do tensors stack in sequence vertically?,depth wise,2003
16857,"Splits input, a tensor with one or more dimensions, into what?",multiple tensors,2003
16858,What is the name of the index that returns a new tensor which indexes the input tensor along dimension dimusing,a Long Tensor,2003
16859,What happens along an axis specified by dim?,Gathers values,2003
16860,What does Alias for torch.movedim() return a new tensor that is?,a narrowed version of input tensor,2003
16861,What are the elements of in out at the  given?,indices,2003
16862,What is the name of the tensor that Splits input into multiple tensors horizontally?,indices_or_sections,6032
16863,What is the name of a tensor that Splits input into multiple tensors horizontally?,Stack,6032
16864,What is the name of the new tensor that indexes the input tensor along dimension dimusing the entries in index?,a Long Tensor,6032
16865,Which tensor indexes the input tensor according to the boolean mask mask?,1-D,6054
16866,What type of tensor indexes the input tensor according to the boolean mask mask?,1-D,6033
16867,What does move the dimension(s) of in out at the  position(s) in source to the position(s) in destination?,Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination,6056
16868,What is the boolean mask mask that returns a new 1-D tensor which indexes the input tensor,a Bool Tensor,6056
16869,What does Alias for torch.transpose() stand for?,Alias for torch.transpose(),6056
16870,Selects values fromin out atwhat?,1-dimensional indices,6056
16871,Splits input into what horizontally according toindices_or_sections?,multiple tensors,6056
16872,What is the name of the index which indexes the input tensor along dimension dimusing the entries in index?,a Long Tensor,6056
16873,Returns what with the elements of in out at the  given indices?,a new tensor,6056
16874,What is the name of the function that returns a new tensor that is a narrowed version of input tensor?,Alias for torch.movedim(),4274
16875,What does Alias for torch.movedim() do?,Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination,4274
16876,Returns what with all the dimensions of inputof size1removed?,a tensor,4274
16877,What kind of version of input tensor is the new tensor?,narrowed,5345
16878,What is used to move the dimension(s) of in out at the  position(s) in source to the position(s) in destination?,Alias for torch.movedim(),5351
16879,What is the name of the index that returns a new tensor that indexes the input tensor along dimension dimusing,a Long Tensor,5351
16880,Returns a new 1-D tensor which indexes the input tensor according to what boolean mask mask,a Bool Tensor,5351
16881,What method returns a new tensor that is a narrowed version of input tensor?,Alias for torch.movedim(),5328
16882,Out-of-place version of torch.Tensor.scatter_add_() Splits the tensor into,chunks,4273
16883,What function returns a new tensor that is a narrowed version of input tensor?,Alias for torch.movedim(),5352
16884,What does Alias for torch.transpose() do?,Alias for torch.transpose(),5470
16885,What is another name forfor torch.transpose()?,Alias for torch.transpose(),5470
16886,What is the name of the function that returns a tensor with all the dimensions of input of size1removed?,Alias for torch.transpose(),5470
16887,Returns a new tensor with the elements of in out at the  given what?,indices,5470
16888,What does Returns a tensor that is a transposed version of input do?,Removes a tensor dimension,5470
16889,What does a torch.Byte Tensor set?,random number generator state,5571
16890,What does Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive)?,a tensor,5571
16891,What does each row contain num_sample indices sampled from the multinomial probability distribution located in the corresponding row of,tensor,5858
16892,What does a torch.Byte Tensor do?,Sets the random number generator state,5660
16893,Returns a tensor filled with random integers generated uniformly between low(inclusive) and what (exclusive)?,high,5660
16894,What is returned when each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row,tensor,5459
16895,What is returned with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input ?,a tensor of the same size as input,5454
16896,What is returned with the same shape as Tensor input?,tensor,5489
16897,What is filled with random integers generated uniformly between low(inclusive) and high(exclusive)?,tensor,5415
16898,What does with torch.save() save to a disk file?,an object,5745
16899,What does Alias for torch.abs() compute of each element in input ?,absolute value,1690
16900,What computes the inverse cosine of each element in input ?,Alias for torch.acos(),1690
16901,What does Alias for torch.acos() return a new tensor with?,inverse hyperbolic cosine,1010
16902,Alias for torch.asinh(). Returns a new tensor with the inverse hyperbolic sine,arctangent,1010
16903,What is the name of the element-wise multiplication performed by Alias for torch.acos?,often s or 1 byte n s or 2,1010
16904,What does the element-wise division often s or 1 byte n s or 2 do?,multiply the result by the scalar valueand add it toinput,1767
16905,What does Alias for torch.acosh() add to each element of inputinput?,scalar other to,1767
16906,What is the element-wise division performed by Alias for torch.acosh()?,often s or 1 byte n s or 2,1767
16907,What is the name of the function that computes the inverse cosine of each element in input ?,Alias for torch.acos(),1767
16908,What type of cosine does Alias for torch.acos() compute?,hyperbolic,1767
16909,What is the element-wise division performed by Alias for torch.acosh?,often s or 1 byte n s or 2,1009
16910,What type of cosine of the elements of input is returned by Alias for torch.acos()?,hyperbolic,1009
16911,What does Alias for torch.acos() perform?,the element-wise multiplication,1009
16912,What type of cosine does Alias for torch.acos() return?,inverse hyperbolic,1009
16913,What does Alias for torch.acos() return?,inverse hyperbolic cosine,1768
16914,What does Alias for torch.acos() compute of each element in input ?,inverse cosine,1768
16915,What does Alias for torch.acosh() add to each element of the inputinput?,scalar other to,5389
16916,What performs often s or 1 byte n s or 2?,the element-wise division,5389
16917,Returns a new tensor with the inverse hyperbolic sine of the elements of input. Alias fortorch,arctangent,5389
16918,What is the name of the element-wise multiplication performed by Alias for torch.acosh?,often s or 1 byte n s or 2,1013
16919,What is the name of the function that adds the scalar other to each element of the inputinput?,Alias for torch.acosh(),1013
16920,What is the name of the function that adds the scalar other to each element of the input?,Alias for torch.acosh(),1013
16921,What does often s or 1 byte n s or 2 perform?,element-wise division,4820
16922,What is the result of the element-wise multiplication often s or 1 byte n s or 2 multiplied by?,scalar value,4820
16923,Computes the bitwise OR of inputandother. Computes the bitwise XOR of inputandother.,AND,4820
16924,In what unit is the element-wise angle of the given input tensor calculated?,radians,5388
16925,What is the name of the element-wise division performed by Alias for torch.acosh?,often s or 1 byte n s or 2,1015
16926,Computes the giveninput tensor. Returns a new tensor with the arcsine of the elements ofin,element-wise angle,1015
16927,What function adds the scalar other to each element of the inputinputand returns a new resulting tensor?,Alias for torch.acosh(),1015
16928,What is the name of the tensor that returns a new tensor with the arcsine of the elements of input,Alias for torch.asin,972
16929,What is the name of the function that returns a new tensor with the arcsine of the elements of input?,Alias for torch.asin(),972
16930,What does often s or 1 byte n s or 2 multiply the result by?,scalar value,4830
16931,What does Computes the bitwise OR of inputandother?,Computes the bitwise OR of inputandother,4830
16932,Alias for torch.asinh(). Returns a new tensor with what of the elements of input,arctangent,4831
16933,Clamps all elements in input into what?,"range[min,max]",4831
16934,Multiply the result by the scalar value and add it toinput. Computes the element-wise angle (in radi,often s or 1 byte n s or 2,4831
16935,Computes what of the giveninput tensor?,the element-wise angle,4831
16936,Computes the element-wise what of the giveninput tensor?,conjugate,4831
16937,What is returned with each of the elements of inputconverted from angles in degrees to radians?,a new tensor,4831
16938,What does divide each element of the inputinput by the corresponding element of other?,Divides each element of the inputinputby the corresponding element of other,4831
16939,Which Alia performs the element-wise multiplication often s or 1 byte n s or 2 multiply the result by the scalar value?,Alia,4831
16940,What does Alias for torch.asinh() return?,arctangent,5371
16941,What computes the element-wise conjugate of the given input tensor?,Alias for torch.clamp(),5371
16942,What does Alias for torch.atan() return?,inverse hyperbolic tangent,1019
16943,What does Alias for torch.clamp() clamp?,"all elements in input into the range[min,max]",1019
16944,What is the name of the tensor returned by Alias for torch.asin?,inverse hyperbolic sine,1018
16945,What element of the elements of input does Alias for torch.asinh() return?,arctangent,1018
16946,With consideration of what is the arctangent of inputi/otheri / textother_iinput,the quadrant,1024
16947,What does Alias for torch.atanh() do?,Computes the bitwise AND of inputandother,1024
16948,Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of inputandother?,Computes the bitwise OR of inputandother,1024
16949,What does Alias for torch.clamp() compute?,the element-wise conjugate of the giveninput tensor,1024
16950,What does Alias for torch.clamp() create a new floating-point tensor with?,the magnitude of inputand the sign of other,1024
16951,What tangent does Alias for torch.atan() return a new tensor with?,inverse hyperbolic tangent,1024
16952,What does the Alias for torch.atanh() do?,Computes the bitwise OR of inputandother,1023
16953,what Computes the bitwise OR of inputandother.,AND,1023
16954,What happens if all elements input evaluate toTrue?,Tests,6768
16955,What does Returns ignoreNaN values?,the median of the values in input,6768
16956,Tests if all elements input evaluate to what?,True,6768
16957,Returns what value of the input tensor in the given dimension(s)dim?,the minimum value of each slice,5560
16958,"Returns what of the values in input , ignoringNaN values?",the median,5560
16959,Returns the maximum value of all elements in input  tensor.,Tests if all elements input evaluate toTrue,5549
16960,What does it do when all elements input evaluate toTrue?,Tests if all elements input evaluate toTrue,5643
16961,Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.,p-norm,5643
16962,What does it test if all elements input evaluate toTrue?,Tests if all elements input evaluate toTrue,5644
16963,"Returns the median of the values in input , ignoring what?",NaN values,5605
16964,"What is returned, ignoringNaN values?",the median of the values in input,5623
16965,What does the variant of torch.quantile() do?,ignores,5623
16966,Returns the log of summed exponentials of each row of the input tensor in the given dimension dim?,p-norm,5651
16967,What does return the log of summed exponentials of each row of the input tensor in the given dimension dim?,the p-norm,5651
16968,"Returns the median of the values in input , what?",ignoringNaN values,5624
16969,What does this function check if all input and other satisfy the condition?,Returns the indices that sort a tensor along a given dimension in ascending order by value,7523
16970,What does Alias for torch.gt() do?,Computesinput,5568
16971,What is the name of the function that computes other text input> text other input>other element-,Alias for torch.gt(),5568
16972,What function checks if all input and other satisfy the condition?,Alias for torch.gt(),7524
16973,What is the name of the function that computes other text inputgeq text other inputother,Alias for torch.ge(),8008
16974,What is the name of the function that computesinput>other text input> text other inputother,Alias for torch.gt(),8008
16975,Returns a new tensor with what representation if each element is finite or not?,boolean elements,8008
16976,What does Alias for torch.ge() do?,Computesinput,1028
16977,What element represents if each element of input is NaN or not?,boolean elements,1028
16978,"What does return a new tensor with if each element of input is ""close"" to the corresponding element of other?",boolean elements,8007
16979,What does Computesinput>other text input> text other input>other element-wise?,Alias for torch.ge(),1027
16980,What is the name of the function that computesinput>other text input> text other input>other,Alias for torch.gt(),1027
16981,What does the Alias for torch.gt() do?,Computesinput,1811
16982,What is the name of the function that computes input>other text input> text other input>othere,Alias for torch.gt(),1811
16983,Tests if each element of input is what infinity or not?,negative,6773
16984,Returns a new tensor with what elements representing if each element of input is NaN or not?,boolean,6773
16985,What is the name of the tensor that returns the smallest element of each row of the input tensor in the given dimension dim,named tuple,6773
16986,What is the name of the tensor wherevaluesis the k th smallest element of each row of the input tensor in the,named tuple,6773
16987,What is the name of the function that computesinputother text inputleq textotherin,Alias for torch.le(),6771
16988,Which window function computes the Kaiser window with window length window_lengthand shape parameterbeta?,Hann,5885
16989,What is the name of the input to the shape shape?,Broadcastsinput,1466
16990,What is returned by Broadcastsinput to the shape shape?,Do cartesian product of the given sequence of tensors,1466
16991,What does it do to return a copy of input?,Compute combinations of lengthrrrof the given tensor,1466
16992,What is the function that returns the cross product of vectors in dimension dimof inputandother?,Compute combinations of lengthrrrof the given tensor,1659
16993,What does dimension dimof inputandother return?,the cross product of vectors,5517
16994,What is the sum of elements of inputin the dimension dim?,cumulative product,5517
16995,What is the cumulative maximum of elements of inputin the dimension dim?,a named tuple,2172
16996,What returns the cumulative maximum of elements of inputin the dimension dim?,named tuple,5518
16997,What returns the cumulative minimum of elements of inputin the dimension dim?,named tuple,5518
16998,"What is returned when a named tuple(values,indices) returns the cumulative product of elements of inputin the dimension dim",cumulative sum of elements of inputin the dimension dim,5518
16999,"If input is  a vector (1-D tensor), then returns what?",a 2-D square tensor,5518
17000,What is returned in dimension dimof inputandother?,the cross product of vectors,5518
17001,"If input is  a vector, then returns a 2-D square tensor Creates a tensor whose diagonals",1-D tensor,5518
17002,"What is returned when a named tuple(values,indices) returns the cumulative minimum of elements of inputin the dimension dim",cumulative product of elements of inputin the dimension dim,5296
17003,What does Returns the cumulative product of elements of inputin the dimension dim?,the cumulative sum of elements of inputin the dimension dim,1638
17004,What performs a batch matrix-matrix product of matrices stored in input andmat2?,batch matrix-matrix product of matrices inbatch1andbatch2.,4795
17005,What does Alias of torch.outer compute the dot product for?,1D tensors,1707
17006,What does Alias of torch.outer() compute the dot product for?,1D tensors,1718
17007,What does Alias for torch.linalg.det() calculate of a square matrix or batches of square matrices,log determinant,1718
17008,What does Alias for torch.linalg.det() call?,Alias for torch.linalg.inv(),5962
17009,What problems does Alias for torch.linalg.slogdet() solve?,least squares and least norm problems,1062
17010,What exposes various helper functions for the__torch_function__protocol?,module,7747
17011,SeeExtending torch for what information about the__torch_function__protocol?,more detail,7747
17012,What is a tuple of functions in the torch API that cannot be overridden by__torch_function__?,publicly available,7747
17013,What does this module expose for the__torch_function__protocol?,various helper functions,7747
17014,What is the name of the module that exposes helper functions for the__torch_function__protocol?,SeeExtending torch,7747
17015,What is publicly available in the torch API but cannot be overridden with__torch_function__?,tuple of functions,7747
17016,What can't be overridden by __torch_function__?,public functions,7747
17017,What cannot be overridden by__torch_function__?,public functions,5183
17018,Why can't a tuple of functions be overridden with torch_function__?,none of the arguments of these functions are tensors or tensor-likes,5183
17019,"What does Dict[Any, List[Callable]] return containing dummy overrides for all overridable functions?",dict,5183
17020,What are functions that are via __torch_function__?,overridable,5183
17021,What is the number of functions that are publicly available in the torch API but cannot be overridden by__torch_function__?,tuple,5183
17022,Why can't a tuple of functions be overridden with __torch_function__?,none of the arguments of these functions are tensors or tensor-likes,5184
17023,Where can a tuple of functions be found?,torch API,5184
17024,Implement a function with what?,checks for__torch_function__overrides,5184
17025,What was the function exposed by the public torch API originally called?,"likepublic_api(*args,**kwargs)on",5184
17026,What is considered non-dispatchable?,exactTensors andParameters,5184
17027,Why are public functions that are publicly available in the torch API but cannot be overridden by__torch_function__?,none of the arguments of these functions are tensors or tensor-likes,5184
17028,What does object :raise if no implementation is found?,TypeError,5184
17029,args(tuple) – what?,Arbitrary positional arguments originally passed intopublic_api,5184
17030,:param what : Iterable or aguments to check for aguments to check for?,relevant_args,5184
17031,"Dict[Any, List[Callable]] Return a dict containing what?",dummy overrides,5184
17032,What is a set of functions that are overridable via __torch_function__?,Set[Callable] Examples List functions that are overridable via __torch_function__,7746
17033,Why are functions that are publicly available in the torch API but cannot be overridden by__torch_function__?,none of the arguments of these functions are tensors or tensor-likes,7746
17034,What are overridable via __torch_function__?,Set[Callable] Examples List functions,7746
17035,Return what that cannot be overridden by__torch_function__?,public functions,7748
17036,object :raises what if no implementation is found?,TypeError,7748
17037,How many functions are publicly available in the torch API but cannot be overridden with__torch_function__?,tuple,7748
17038,This module exposes what for the__torch_function__protocol?,various helper functions,7748
17039,For more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__tor,SeeExtending torch,7748
17040,Arbitrary keyword arguments originally passed intopublic_api. what (tuple) – Arbitrary positional arguments originally passed intopublic_api?,kwargs,7748
17041,Check for what in the elements of the elements of the elements of the elements of the elements of the elements of the elements of the elements of the elements of,__torch_function__ implementations,7748
17042,How many of the arguments of torch functions are tensors or tensor-likes?,none,871
17043,List functions that are what via __torch_function__ A dictionary that maps namespaces that contain overridable functions to functions,overridable,4124
17044,"Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueifsTrueif",bool,4124
17045,A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be what?,overridden,788
17046,A dictionary that maps namespaces that contain what to functions in that namespace that can be overridden?,overridable functions,788
17047,What are lambda functions useful for for a type that defines__torch_function__?,testing API coverage,788
17048,Arbitrary positional arguments originally passed intopublic_api?,args(tuple),788
17049,"Dict[Any, List[Callable]] Return a dict containing what for all overridable functions?",dummy overrides,5835
17050,"What does the Dict[Any, List[Callable]] return a dict containing for all overridable functions?",dummy overrides,4122
17051,What are lambda functions useful for testing for a type that defines__torch_function__?,API coverage,791
17052,A dictionary that maps overridable functions to lambda functions that have the same signature as the real function and unconditionally return -1?,PyTorch API,791
17053,What is relevant_args?,iterable,791
17054,What is a dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real,Example,791
17055,A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function unconditionally,-1,5180
17056,What does a dict contain for all overridable functions?,dummy overrides,5180
17057,"What is a function exposed by the public torch API called likepublic_api(*args,**kwargs)on which arguments are",public_api(function),5180
17058,What are lambda functions useful for?,testing API coverage,5180
17059,Arbitrary positional arguments originally passed intopublic_api. kwargs (tuple) – Arbitrary keyword arguments originally passed into,args,5180
17060,A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditional,-1,2133
17061,Return a dict containing what for all overridable functions?,dummy overrides,5179
17062,What implement a function with checks for__torch_function__overrides?,"Dict[Callable, Callable] Examples",5179
17063,What implements a function with checks for__torch_function__overrides?,"Dict[Callable, Callable] Examples",790
17064,What language implements torch::autograd::handle_torch_function?,C++,790
17065,A dictionary that maps what in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return,overridable functions,790
17066,What is the equivalent of this function in the C++ implementation?,handle_torch_function,5790
17067,"What does Checks if something is a Tensor-like, including an exactTensor?",bool,5790
17068,"What – Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are",public_api(function),5790
17069,Example Check for what?,__torch_function__ implementations in the elements of an iterable,5790
17070,"If no implementation is found, what happens to TypeError?",if no implementation is found,5790
17071,"If properties, their__get__method must be passed in, what may be needed?",may be needed,5790
17072,What is an example of a function that implements checks for__torch_function__overrides?,Dict,2137
17073,In what language is the equivalent of the torch function implemented?,C++,2339
17074,What is the name of the iterable of arguments to check for __torch_function__ methods?,iterable,2134
17075,What example implements a function with checks for__torch_function__overrides?,Dict,2136
17076,What is the name of the function exposed by the public torch API?,public_api(function),10518
17077,What is true if any of the elements of relevant_args have __torch_function__ implementations?,True if any of the elements of relevant_args have __torch_function__ implementations,10518
17078,What is checked in the elements of an iterable?,__torch_function__ implementations in the elements of an iterable,10518
17079,"Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method,",kwargs(tuple),10518
17080,Result from what?,callingimplementationor an__torch_function__method,10518
17081,What is iterable of arguments to check for __torch_function__ methods?,relevant_args(iterable),10518
17082,"If no implementation is found, object :raises what?",TypeError,10518
17083,Relevant_args: Iterable or aguments to check for what?,__torch_function__ methods,10518
17084,What sometimes don’t contain a__module___?,Methods/properties,10518
17085,What is the iterable of arguments to check for __torch_function__ methods?,relevant_args(iterable),3671
17086,What language implements a function with checks for__torch_function__overrides?,C++,3671
17087,What is args(tuple)?,Arbitrary positional arguments originally passed intopublic_api,3671
17088,What is the name of the Arbitrary keyword arguments originally passed intopublic_api?,kwargs(tuple),3671
17089,What does object raise if no implementation is found?,TypeError,3671
17090,What does a function implement?,checks for__torch_function__overrides,2338
17091,What language implements the equivalent of the torch function?,C++,5789
17092,In what language is the equivalent of handle_torch_function found?,C++ implementation,5789
17093,Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) – Arbitrary keyword arguments originally passed into,args,8894
17094,When does this occur?,whenever there’s a__torch_function__attribute on the type of the input,8894
17095,Returns what if the function passed in is a handler for a method or property belonging totorch.Tensor?,True,8894
17096,A subclass of tensor is generally a what?,Tensor-like,8894
17097,What sometimes don’t contain a__module__slot?,Methods/properties,8894
17098,What must the first passed-in argument be?,instance of torch.Tensor,8894
17099,What does Wrap a given function with?,Wraps a given function with__torch_function__-related functionality,8894
17100,Arbitrary keyword arguments originally passed intopublic_api. args(tuple) – Arbitrary positional arguments originally passed intopublic_,kwargs,8894
17101,What are non-dispatchable?,exactTensors andParameters,8894
17102,What does the example check for?,__torch_function__ implementations in the elements of an iterable,8894
17103,:param relevant_args: Iterable or aguments to check for what?,__torch_function__ methods,8894
17104,What ReturnsTrueif the passed-in input is a Tensor-like?,ReturnsTrueif the passed-in input is a Tensor-like,8894
17105,Built-in or user types can be made Tensor-like by what?,implementing __torch_function__,8894
17106,"What type of argument is used to check if something is a Tensor-like, including an exactTensor?",bool,8894
17107,What aren’t usually Tensor-like?,Built-in or user types,8894
17108,What does a kwargs(tuple) result from?,callingimplementationor an__torch_function__method,8894
17109,When does TypeError occur?,if no implementation is found,8894
17110,:type relevant_args: iterable True <sep> if any of the elements of relevant_args have __torch,if any of the elements of relevant_args have __torch_function__ implementations,8894
17111,"For properties, what must be passed in?",their__get__method must be passed in,8894
17112,What is a callable that returns an iterable of Tensor-likes?,dispatcher(Callable),8894
17113,What is the acronym for Arbitrary keyword arguments originally passed intopublic_api?,kwargs,10517
17114,"What is the function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are",public_api(function),10517
17115,What are Arbitrary positional arguments originally passed intopublic_api?,args,10517
17116,"What check if something is a Tensor-like, including an exactTensor?",bool,10517
17117,Where does the result come from?,callingimplementationor an__torch_function__method,8892
17118,What is the name of the object that raises TypeError if no implementation is found?,Example,8892
17119,What is the name of the arbitrary positional arguments originally passed intopublic_api?,args,8892
17120,What is the name of the object used by the torch API?,object,10516
17121,What is the function exposed by the public torch API called?,public_api(function),10516
17122,What is a tuple of arbitrary positional arguments originally passed intopublic_api?,args,10516
17123,Result from callingimplementationor?,an__torch_function__method,10516
17124,What type of object does an__torch_function__method return?,object,10516
17125,kwargs(tuple) – Arbitrary keyword arguments originally passed into what?,public_api,9764
17126,:type relevant_args: What type of args is used to check for __torch_function__ methods?,iterable,9764
17127,Result from callingimplementationor what?,an__torch_function__method,5176
17128,Param relevant_args: Iterable or what to check for __torch_function__ methods?,aguments,5176
17129,:type relevant_args: iterable True what?,if any of the elements of relevant_args have __torch_function__ implementations,5176
17130,What do you check in the elements of an iterable?,__torch_function__ implementations,10570
17131,What happens if any of the elements of relevant_args have __torch_function__ implementations?,if any of the elements of relevant_args have __torch_function__ implementations,10570
17132,What type of arguments are used to check for a__torch_function__attribute on the type of the input?,Examples,10570
17133,What raises if no implementation is found?,TypeError,187
17134,:param relevant_args: Iterable or what to check for __torch_function__ methods?,aguments,187
17135,Check for what in the elements of an iterable?,__torch_function__ implementations,1563
17136,What is used to check for __torch_function__ methods?,aguments,1563
17137,"Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in",bool,1563
17138,:type relevant_args: iterable True what if any of the elements of relevant_args have __torch,if any of the elements of relevant_args have __torch_function__ implementations,1563
17139,For what reasons is a property's__get__method required?,following reasons,1563
17140,:type relevant_args: iterable True: if any of the elements of relevant_args have __torch,if any of the elements of relevant_args have __torch_function__ implementations,188
17141,What type of methods does iterable or aguments check for?,__torch_function__,188
17142,What is TypeError?,if no implementation is found,188
17143,What returns true if the passed-in input is a Tensor-like?,ReturnsTrueif the passed-in input is a Tensor-like,1584
17144,What does an exactTensor do?,Checks if something is a Tensor-like,1584
17145,What is generally a Tensor-like?,A subclass of tensor,1584
17146,"bool Checks if something is a Tensor-like, including what?",exactTensor,8979
17147,When does ReturnsTrueif the passed-in input is a Tensor-like?,whenever there’s a__torch_function__attribute on the type of the input,8003
17148,A subclass of tensor is generally what?,Tensor-like,8003
17149,What aren't usually Tensor-like?,Built-in or user types,8003
17150,How can built-in or user types be made Tensor-like?,implementing __torch_function__,8003
17151,What is the function passed in for a method or property belonging totorch.Tensor?,a handler,8003
17152,What must be passed in for properties?,their__get__method,8003
17153,What sometimes don't contain a__module__slot?,Methods/properties,8003
17154,What does the first passed-in argument need to be?,instance of torch.Tensor,8003
17155,"If the function passed in is a handler for a method or property belonging totorch.Tensor, as passed into_",True,8003
17156,What does bool check if something is a Tensor-like?,"Checks if something is a Tensor-like, including an exactTensor",8003
17157,What Returns True if the passed-in input is a Tensor-like?,ReturnsTrueif the passed-in input is a Tensor-like,8003
17158,Why is a__get__method passed in?,Methods/properties sometimes don’t contain a__module__slot,8003
17159,"What does bool do if something is a Tensor-like, including an exactTensor?",Checks,8002
17160,When does ReturnsTrueif the passed-in input is a Tensor-like occur?,whenever there’s a__torch_function__attribute on the type of the input,1583
17161,Built-in or user types can be made Tensor-like by implementing what?,__torch_function__,1583
17162,What does bool do?,Checks if something is a Tensor-like,8980
17163,Why is a__get__method needed?,Methods/properties sometimes don’t contain a__module__slot,8980
17164,What is another name for if something is a Tensor-like?,Checks,5765
17165,What checks if something is a Tensor-like?,Checks,5765
17166,What does ReturnsTrue if the passed-in input is a Tensor-like?,ReturnsTrueif the passed-in input is a Tensor-like,789
17167,Relevant_args: Iterable or what to check for __torch_function__ methods?,aguments,789
17168,A dictionary that maps namespaces that contain what?,overridable functions,789
17169,A dictionary that maps overridable functions in the PyTorch API to lambda functions unconditionally return what?,-1,789
17170,What is the current state of a Tensor-like?,Currently,789
17171,When does ReturnsTrue if the passed-in input is a Tensor-like?,whenever there’s a__torch_function__attribute on the type of the input,5697
17172,Returns True if the passed-in input is what?,a Tensor-like,5697
17173,What is the function passed in?,a handler for a method or property belonging totorch,5697
17174,What does the return true if the function passed in is a handler for a method or property belonging totorch.Tens,Note,5697
17175,What subclass is generally a Tensor-like?,tensor,5697
17176,What does Returns True if the function passed in is a handler for a method or property belonging totorch.Tens,Note,5697
17177,What returns true if the function passed in is a handler for a method or property belonging totorch.Tensor?,Returns True,5235
17178,Methods/properties sometimes don't contain a__module__slot. They require that the first passed-in argument,an instance of torch.Tensor,5235
17179,What is an example of a function that wraps a given function with?,Wraps a given function with__torch_function__-related functionality,5235
17180,When does this happen?,whenever there’s a__torch_function__attribute on the type of the input,2057
17181,A subclass of tensor is generally a subclass of what?,Tensor,856
17182,What is a subclass of tensor generally called?,Tensor-like,855
17183,What can built-in or user types be made Tensor-like by implementing?,__torch_function__,1475
17184,What does examples wrap a given function with?,Examples Wraps a given function with__torch_function__-related functionality,1475
17185,What type of property can be made by implementing __torch_function__?,Tensor,1479
17186,What type of argument must be passed in for a method to be considered a Tensor?,Examples,1479
17187,Why is the__get__method passed in?,Methods/properties sometimes don’t contain a__module__slot,1479
17188,What does the first passed-in argument need to be an instance of torch.Tensor?,Examples,1479
17189,What can be made Tensor-like by implementing?,__torch_function__,1480
17190,Examples Wraps what?,a given function with__torch_function__-related functionality,1480
17191,What does it usually suffice to express your code as?,a series of functions,1480
17192,"If you need a function to work for Tensor-likes, then this function is available.",rare situation where this is not the case,1480
17193,"If you’re wrapping a low-level library and you need it to work for Tensor-likes, what is available?",this function is available,1480
17194,Why is the__get__method needed?,Methods/properties sometimes don’t contain a__module__slot,5236
17195,What is a callable that returns an iterable of Tensor-likes passed into the function?,dispatcher(Callable),5236
17196,What does Example Wrap a given function with?,Examples Wraps a given function with__torch_function__-related functionality,5236
17197,What does this decorator do to your code?,reduce the performance of your code,5236
17198,What should you express your code as?,"a series of functions that, themselves, support __torch_function__",5236
17199,What is an example of a situation where a function that supports __torch_function__ is available?,rare situation where this is not the case,5236
17200,What is an example of a situation where this function is not available?,if you’re wrapping a low-level library and you also need it to work for Tensor-likes,5236
17201,What type of function is available if you're wrapping a low-level library and you need it to work for Tensor-likes,Examples,5236
17202,What does a method/property require that the first passed-in argument be?,an instance of torch.Tensor,4244
17203,What wraps a given function with?,Examples Wraps a given function with__torch_function__-related functionality,4244
17204,What must be the first passed-in argument?,an instance of torch.Tensor,7449
17205,What does example wrap a given function with?,Wraps a given function with__torch_function__-related functionality,2341
17206,What does dispatcher(Callable) do?,Wraps a given function with__torch_function__-related functionality,8441
17207,What does dispatcher(Callable) return?,Note,8441
17208,Returns a 3-dimensional view of each input tensor with what dimensions?,zero,5274
17209,"What is the second input tensor out(Tensor,optional)?",output tensor,1700
17210,"Out(Tensor,optional) returns what?",output tensor,5383
17211,"Wheninput is on the CPU, the implementation of torch.sinh may use what library?",Sleef library,5383
17212,What does the Sleef library do?,Seeherefor details,5383
17213,shifts is the number of places by which the elements of the tensor are shifted?,python:ints,9674
17214,What must be a tuple of the same size if shifts is a tuple?,dims,9674
17215,"Ifrepsspecifies fewer dimensions thaninputhas, then ones are what?",prepended torepsuntil all dimensions are specified,3634
17216,"What is an example of an input that is treated as (1, 1, 2, 2) if it has fewer dimensions thanrepsspecifies?",ifinputhas shape,3634
17217,When isinput is treated as if it were unsqueezed at dimension zero until it has as many dimensions asrepsspecifies?,ifinputhas fewer dimensions thanrepsspecifies,3634
17218,"What is treated as if it had the shape (1, 1, 4, 2)?",ifinputhas shape,3634
17219,What is the difference between ifinputhas has fewer dimensions thanrepsspecifies?,Note,3634
17220,"Ifrepsspecifies what thaninputhas, then ones are prepended torepsuntil all dimensions are specified?",fewer dimensions,3634
17221,Ifinputhas shape andrepsis are treated as what?,"(1, 1, 2, 2).",3634
17222,What is another example of a condition whereinput is is treated as if it were unsqueezed at dimension zero until it has as many,ifinputhas fewer dimensions thanrepsspecifies,3634
17223,"Ifinputhas shape (what is the number of dimensions) andrepsis (3, 3, 2, 2), theninput is treated as","4, 2)",3634
17224,What is the difference between ifinputhas fewer dimensions thanrepsspecifies and If input is  treated as if it had,Note,3634
17225,What is the shape of a tensor?,Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive),5862
17226,What is the name of the function that returns the initial seed for generating random numbers as a Python long?,Sets the seed for generating random numbers,5862
17227,Sets the seed for generating random numbers. Returns the random number generator state as a torch.Byte Tensor.,random number generator state,5862
17228,"Ifrepsspecifies how many dimensions thaninputhas, then ones are prepended torepsuntil all dimensions are specified.",fewer,1849
17229,What is returned along a given dimension?,theklargest elements of the giveninput tensor,5688
17230,"What are the indices of the named tuple of(values, indices)?",the indices of the elements in the originalinput tensor,5688
17231,The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted?,input(Tensor),5688
17232,"What returns the k in ""top-k""?",k(int),5688
17233,Which elements of the given input tensor are returned along a given dimension?,theklargest elements,5688
17234,"What is returned, where the indicesare the indices of the elements in the originalinput tensor?","A named tuple of(values, indices)",5688
17235,When is the last dimension of the input chosen?,Ifdimis not given,5689
17236,What returns theksmallest elements of the given input tensor?,IflargestisFalsethen,5689
17237,What happens when the last dimension of the input is chosen?,Ifdimis not given,5689
17238,The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted what?,input(Tensor),5689
17239,"What are the indices in a named tuple of(values, indices)?",the indices of the elements in the originalinput tensor,5689
17240,"What is the k in “top-k” dim(int,optional) – the dimension to sort along largest(bool,",k(int),5689
17241,What will make sure that the returnedkelements are themselves sorted input(Tensor) – the input tensor?,boolean optionsortedIf True,5689
17242,What element of the given input tensor is returned along a given dimension?,theklargest elements,5689
17243,"What is returned, where the indicesare the indices of the elements in the original input tensor?","A named tuple of(values, indices)",5689
17244,"If the last dimension of the input is not given, what is the last dimension of theinput?",Ifdimis,3610
17245,"What are the indices in the named tuple of(values, indices)?",the indices of the elements in the originalinput tensor,3610
17246,"What is the k in ""top-k"" dim(int,optional)?",k(int),3610
17247,"What is not given, the last dimension of the input is chosen?",Ifdimis,3610
17248,"Ifdimis is not given, the last dimension of the inputs chosen.",IflargestisFalsethen theksmallest elements,3610
17249,The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted.,input(Tensor),3610
17250,"What is the k in “top-k” dim(int,optional)?",k(int),3610
17251,What returns theksmallest elements?,IflargestisFalsethen,3627
17252,What controls whether to return largest or smallest elements?,k(int),3627
17253,What are theksmallest elements returned?,IflargestisFalsethen,3627
17254,What is returned when largestisFalsethen theksmallest elements are returned?,"A named tuple of(values, indices)",3627
17255,"What are the indices of a named tuple of(values, indices)?",the indices of the elements in the originalinput tensor,819
17256,What is used in the 2D average-pooling operation?,ink,1208
17257,What type of average-pooling operation does inkHkWkH times kWkHkWregions by step sizes,2D,1191
17258,What type of average-pooling operation does inkTkHkWkT times kH times,3D,1168
17259,What type of operation does inkHkWkH times kWkHkWregions by step sizes?,2D average-pooling,1168
17260,What type of pooling operation does inkHkWkH times kWkHkWregions by step sizes?,2D average-pooling operation,1169
17261,What type of operation does inkTkHkWkT times kH times kWkT,3D average-pooling operation,1172
17262,What does the 3D max pooling over an input signal composed of several input planes compute?,a partial inverse ofMaxPool1d,1172
17263,What does MaxPool2d compute?,partial inverse,1172
17264,What does inkTkHkWkT times kH times kWkTkH,3D average-pooling operation,1173
17265,What is the result of the partial inverse of MaxPool2d?,Computes a partial inverse ofMaxPool2d,1648
17266,What is the function that computes a partial inverse of MaxPool3d?,Computes a partial inverse ofMaxPool2d,1648
17267,What does the 3D max pooling do?,Computes a partial inverse ofMaxPool1d,1229
17268,What is the result of calculating a partial inverse of MaxPool1d?,Computes a partial inverse ofMaxPool1d,1229
17269,What does the function do that computes a partial inverse of?,Computes a partial inverse ofMaxPool2d,1229
17270,What is the result of Computed a partial inverse of MaxPool3d?,Computes a partial inverse ofMaxPool3d,1214
17271,What is the result of Computed a partial inverse of MaxPool1d?,Computes a partial inverse ofMaxPool1d,1644
17272,What is the inverse of MaxPool3d?,Computes a partial inverse ofMaxPool3d,1644
17273,What is another name for MaxPool3d?,Computes a partial inverse ofMaxPool3d,1647
17274,What does the HardTanh function apply element-wise?,In-place version ofrelu(),3829
17275,In-place version ofrelu(). Applies what element-wise function?,HardTanh function,3829
17276,In-place version ofhardtanh(). Applies what function element-wise?,hardswish function,3829
17277,What type of function does the HardTanh function apply?,element-wise,3829
17278,What is the hardswish function called?,In-place version ofelu(),1256
17279,What type of function is applied element-wise?,hardswish,1256
17280,"ELU(x)=max(max(max(0,x) + min(0,(exp(x)","0,x",1256
17281,What is the hardswish function?,element-wise,3827
17282,What type of function does the hardswish function apply?,element-wise,3827
17283,"ELU(x)=max(max(max(0,x)+min(0,(exp(x","0,x",1253
17284,What is the name of the function that applies element-wise?,In-place version ofelu(),1253
17285,"ELU(x)=max(max(x)+min(0,(exp(x)1))","0,x",1244
17286,Where is the version of ofelu() used?,In-place,1244
17287,Where is the version ofelu() used?,In-place,3826
17288,In-place version ofelu(). Applies what?,element-wise,3826
17289,What is the value of CELU?,x,1242
17290,Where is the version ofleaky_relu()?,In-place,1242
17291,What type of transformation is applied to the incoming data?,bilinear,1238
17292,What returns a LongTensor with zeros everywhere except where the index of last dimension matches the corresponding value of the input tens,"tensor of shape(*,num_classes)",840
17293,What is the cosine similarity between?,x1 and x2,5814
17294,What does the function measure Binary Cross Entropy between?,target and output logits,2648
17295,What is a negative log likelihood loss?,Poisson,2648
17296,What does this criterion combine into a single function?,log_softmaxandnll_lossin,2648
17297,What type of negative log likelihood loss is seen in HingeEmbeddingLossfor details?,Gaussian,2648
17298,What loss is the Gaussian negative log likelihood loss?,Connectionist Temporal Classification,2648
17299,What is the criterion that combineslog_softmaxandnll_lossin a single function?,Connectionist Temporal Classification loss,7487
17300,What is the criterion for the Connectionist Temporal Classification loss?,combineslog_softmaxandnll_lossin,7487
17301,What is the MultiLabelSoftMarginLossfor details?,MultiLabelSoftMarginLossfor details,7487
17302,What does MultiLabelSoftMarginLossfor details?,MultiLabelMarginLossfor details,7487
17303,What is the criterion for log_softmaxandnll_lossin?,combineslog_softmaxandnll_lossin a single function,7487
17304,What is the criterion that combines log_softmaxandnll_lossin a single function?,The Connectionist Temporal Classification loss,7487
17305,What is this criterion?,combineslog_softmaxandnll_lossin a single function,2649
17306,What is the term for the loss of the Gaussian negative log likelihood?,Connectionist Temporal Classification loss,2649
17307,What is the name of the function that measures Gaussian negative log likelihood loss?,SeeHingeEmbeddingLoss,2644
17308,What is the criterion that measures Binary Cross Entropy between target and output logits?,combineslog_softmaxandnll_lossin a single function,2644
17309,What is a single function that measures Binary Cross Entropy between target and output logits?,combineslog_softmaxandnll_lossin,2643
17310,What is the loss that measures the element-wise mean squared error?,Connectionist Temporal Classification loss,2643
17311,What does TheKullback-Leibler divergence Loss Function measure?,MarginRankingLoss,7377
17312,What does TheKullback-Leibler divergence Loss Function do?,SeeMarginRankingLossfor details,7377
17313,What is the name of the negative log likelihood loss?,Poisson,4871
17314,What type of negative log likelihood loss is SeeHingeEmbeddingLossfor details?,Gaussian,4871
17315,What is the MultiLabelMarginLossfor details?,MultiLabelMarginLossfor details,4871
17316,What is the name of the loss criterion that combineslog_softmaxandnll_lossin a single function?,Poisson negative log likelihood loss,4871
17317,What is the name of the criterion that combineslog_softmaxandnll_lossin a single function?,SeeCosineEmbeddingLoss,4871
17318,What is the name of the criterion that combines log_softmaxandnll_lossin a single function?,SeeCosineEmbeddingLoss,5794
17319,What is the criterion for combining log_softmaxandnll_lossin?,combineslog_softmaxandnll_lossin a single function,5794
17320,What does TheKullback-Leibler measure?,the element-wise mean squared error,5797
17321,What is theKullback-Leibler divergence Loss Function?,SeeHingeEmbeddingLossfor details,5797
17322,What is another name for MultiLabelMarginLoss?,MultiLabelMarginLossfor details,6918
17323,What is another name for the Gaussian negative log likelihood loss?,SeeHingeEmbeddingLossfor details,6918
17324,What is the name of TheKullback-Leibler divergence Loss Function?,SeeMarginRankingLossfor details,6918
17325,What is the name of the function that measures the mean element-wise absolute value difference?,multi_margin_loss,6918
17326,What loss is Gaussian negative log likelihood loss?,The Connectionist Temporal Classification loss,6918
17327,What is the name of the loss function that takes the mean element-wise absolute value difference?,SeeHingeEmbeddingLoss,6918
17328,What is the name of the loss function that measures the mean element-wise mean squared error?,MarginRankingLoss,6918
17329,What is the term for a negative log likelihood loss?,Gaussian negative log likelihood loss,2678
17330,What is the name of the negative log likelihood loss function?,MultiLabelSoftMarginLoss,2678
17331,What is the function that measures the element-wise mean squared error?,Measures the element-wise mean squared error,4236
17332,Measures what?,the element-wise mean squared error,4236
17333,Rearranges elements in what?,a tensor of shape,5107
17334,Down/up samples the input to what?,Upsamples the input to either the givensizeor the givenscale_factor Upsamples the input,5107
17335,"Given an inputand a what, computes theoutputusinginputvalues and pixel locations fromgrid?",flow-fieldgrid,5107
17336,"What does rearranging elements in a tensor of shape(,C,Hr,Wr)(*,",Reverses thePixelShuffleoperation,5107
17337,What is given a batch of in a 2D or 3D flow field?,affine matricestheta,5107
17338,What is thedownscale_factor?,r,5703
17339,What tensor is rearranged by rearranging elements in a tensor of shape?,Pads,5106
17340,How does Pads tensor sample the input?,Down/up,4721
17341,What does the flow-fieldgrid do?,computes theoutputusinginputvalues and pixel locations fromgrid,4721
17342,What is the name of the tensor that down/up samples the input to?,Pads tensor,4721
17343,What dot product version of this function does not support anoutparameter?,1-dimensional,4482
17344,What is the first tensor to be multiplied other(Tensor)?,input(Tensor),4482
17345,Inputs are valid for broadcasting even though what are different?,final two dimensions,4482
17346,What does inputalong the givendim do?,Counts the number of non-zero values in the tensor,1937
17347,Intortuple of what is optional?,python:ints,1937
17348,What cannot handle huge sparse matrices?,thattorch.linalg.svd(),4435
17349,Why should you use the full-rank SVD implementationtorch.linalg.svd() for dense matrices?,10-fold higher performance characteristics,4384
17350,What can the low-rank SVD be useful for large sparse matrices?,thattorch.linalg.svd(),3731
17351,"A (Tensor) is the input tensor of size(,m,n)(*, m, n",q,3731
17352,Why should you use full-rank SVD for dense matrices?,10-fold higher performance characteristics,3731
17353,"What is the input tensor of size(,m,n)(*,m,n)(*,m,n)",A (Tensor),7115
17354,What is the input of A (Tensor)?,tensor,7115
17355,Why should you use full-rank SVD implementationtorch.linalg.svd() for dense matrices?,10-fold higher performance characteristics,7907
17356,"What is the tensor of size(,m,n)(*, m, n)(,m,n",q,7907
17357,What type of matrices can low-rank SVD be useful for?,huge sparse matrices,7127
17358,How much higher performance characteristics do full-rank SVD implementations of dense matrices have?,10-fold,5202
17359,What is a slightly overestimated rank of A. conduct?,q,760
17360,What is defined by expanding theiiithinput over dimensions defined by other inputs?,theiiithgrid,6185
17361,What is tensors(list of Tensor)?,list of scalars or 1 dimensional tensors,6185
17362,"What will be treated as tensors of size(1,)(1,)(1,)automatically?",Scalars,6185
17363,What can NNNtensors be?,scalar or 1-dimensional vector,6184
17364,How is theiiithgrid defined?,expanding theiiithinput over dimensions defined by other inputs,6184
17365,What is a torch.Tensoris?,multi-dimensional matrix,8766
17366,What type of torch is float64ortorch?,double torch,11062
17367,What are tensor views?,tensor views,2237
17368,What are some examples of a torch.Tensor attributes?,"thetorch.dtype,torch.device, and torch.layout attributes",2237
17369,What does current implementation of torch.Tensor introduce?,memory overhead,2237
17370,What is another name for a torch.Tensor?,Note,2236
17371,What are some of the attributes of a torch.Tensor?,"thetorch.dtype,torch.device, and torch.layout attributes",2563
17372,"For more information on thetorch.dtype,torch.device, and torch.layout attributes of what",a torch.Tensor,2563
17373,What kind of memory usage could be caused by the current implementation of torch.Tensor?,unexpectedly high,4431
17374,What Returns a new Tensor with dataas the tensor data?,new_tensor,7886
17375,What does the new Tensor return with dataas?,tensor data,6579
17376,Returns a new Tensor with dataas what?,tensor data,5336
17377,What is the name of the function that is used to log in a system?,Alias for torch.special.logit(),1037
17378,What is the name of the function used by Alias for torch.special.logit()?,Alias for torch.special.logit(),1037
17379,What is another name for COO format?,Coordinate format,4962
17380,What are the indices of specified elements collected in COO format?,"inindicestensor of size(ndim,nse)",4962
17381,What are the corresponding values collected invaluestensor of size(nse)and with?,arbitrary integer or floating point number element type,10862
17382,What is added to the memory consumption of a sparse COO tensor from storing other tensor data?,a constant overhead,10862
17383,The corresponding values are collected invaluestensor of what?,"size(nse,)",10847
17384,What does storing other tensor data add to the memory consumption of a sparse COO tensor?,a constant overhead,11385
17385,What does ndimis indicate about a tensor?,dimensionality,11384
17386,What type of tensor is at least(ndim*8+sizeofelementtypeinbytes>)*nse,a sparse COO tensor,11384
17387,What is the dimensionality of the tensor andnseis?,number of specified elements,11384
17388,What tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?,strided tensor,4408
17389,What type of tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes,strided,7175
17390,How many non-zero 32-bit floating point numbers does a 10 000 x 10 000 tensor have?,100 000,2532
17391,What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers?,at least(2*8+4)*100000=2000000bytes,2532
17392,What is the inputi of a sparse tensor?,inputiis NOT a list of index tuples,6150
17393,PyTorch hybrid COO tensor extends the sparse COO tensor by allowing thevaluestensor to,multi-dimensional tensor,6150
17394,What is the inputiis NOT a list of?,index tuples,849
17395,What is the inputi?,inputiis NOT a list of index tuples,4486
17396,The inputiis NOT a list of what?,index tuples,4487
17397,What extends the sparse COO tensor by allowing thevaluestensor to be a multi-dimensional tensor,PyTorch,4959
17398,What element does PyTorch hybrid COO tensor have?,typetorch.int64,5029
17399,The indices of specified elements are collected inindicestensor of what?,size,10863
17400,What are the tensor values collected with?,arbitrary integer or floating point number element type,10863
17401,"The corresponding tensor values are collected invaluestensor of size(sparse_dims,nse)and","nse,dense_dims",10863
17402,What are the indices of specified elements collected?,inindicestensor,10863
17403,What are tensor values collected invaluestensor of?,"size(nse,dense_dims)",10846
17404,What are the corresponding tensor values collected invaluestensor of size?,"nse,dense_dims",10846
17405,What would we write to create a 2 + 1-dimensional tensor?,ifsis a sparse COO tensor,4444
17406,What are the values of a sparse tensor stored as?,strided tensors,4444
17407,What would we write if we want to create a sparse tensor?,ifsis a sparse COO tensor,8296
17408,What is the location of the 2 + 1-dimensional tensor?,"entry [3, 4] at location (0, 2)",8296
17409,"What would we write to create a 2 + 1-dimensional tensor with the entry [3, 4 at location (0, 2), entry [",ifsis a sparse COO tensor,8296
17410,What is the name of the function that stores sparse indices?,s.indices(),6143
17411,What is the COO tensor?,sparse,10864
17412,The indices of specified elements are collected what?,inindicestensor,10864
17413,What do we have the following invariants?,ifsis a sparse COO tensor,3716
17414,"PyTorch sparse COO tensor format permitsuncoalescedsparse tensors, where",duplicate coordinates,3716
17415,What is the meaning of strided tensors in a sparse COO tensor?,Note,3715
17416,What is the name of the function where sparse indices are stored explicitly?,s.indices(),4165
17417,What will the coalescing process accumulate into a single value using summation?,multi-valued elements,4165
17418,What are the values of a hybrid tensor?,K-dimensional tensors,8297
17419,What format permitsuncoalescedsparse tensors?,PyTorch sparse COO tensor,8297
17420,What is the definition of a dimensional tensor?,(2 + 1),8297
17421,s.values().layout==torch.strided- values are stored as what?,strided tensors,10637
17422,What are s.values().layout==torch.strided- values stored as?,strided tensors,10634
17423,What does torch.Tensor.is_coalesced()returnsTrue?,Note,10634
17424,Dense dimensions always follow what dimensions?,sparse,10633
17425,What permits uncoalescedsparse tensors?,PyTorch sparse COO tensor format,4369
17426,PyTorch sparse COO tensor format permitsuncoalescedsparse tensors where there,duplicate coordinates,2108
17427,What type of uncoalesced tensor is created when multiple values are specified for the same index1?,1-D,2109
17428,What is the value at an index in a PyTorch sparse COO tensor?,the sum of all duplicate value entries,4987
17429,What do most operations work identically given a coalesced or uncoalesced sparse tensor?,coalesced or not,4987
17430,PyTorch sparse COO tensor format permits what in the indices?,duplicate coordinates,4987
17431,What does torch.Tensor.is_coalesced() return true?,Note,11389
17432,What does torch.is_coalesced()returnsTrue?,Note,11389
17433,Most operations will work identically given what two types of tensors?,coalesced or uncoalesced sparse tensor,11390
17434,Most operations will work identically given what?,coalesced or uncoalesced sparse tensor,3726
17435,Are the indices of specified tensor elements unique or unique?,unique,10867
17436,Which tensor is coalesced or not?,sparse tensor,10867
17437,What can be acquired using methodstorch.Tensor.indices() and usetorch.Tensor.values(),COO format data,10867
17438,What is used to acquire the COO format data of an uncoalesced tensor?,usetorch,10867
17439,Which indices are sorted in lexicographical order?,torch,10856
17440,What returns True?,torch.Tensor.is_coalesced(),10971
17441,How is the addition of sparse COO tensors implemented?,concatenating the indices and values tensors,2549
17442,What dimension can be acquired using methodstorch.Tensor.sparse_dim() and torch.Tens,dense,7209
17443,What type of tensor can be acquired using methodstorch.Tensor.indices() and torch.Tens,sparse COO tensor,2971
17444,What type of COO tensor is a torch.Tensorinstance?,sparse,2971
17445,"For acquiring the COO format data of an what type of tensor, usetorch.Tensor._values",uncoalesced,7208
17446,What can one construct of a sparse COO tensor using thetorch.Tensor.coalesce(),coalesced copy,1833
17447,"In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be what?",zero,5938
17448,What are there operations that may interpret the fill value differently?,operations that may interpret the fill value differently,5938
17449,A CSR sparse tensor consists of three what?,1-D tensors,6913
17450,What consists of compressed row indices?,Thecrow_indicestensor,762
17451,What is the size of a CSR sparse tensor?,sizesize[0]+1,762
17452,What does the tensor encode the index invaluesandcol_indicesdepending on?,where the given row starts,762
17453,What is the CSR sparse tensor?,1-D tensor of sizesize[0]+1,763
17454,What consists of three 1-D tensors?,CSR sparse tensor,763
17455,What should the index tensorscrow_indicesandcol_indices have element type?,eithertorch.int64(default) ortorch.int32,763
17456,Thecrow_indicestensor is a 1-D tensor of what size?,sizesize[0]+1,763
17457,What do you want to use if you want to usetorch.int32?,MKL-enabled matrix operations,763
17458,What does the index invaluesandcol_indices depend on?,where the given row starts,7384
17459,What does Thecrow_indicestensor consist of?,compressed row indices,7384
17460,What is Thecrow_indicestensor?,1-D tensor of sizesize[0]+1,7384
17461,What is the last element of Thecrow_indicestensor?,the number of non-zeros,7384
17462,Thecrow_indicestensor encodes the index invaluesandcol_indicesdepending on what?,where the given row starts,7384
17463,What does thecol_indicestensor contain?,column indices,7384
17464,Thecrow_indicestensor consists of what?,compressed row indices,7384
17465,"If you want to use what -enabled matrix operations, usetorch.int32.",MKL,7384
17466,What does Thecol_indicestensor contain?,column indices of each value,6915
17467,What is Thecol_indicestensor?,1-D tensor of sizennz,6915
17468,What is the value of the CSR tensor?,1-D tensor of sizennz,6915
17469,What does usetorch.int32 do?,MKL-enabled matrix operations,6915
17470,What is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present?,Thesizeargument,6915
17471,Thecrow_indicestensor consists of compressed row indices. This is what?,1-D tensor of sizesize[0]+1,6915
17472,What does the CSR sparse tensor encode the index invaluesandcol_indicesdepending on?,where the given row starts,6915
17473,Thecol_indicestensor contains the column indices of each value. This is a 1-D tensor of what?,sizennz,6915
17474,Thevaluestensor contains the values of the CSR tensor. This is a what?,1-D tensor of sizennz,6915
17475,What must the user supply?,row and column indices and values tensors separately,6915
17476,What should the index tensorscrow_indicesandcol_indices have?,element type eithertorch.int64(default) ortorch.int32,7447
17477,What is the default linking of pytorch with?,MKL LP64,7447
17478,What is the default linking of pytorch being with MKL LP64?,usetorch.int32,7382
17479,Thecol_indicestensor contains the column indices of each value. This is a what?,1-D tensor,7383
17480,What does the user have to do to construct a sparse CSR matrices?,The user must supply the row and column indices and values tensors separately,7383
17481,"If you want to use matrix operations, usetorch.int32. This is as a result of the default linking of py",MKL,4404
17482,What should be used if you want to use MKL-enabled matrix operations?,usetorch.int32,4404
17483,What is optional and will be deduced from thecrow_indicesandcol_indicesif it is not present?,Thesizeargument,4404
17484,What is currently the only math operation supported on CSR tensors?,thetensor.matmul()method,4404
17485,What is the sparse matrix-vector multiplication?,currently the only math operation supported on CSR tensors,4404
17486,Thevaluestensor contains the values of the CSR tensor of sizennz.,1-D,7446
17487,"If you want to use matrix operations, usetorch.int32.",MKL,7124
17488,What is optional and will be deduced from thecrow_indicesandcol_indices if it is not present?,Thesizeargument,6003
17489,Thesizeargument is optional and will be deduced from what if it is not present?,thecrow_indicesandcol_indices,6003
17490,What can be directly constructed by using thetorch._sparse_csr_tensor()method?,Sparse CSR matrices,6003
17491,What is the only math operation supported on CSR tensors?,thetensor.matmul()method,7295
17492,What does M[layout]denote?,matrix,7097
17493,HereT[layout]denotes what with a given layout?,tensor,7097
17494,What is a scalar?,float or 0-D PyTorch tensor,7097
17495,What type of scalar does fdenote?,float or 0-D PyTorch tensor,7096
17496,What is an example of a PyTorch operation?,Sparse grad?,7096
17497,What is the layout signature torch.mv() no?,M[sparse_coo]@V[strided]->V[strided],7096
17498,What is the layout signature torch.mv()?,no M[sparse_coo]@V[strided]->V[strided],7096
17499,What is the default value for M[sparse_coo]@V[strided]->V[strided,no,10242
17500,What is M[sparse_coo]@V[strided]->V[strided] torch?,no,10242
17501,What is the name of the grad?,Sparse,6005
17502,What is a Sparse grad?,Layout signature,6005
17503,What does mv() no?,M[sparse_coo]@V[strided]->V[strided] torch,4065
17504,What does torch.mv() no M[sparse_coo]@V[strided]->V[stri,Layout signature,4065
17505,What is the value of the M[sparse_coo]@V[strided]->V[strided,no,11103
17506,M[sparse_coo]@V[strided]->V[strided] torch.mv(),no,11103
17507,What does M[sparse_coo]@V[strided]->V[strided] torch do?,M[sparse_coo]@V[strided]->V[strided] torch,4187
17508,What does torch.mv() do?,no,11106
17509,What is the value of torch.mv()?,no,11106
17510,What is the name of the command that does not exist?,M[sparse_csr]@V[strided]->V[strided] torch,10246
17511,What is the name of the torch that is not sparse_csr?,M[sparse_csr]@V[strided]->V[strided] torch,10246
17512,What is the name of the command?,M[sparse_coo]@M[strided]->M[strided] torch,4183
17513,What does matmul() no?,M[sparse_csr]@M[strided]->M[strided] torch,4183
17514,What does M[sparse_csr]@V[strided]->V[strided] torch do,M[sparse_csr]@V[strided]->V[strided] torch,4192
17515,What does matmul() not do?,M[sparse_csr]@M[strided]->M[strided] torch,11093
17516,What is the name of the M[sparse_coo]@M[strided]->M[strided,M[sparse_coo]@M[strided]->M[strided] torch,10240
17517,What is the default value of M[sparse_csr]@M[strided]->M[stride,no,11095
17518,Is M[sparse_csr]@M[strided]->M[strided] torch?,no,10245
17519,What does M[sparse_csr]@M[strided]->M[strided] torch?,M[sparse_csr]@M[strided]->M[strided] torch,10245
17520,Where is M[sparse_csr] located?,M[strided]->M[strided] torch,4191
17521,What does M[sparse_csr]@M[strided]->M[strided] torch do,M[sparse_csr]@M[strided]->M[strided] torch,4191
17522,What does torch.mm() not do?,M[sparse_coo]@M[strided]->M[strided] torch,11098
17523,What is the name of the component that is not present at the start of a mission?,M[sparse_coo],10241
17524,What is the name of the sparse_coo at M[strided]->M[strided] torch,M[sparse_coo]@M[strided]->M[strided] torch,10241
17525,What does M[sparse_coo]@M[strided]->M[strided] torch do?,M[sparse_coo]@M[strided]->M[strided] torch,4185
17526,What does smm() no?,M[sparse_coo]@M[strided]->M[sparse_coo] torch,11224
17527,What is the name of the command that is used to send a message to a target?,M[sparse_coo],11441
17528,What is the name of the component that is at the end of a mission?,M[sparse_coo],4184
17529,What does M[sparse_coo]@M[strided]->M[sparse_coo],no,10238
17530,What does hspmm() no M[sparse_coo]@M[strided]->M[h,M[sparse_coo],4181
17531,What does M[sparse_coo]@M[strided]->M[hybridsparse,no,10237
17532,What is the name of the component that is at the end of a stride?,M[sparse_coo],4180
17533,What does ->M[hybridsparse_coo] torch.bmm() no T[sparse_,M[sparse_coo]@M[strided],4180
17534,What does svd_lowrank() use?,SVD,9327
17535,What does f*M[sparse_coo]+f*(M[sparse_coo]@M,no,10250
17536,What does the “Sparse grad?” column indicate?,if the PyTorch operation supports backward with respect to sparse matrix argument,10250
17537,"All PyTorch operations, excepttorch.smm(), support backward with respect to what argument?",strided matrix,10250
17538,"All PyTorch operations, excepttorch.smm(), support backward with respect to what?",strided matrix arguments,4182
17539,What does the PyTorch operation do to support backward with respect to strided matrix arguments?,Note,4182
17540,"What do all PyTorch operations, excepttorch.smm(), support backward with respect to strided matrix arguments?",Note,10239
17541,"What does the ""Sparse grad?"" column indicate?",if the PyTorch operation supports backward with respect to sparse matrix argument,9326
17542,What does the Tensor use?,sparse storage layout,7074
17543,What returns the number of dense dimensions in a sparse tensorself?,Tensor.dense_dim,7074
17544,What returns the number of sparse dimensions in a sparse tensorself?,Tensor.sparse_dim,7074
17545,What _ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,7074
17546,What Returns a coalesced copy ofselfifselfis anuncoalesced tensor?,Tensor.coalesce,7074
17547,What Return the number of dense dimensions in a sparse tensorself?,Tensor.dense_dim,7074
17548,What Return the number of sparse dimensions in a sparse tensorself?,Tensor.sparse_dim,7074
17549,What is returned by the sparse tensormask?,newsparse tensor,7074
17550,What _ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number,Tensor.sparse_resize_and_clear,7074
17551,Tensor.is_coalesced Returns what?,Trueifselfis a sparse COO tensorthat is coalesced,7074
17552,What creates a strided copy ofself?,Tensor.to_dense,7074
17553,What Returns the tensor containing the compressed row indices?,Tensor.crow_indices,7074
17554,The following Tensor methods are related to what tensors?,sparse,7073
17555,What Returns the number of dense dimensions in a sparse tensorself?,Tensor.dense_dim,6331
17556,What does Tensor.sparse_dim return in a sparse tensorself?,sparse dimensions,3895
17557,IsTrueif the Tensor uses sparse storage layout?,False,3895
17558,What does a tensor use to convert a tensor to compressed row storage format?,Tensor.indices,3895
17559,What does Tensor.is_sparse IsTrueif the Tensor uses?,sparse storage layout,6467
17560,What does Tensor.sparse_dim return the number of sparse dimensions in?,a sparse tensorself,6467
17561,IsTrue if the Tensor uses sparse storage layout?,False,3896
17562,IsTrue if the Tensor uses what?,sparse storage layout,3896
17563,What returns the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize_and_clear,3896
17564,What does Tensor.sparse_dim Return in a sparse tensorself?,sparse dimensions,3896
17565,What is the name of the tensor method that resizes itselfsparse tensorto the desired size and the number,Tensor.sparse_resize_and_clear,3896
17566,What return the number of sparse dimensions in a sparse tensorself?,sparse,6330
17567,What tensor with values from a strided tensorselffiltered by the indices of the sparse ten,newsparse tensor,6669
17568,What Returns the number of sparse dimensions in a sparse tensorself?,Tensor.sparse_dim,6669
17569,What does Tensor.is_coalesced return if a sparse COO tensorthat is coalesced?,Trueifselfis,6669
17570,What Returns trueifselfis a sparse COO tensorthat is coalesced?,Tensor.to_dense,6669
17571,Return the number of dense dimensions in what?,a sparse tensorself,5194
17572,Return the number of what in a sparse tensorself?,sparse dimensions,5198
17573,What removes all specified elements from a sparse tensorself?,Tensor.sparse_resize_and_clear,5198
17574,What tensor with values from a strided tensorself filtered by the indices of the sparse,newsparse tensor,6668
17575,Return the number of sparse dimensions in what?,a sparse tensorself,5197
17576,What returns a strided tensorselffiltered by the indices of the sparse tensormask?,newsparse tensor,5410
17577,What returns a sparse COO tensorthat is coalesced?,Trueifselfis,5410
17578,What type of tensor does Tensor.coalesce return?,coalesced copy,6439
17579,What is another name for sparse tensorto?,Tensor.sparse_resize_and_clear,6439
17580,What does Tensor.sparse_resize_ do?,Resizesselfsparse tensorto,6721
17581,What _ Resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,5412
17582,What type of copy of the tensor is returned?,sparse,5412
17583,What does Tensor.sparse_resize do?,Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions,6677
17584,What is the name of the Tensor method that returns the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize_and_clear,1901
17585,What is returned as a coalesced copy of an uncoalesced tensor?,selfifselfis,1901
17586,What is the name of the tensor method that resizes itselfsparse tensorto?,Tensor.sparse_resize_and_clear,6195
17587,What is anuncoalesced tensor?,selfifselfis,5190
17588,What do a sparse COO tensor methods return?,Return the values tensor,5205
17589,What removes all specified elements from a sparse tensorselfand resizes itselfto the desired size and the number of spar,Tensor.sparse_resize_and_clear,6745
17590,What does tensor.sparse_resize_and_clear_ Resizesselfto?,the desired size and the number of sparse and dense dimensions,5292
17591,What type of copy of self is returned if selfis anuncoalesced tensor?,coalesced,5292
17592,What is returned if selfis anuncoalesced tensor?,Trueifselfis a sparse COO tensorthat,5292
17593,What removes all specified elements from a sparse tensorselfand resizesselfto the desired size and number of spars,Tensor.sparse_resize_and_clear,7077
17594,What type of copy of self does Tensor.coalesce return?,coalesced,6298
17595,What returns true if selfis a sparse COO tensorthat is coalesced?,coalesced,6449
17596,What does Tensor.is_coalesced return?,Trueifselfis,6449
17597,What does the tensor return when selfis a sparse CSR tensor of layoutsparse_cs,column indices of theselftensor,6449
17598,What returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR,Tensor.crow_indices,6449
17599,"When selfis a sparse CSR tensor of layoutsparse_csr, what type of tens",a sparse CSR tensor,6449
17600,What resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,6299
17601,What removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of spar,Tensor.sparse_resize_and_clear,5293
17602,What resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?,Tensor.sparse_resize,6676
17603,What method is specific tosparse CSR tensors?,Tensor.crow_indices,6676
17604,What does it do to the desired size and the number of sparse and dense dimensions?,Resizesselfsparse tensorto,5165
17605,What does Tensor.sparse_resize_and_clear_ resizesselfto?,the desired size and the number of sparse and dense dimensions,5165
17606,What is the name of the method that returns true if selfis a sparse COO tensorthat is coalesced?,coalesced,5165
17607,What are the following methods?,specific tosparse CSR tensors,5165
17608,What does tensor.sparse_resize_and_clear_ resize selfto?,the desired size and the number of sparse and dense dimensions,6680
17609,What is returned when selfis a sparse CSR tensor of layoutsparse_csr?,tensor containing the compressed row indices of theselftensor,5674
17610,What indices does theselftensor return when selfis a sparse CSR tensor of layoutsparse,column,5674
17611,What returns the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layouts,the tensor,5674
17612,What does tensor.crow_indices return when selfis a sparse CSR tensor of layoutspar,tensor containing the compressed row indices of theselftensor,6448
17613,What is returned when a COO tensorthat is coalesced?,Trueifselfis,6448
17614,"When selfis a sparse CSR tensor of layoutsparse_csr, what is it?",a sparse CSR tensor,5698
17615,ReturnsTrueifselfis a sparse COO tensorthat is what?,coalesced,5698
17616,What indices of theselftensor is returned when selfis a sparse CSR tensor of layoutspars,column,2028
17617,What does the tensor return whenselfis a sparse CSR tensor of layoutsparse_cs,column indices of theselftensor,2028
17618,Creates what type of self?,strided copy,2028
17619,What does the Tensor.col_indices return whenselfis a sparse CSR tensor of layoutspars,column indices of theselftensor,6317
17620,What does Tensor.crow_indices return when selfis a sparse CSR tensor of layoutspars,the tensor containing the compressed row indices of theselftensor,6317
17621,What type of indices does theselftensor return when selfis a sparse CSR tensor of layoutspar,column,7081
17622,What does Tensor.col_indices return whenselfis a sparse CSR tensor of layoutsparse,tensor containing the column indices of theselftensor,6302
17623,What does the selftensor return when selfis a sparse CSR tensor of layoutsparse_cs,the tensor containing the column indices,5672
17624,In what format is the a sparse tensor constructed?,CSR,1842
17625,What does sparse.sum return?,sum of each row of the sparse Tensor inputin the given dimensionsdim,1842
17626,What does the sparse.addmm function support for sparse matrixmat1?,backward,1842
17627,What does sparse.mm perform of the sparse matrixmat1 and the (sparse or strided) matrixmat2,matrix multiplication,1842
17628,What is the name of the function that supports backward for sparse matrixmat1?,sparse.mm,1841
17629,Where are specified values in the a sparse tensor?,givencrow_indicesandcol_indices,1846
17630,The function sparse.addmm supports which direction for sparse matrixmat1?,backward,1846
17631,What returns the sum of each row of the sparse Tensor inputin the given dimensionsdim?,sparse,10737
17632,What is the return value of the sparse Tensor input?,sum of each row of the sparse Tensor inputin the given dimensionsdim,5668
17633,What do the followingtorchfunctions support?,sparse tensors,7099
17634,What is the equivalent of totorch.full_like?,totorch.full,5485
17635,"Returns what with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)",a tensor,5485
17636,"What is dtype(torch.dtype, optional)?",the desired data type of returned Tensor,5485
17637,What does ifNone do?,defaults to the dtype of input,5485
17638,"Default: ifNone, defaults to what?",the device of input,9342
17639,What is the latest version of STFT?,1.8.0,5889
17640,When will return_complex=Trueas be used?,pytorch release,2620
17641,What cantorch.view_as_real() be used for?,to recover a real tensor with an extra last dimension,2620
17642,What does return_complex=Trueas do in a future pytorch release?,return complex tensors,2620
17643,In what version of Python must return_complex always be given explicitly for real inputs?,1.8.0,2620
17644,What function can be used to recover a real tensor with an extra last dimension for real and imaginary components?,thattorch.view_as_real(),2620
17645,From what version of pytorch did return_complex always be given explicitly for real inputs?,1.8.0,2620
17646,What version of pytorch does return_complex need to be given explicitly for real inputs?,1.8.0,8152
17647,What version of pytorch does return_complex always have to be given explicitly for real inputs?,1.8.0,8152
17648,What can Torch.view_as_real() be used to recover?,real tensor,4510
17649,What does this method compute?,wheremmmis the index of the sliding window,3659
17650,What is the default value for a time sequence?,Ifhop_lengthisNone,3659
17651,Input must be either a 1-D time sequence or a 2-D batch of time sequences?,1-D time sequence or a 2-D batch of time sequences,2621
17652,"In a future pytorch release, this function will only do what?",return complex tensors,2621
17653,What is treated as equal tofloor(n_fft/4)?,Ifhop_lengthisNone,3660
17654,What is treated as equal ton_fft?,Ifwin_lengthisNone,3660
17655,"IfwindowisNone(default), what is it treated as?",if having111everywhere in the window,3660
17656,What is wheremmmis?,the index of the sliding window,2622
17657,"Ifhop_lengthisNone(default), it is treated as equal what?",tofloor(n_fft/4),4512
17658,"Ifwin_lengthisNone(default), it is treated as equal what?",ton_fft,4512
17659,For what components can a real tensor have an extra last dimension?,real and imaginary components,4512
17660,Windowwill be padded on both sides to what before being applied?,lengthn_fft,4512
17661,What is the default setting that allows input to be padded on both sides so that thettt-th frame is centered?,IfcenterisTrue,4512
17662,What type of input must be used?,1-D time sequence or a 2-D batch of time sequences,9681
17663,"Ifhop_lengthisNone(default), it is treated as equal to what?",floor(n_fft/4),6948
17664,"Ifwin_lengthisNone(default), it is treated as what?",equal ton_fft,6948
17665,What is the interface of the STFT modeled after?,thelibrosastft function,6948
17666,What does wheremmmis?,the index of the sliding window,6948
17667,inputmust be either a what?,1-D time sequence or a 2-D batch of time sequences,6948
17668,"Ifwin_lengthn_ffttextwin_lengthn_fft,windowwill be padded on",lengthn_fft,11410
17669,What can a window be of sizewin_length?,1-D tensor,11410
17670,Inputwill be padded on both sides so that thettt-th frame is centered at what?,IfcenterisTrue,11410
17671,What is the default value for a window?,IfwindowisNone,11410
17672,What is the default setting for windows?,reflect,11410
17673,"IfwindowisNone(default), it is treated as if what?",having111everywhere in the window,3652
17674,"Ifwin_lengthisNone(default), windowwill be padded on both sides to what before being applied?",lengthn_fft,3652
17675,Windowcan be a what?,1-D tensor of sizewin_length,3652
17676,Inputwill be padded on both sides so that thettt-th frame is centered at timethop_lengtht,IfcenterisTrue,3652
17677,See torch.nn.functional.pad() for what?,all available options,3652
17678,What is the default setting for the padding method used oninputwhencenterisTrue?,"""reflect""",3652
17679,For what type of input is IfonesidedisTrue default?,real input,3652
17680,What is the window padded on both sides to before being applied?,lengthn_fft,3650
17681,What is the tensor of sizewin_length?,1-D,3650
17682,"Ifwin_lengthisNone(default), what is it treated as?",equal ton_fft,3650
17683,What is the default value for a window to be treated as equal tofloor(n_fft/4)?,Ifhop_lengthisNone,3616
17684,What tensor of sizewin_length can a window be?,1-D,11409
17685,Ifwin_lengthn_ffttextwin_lengthn_fftwin_lengthn_ff,lengthn_fft,11409
17686,What is the default value of a window?,IfwindowisNone,11409
17687,What is treated as if having111everywhere in the window?,IfwindowisNone,11383
17688,Inputmust be either a what?,1-D time sequence or a 2-D batch of time sequences,11383
17689,What is the default value for a sliding window?,Ifhop_lengthisNone,11383
17690,"Ifwin_lengthisNone(default), what will the window be padded on both sides to before being applied?",lengthn_fft,11383
17691,What is the default for input to be padded on both sides?,IfcenterisTrue,11383
17692,What begins at timethop_lengtht times texthop_lengththop_length?,thettt-th frame,11383
17693,Input will be padded on both sides so that thettt-th frame is centered at what?,IfcenterisTrue,3605
17694,What determines the padding method used oninputwhencenterisTrue?,pad_mode,10373
17695,What does IfnormalizedisTrue return?,normalized STFT results,10373
17696,What is the output of IfFalse?,ainput.dim()+2dimensional real tensor,10373
17697,"Ifreturn_complexisTrue(default if input is complex), the return is what?",ainput.dim()+1dimensional complex tensor,10373
17698,What is the name of the padding method used oninputwhencenterisTrue?,Seetorch.nn.functional.pad(),10372
17699,"If true, the output is what?",ainput.dim()+2dimensional real tensor,10372
17700,What is the default value of the padding method used oninputwhencenterisTrue?,reflect,3617
17701,What is the default value that is treated as equal tofloor(n_fft/4)?,Ifhop_lengthisNone,3617
17702,What is the name of the method that determines the padding method used oninputwhencenterisTrue?,Seetorch.nn.functional.pad(),10371
17703,What is the default padding method used oninputwhencenterisTrue?,reflect,10371
17704,What makes onesidedoutput not possible?,if the input or window tensors are complex,3631
17705,What is the return value of Ifreturn_complexisTrue?,ainput.dim()+1dimensional complex tensor,3629
17706,What is the default if input is complex?,Ifreturn_complexisTrue,3629
17707,What returns the normalized STFT results?,IfnormalizedisTrue,3629
17708,What is the optional batch size of input?,the number of frequencies where STFT is applied,3629
17709,What is the input tensor n_fft(int) – size of Fourier transform?,input(Tensor),3629
17710,What function returns the normalized STFT results?,IfnormalizedisTrue,3630
17711,What controls the padding method used whencenterisTrue?,pad_mode,3630
17712,What is the output of ainput.dim()+2dimensional real tensor?,ainput.dim()+2dimensional real tensor,3630
17713,input(Tensor) – the input tensor n_fft(int) – the input ten,Fourier transform,3630
17714,What is the option to padinputon both sides so that thettt-th frame is centered at timethop_lengtht,"center(bool,optional)",3630
17715,IfnormalizedisTrue(default isFalse) returns the normalized STFT results?,Default,3630
17716,Ifreturn_complexisTrue(default if input is complex) the return is what?,ainput.dim()+1dimensional complex tensor,3628
17717,"If return_complexisTrue, what is the return?",ainput.dim()+1dimensional complex tensor,3635
17718,What returns ainput.dim()+1dimensional complex tensor?,Ifreturn_complexisTrue,3635
17719,What is the default return value for ainput.dim()+2dimensional real tensor?,IfFalse,3635
17720,If return_complexisTrue(default if input is complex) what is the return?,ainput.dim()+1dimensional complex tensor,3635
17721,What does return if return_complexis true?,a complex tensor of size,5505
17722,Returns either a complex tensor of size(NT)(* times N times T,a real tensor of size,5505
17723,What does TTT represent?,the total number of frames,5505
17724,Calling with the previous signature may cause error or return what?,incorrect result,5505
17725,"Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional)",ton_fft,7522
17726,"What is window(Tensor,optional)?",optional window function,11398
17727,What is the default window function of all111s?,Default:None,11398
17728,What is the padding method used?,whencenterisTrue,11398
17729,What is the default to return the normalized STFT results?,Default:False onesided,11398
17730,Default:True for what?,realinputandwindow,11398
17731,"Default:False onesided(bool,optional) – controls whether to return how many results to avoid redundancy for real",half,11398
17732,What is the name of the tensor function that returns a complex tensor?,return_complex,11398
17733,Default:None(treated as window of what number) – the optional window function?,all111s,9959
17734,What is the default window of all111s?,Default:None,9525
17735,What is the default value of the window function?,True,9525
17736,What is used to center thetttt-th frame?,padinputon,9960
17737,What is the default value for window of all111s?,Default:None,9960
17738,"What does false onesided(bool,optional) avoid for real inputs?",redundancy,9960
17739,"Default:True pad_mode(string,optional) – controls the padding method used what?",whencenterisTrue,11397
17740,What is the default name of the padding method used whencenterisTrue?,"""reflect""",11397
17741,What is the default whencenterisTrue?,"Default:""reflect""",11397
17742,What controls the padding method used?,whencenterisTrue,9526
17743,"What is center(bool,optional)?",whether to padinputon both sides,9526
17744,What is the name of the function that returns a complex tensor?,return_complex,9526
17745,Window(Tensor) is what type of window function?,optional,11405
17746,"Default:""reflect"" normalized(bool,optional) – controls whether to return what?",normalized STFT results,9056
17747,What is the default pad_mode?,whencenterisTrue,9056
17748,What is a tensor with an extra last dimension for the real and imaginary components?,return_complex,10370
17749,What is the default value for the return of half of results?,Truefor realinputandwindow,10267
17750,A tensor containing the STFT result with what described above Tensor?,shape,10267
17751,What does false onesided avoid for real inputs?,redundancy,10267
17752,What controls whether to return half of results to avoid redundancy for real inputs?,"Default:Truefor realinputandwindow,Falseotherwise",10267
17753,"What is dim(int,list of python:int,optional)?",the dimension or dimensions to approximate the gradient over,7537
17754,"What does spacing(scalar,list of scalar,list of Tensor,optional) represent?",implicitly or explicitly represents coordinates,7537
17755,"If the number of bins is at leastminlength and If input is  empty, then the result is tensor of sizeminlength",zeros,1935
17756,"Ifnis the value at what position, out[n]+=weights[i]ifweightsis specified elseout[n]+",positioni,1935
17757,"What is the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=",Note,1935
17758,"Ifminlengthis specified, the number of bins is at leastminlengthand what?",If input is  empty,1935
17759,Ifnis the value at what?,"positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",1935
17760,"The number of bins is one larger than the largest value in input unlessinput is empty, in which case the result is a ten",size 1,1936
17761,Ifnis the value at what position?,positioni,1936
17762,What device may produce nondeterministic gradients when given tensors?,CUDA device,1936
17763,"Ifnis the value at what positioni,out[n]+=weights[i]ifweightsis specified elseout[n]","positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",1936
17764,What is a tensor of shapeSize([max(input)+1])?,If input is  non-empty,1936
17765,"If the number of bins is at leastminlength, then the result is a tensor of sizeminlengthfilled with zeros.",Ifminlengthis specified,1936
17766,Input(Tensor) – 1-d int tensor weights(Tensor) – what?,optional,1936
17767,What should the input tensor weights(Tensor) be of?,same size,1936
17768,"What is the optional, minimum number of bins?",minlength(int),1936
17769,"Minlength(int) – optional, minimum number of bins. Should be what?",non-negative,1936
17770,What is the size of the bins?,size 1,7204
17771,"Ifminlengthis specified, the number of bins is at leastminlength and If input is  empty, what is the result?",tensor of sizeminlengthfilled with zeros,7204
17772,Where can you find more information about this operation?,SeeReproducibility,7204
17773,"If the number of bins is at leastminlength and If input is  empty, then the result is a tensor of size",Ifminlengthis specified,7204
17774,This operation may produce what when given tensors on a CUDA device?,nondeterministic gradients,7204
17775,What is the minimum number of bins?,minlength(int),4429
17776,What should minlength(int) be?,non-negative,4429
17777,What is the weight for each value in the input tensor?,optional,4429
17778,Fillsselftensor with elements samples from the normal distribution parameterized what?,bymeanandstd,2430
17779,What is filled with elements samples from the normal distribution parameterized bymeanandstd?,Fillsselftensor,2430
17780,What always copiesdata?,torch.tensor(),11235
17781,What does torch.tensor() use if you want to avoid a copy of data?,NumPyndarray,1853
17782,What type of tensor do you want to avoid a copy of?,NumPyndarray,1853
17783,What does torch.tensor()always copiesdata?,Tensordata,10950
17784,What does torch.tensor use if you want to avoid a copy?,NumPyndarray,8221
17785,What does torch.tensor.as_tensor() use if you want to avoid a copy?,NumPyndarray,11236
17786,What does torch.tensor do if you have a NumPyndarray and want to avoid a copy?,Warning,11236
17787,What does torch.as_tensor() use if you want to avoid a copy?,NumPyndarray,11236
17788,What does torch.tensor.as_tensor() do?,Warning,11236
17789,What do you need to do to avoid a copy of a Tensordata?,"change itsrequires_gradflag, userequires_grad_()ordetach()",11050
17790,ByteTensor is sometimes referred to as what?,binary16,11050
17791,How can a tensor be constructed from a Pythonlistor sequence?,thetorch.tensor(),11050
17792,"If you have a numpy array and want to avoid a copy, what is the best way to avoid a copy?",usetorch.as_tensor(),11050
17793,What does Torch.tensor() construct when data is a tensorx?,leaf variable,8353
17794,What is the equivalent totorch.tensor(x)?,tox.clone().detach(),8353
17795,What are the equivalents of clone() and detach()?,usingclone()anddetach(),8353
17796,"What can be a list, tuple, NumPyndarray, NumPyndarray, and other types",scalar,8353
17797,What infers data type fromdata?,ifNone,9220
17798,What default infers data type fromdata?,ifNone,9220
17799,What is the name of the pinned memory that is allocated to the returned tensor?,pin_memory,10589
17800,What does the second row of the arowbycolmatrix contain?,column coordinates,5564
17801,Returns the indices of the upper triangular part of arowbycolmatrix in what format?,2-by-N Tensor,5564
17802,What is the upper triangular part of arowbycolmatrix defined as?,the elements on and above the diagonal,5564
17803,When running on what platform must row*col be less than259259259 to prevent overflow during calculation?,CUDA,8375
17804,What is the name of the number of rows in the 2-D matrix?,row(int),10624
17805,"What is ifNone,torch.long?",Default,10624
17806,What is the default if not provided?,Default,10624
17807,What is the default value for offset(int) if not provided?,Default,4449
17808,"When running on what platform,row*colmust be less than259259259 to prevent overflow during calculation?",CUDA,4449
17809,"What is layout(torch.layout, optional)?",currently only supporttorch.strided,4449
17810,What is an example of a CUDA tensor type?,Example:,4449
17811,What is the default value of offset(int)?,if not provided,10289
17812,What is the default value of offset(int) if not provided?,Default,10289
17813,What is the layout of a tensor?,currently only supporttorch.strided,9223
17814,What is an example of a tensor layout?,Example:,9223
17815,What has the same sign as the divisorother?,The remainder,7036
17816,What is used for how division by zero is handled?,Seetorch.fmod(),7036
17817,What is the input for the dividend other(TensororScalar)?,Tensor,7036
17818,Input(Tensor) – the dividend other(TensororScalar) – the divisor out(T,the output tensor,6137
17819,What computes the element-wise remainder of division equivalently to the C library functionfmod()?,torch.fmod(),4363
17820,The input(Tensor) – the dividend other(TensororScalar) – the divisor out(T,the output tensor,4363
17821,What is another name forfor torch.div?,Alias for torch.div(),1026
17822,What is the name of the number of rows in a 2-D tensor?,n(int),5267
17823,Returns a 2-D tensor with what else?,zeros,5267
17824,"What is the number of rows m(int,optional) – the number of columns with default beingn out(Tensor",n,5268
17825,What is the default setting for a 2-D tensor with ones on the diagonal and zeros elsewhere?,Default:torch.strided,5268
17826,What does n(int) mean?,"the number of rows m(int,optional)",9953
17827,"What is the number of columns with default beingn out(Tensor,optional) – the output tensor?",m,9866
17828,"What is the default beingn out(Tensor,optional)?",output tensor,9866
17829,What is the name of Alias for torch.special.erfinv?,Alias for torch.special.erfinv(),1035
17830,What is another name forfor torch.special.erfinv()?,Alias for torch.special.erfinv(),1035
17831,What does seetorch.i0() do?,Let I_0 be the zeroth order modified Bessel function of the first kind,4093
17832,"Ifwindow_lengthis what, then the returned window is a single element tensor containing a one?",one,4093
17833,Let I_0 be the modified Bessel function of the first kind?,zeroth order,4093
17834,"What is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?",Callingtorch.kaiser_window,4092
17835,What does theperiodicargument do to produce a periodic window as input to functions liketorch.stft()?,Note,4092
17836,What is intended as a helpful shorthand to produce a periodic window as input to functions liketorch.stft()?,Theperiodicargument,1520
17837,What is equivalent to callingtorch.kaiser_window?,Callingtorch.kaiser_window,1520
17838,"What is equivalent to callingtorch.kaiser_window(L,B,periodic=True)?",callingtorch.kaiser_window,1520
17839,What is window_length(int)?,length of the window,1520
17840,What does window_length(int) represent?,length of the window,1520
17841,What is the term for a dense layout?,Onlytorch.strided,1520
17842,"If the returned window is a single element tensor containing a one, what is the result?",Ifwindow_lengthis one,1519
17843,What is intended as a useful shorthand to produce a periodic window as input to functions liketorch.stft()?,Theperiodicargument,1679
17844,"If False, returns a window suitable for use in filter design. beta(float,optional) – shape parameter for the window.",symmetric,1679
17845,"What is the equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?","callingtorch.kaiser_window(L+1,B,periodic=False)",1679
17846,What is the returned window ifwindow_lengthis one?,a single element tensor containing a one,4380
17847,What does window_length(int) mean?,length of the window,4381
17848,"What does beta(float,optional) provide for the window?",shape parameter,4381
17849,"Ifwindow_lengthis one, then the returned window is what?",a single element tensor containing a one,4381
17850,"Beta(float,optional) – what parameter for the window?",shape parameter,10403
17851,What returns a periodic window suitable for use in spectral analysis?,If True,10403
17852,What is the returned window if window_lengthis one?,a single element tensor containing a one,3655
17853,What is supported for a dense layout?,Onlytorch.strided,8957
17854,"What parameter does beta(float,optional) provide for the window?",shape parameter,8957
17855,What is the default value of the window tensor?,ifNone,8957
17856,What type of tensor does ifNone use?,Default,9222
17857,What is the function that computes the then-th power of a square matrix for an integern?,Computes then-th power of a square matrix for an integern,1808
17858,How does reordering the multiplications work?,multiplies two or more matrices,1808
17859,Computes what power of a square matrix for an integern?,then-th power,1808
17860,How does this function work?,Efficiently multiplies two or more matrices,1808
17861,What is the result of the multiplicative inverse of of torch.tensordot()?,"the solutionXto the systemtorch.tensordot(A, X) = B",1787
17862,Computes the inverse of torch.tensordot()?,multiplicative,1787
17863,Computes the solutionXto what?,systemtorch.tensordot,1714
17864,What type of tensor is constructed in COO(rdinate) format?,a sparse tensor,1844
17865,Constructs a sparse tensor in COO(rdinate) format with what?,specified values at the given indices,5677
17866,"Can be a list, NumPyndarray, scalar, and what other types?","tuple, NumPyndarray, scalar, and other types",7559
17867,What will the indices be cast to internally?,a torch.LongTensor,7559
17868,What is the second dimension of the indices?,the number of non-zero values,7559
17869,indices(array_like) – Initial data for the tensor. Can be what?,"a list, tuple, NumPyndarray, scalar, and other types",7559
17870,Where will the indices be cast?,a torch.LongTensorinternally,7559
17871,What are initial values for the tensor?,values,11327
17872,What are some types of values?,"a list, tuple, NumPyndarray, scalar",11327
17873,"Default: if what, infers data type fromvalues?",None,10698
17874,What infers data type fromvalues?,if None,10698
17875,"Default: if what, infers data type fromvalues. device(torch.device, optional) – the desired",None,9217
17876,"What is the tensor to compute OR with out(Tensor,optional) – the output tensor?",Example,1745
17877,What is the solution to the system of linear equations represented byAX=BAX = BAX=Band?,LU factorization of A,7564
17878,What function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band?,LUcontainsLandUfactors,7564
17879,What does this function return the solution to?,the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A,7564
17880,What does LUcontainsLandUfactors do for LU factorization ofA?,LUcontainsLandUfactors,7564
17881,"What types of inputs does torch.solve(B, A)support?",real-valued and complex-valued inputs,11220
17882,"What should X=torch.solve(B,A)solution be replaced with?",Note Irrespective of the original strides,11220
17883,What is the name of the strides that are used to transpose the returned matrices?,"A.contiguous().transpose(-1, -2)",11220
17884,"To get the LU factorization of the input, what may be used with torch.lu_solve() and torch.",seetorch.lu(),11220
17885,"What does the input matrixBBBof size(,m,k)(*, m, k)(,m,k",zero or more batch dimensions,11220
17886,"What is an input square matrix of size(,m,m)(*, m, m)(,m,m)",A(Tensor),11220
17887,What is LUcontainsLandUfactors for?,LU factorization ofA,4061
17888,What does LUcontainsLandUfactors for?,Warning,4061
17889,What does LUcontainsLandUfactors for LU factorization ofA?,LUcontainsLandUfactors,4062
17890,"What is input matrixBBBof size(,m,k)(*, m, k)(,m,k)",zero or more batch dimensions,4062
17891,"What does torch.solve(B, A)notify when inputs are batches of 2D matrices?",Warning,11219
17892,What does torch.linalg.solve() use to get the LU factorization?,seetorch.lu(),11217
17893,"What should be replaced with X=torch.solve(B,A)?",Note,11217
17894,"What should be replaced with X=torch.solve(B,A).solution?",Note,8451
17895,What is the name of the strides that will be transposed?,"A.contiguous().transpose(-1, -2)",8451
17896,What does input(Tensor) represent?,zero or more batch dimensions,8451
17897,"A(Tensor) – input square matrix of size(,m,m)(*, m, m)(",zero or more batch dimensions,8451
17898,What does the cublasHandle_t pointer to current cuBLAS handle return?,the currently selectedStreamfor a given device,3939
17899,What is returned when the index of a currently selected device is returned?,currently selectedStreamfor a given device,3939
17900,What is the defaultStream for a given device?,defaultStreamfor a given device,1882
17901,Returns the what?,currently selectedStreamfor a given device,1882
17902,Sets what if PyTorch's CUDA state has been initialized?,current device,1882
17903,What does CUDA semantics do?,Checks if peer access between two devices is possible,1508
17904,Returns the currently selectedStreamfor a given device.,defaultStreamfor a given device,5528
17905,What does cublasHandle_t return?,the currently selectedStreamfor a given device,1881
17906,What does cublasHandle_t pointer to return?,defaultStreamfor a given device,1881
17907,What stream does cublasHandle_t return for a given device?,defaultStream,5501
17908,What does the index of a currently selected device return?,currently selectedStreamfor a given device,1581
17909,Returns the index of a currently selected device. Returns the what?,currently selectedStreamfor a given device,5500
17910,Returns the currently selectedStreamfor a given device. Returns what?,defaultStreamfor a given device,5500
17911,What does the currentStream for a given device return?,the currently selectedStreamfor a given device,5534
17912,Returns the index of a currently selected device.,currently selectedStreamfor a given device,5535
17913,What is returned when a device is selected?,Gets the cuda capability of a device,5535
17914,What gencode flags does this library return?,NVCC,5532
17915,Releases what currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-s,all unoccupied cached memory,5139
17916,What type of memory allocator statistics does this return a dictionary of?,CUDA,5139
17917,What does it do to all unoccupied cached memory currently held by the caching allocator?,Releases all unoccupied cached memory currently held by the caching allocator,5140
17918,What does this function do for a process?,Set memory fraction,5140
17919,What is seememory_reserved()?,Deprecated,5603
17920,What is the name of the function that returns the maximum GPU memory occupied by tensors in bytes for a given device?,seemax_memory_reserved(),5603
17921,Set memory fraction for a process. Deprecated; what?,seememory_reserved(),5307
17922,What does seememory_reserved do for a process?,Set memory fraction,5307
17923,What is the name of the function that returns the memory fraction for a process?,seemax_memory_reserved(),5307
17924,What is the name of the function that returns the maximum GPU memory managed by the caching allocator for a given device?,seemax_memory_reserved(),5526
17925,What function provides greater precision than the exponential of the elements minus 1 of input?,exp(x) - 1,10352
17926,What is the function that computes the exponential of the elements minus 1 of input?,base two exponential function of input,2305
17927,input is clamped to what when eps is not None?,eps,2302
17928,"If increasing is true, the order of the columns is reversedx0,x1,...,x(N1),x(N2)",True,2684
17929,What is generated by the output matrix?,Vandermonde matrix,2684
17930,For whom is a Vandermonde matrix named?,Alexandre-Theophile Vandermonde,2684
17931,What type of input tensor is x(Tensor)?,1-D,2685
17932,"What are the powers of the input vectorx(N1),x(N2),...,x0x(N-1)",elementwise,2685
17933,What is x(Tensor)?,1-D input tensor,2685
17934,"If increasing is true, the order of the columns is reversedx0,x1,...,x(N1)x0, x",True,2685
17935,"What does N(int,optional) mean?",Number of columns in the output,2685
17936,What is the square array returned if N is not specified?,N=len(x),2685
17937,What is an example of a Vandermonde matrix?,Tensor,2685
17938,What does a Vandermonde matrix do?,Generates a Vandermonde matrix,2685
17939,What is a Vandermonde matrix named for?,Alexandre-Theophile Vandermonde,2685
17940,What is the name of the order of the powers of the columns in a Vandermonde matrix?,"increasing(bool,optional)",2685
17941,"If increasing is True, what happens to the powers of the columns in a Vandermonde matrix?",the powers increase from left to right,2685
17942,What is the name of the matrix with a geometric progression in each row named for Alexandre-Theophile Vandermonde?,Vandermonde matrix,2685
17943,"If increasing is True, the columns of the Vandermonde matrix are reversed if what?",False,2685
17944,"If increasing is False, the columns arex0,x1,...,x(N1)x0, x1,",True,2685
17945,"If increasing is True, the order of the columns of the output matrix is what?",reversed,6994
17946,What is the name of the matrix with a geometric progression in each row?,Alexandre-Theophile Vandermonde,6994
17947,"What is increasing(bool,optional)?",Order of the powers of the columns,9607
17948,"If increasing is true, the columns arex0,x1,...,x(N-1)x0,x1,...,x(N-",True,9607
17949,What is the name of the matrix where the powers increase from left to right?,Vandermonde matrix,9607
17950,"If increasing is what, the first column isx(N1)x(N-1)x(N1)?",False,9607
17951,"What is N(int,optional)?",Number of columns in the output,4293
17952,What does it do to a mantissa and exponent tensor?,Decomposesinputinto mantissa and exponent tensors,2085
17953,What type of inputs does mantissa support?,float inputs,2085
17954,"What is the input tensor out(tuple,optional)?",output tensors,2085
17955,What does inputinto mantissa and exponent tensors do?,Decomposes,2085
17956,What type of inputs does the mantissa support?,float inputs,2085
17957,What is the name of the function that performs the element-wise division of often s or 1 byte n s or 2?,Warning,4818
17958,What does the element-wise division of often s or 1 byte n s or 2 do?,Warning,4818
17959,"The input,tensor1, andtensor2must bebroadcastable.",shapes,7288
17960,What is the tensor to be added tensor1(Tensor)?,input(Tensor),7288
17961,"For inputs of typeFloatTensororDoubleTensor,valuemust be a what?",real number,4819
17962,What is the element-wise division performed by addcdiv?,often s or 1 byte n s or 2,4819
17963,"What of input,tensor1, andtensor2must bebroadcastable?",shapes,4819
17964,What is the tensor to be added tensor1(Tensor) – the numerator tensor,input(Tensor),4819
17965,What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Futureobjects?,aFuturetype,7787
17966,What does the package expose a set of APIs for?,add callback functions and set results,7787
17967,What is the Futuretype primarily used by?,theDistributed RPC Framework,7787
17968,What is theFuturetype primarily used by?,theDistributed RPC Framework,7788
17969,What wrapper encapsulates an asynchronous execution of a callable?,a torch._C.Future,7788
17970,What does a torch._C.Future expose to add callback functions and set results?,APIs,7788
17971,"What can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",Multiple callbacks,7788
17972,"If thisFutureis already completed, the given callback will be run what?",inline,7788
17973,What encapsulates an asynchronous execution and a set of utility functions to simplify operations onFutureobjects?,aFuturetype,7788
17974,What does the wrapper expose a set of APIs for?,add callback functions and set results,8440
17975,What does a torch._C.Future expose?,APIs,8440
17976,What will be run when theFutureis completed?,Append the given callback function to thisFuture,1160
17977,What cannot be guaranteed when multiple callbacks are added to the sameFuture?,the order in which they will be executed,1161
17978,"The callback must take one argument, which is the reference to what?",thisFuture,3513
17979,"If thisFutureis already completed, the given callback will be run what way?",immediately inline,3513
17980,This method will record events on all the relevant current streams and use them to ensure proper scheduling for all the consumers of what?,thisFuture,3513
17981,What is similar to the non-blocking behavior ofwait()?,non-blocking behavior ofwait(),3513
17982,The callback will be invoked with some dedicated streams set as what?,current,3513
17983,"To enforce a certain order, what can be added to the sameFuture?",chaining:fut.then(cb1).then(cb2),3513
17984,"If theFuture's value contains tensors that reside on GPUs, the callback might be invoked when?",while the async kernels that are populating those tensors haven’t yet finished executing on the device,3513
17985,What will the callback function be run when theFutureis completed?,thisFuture,1159
17986,What does this method behave in the same way asthen()?,GPU tensors,8423
17987,"What does callback(Future) take in one argument, is the reference to this Future?",Note,8423
17988,What is the reference to the Future?,aCallable,9017
17989,"What does aCallable that takes in one argument, is the reference to this Future?",Note,9016
17990,"What is aCallable that takes in one argument, is the reference to this Future?",callback(Future),9018
17991,What does aFuturecannot be marked twice?,aFuturecannot be marked completed twice,9018
17992,What must be carefully taken care of if the callback function throws an error?,callback function throws,4470
17993,What does Future.done() return if it has a result or an exception?,ReturnTrueif thisFutureis done,4470
17994,What if it has a result or an exception?,AFutureis done,4470
17995,"If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that are",tensors,4470
17996,What is done if it has a result or an exception?,AFuture,4471
17997,What does thisFuture return if it has a result or an exception?,ReturnTrueif thisFutureis done,4471
17998,"When calling wait()/value() on thisFuture, the exception set here will be raised what?",inline,4471
17999,"If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that",tensors,4471
18000,"If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven what?",if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device,8424
18001,"If the value contains tensors that reside on GPUs,Future.done() will returnTrueeven what?",if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device,3521
18002,What will mark thisFutureas completed with an error and trigger all attached callbacks?,thisFuture,5822
18003,"When calling wait()/value() on thisFuture, the exception set here will be raised what way?",inline,5822
18004,What does the exception for thisFuture mean?,aFuturecannot be marked completed twice,5822
18005,"To enforce a certain order, what can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",chaining:fut.then(cb1).then(cb2),5822
18006,What is the exception for thisFuture?,aFuturecannot be marked completed twice,10598
18007,"To enforce a certain order, what can be used to add multiple callbacks to the sameFuture?",chaining:fut.then(cb1).then(cb2),10598
18008,What will mark thisFutureas completed and trigger all attached callbacks?,Set the result for thisFuture,5833
18009,Is aFuture able to be marked twice?,aFuturecannot be marked completed twice,5833
18010,What does the callback function do to thisFuture?,Append,5833
18011,"If theFuture's value contains what that reside on GPUs, the callback might be invoked while the async kernels",tensors,5833
18012,"To enforce a certain order in which callbacks will be executed, what can be used?",chaining:fut.then(cb1).then(cb2),5833
18013,What can the callback function use to get the value of thisFuture?,thevalue()method,5833
18014,What does thisFuture not mark completed twice?,aFuturecannot be marked completed twice,5821
18015,How many times can aFuture not be marked completed?,twice,5832
18016,What does thisFuture not do?,aFuturecannot be marked completed twice,5832
18017,What is the result object of thisFuture?,result(object),10600
18018,What takes thisFutureas the only argument?,aCallable,5920
18019,What happens if the callback function throws?,the future returned bythenwill be marked appropriately with the encountered error,5920
18020,What is aCallable that takes thisFutureas the only argument?,callback,9015
18021,A newFutureobject that holds what will be marked as completed when the givencallbackfinishes?,return value,9015
18022,What does aCallable that takes thisFutureas the only argument do?,Note,9015
18023,What argument does callback(Callable) take as the only argument?,thisFuture,9015
18024,A newFutureobject that holds the return value of thecallback will be marked as what when the givencallbackfinishes?,completed,9015
18025,What do callback(Callable) and callback(Callable) take?,Note,9015
18026,When will a newFutureobject that holds the return value of thecallbackand be marked as completed?,when the givencallbackfinishes,827
18027,What do you need to do to mark a newFutureobject as completed when the givencallbackfinishes?,Note,827
18028,What happens if a callback function throws an error?,the future returned bythenwill be marked appropriately,4472
18029,What happens in other cases?,thisFuturemay not yet hold a value and callingvalue()could fail,4473
18030,What is already being taken care of within callbacks?,bythen(),4473
18031,"If the value contains tensors that reside on GPUs, this method willnotperform any additional synchronization. This should be done",separately,4473
18032,"If the value contains what that reside on GPUs, this method willnotperform any additional synchronization?",tensors,4579
18033,Where is the synchronization of callbacks already taken care of?,bythen(),4578
18034,"If the value contains tensors that reside on GPUs, this method will not perform what?",synchronization,4578
18035,What could cause callingvalue() to fail?,thisFuturemay not yet hold a value,3525
18036,What does callback(Callable) take as the only argument?,thisFuture,3525
18037,When will a newFutureobject that holds the return value of thecallback be marked as completed?,when the givencallbackfinishes,3525
18038,What is a method that can be called after a call towait() has been completed?,Obtain the value of an already-completed future,3525
18039,What does the future's value contain that reside on GPUs?,tensors,3525
18040,What are some dedicated streams set as?,current,3525
18041,What is the behavior of wait() similar to?,non-blocking behavior,3525
18042,"If the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernel",if the callback returns a value that contains tensors that reside on a GPU,3525
18043,What happens when callingvalue() fails?,thisFuturemay not yet hold a value and callingvalue()could fail,7733
18044,How is synchronization taken care of within callbacks?,bythen(),7734
18045,Where should this method be called after a call towait() has completed?,inside a callback function passed tothen(),7734
18046,"If the value contains tensors that reside on GPUs, then this method willnot perform any additional synchronization. This should be done",separately,7734
18047,What method throws an error if the function creating the value has thrown an error?,thisvalue()method,7734
18048,What may not yet hold a value and callingvalue()could fail?,thisFuture,7734
18049,How long should thisFuture be blocked?,until the value of thisFutureis ready,7734
18050,What does the value contain that reside on GPUs?,tensors,3518
18051,What holds the value held by?,thisFuture,3518
18052,What does futures(list) return to a list of the passed in Futures?,aFutureobject,3518
18053,"If the function (callback or RPC) creating the value has thrown an error, what will also throw an error?",thiswaitmethod,3518
18054,What does non-blocking mean?,wait()will insert the necessary instructions in the current streams,3518
18055,When is the combinedFuture completed?,when all of the sub-futures are completed,3518
18056,What is the name of the callback for which towait() is already taken care of?,bythen(),3520
18057,What should be done until the value of thisFuture is ready?,Block,3520
18058,What is the name of the method that performs the synchronization of tensors that reside on GPUs?,towait(),3520
18059,When does thisvalue() method block?,until the value of thisFutureis ready,3520
18060,How long until the value of thisFutureis ready?,Block,1447
18061,How long does it take to block the value of thisFuture?,until the value of thisFutureis ready,1447
18062,Collects the providedFutureobjects into what kind of Future?,combined,7354
18063,Returns aFutureobject to a list of the passed in what?,Futures,7354
18064,What does thiswaitmethod do when all of the sub-futures are completed?,Collects the providedFutureobjects into a single combinedFuture,7354
18065,What is a list of Futureobjects?,futures(list),7355
18066,Collects the providedFutureobjects into a single what?,combinedFuture,7355
18067,What does futures(list) return?,a list ofFutureobject,7355
18068,What is futures(list)?,a list ofFutureobjects,9434
18069,What does futures(list) do?,Waits for all provided futures to be complete,9434
18070,The method will throw an error ifwaiton what?,anyFuturethrows,9434
18071,When is a combinedFuture completed?,when all of the sub-futures are completed,1619
18072,"If any futures encounter an error, the method will do what?",exit early,1619
18073,What can the dividend and divisor contain?,both for integer and floating point numbers,7034
18074,What does the dividend and divisor support?,"Supportsbroadcasting to a common shape,type promotion, and integer and float inputs",7034
18075,"When the divisor is what, returnsNaN for floating point dtypes on both CPU and GPU?",zero,7034
18076,What is the dividend other(TensororScalar)?,input(Tensor),7034
18077,What is the dividend other(TensororScalar) – the divisor?,Tensor,7034
18078,What happens when the divisor is zero on CPU?,raisesRuntimeError,6138
18079,"What is the divisor out(Tensor,optional) a part of?",the output tensor,6138
18080,"What does p(float,optional) return?",the norm to be computed,5657
18081,What is considered close If actual and expected are  real-valued and finite?,Asserts,1336
18082,"If actual and expected are  valued, they are considered close if both their real and imaginary components are considered close according to the definition above.",complex,1336
18083,"If actual and expected are  close, they are considered close if and they have the same device(if check_device is True), same",real-valued and finite,1336
18084,"If actual and expected are  close, they are considered close if both their real and imaginary components are considered close according to the definition above.",complex-valued,1336
18085,rtol(Optional[float]) – What is rtol?,Relative tolerance,9292
18086,What does rtol(Optional[float]) do?,specified atol must also be specified,9292
18087,What does check_dtype(bool) assert?,corresponding tensors have the same dtype,9292
18088,What are the inputs if their set of keys do not match?,Mapping’s,9066
18089,"What happens if the inputs are Mapping’s, but their set of keys do not match?",corresponding tensors do not have the same shape,9066
18090,Check_stride(bool) – If True(default) asserts that corresponding tensors have what?,same stride,9066
18091,Can be passed as callable in which case it will be called with the what?,mismatching tensors,9066
18092,What is a UsageError?,a torch.Tensorcan’t be constructed from an array-or-scalar-like,9066
18093,"If any tensor is quantized or sparse, this is a what?",temporary restriction,9066
18094,What does UsageError occur if?,only rtol or atol is specified,9066
18095,"If the inputs are Sequence’s, but their length does not match?","If the inputs are Sequence’s, but their length does not match",9066
18096,"If check_dtype is disabled, tensors with different type’s are what?",promoted to a common dtype,9066
18097,What should you do if the values of corresponding tensors mismatch?,See below for details,9066
18098,"If corresponding tensors are not on the same device, what is the cause of Assertion Error?",If Check_device,9066
18099,What error occurs if corresponding tensors do not have the same dtype?,If Check_dtype,9066
18100,What happens if the values of corresponding tensors have the same stride?,Assertion Error,9066
18101,What is the value of the default rtol and atol?,0.0 0.0,1328
18102,The namespace of diagnostic information that will be passed to msg if its a callable has what attribute?,number_of_elements,1326
18103,What is the namespace of diagnostic information that will be passed to msg if its a callable has the following attributes?,total_mismatches,1326
18104,What is the name for total mismatches divided by number of elements?,mismatch_ratio,1326
18105,What does the torch package return if obj is a PyTorch tensor?,PyTorch storage object,7329
18106,What does Returns the default torch.Tensortype return?,total number of elements in the input tensor,5223
18107,What is the default torch.Tensortype?,current default floating point torch.dtype,2696
18108,What is returned by the default torch.Tensortype to floating point tensor typet?,the total number of elements in the input tensor,5844
18109,What tensor of size end start step left l ceil fractextend - text,1-D,1908
18110,What is the name of the Tensor input that creates a view of an existing torch.Tensor input,"specified size,stride and storage_offset",1945
18111,What is the tensor of size end start step left l ceil fractextend -,1-D,1946
18112,What type of tensor of sizestepswhose values are evenly spaced from start to end inclusive?,one-dimensional tensor,1946
18113,"Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.",Con,1946
18114,What type of tensor does a numpy.ndarray create?,one-dimensional,2031
18115,What happens to the given sequence of seq tensor in the given dimension?,Concatenates,1815
18116,"Splits input, a tensor with three or more dimensions, into what according toindices_or_sections?",multiple tensors depth wise,1815
18117,What function moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination?,Alias for torch.movedim(),1815
18118,What is the name of the function that moves the dimension(s) of in out at the  position(s) in source to the position(s),Alias for torch.movedim(),2671
18119,Gathers values along what specified by dim?,axis,2671
18120,Which version of torch.Tensor.scatter_add_() Splits the tensor into chunks?,Out-of-place version of torch.Tensor.scatter_(),2671
18121,What do tensors do in sequence depth wise?,Stack tensors,6055
18122,"What does Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to?",indices_or_sections,6055
18123,What happens when a tensor is returned with all the dimensions of inputof size1removed?,Concatenates a sequence of tensors along a new dimension,5329
18124,What function returns a tensor with all the dimensions of inputof size1removed?,Alias for torch.transpose(),5329
18125,What does Alias for torch.transpose() call?,Alias for torch.transpose(),1033
18126,What dimensions does Alias for torch.transpose() transpose?,0 and 1,1033
18127,Returns a new tensor with the elements of in out atwhat?,given indices,5346
18128,"Each element sampled from what distribution with rate parameter given by the corresponding element in input i.e., Returns a tens",Poisson,2205
18129,What is returned when a tensor is filled with random integers generated uniformly between low(inclusive) and high(exclusive)?,tensor with the same shape,5490
18130,Returns what with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive,a tensor,5446
18131,Alias for torch.acos(). Returns a new tensor with what cosine of the elements of,inverse hyperbolic,1003
18132,What is the name of the element-wise division performed by Alias for torch.acosh()?,often s or 1 byte n s or 2,1004
18133,Alias for torch.abs() Computes the what cosine of each element in input ?,inverse,1004
18134,What does Alias for torch.acosh() add?,scalar other to each element of the inputinput,1004
18135,What does Alias for torch.abs() do?,Alias for torch.,1004
18136,What is the name of the function that adds the scalar other to each element of the input input?,Alias for torch.acosh(),1014
18137,Computes the bitwise what of inputandother?,OR,1025
18138,Computes the what of inputandother?,bitwise XOR,1025
18139,Clamps all elements in input into what range?,"range[min,max]",1025
18140,"Create a new what with the magnitude of inputand the sign of other, elementwise?",floating-point tensor,1025
18141,What are the elements of inputconverted from?,angles in degrees to radians,1025
18142,What happens to each element of the inputinput by the corresponding element of other?,Divides each element of the inputinputby the corresponding element of other,1025
18143,What is another name for Alias for torch.special.erf()?,Alias for torch.special.erfc(),1025
18144,What element of the elements of input does Alias for torch.asinh() return a new tensor with,arctangent,1025
18145,Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Alias fort,atan(),1025
18146,The arctangent of inputi/otheri is considered with consideration of what?,quadrant,1025
18147,What function returns a new tensor with the exponential of the elements of the input Tensor input?,Alias for torch.special.erfinv(),1025
18148,Returns a new tensor with what of the elements of the input Tensor input?,exponential,1025
18149,What is the name of the function that returns a new tensor with the exponential of the elements of the input Tensor input,Alias for torch.special.exp2(),1025
18150,What is Alias for torch.special.exp2()?,Alia,1025
18151,"What is the name of the function that clamps all elements in input into the range[min,max]?",Alias for torch.clamp(),1735
18152,What computes the element-wise conjugate of the giveninput tensor?,Alias for torch.clamp(),1736
18153,What does Alias for torch.clamp() do?,"Clamps all elements in input into the range[min,max].",1736
18154,What does Alias for torch.asinh() return a new tensor with?,arctangent,1736
18155,"Clamps which elements in input into the range[min,max]?","all elements in input into the range[min,max].",5396
18156,Who returns a new tensor with the inverse hyperbolic tangent of the elements of input?,Alias for torch.atanh(),5396
18157,What function computes the element-wise conjugate of the giveninput tensor?,Alias for torch.clamp(),5396
18158,Who returns a new tensor with the arctangent of the elements of input?,Alias for torch.asinh(),5396
18159,What is the name of the function that returns a new tensor with the exponential of the elements of input?,Alias for torch.special.erfinv(),5396
18160,"Returns what value, ignoringNaN values?",the median of the values in input,5586
18161,The variant of torch.quantile()ignores what ifNaN values in input did not exist?,quantilesqas,5635
18162,What does return a copy of input?,Compute combinations of lengthrrrof the given tensor,5544
18163,"What is returned when a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the",the cumulative product of elements of inputin the dimension dim,5544
18164,What returns the cumulative product of elements of inputin the dimension dim?,Returns the cumulative product of elements of inputin the dimension dim,5915
18165,"What does the named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim",the cumulative product of elements of inputin the dimension dim,2173
18166,Returns what of inputin the dimension dim?,the cumulative product of elements,5916
18167,Returns what of elements of inputin the dimension dim?,cumulative sum,5297
18168,Compute combinations of lengthrrof the given tensor. Returns the cross product of vectors in dimension dimof inputand,Compute combinations of lengthrrrof the given tensor,5297
18169,"Computes what product, denoted by otimes, of inputandother?",Kronecker,5297
18170,What does the batch matrix-matrix product of matrices stored in input andmat2 return?,matrix product of theNNN2-D tensors,4776
18171,Where is the equivalent of the torch::autograd::handle_torch_function?,C++ implementation,872
18172,What is a dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden?,__torch_function__,4123
18173,"What is checked if something is a Tensor-like, including an exactTensor?",bool,2135
18174,"ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there’s a_",whenever there’s a__torch_function__attribute on the type of the input,2135
18175,What is another name for Arbitrary keyword arguments originally passed intopublic_api?,kwargs,8893
18176,Check for what in the elements of an iterable. Considers exactTensors andParameters non-dispatchable.,__torch_function__ implementations,2281
18177,What may be needed to pass in a property's__get__method?,following reasons,2281
18178,"What checks if something is a Tensor-like, including an exactTensor?",Checks,5766
18179,A subclass of what is generally a Tensor-like?,tensor,5766
18180,What does this wrap a given function with?,Wraps a given function with__torch_function__-related functionality,2330
18181,"Ifrepsspecifies how many dimensions than inputhas, then ones are prepended torepsuntil all dimensions are specified.",fewer,1850
18182,"Ifinputhas shape, andrepsis treated as what?","1, 1, 2, 2).",1850
18183,What is an example of a tensor that is treated as if it were unsqueezed at dimension zero until it has as many,ifinputhas fewer dimensions thanrepsspecifies,1850
18184,What is an example of a tensor that has fewer dimensions thanrepsspecifies?,ifinputhas shape,1850
18185,What is the number of repetitions per dimension?,reps,1850
18186,What is an example of a tuple function?,Example,1850
18187,What type of convolution over an input signal composed of several input planes?,1D,1194
18188,What does a 3D max pooling over an input signal comprised of several input planes do?,Computes a partial inverse ofMaxPool2d,1230
18189,"Applies the element-wise functionReLU6(x)=min(max(0,x),6)textReLU6",hardswish function,1257
18190,In-place version ofelu()?,In-place version ofelu(),1257
18191,"What is the value of CELU(max(0,x)+min(0,(exp(x/)",x,1243
18192,Randomized leaky ReLU.,In-place version ofrrelu(),1243
18193,What function applies element-wise?,hard shrinkage,1243
18194,What is in-place version ofrrelu()?,Randomized leaky ReLU,3828
18195,What is the in-place version ofrrelu()?,gated linear unit,3828
18196,What is the gated linear unit?,leaky_relu(),3828
18197,What is a learnable parameter?,weight,3828
18198,Randomized leaky ReLU. In-place version of what?,rrelu(),3828
18199,What does TheKullback-Leibler divergence Loss measure?,the element-wise mean squared error,2650
18200,What is the criterion for combining log_softmaxandnll_lossin a single function?,combineslog_softmaxandnll_lossin a single function,2650
18201,What is the name of the function that measures the Gaussian negative log likelihood loss?,SeeHingeEmbeddingLoss,2650
18202,Function that uses what if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?,squared term,2650
18203,What is the name of the function that uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise,SeeSoftMarginLoss,2650
18204,"If the first argument is 2-dimensional and the second argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched",1,3491
18205,What type of dimensions are broadcastable?,non-matrix,6972
18206,"If both tensors are what, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix",1-dimensional,6972
18207,The matrix-matrix product is returned if the first argument is what dimension?,1-dimensional,6972
18208,"If both arguments are at least 1-dimensional and at least one argument is N-dimensional, what is returned?",batched matrix multiply,6972
18209,What must the non-matrix dimensions be?,broadcastable,3399
18210,a torch.Tensoris is a what?,multi-dimensional matrix,8767
18211,What type of torch does Torch define?,LongTensor Boolean torch,7940
18212,What is the name of the float torch?,float32ortorch,2668
18213,How many bits of floating point torch does float32ortorch.float torch.FloatTensor torch.cuda.,32,149
18214,What is 1 torch?,16-bit floating point,123
18215,What is a torch called?,float16ortorch.half torch,11057
18216,What operator supports Brain Floating Point?,EmbeddingBag operator,155
18217,What is torch.Tensoris an alias for?,default tensor type,145
18218,What is used to avoid a copy of a tensordata?,userequires_grad_()ordetach(),861
18219,What does thattorch.autogradrecords operations do on a tensor?,automatic differentiation,861
18220,What type of storage does the tensor class provide?,"multi-dimensional,stridedview",862
18221,What is used to avoid a copy of a Tensordata?,userequires_grad_()ordetach(),862
18222,What is used to avoid a copy of a numpy array?,usetorch.as_tensor(),862
18223,A tensor of what can be constructed by passing a torch.dtypeand/or a torch.device to a construct,specific data type,862
18224,What are Tensor Views?,tensor views,862
18225,What are some examples of a torch.Tensor Attributes?,"thetorch.dtype,torch.device, and torch.layout attributes",862
18226,What suffix mark methods which mutate a tensor?,underscore,2561
18227,What is the term for a tensor'storch.deviceand/ortorch.dtype?,Warning,2561
18228,What is a tensor'storch.deviceand/ortorch.dtype?,Warning,864
18229,What are there a few main ways to create?,tensor,7393
18230,What is one way to create a tensor with specific size?,*tensor creation ops,7393
18231,What does Tensor.new_full return a Tensor of sizesizefilled?,withfill_value,7393
18232,What is added to a scalar or tensor toselftensor?,Add a scalar or tensor toselftensor,6212
18233,What does Tensor.argmax stand for?,Tensor.argmax,6212
18234,What is the name of the element that adds a scalar or tensor toselftensor?,addbmm,6212
18235,What does Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor,addcdiv,3894
18236,What Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor?,addbmm,5343
18237,What does add do to a scalar or tensor?,Add a scalar or tensor toselftensor,5343
18238,What does Tensor.ndim Alias fordim() Tensor.real Return a new tensor containing?,real values of theselftensor,6560
18239,What is added to a scalar or tensor?,Add a scalar or tensor toselftensor,6616
18240,What Seetorch.addbmm() Tensor?,addbmm,6616
18241,What is the name of the scalar or tensor toselftensor?,Tensor,5289
18242,What does add a scalar or tensor toselftensor do?,Add a scalar or tensor toselftensor,3825
18243,What does Seetorch.addbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.,addbmm,6203
18244,What do you add to a tensor?,scalar or tensor toselftensor,6211
18245,What is replaced by each element in the tensor?,the value returned bycallable,6211
18246,What represent a multi-dimensional array containing elements of a single data type?,PyTorch,4978
18247,What is another name for multi-dimensional arrays where the contiguous memory storage of array elements turns out to be suboptimal?,sparse arrays,4978
18248,What is a sparse array format?,Note,4978
18249,What are stored contiguously in memory?,array elements,4979
18250,What represents a multi-dimensional array containing elements of a single data type?,PyTorch providestorch.Tensorto,4979
18251,What term is used to denote unspecified elements in sparse arrays?,fill value,4979
18252,What is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?,strided tensor,7176
18253,What is the size of a strided tensor?,10 000 x 10 000,7176
18254,What is the inputinot a list of?,index tuples,6149
18255,"Where is the entry [5, 6] in a sparse COO tensor?","(1, 0",1124
18256,What would we do to create a sparse COO tensor?,write,1124
18257,What can be found in the indices of PyTorch sparse COO tensors?,duplicate coordinates,4986
18258,What returns true?,torch.Tensor.is_coalesced(),10972
18259,How can the scalar multiplication on an uncoalesced sparse tensor be implemented?,multiplying all the uncoalesced values with the scalar,8994
18260,What nonlinear operation cannot be implemented by applying the operation to uncoalesced data?,a square root,8389
18261,The CSR sparse tensor encodes the index invaluesandcol_indicesdepending on what?,where the given row starts,6914
18262,"PyTorch, operation Sparse grad?",PyTorch,4971
18263,Which PyTorch operations support backward with respect to sparse matrix argument?,"All PyTorch operations, excepttorch.smm(), support",11099
18264,"All PyTorch operations, excepttorch.smm(), support backward with respect to sparse matrix argument. <sep>","All PyTorch operations, excepttorch.smm(), support backward with respect to",4186
18265,What does PyTorch not support with the layout signatureM[strided]@M[sparse_coo]?,PyTorch does not support matrix multiplication,11070
18266,Applications can still compute this using what?,matrix relationD@S==(S.t()@D.t()).t(),11070
18267,The following methods are specific tosparse what tensors?,CSR,6672
18268,What returns the indices of theselftensor whenselfis a sparse CSR tensor of layoutspars,Tensor.col_indices,6440
18269,Return what tensor of a sparse COO tensor?,indices,5191
18270,What does the tensor containing return when selfis a sparse CSR tensor of layoutsparse_,column indices,5191
18271,What Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse C,Tensor.crow_indices,6746
18272,What is returned when a sparse COO tensorthat is coalesced?,Trueifselfis,7078
18273,What returns a tensor with the same size as inputfilled withfill_value?,a tensor with the same size as inputfilled withfill_value,5486
18274,What cantorch.view_as_real() be used to recover?,real tensor,4511
18275,"In a future pytorch release, this function will only what?",return complex tensors,8153
18276,What expression does the STFT compute?,wheremmmis the index of the sliding window,8153
18277,The input must be either a what?,1-D time sequence or a 2-D batch of time sequences,6947
18278,What is the default value for n_fft/4?,Ifhop_lengthisNone,9682
18279,What is the default value for input when centerisTrue?,reflect,9682
18280,"Ifwin_lengthisNone(default),windowwill be padded on both sides to what before being applied?",lengthn_fft,3651
18281,Ifwin_lengthisNone(default) is treated as what?,equal ton_fft,3651
18282,Default:None(treated as window of what number) – the optional window function.,all111s,5506
18283,What is the input tensor n_fft(int) – size of Fourier transform hop_length(int,input(Tensor),8194
18284,What does reordering the multiplications do?,multiplies two or more matrices,1730
18285,Computes the inverse of a square matrix what?,if it exists,1731
18286,What is returned when a library is compiled for CUDA?,Gets the cuda capability of a device,3940
18287,Returns what stream for a given device?,currently selectedStreamfor a given device,3941
18288,What is returned when a device is compiled?,Gets the cuda capability of a device,5529
18289,Sets the current stream.This is a wrapper API to what?,set the stream,5529
18290,What does the wrapper API do?,Waits for all kernels in all streams on a CUDA device to complete,2704
18291,What does seememory_reserved() do?,Set memory fraction,5309
18292,What is the name of the function that computes the inverse error function of input?,Computes the inverse error function of input,7760
18293,What does Future.done() return if thisFutureis done?,ReturnTrue,9706
18294,What can be used to enforce a certain order?,chaining:fut.then(cb1).then(cb2),10599
18295,The callback will be invoked with dedicated streams set as what?,current,3524
18296,When does block occur?,until the value of thisFutureis ready,1448
18297,What happens if the value contains tensors that reside on GPUs?,wait()will insert the necessary instructions in the current streams,1448
18298,"If the value contains what that reside on GPUs, an additional synchronization is performed with the kernels (executing on the device) which may be",tensors,7353
18299,What does non-blocking do to ensure that further operations enqueued on those streams will be properly scheduled after the async,wait()will insert the necessary instructions in the current streams,7353
18300,What will throw an error if the function (callback or RPC) creating the value has thrown an error?,thiswaitmethod,7353
18301,When may the available functions change?,PyTorch releases,8203
18302,"What does union[bool,str] – If True, two NaN values will be considered equal?",equal_nan,8203
18303,"If ""relaxed"", complex values are considered as NaN  what?",if either the real or imaginary component is NaN,8203
18304,When are complex values considered as NaN if either the real or imaginary component is NaN?,"""relaxed""",3392
18305,What can actual and expected do?,be Tensor’s or any array-or-scalar-like of the same type,3392
18306,What can be Sequence’s or Mapping’s in which case they are considered close if their structure matches and all their elements are,be Sequence’s or Mapping’s,3392
18307,Absolute tolerance must also be specified if what must also be specified?,specified rtol,3392
18308,Which tensors are on the same device?,corresponding tensors,3392
18309,"What is optional[Union[str,Callable[[Tensor,Tensor,Dial,Tens",msg,3392
18310,Returns what if the inputs a single element tensor?,True if the inputs a single element tensor,5211
18311,"Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size. Returns",Returns a tensor,5211
18312,Random sampling creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.,Random sampling creation ops,2150
18313,Returns a what type of tensor of size end start step left l ceil fractextend,1-D tensor,5840
18314,Returns a tensor filled with what value 1?,scalar,5845
18315,What is created of sizestepswhose values are evenly spaced from start to end inclusive?,one-dimensional tensor,5089
18316,What is constructed in COO(rdinate) format with specified values at the given indices?,a sparse tensor,1851
18317,"What type of tensor of sizestepswhose values are evenly spaced from start to end, inclusive?",one-dimensional,1851
18318,"Splits input, a tensor with three or more dimensions, into what?",multiple tensors depth wise,1816
18319,Returns what where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row,a tensor,5853
18320,What returns a random permutation of integers from 0 to n -1?,random permutation of integers from 0 to n -1,5853
18321,Returns what tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive),a tensor,5853
18322,What is returned with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive,a tensor,2206
18323,Each element sampled from a what distribution with rate parameter given by the corresponding element in input ?,Poisson,2206
18324,Where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of ten,a tensor,5460
18325,Returns a tensor with what shape as Tensor input?,same shape,5460
18326,What is returned if a tensor is filled with random integers generated uniformly between low(inclusive) and high(exclusive),a tensor,5447
18327,Computes the what value of each element in input ?,absolute value,1691
18328,Computes the giveninput tensor's what?,element-wise angle,4821
18329,Alias for torch.asinh(). Returns a new tensor with the what of the elements ofin,arctangent,4821
18330,"Clamps which elements in input into the range[min,max].?","all elements in input into the range[min,max].",4821
18331,Alias for torch.clamp(). Computes the what of the giveninput tensor?,element-wise conjugate,4821
18332,"Create a new what tensor with the magnitude of inputand the sign of other, elementwise?",floating-point tensor,4821
18333,Computes the what cosine of each element in input ?,inverse,1769
18334,Computes the inverse cosine of each element in input . Computes the smallest integer greater than or equal to each element.,Alias for torch.clamp(),1769
18335,Who returns a new tensor with the inverse hyperbolic sine of the elements of input?,Alias for torch.asin(),5372
18336,What does Alias for torch.special.erf() do?,Alias for torch.special.erfc(),5372
18337,What type of tensor is created with the magnitude of input?,floating-point tensor,5390
18338,Tests if all elements input evaluate what?,toTrue,5550
18339,Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. Returns the mean,p-norm of (input-other) Returns the log of summed exponentials,5550
18340,"Returns the median of the values in input , doing what?",ignoringNaN values,5550
18341,Computes what quantiles of each row of the input tensor along the dimension dim?,q-th,5550
18342,What is the result of Computes the q-th quantiles of each row of the input tensor along the dimension dim,variant of torch.quantile()that “ignores”NaN values,5550
18343,Compute combinations of lengthrrrof the given tensor. Returns a copy of input. Returns a copy of,Compute combinations of lengthrrrof the given tensor,5277
18344,Returns what wherevaluesis the cumulative minimum of elements of inputin the dimension dim?,"a named tuple(values,indices)",1660
18345,Compute combinations of lengthrrrof the given tensor. Returns the cross product of vectors in dimension dimof input,Compute combinations of lengthrrrof the given tensor,1931
18346,Performs what of matrices stored inbatch1andbatch2 with a reduced add step (all matrix multiplications get,a batch matrix-matrix product,4780
18347,Performs what of matrices stored in input andmat2?,a batch matrix-matrix product,4777
18348,What does Alias for torch.linalg.slogdet() do?,Alias,4777
18349,Alias for torch.linalg.matrix_power() Returns what of a 2-D tensor?,numerical rank,5966
18350,What is the householder_product of Alias for torch.linalg?,householder_product,5966
18351,Alias for torch.linalg.matrix_power() Returns the what of a 2-D tensor,numerical rank,1721
18352,What does a dictionary that maps overridable functions in the PyTorch API unconditionally return?,-1,5836
18353,What – Arbitrary keyword arguments originally passed intopublic_api?,kwargs(tuple),5836
18354,"Checks if something is a Tensor-like, including an exactTensor-like, including an exactTensor",bool,5836
18355,These lambda functions are useful for what?,testing API coverage,792
18356,When does object :raises TypeError :,if no implementation is found,792
18357,Where are overridable functions found?,PyTorch API,792
18358,What is the term for a function with checks for__torch_function__overrides?,Dict,792
18359,What aren't usually used in a Tensor-like class?,Built-in or user types,792
18360,What – Iterable of arguments to check for __torch_function__ methods?,relevant_args(iterable),3672
18361,"if no implementation is found, object :raises what?",TypeError,3672
18362,"Returns True if the function passed in is a handler for a method or property belonging totorch.Tensor,",Note,3672
18363,What Examples Implement a function with checks for__torch_function__overrides?,"Dict[Callable, Callable]",5181
18364,Return a dict containing what?,dummy overrides,5181
18365,What is generally a subclass of?,tensor,5181
18366,"Checks if something is a Tensor-like, including an exactTensor. ReturnsTrue if any of the",bool,2340
18367,"What does Dict[Callable, Callable] Examples Implement?",a function with checks for__torch_function__overrides,2138
18368,"Result from what, as appropriate?",callingimplementationor an__torch_function__method,2138
18369,:type relevant_args: iterable <sep> True if any of the elements of relevant_args have __torch,True if any of the elements of relevant_args have __torch_function__ implementations,2138
18370,What operation does inkTkHkWkT times kWkTkHkWregions by step sizes,3D average-pooling,2368
18371,What does 3D fractional max pooling over an input signal composed of several input planes do?,Thre,2368
18372,What product is returned if both tensors are 1-dimensional?,Matrix product of two tensors,4214
18373,The non-matrix dimensions are Broadcasted and therefore must be what?,broadcastable,4214
18374,"What only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions?",the broadcasting logic,4214
18375,What is a GPU tensor?,CPU tensor,1500
18376,What type of torch?,64-bit floating point,166
18377,What is sometimes referred to as binary16?,Sometimes referred,166
18378,What is Brain Float sometimes referred to as?,Brain Float,11063
18379,How many bits floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.,16,11041
18380,"Brain Floating Point uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of range",Use,11041
18381,How many bits are floating point1?,16,124
18382,"Useful when precision is important at the expense of range, since it has the same number of exponent bits asfloat32 quantized 4-bit integer is",range is important,11047
18383,What is the current state of the torch?,Currently it,11047
18384,Where is EmbeddingBag only supported?,EmbeddingBag operator,127
18385,What is a float32 quantized 4-bit integer stored as?,8-bit signed integer,132
18386,What type of integer (signed) is torch?,16-bit,132
18387,What is an EmbeddingBag operator?,torch.Tensoris,10946
18388,What is a quantized 4-bit integer stored as?,8-bit signed integer,179
18389,If you have a Tensordataand just want to change what?,"itsrequires_gradflag, userequires_grad_()ordetach",179
18390,A tensor can be constructed from what?,Pythonlistor,11030
18391,A tensor can be constructed from what sequence?,Pythonlistor,11026
18392,What do you have if you want a tensor to be constructed from a Pythonlistor sequence?,Tensordata,10947
18393,What sequence can a tensor be constructed from?,Pythonlistor,10947
18394,What is a way to avoid a copy of a numpy array?,usetorch.as_tensor(),10954
18395,A tensor of what can be a tensor of?,specific data type,10954
18396,What does Add a scalar or tensor toselftensor do?,Add a scalar or tensor toselftensor,5813
18397,What does Alias forabs() Tensor.absolute stand for?,Alias forabs() Tensor.absolute,1038
18398,"What can be constructed by providing the two tensors of indices and values, as well as the size of the sparse",A sparse COO tensor,850
18399,What dimensionality of a tensor is the sum of the number of sparse and dense dimensions?,M+K==len(s.shape)==s.ndim,4445
18400,What permitsuncoalescedsparse tensors?,PyTorch sparse COO tensor format,4988
18401,Most operations will work identically given what two types of sparse tensors?,coalesced or uncoalesced sparse tensor,4988
18402,What is an example of a sparse COO tensor format that permitsuncoalescedsparse tens,"For example, one",10865
18403,What is the location of a 2 + 1)-dimensional tensor?,"(1, 0",6144
18404,What is the sum of the number of sparse and dense dimensions in a tensor?,M+K==len(s.shape)==s.ndim,6144
18405,"Suppose we want to create a 2 + 1-dimensional tensor with the entry [3, 4] at location (0, 2), entry",Note For the most part,6144
18406,What sparse COO tensor format permitsuncoalescedsparse tensors?,PyTorch,3717
18407,"What kind of uncoalesced tensor can one specify multiple values,3and4, for the same index1 that leads to?",1-D,10635
18408,What is an example of a sparse COO tensor?,sparse COO tens,10635
18409,Addition of what is implemented by simply concatenating the indices and values tensors?,sparse COO tensors,10638
18410,What is the lexico of sparse tensors?,lexico,10638
18411,What can be acquired using methodstorch.Tensor.indices() and torch.Tensor.indices()?,COO format data,11391
18412,What is the output of torch.Tensor.coalesce()method?,a sparse tensor,3727
18413,What can one acquire only when a sparse COO tensor is acquired?,COO format data,3727
18414,What can be acquired using methodstorch.Tensor.indices() and torch.Tensor.values()?,COO format data,4101
18415,What cannot be implemented by applying the operation to uncoalesced data becausesqrt(a+b)==sqrt,nonlinear operation,4101
18416,What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?,lexicographical,2550
18417,"What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",lexicographical,4636
18418,What type of nonlinear tensor can be implemented by multiplying all the uncoalesced values with the scalar because,nonline,4636
18419,Where*is what?,the optional batch size of input,3632
18420,Returns the indices of the upper triangular part of arowbycolmatrix in what?,2-by-N Tensor,5565
18421,The upper triangular part of the matrix is defined as what?,elements on and above the diagonal,5565
18422,"Ifoffset=0, all elements on and above the main diagonal are retained?","Ifoffset= 0, all elements on and above the main diagonal are retained",5565
18423,"The set of indices(i,i)lbrace (i, i) rbrace(i,",main diagonal,5565
18424,Why does row*colmust be less than259259259?,to prevent overflow during calculation,5565
18425,What is currently only supporttorch supported?,layout,5565
18426,What does Sets the seed for generating random numbers to a random number for the current GPU?,Sets the seed for generating random numbers to a random number for the current GPU,7784
18427,Returns the currently selectedStreamfor a given device. Returns what for a given device?,defaultStream,5530
18428,Gets the cuda capability of a device. Gets the name of a device. Gets the properties of a device. Return,Gets the name of a device,1509
18429,What does comm.broadcast broadcast to specified GPU devices?,comm.broadcast Broadcasts a tensor,1509
18430,comm.broadcast Broadcasts a tensor to specified GPU devices?,comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPU,1509
18431,Which entity selects a given stream?,Context-manager,1883
18432,What does comm.reduce_add Sums tens tens?,comm.reduce_add Sums tens,1883
18433,comm.reduce_add Sums tensors from multiple GPUs?,comm.s,1582
18434,Returns what this library was compiled for?,list CUDA architectures,5502
18435,Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the properties,cuda,5536
18436,What Gathers tensors from multiple GPUs?,comm.gather,5536
18437,Releases what currently held by the caching allocator?,all unoccupied cached memory,5509
18438,Computes the error function of input. The error function is defined as follows: input(Tensor) – the input ten,output tensor,7761
18439,What is an example of a Computes the exponential of the elements minus 1 of input?,Computes the exponential of the elements minus 1 of input,7761
18440,What is the name of the function that Computes the inverse error function of input?,Computes the inverse error function of input,1757
18441,Computes the elements minus 1 of input. Note This function provides greater precision than exp(x) - 1 for small values of,exponential,1757
18442,Computes the first kind of what function for each element of input?,exponentially scaled zeroth order modified Bessel function,9667
18443,Computes the first kind for each element of input. input(Tensor) – the input tensor. out(,exponentially scaled zeroth order modified Bessel function,9671
18444,This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of what?,thisFuture,9707
18445,"If the value contains tensors that reside on GPUs, what will Future.done() returnTrueeven?",if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device,3522
18446,What is a limitation of thisFuture?,aFuturecannot be marked completed twice,3522
18447,What does is_tensor return ?,True or False,9723
18448,Is_tensor returns true or false if obj is  a PyTorch tensor?,True,9722
18449,Is_tensor Returns what?,True or False,9722
18450,What  does Is_tensor Return ?,True or False,9722
18451,Is_storage Returns True or False if obj is  a PyTorch storage object?,True,9721
18452,When is_storage return True?,if the  object is a PyTorch storage object.,9721
18453,What does Is_storage Return?,True or False,9721
18454,What happens if the data type of input is a complex data type?,is_complex Returns True,9710
18455,What Returns True if the data type of input is a complex data type?,is_complex,9710
18456,What happens if the data type of input is a floating point data type?,is_floating_point Returns True,9711
18457,What Returns True if the data type of input is a floating point data type?,is_floating_point,9711
18458,What happens if the input is a single element tensor which is not equal to zero after type conversions?,is_nonzero Returns True,9715
18459,What Returns True if the input is a single element tensor which is not equal to zero after type conversions?,is_nonzero,9715
18460,What sets the default floating point dtype to d?,set_default_dtype,10664
18461,How to get  the current default floating point torch.dtype?,use  get_default_dtype,9454
18462,What is function that returns the current floating point torch.dtype?,get_default_dtype,9454
18463,What is fuction to  get current floating point torch.dtype?,get_default_dtype,9454
18464,What sets the default torch.Tensor type to floating point tensor type t?,set_default_tensor_type,10665
18465,What  type of  Tensor Type  does  set_default_tensor_type Sets the default torch.Tensor type to?,floating point,10665
18466,numel Returns the total number of elements in what?,the input tensor,10282
18467,What returns the total number of elements in the input tensor?,numel,10282
18468,What is method  that sets options for printing?,set_printoptions,10672
18469,What disables denormal floating numbers on the CPU?,set_flush_denormal,10667
18470,What does  set_flush_denormal do,set_flush_denormal Disables denormal floating numbers on CPU.,10667
18471,What is the use of tensor?,tensor Constructs a tensor with data.,10811
18472,what is  the method  to Constructs a tensor withdata?,tensor,10811
18473,What does sparse_coo_tensor construct?,a sparse tensor,10739
18474,What Constructs asparse tensor in COO(rdinate) format?,sparse_coo_tensor,10739
18475,In what format does sparse_coo_tensor construct asparse tensor?,COO(rdinate),10739
18476,What does as_tensor convert the data into?,a torch.Tensor,8903
18477,What converts the data into a torch.Tensor?,as_tensor,8903
18478,"What type of view creates a view of an existingtorch.Tensorinput with specifiedsize,stride andstor",as_strided,8900
18479,"What creates a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_off",as_strided,8900
18480,What does from_numpy create a Tensor from?,a numpy.ndarray,9405
18481,What does from_numpy create?,a Tensor,9405
18482,What does zeros_like return?,a tensor filled with the scalar value 0,11470
18483,what type of function does one return?,tensor filled with the scalar value 1,10300
18484,what does ones Return return?,a tensor,10300
18485,What does one return that is filled with the scalar value 1  ?,a tensor,10301
18486,What does ones_like return?,a tensor,10301
18487,What does arange return?,a 1-D tensor,8872
18488,What is the use of  ops range?,a 1-D tensor,10556
18489,What  does range Return?,a 1-D tensor,10556
18490,What creates a one-dimensional tensor of size steps?,logspace,9824
18491,What is logarithmic scale used to create a one-dimensional tensor of size steps?,basebase,9824
18492,What creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end?,linspace,9801
18493,What returns a 2-D tensor with ones on the diagonal and zeros elsewhere?,eye,9313
18494,empty Returns a tensor filled with what?,uninitialized data,9249
18495,What does empty_strided Return ?,empty_strided Returns a tensor filled with uninitialized data.,9252
18496,What type of data does empty_strided return?,uninitialized data,9252
18497,empty_strided Returns a tensor filled with what?,uninitialized data,9252
18498,Returns an uninitialized tensor with the same size as input?,empty_like,9251
18499,What does full create a tensor of?,with fill_value,9406
18500,What is the  size filled with fill_value?,tensor,9406
18501,What does Create a tensor of  size filled with fill_value.?,full,9406
18502,What returns a tensor with the same size as input filled with fill_value?,full_like,9407
18503,full_like Returns a what with the same size as input filled with fill_value?,tensor,9407
18504,What type of tensor does quantize_per_tensor convert to?,float tensor,10540
18505,What is the scale of a quantized tensor?,zero point,10540
18506,What type of tensor does quantize_per_tensor convert a float tensor to?,quantized tensor with given scale and zero point.,10540
18507,What type of tensor does quantize_per_tensor convert?,float,10540
18508,Quantize_per_channel Converts a float tensor to what?,per-channel quantized tensor with given scales and zero points.,10539
18509,What is the value of the scales used to convert a float tensor to a per-channel quantized tensor,zero points,10539
18510,What does quantize_per_channel convert a float tensor to?,quantized tensor with given scales and zero points.,10539
18511,What returns an fp32 Tensor by dequantizing a quantized Tensor?,dequantize,9160
18512,what part of a complex tensor is equal to real?,real part,9103
18513,what part of a complex tensor is equal to imag?,imaginary part,9103
18514,What are the elements of polar Construct a complex tensor?,Cartesian coordinates,10434
18515,What is a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand,polar Constructs,10434
18516,What are the elements of polar Constructs a complex tensor?,Cartesian coordinates,10434
18517,What does heaviside compute for each element in input?,Heaviside step function,9508
18518,What Computes the Heaviside step function for each element in input?,heaviside,9508
18519,What concatenates the given sequence of seq tensors in the given dimension?,cat,9027
18520,What does cat concatenate in a given dimension?,given sequence of seq tensors,9027
18521,cat Concatenates the given sequence of seq tensors in what?,given dimension,9027
18522,What does a chunk do?,Splits a tensor into a specific number of chunks,9077
18523,What splits a tensor into a specific number of chunks?,chunk,9077
18524,"According to What ,does dsplit Split input's depthwise tensors ?",indices_or_sections,9201
18525,How many dimensions does dsplit Splits input have?,three or more dimensions,9201
18526,dsplit Splits input is divided into multiple tensors according to what?,indices,9201
18527,What creates a new tensor by horizontally stacking the tensors in tensors?,column_stack,9095
18528,How does column_stack create a new tensor?,horizontally stacking,9095
18529,Along what axis are dstack Stack tensors in sequence depthwise?,third axis,9205
18530,What is  method  that  stack tensors in sequence depthwise?,dstack,9205
18531,What does dstack do?,dstack Stack tensors in sequence depthwise,9205
18532,What does gather do?,Gathers values along an axis specified bydim,9442
18533,What is the axis specified by gather?,bydim,9442
18534,What is the method  that  splits a tensor horizontally?,hsplit,9530
18535,What does hsplit Splits input have?,A tensr with one or more dimensions,9530
18536,How does hsplit Splits input split into multiple tensors?,horizontally into multiple tensors,9530
18537,How are hstack Stack tensors sequenced?,horizontally,9532
18538,In what way hstack Stack tensors in sequence ?,horizontally,9532
18539,What returns a new tensor which indexes the input tensor along dimension dim using the entries inindex?,index_select,9609
18540,What returns a new 1-D tensor which indexes the input tensor according to the boolean mask?,masked_select,9874
18541,What type of tensor does masked_select return?,1-D tensor which indexes the input tensor according to the boolean mask,9874
18542,What is the boolean mask?,a Bool Tensor,9874
18543,What does masked_select return?,1-D tensor,9874
18544,What does movedim do?,Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination,9937
18545,What Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination?,movedim,9937
18546,What is moveaxis?,Alias for torch.movedim,9936
18547,What is the Alias for torch.movedim()?,moveaxis,9936
18548,What is a narrowed version of input tensor?,a new tensor,9971
18549,What type of return does reshape return?,"tensor with the same data and number of elements as input, but with the specified shape.",10593
18550,What returns a tensor with the same data and number of elements as input?,reshape,10593
18551,What does reshape return with the same data and number of elements as input?,a tensor with the specified shape,10593
18552,What is the Alias of torch.vstack()?,row_stack,10625
18553,What is row_stack?,Alias of torch.vstack,10625
18554,What is the Alias for torch.vstack()?,row_stack,10625
18555,What is out-of-place version of torch.Tensor.scatter_()?,Out-of-place version of torch.Tensor.scatter_(),10645
18556,What is a version of of torch.Tensor.scatter_()?,scatter Out-of-place,10645
18557,What is out-of-place version of torch.Tensor.scatter_add_()?,Out-of-place version of torch.Tensor.scatter_add_(),10646
18558,What version of of torch.Tensor.scatter_add_()?,scatter_add Out-of-place,10646
18559,What is the Out-of-place version of of torch.Tensor.scatter_add_()?,scatter_add,10646
18560,What does split do to a tensor?,split Splits the tensor into chunks,10742
18561,split Splits the tensor into what?,chunks,10742
18562,The tensor returns a tensor with all the dimensions of input of what?,size 1,10745
18563,What does squeeze Returns return?,a tensor,10746
18564,What Concatenates a sequence of tensors along a new dimension?,stack,10748
18565,stack Concatenates a sequence of what?,tensors,10748
18566,What concatenates a sequence of tensors along a new dimension?,stack,10748
18567,What is Alias for torch.transpose()?,swapdims,10790
18568,What is  swapaxes?,Alias for torch.transpose(),10789
18569,what  input  tensor does   t Expect o be and transposes dimensions 0 and 1?,2-D tensor,10793
18570,What are the elements of input to  ake?,the given indices,10794
18571,Take_along_dim Selects values from what indices from the given dim?,1-dimensional indices,10795
18572,What selects values from input at the 1-dimensional indices from indices along the given dim?,take_along_dim,10795
18573,What does tensor_split do?,tensor_split Splits a tensor into multiple sub-tensors,10813
18574,What are all sub-tensors of a tensor for tensor_split?,views of input,10813
18575,What does tensor_split use to split a tensor into multiple sub-tensors?,indices  or number of sections specified by indices_or_sections,10813
18576,What Constructs a tensor by repeating the elements of input?,tile,10918
18577,What does transpose return?,tensor which is transpose of input tensor,11255
18578,What returns a tensor that is a transposed version of input?,transpose,11255
18579,What removes a tensor dimension?,unbind,11280
18580,How to remove a tensor dimension?,use unbind,11280
18581,What does unsqueeze return a new tensor with?,a dimension of size one inserted at the specified position,11286
18582,What does unsqueeze Return?,a new tensor with a dimension of size one inserted at the specified position,11286
18583,What does vsplit Split input's tensors vertically vary according to?,indices_or_sections,11351
18584,What is input to vsplit?,a tensor with two or more dimensions,11351
18585,How are vstack Stack tensors organized?,row wise,11352
18586,How are vstack Stack tensors sequenced?,vertically,11352
18587,What does  vstack do?,vstack Stack tensors in sequence vertically,11352
18588,vstack Stack tensors in sequence vertically based on what?,row wise,11352
18589,What is the return value of elements selected from either x or y?,tensor,11367
18590,What does Generator create and return that manages the state of the algorithm?,generator object,2687
18591,To what  does seed Set the seed for generating random numbers?,non-deterministic random number,10655
18592,What does set the seed for generating random numbers?,manual_seed,9868
18593,what for manual_seed Set the seed ?,for generating random numbers,9868
18594,What does return the initial seed for generating random numbers as a Python  long?,initial_seed,9619
18595,initial_seed Returns the initial seed for generating random numbers as  what?,Python long,9619
18596,What does initial_seed return for generating random numbers as a Python  long?,initial seed,9619
18597,What returns the random number generator state as a torch.ByteTensor?,get_rng_state,9462
18598,What is random number generator state returned by get_rng_state?,torch.ByteTensor,9462
18599,What does set_rng_state set?,random number generator state,10674
18600,What draws binary random numbers from a Bernoulli distribution?,bernoulli,8953
18601,What does bernoulli draw from a Bernoulli distribution?,binary random numbers,8953
18602,What returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located,multinomial,9947
18603,What does multinomial return if each row containsnum_samplesindices sampled from the multinomial probability distribution?,tensor,9947
18604,What returns a tensor of random numbers drawn from separate normal distributions?,normal,10264
18605,What does poisson use to return a tensor of the same size?,rate parameter given by the corresponding element in input i,10432
18606,What distribution is the tensor sampled from by poisson?,Poisson distribution,10432
18607,What parameter is given by the corresponding element in input for poisson?,rate parameter,10432
18608,From what distribution is each element sampled by poisson?,Poisson distribution,10432
18609,What kind of distribution is  referred by rand to return tensor?,uniform distribution,10548
18610,What returns a tensor filled with random numbers from a uniform distribution?,rand,10548
18611,rand Returns a tensor filled with what?,random numbers,10548
18612,What does rand_like return?,a tensor,10549
18613,What returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?,randint,10550
18614,What returns a tensor with the same shape as Tensorinput filled with random integers generated uniformly betweenlow(inclusive,randint_like,10551
18615,What does randn return?,tensor,10552
18616,Which distribution  randn_like  refers for returning  a tensor ?,normal distribution,10553
18617,What does randn_like return?,a tensor,10553
18618,What returns a random permutation of integers from 0 to -1?,randperm,10555
18619,What is engine that generates Sobol sequences?,The torch.quasirandom.SobolEngine,10545
18620,What type of sequence is generated by the SobolEngine ?,quasirandom  (scrambled) Sobol sequences.,10545
18621,What does save do?,save Saves an object to a disk file,10644
18622,What saves an object to a disk file?,save,10644
18623,save Saves an object to what type of file?,disk,10644
18624,load Loads an object saved from a file  saved using what method?,with torch.save(),9805
18625,What does load Load  from a file?,the object from a file  earlier saved with torch.save(),9805
18626,What returns the number of threads used for inter-op parallelism on CPU?,get_num_interop_threads,9461
18627,What does get_num_interop_threads return?,the number of threads used for inter-op parallelism on CPU,9461
18628,What sets the number of threads used for interop parallelism?,set_num_interop_threads,10670
18629,What is program that disabled gradient calculation?,no_grad Context-manager,10251
18630,What did no_grad Context-manager disable?,gradient calculation,10251
18631,What does the enable_grad Context-manager enable?,gradient calculation,9256
18632,What is set_grad_enabled?,Context-manager,10668
18633,What happens if grad mode is currently enabled?,is_grad_enabled Returns True,9712
18634,What mode is currently enabled by is_grad_enabled?,grad mode,9712
18635,What does inference_mode Context-manager enable or disable?,inference mode,9615
18636,What Returns True if inference mode is currently enabled?,is_inference_mode_enabled,9713
18637,What does abs compute of each element in input?,absolute value,8783
18638,What computes the absolute value of each element in input?,abs,8783
18639,What is the absolute ?,Alias for torch.abs(),8785
18640,What is the  Alias for torch.abs()?,absolute,8785
18641,What does acos compute?,inverse cosine,8788
18642,What is Alias for torch.acos?,arccos,8875
18643,acosh Returns a new tensor with what of the elements of input?,inverse hyperbolic cosine,8791
18644,What is the Alias for torch.acosh()?,arccosh,8876
18645,"What performs the element-wise division of tensor 1 by tensor 2 , multiply the result by the scalarvalueand add it to input?",addcdiv,8832
18646,How does addcdiv perform the element-wise division of of tensor 1 by tensor 2?,multiply the result by the scalarvalue,8832
18647,What performs the element-wise multiplication of tensor 1 by tensor 2?,addcmul,8833
18648,to what inputs the element-wise multiplication performed by addcmul?,element-wise multiplication of tensor 1 by tensor 2,8833
18649,What does angle compute?,the element-wise angle,8867
18650,In what units is the element-wise angle of a given input tensor computed?,radians,8867
18651,What returns a new tensor with arcsine of each element ?,arcsine,8904
18652,What type of sine does asinh return?,hyperbolic,8906
18653,Asinh Returns a new tensor with what of the elements of input?,inverse hyperbolic sine,8906
18654,Atan returns a new tensor with what of the elements of input?,arctangent,8908
18655,Atanh Returns a new tensor with the inverse of what type of tangent of the elements of input?,hyperbolic,8911
18656,Atanh Returns a new tensor with what of the elements of input?,inverse hyperbolic tangent,8911
18657,What is the  Alias for torch.atanh()?,arctanh,8877
18658,What is the element-wise arctangent of input?,atan2,8910
18659,What Computes the bitwise NOT of the given input tensor?,bitwise_not,8968
18660,What computes the bitwise AND of input and other?,bitwise_and,8967
18661,What is bitwise_or for?,Computes the bitwise OR of input and other,8969
18662,What computes the bitwise XOR of input and other?,bitwise_xor,8970
18663,What does ceil return on elements of input?,"a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element",9050
18664,What is the ceil of the elements of input?,the smallest integer greater than or equal to each element,9050
18665,What clamps inputs to a range bentween min and max?,clamp Clamps inputs to a range bentween min and max,9078
18666,What is the Alias for torch.clamp()?,clip,9083
18667,What does conj compute?,the element-wise conjugate of the given input tensor,9115
18668,What Computes the element-wise conjugate of the given input tensor?,conj,9115
18669,"What Create a new floating-point tensor with the magnitude of input and the sign of other, element wise?",copysign,9124
18670,What Returns a new tensor with the cosine of the elements of input?,cos,9126
18671,cosh Returns a new tensor with what type of cosine?,hyperbolic,9127
18672,What returns a new tensor with the hyperbolic cosine of the elements of input?,cosh,9127
18673,What returns a new tensor with each of the elements of input converted from angles in degrees to radians?,deg2rad,9155
18674,What divides each element of the input input by the corresponding element of the other?,div,9188
18675,What divides each element of the input input by the corresponding element of other?,div,9188
18676,digamma Computes the logarithmic derivative of what on input?,gamma function,9181
18677,digamma Computes what derivative of the gamma function on input?,logarithmic derivative,9181
18678,What computes the logarithmic derivative of the gamma function on input?,digamma,9181
18679,exp Returns a new tensor with what of the elements of the input tensorinput?,exponential,9291
18680,What is the Alias for torch.special.expm1?,expm1,9293
18681,What is the Alias for torch.special.expm1()?,expm1,9293
18682,What does torchvision.transforms.CenterCrop do?, Crops the given image at the center,38
18683,What is the purpose of torchvision.transforms.CenterCrop ?, Crops the given image at the center,38
18684,What is torchvision.transforms.CenterCrop used for?, Crops the given image at the center,38
18685,is center cropping available?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18686,is there center cropping available in torchvision?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18687,is there center cropping available in torchvision?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18688,which library supports center cropping?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18689,is there center cropping in pytorch?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18690,center cropping supported?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18691,centter cropping supported in pytorch?, torchvision.transforms.CenterCrop Crops the given image at the center,38
18692,what is the use of torchvision.transforms.ColorJitter(brightness?," Randomly change the brightness, contrast, saturation and hue of an image",39
18693,what is  torchvision.transforms.ColorJitter(brightness for?," Randomly change the brightness, contrast, saturation and hue of an image",39
18694,what is  torchvision.transforms.ColorJitter(brightness used for?," Randomly change the brightness, contrast, saturation and hue of an image",39
18695,when is torchvision.transforms.ColorJitter(brightness used ?," Randomly change the brightness, contrast, saturation and hue of an image",39
18696,when is ColorJitter of torchvision.transforms used  for?," Randomly change the brightness, contrast, saturation and hue of an image",39
18697,when is ColorJitter of torchvision.transforms user?," Randomly change the brightness, contrast, saturation and hue of an image",39
18698, is ColorJitter in torchvision.transforms ?," yes,  it Randomly change the brightness, contrast, saturation and hue of an image",39
18699, is ColorJitter present in torchvision.transforms ?," yes,  it Randomly change the brightness, contrast, saturation and hue of an image",39
18700, is ColorJitter available in torchvision.transforms ?," yes,  it Randomly change the brightness, contrast, saturation and hue of an image",39
18701, ColorJitter available in torchvision.transforms ?," yes,  it Randomly change the brightness, contrast, saturation and hue of an image",39
18702, ColorJitter is in torchvision.transforms ?," yes,  it Randomly change the brightness, contrast, saturation and hue of an image",39
18703, which function in torchvision.transforms randomly changes brightness?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18704, which function in torchvision.transforms randomly changes conrast?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18705, which function in torchvision.transforms randomly changes saturation?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18706, which function in torchvision.transforms randomly changes hue?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18707, ColorJitter in torchvision.transforms randomly changes hue?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18708, ColorJitter in torchvision.transforms randomly changes saturation?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18709, ColorJitter in torchvision.transforms randomly changes brightness?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18710, ColorJitter in torchvision.transforms randomly changes saturation?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18711, ColorJitter in torchvision.transforms randomly changes contrast?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18712, can contrast of image be randomly changed?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18713, can saturation of image be randomly changed?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18714, can brightness of image be randomly changed?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18715, can hue of image be randomly changed?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18716, is torchvision.transforms.ColorJitter used to change hue randomly?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18717, is torchvision.transforms.ColorJitter used to change saturation randomly?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18718, is torchvision.transforms.ColorJitter used to change contrast randomly?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18719, is torchvision.transforms.ColorJitter used to change brightness randomly?," torchvision.transforms.ColorJitter  Randomly change the brightness, contrast, saturation and hue of an image",39
18720, what values can be used for brightness_factor in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18721, which values can be used for brightness_factor in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18722, brightness_factor in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18723, brightness_factor value in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18724, brightness_factor value in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18725, brightness_factor value  can be negtive in torchvision.transforms.ColorJitter?," brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.",39
18726, hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18727, what can be hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18728, what can be hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18729, what is minimum  hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18730, what is maximum  hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18731, what is maximum  hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18732, what is maximum  hue value in torchvision.transforms.ColorJitter?," hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.",39
18733, why pytorch uses caching memory allocator?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18734, why does Pytorch utilise a memory allocator for caching?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18735, What is the purpose of Pytorch's use of a memory allocator for caching?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18736, What is the objective of Pytorch's caching usage of a memory allocator? , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18737, What is the goal of Pytorch's use of a memory allocator for caching?  , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18738, What is the objective of Pytorch's caching usage of a memory allocator?  , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18739, does  pytorch uses caching memory allocator?  , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18740, Does Pytorch make use of a memory allocator for caching? , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18741, Is a memory allocator used by Pytorch for caching? , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18742, Pytorch employs a memory allocator for caching. , PyTorch uses a caching memory allocator to speed up memory allocations.,20
18743," For caching, Pytorch makes use of a memory allocator. ", PyTorch uses a caching memory allocator to speed up memory allocations.,20
18744," For caching, Pytorch makes use of a memory allocator?", PyTorch uses a caching memory allocator to speed up memory allocations.,20
18745, why nvidia-smi doesn't show true usage while using pytorch?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18746, Why does nvidia-smi fail to provide actual use when using Pytorch?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18747, Why does nvidia-smi fail to report real utilisation when Pytorch is used?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18748,"When Pytorch is utilised, why does nvidia-smi fail to show true utilisation?", PyTorch uses a caching memory allocator to speed up memory allocations.,20
18749,Why does nvidia-smi fail to display actual usage when Pytorch is used?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18750,Why doesn't nvidia-smi show actual consumption when Pytorch is used?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18751,Why does nvidia-smi fail to display actual usage when Pytorch is used?, PyTorch uses a caching memory allocator to speed up memory allocations.,20
18752,how to know about python subprocesses which are still alive?, You may find them via ps -elf | grep python,20
18753,How can I find out about Python subprocesses that are still active?, You may find them via ps -elf | grep python,20
18754,How can I learn about Python subprocesses that are still running?, You may find them via ps -elf | grep python,20
18755,How can I find out more about Python subprocesses that are still active?, You may find them via ps -elf | grep python,20
18756,How can I learn more about still-running Python subprocesses?, You may find them via ps -elf | grep python,20
18757,How can I learn more about Python subprocesses that are still running?, You may find them via ps -elf | grep python,20
18758,How to manually kill python subprocess?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18759,How can I manually terminate a Python subprocess?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18760,How can I manually end a Python subprocess?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18761,How to find and  manually end a Python subprocess?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18762, find and  manually end a Python subprocess?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18763, find and   end a Python subprocess manually?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18764, find and   terminate a Python subprocess manually?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18765, how to find and   terminate a Python subprocess manually?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18766, gpu not freed poperly?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18767, why gpu not freed poperly?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18768, why gpu is not freed poperly?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18769, why my gpu is not freed poperly?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18770,  gpu is not freed poperly as per nvidia-smi?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18771,  why gpu is not freed poperly as per nvidia-smi?, You may find them via ps -elf | grep python and manually kill them with kill -9 [pid],20
18772,  why gpu is not freed after python quits?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18773,   gpu is not freed even after python quits?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18774,   gpu not freed on quitting python?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18775, why  gpu not freed on quitting python?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18776, Why isn't the GPU released when you exit Python?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18777," When Python is terminated, why isn't the GPU freed?"," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18778, Why isn't the GPU released when Python is terminated?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18779," When Python is ended, why isn't the GPU released?"," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18780," When Python is ended, why isn't the GPU released?"," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18781, Why isn't the GPU released when Python is finished?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18782, Why isn't the GPU made available after Python is finished?," If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].",20
18783,calculating loss for entire batch using nllloss in 0 4 0,"I believe you should use criterion test(dec outs.view(-1, vocab size),targets.view(-1)) to test your hypothesis.( C ) = vocab size, and ( N ) = batch size*seq length in your case. I'm assuming that the sequence length in each batch is the same. If not, you'll need to use pack padded sequence and hide the pad token loss.",2087
18784,masked fill operates weirdly,"The error message indicates that there is a size issue on the third dimension, with one size 8 and the other size 9.Before the procedure, I would print the size of the tensors to double-check the dimensions.",2613
18785,creating custom nn module for softmargin softmax,"Avoid using for loops whenever feasible, and attempt to vectorize your code as much as possible.",4886
18786,gru autoencoder is not working,Target tensor was wrong,3198
18787,issue with multiple gpu loss convergence," mention dim=1 in the data parallel, that is the dimension I need to scatter my inputs.",3201
18788,building from source keeps failing ubuntu 18 04 02 lts no gpu,Create new anaconda  envinorment ,9510
18789,segmentation fault core dumped with personnal nn function,Update version from v1.0 to v1.0.1,7230
18790,libtorch cmake error on centos7,,5958
18791,cmake error and fatal error lnk1181 building from source on windows 10,"So, I noticed that your build directory has some empty space. You can simply avoid that as a workground to ensure that the build passes. ",8315
18792,data float 1 segfaults when cudatype,The Tensor class despreately needs documentation!,3099
18793,how to collect libtorch package like the official release when building from source,"I believe that using an adequate CMake + make install, such as the Android build, should suffice. For this, you'll need to turn off Python.The suggested solution there, creating Python and selecting the lib and include, also works fine. That's how Libtorch 1.0 was made (by extracting from the whl...).",3271
18794,cuda is available true with python false with c,use libtorch,2179
18795,how to self define a backward function for a net in libtorch i tested some code but failed,"The Functions on the Python side have changed slightly, as you can see in the tutorial.It's a little more complicated in cpp. A Function can only be used in one direction, and its apply method should be used. It can be done in pure autograd by executing operations on variables, or it can be wrapped and a reverse function supplied.If you want a custom backward, you'll need two functions. In this case, the forward function is DelayedError, and the backward function is Error.",2900
18796,is there any way to skip steps in a dataloader,Bulding New desgin for DataLoader,8462
18797,understanding model to device,"Yes, as noted in this post, your assumption should be valid, as the model reference would be passed and its parameters (and buffers) updated in place.To test it with your configuration, you might use the code snippet in the linked post.",8537
18798,update weight with same netoworks output,"Previous PyTorch versions allowed this wrong gradient calculations, which is why no errors were raised.",6894
18799,update network after differentiation with autograd grad,Maybe the integration in pytorch geometric is breaking,8068
18800,create a f score loss function,AFAIK f-score is ill-suited as a loss function for training a network.,895
18801,how do i backpropagate through a modules parameters,"It functionalizes the model, where it’s parameters can be detached and backproped through",11453
18802,how am i supposed to cache tensors that require grad but arent learnable module params that are replaced by a different tensor several times each forward pass,"Activations and outputs are not nn. Parameters aren't being assigned as model attributes. Instead, I added them to the forward procedure as optional key word arguments and returned the activations as well.",3204
18803,torch no grad makes any difference,"Only if you utilise specific Modules that behave differently in eval mode, such as dropout or batchnorm, will eval mode make a difference. However, even in such instance, the runtime may not alter significantly. The no grad setting disables autograd, resulting in a significant reduction in memory use.",2927
18804,training breaks in pytorch 1 5 0 throws inplace modification error,"There is no other output when set detect anomaly is used. Make sure you're using the most recent version of Pytorch, as we recently fixed a bug where warnings weren't showing up in colab. Alternatively, you can run your code from the command line to get the forward code. The code performs numerous in-place and viewing operations. It also collaborates with...",5873
18805,custom loss function class,BCELossWithLogits does,2986
18806,best way to downsample batch image tensors, use nn.PixelShuffle(),5116
18807,unexpected data error in the ms coco dataset valueerror all bounding boxes should have positive height and width,Filter  biunding box whcich has width 0 ,1407
18808,softmax returns only 1s and 0s during inference,Print the logits values because  ploting image won't help ,2598
18809,mnist server down,"Version is not stable , Download the MNIST file ",3523
18810,how to visualize model in pytorch,read Saving and Loading Models PyTorch Tutorials 1.8.1+cu102 documentation ,4343
18811,vgg16 using cifar10 not converging,"There is no other output when set detect anomaly is used. Make sure you're using the most recent version of Pytorch, as we recently fixed a bug where warnings weren't showing up in colab. Alternatively, you can run your code from the command line to get the forward code. The code performs numerous in-place and viewing operations. It also collaborates with...",3884
18812,dataparallel trained on one gpu but inference used on multiple gpus,Load model before DP Construction ,7389
18813,runtimeerror each element in list of batch should be of equal size,Downgrade pytroch version to 1.5 from 1.6 ,3901
18814,valueerror not enough values to unpack expected 3 got 2,Lstm returns 3-tuple instead you are unpackingit as 2  ,11465
18815,nn nllloss valueerror dimension mismatch,"Know the dataset inside out , last bacth conatins only 4 labels it is excpecting 10 ",8044
18816,nn embedding input indices meaning,embedding.weight.,2232
18817,low gpu utilization for sequence task,Eliminate the inner loop with bmm,1411
18818,conflict between libtorch and grpc,"Compiling libtorch with the protobuf library that grpc uses, or compiling grpc with the protobuf library that libtorch uses, is currently the easiest approach.",7416
18819,multidimensional slice in c,Curretly it is not avaiable,8268
18820,where is the implementation of tensor slice,Use this link https://github.com/pytorch/pytorch/blob/3ad1bbe16a3c1d6bb9566f09229afd63022a82df/aten/src/ATen/native/TensorShape.cpp#L655,3003
18821,include directory structure,us ethis link https: //pytorch.org/cppdocs/frontend.html#end-to-end-example 9,3299
18822,at cuda memory leak when loading model,"ASAN doesn’t work with CUDA, use only CPU functionality",903
18823,solved thread safety issue in torch load,I believe the error occurred because I used the release version of the torch library and compiled in debug mode.,3243
18824,dataparallel output differs from its module,Error due to floating point precision ,5927
18825,use 4 gpu to train model loss batch size batch size 4, turn  list to tensor ,6805
18826,torch distributed class definitions,us this link https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/Types.hpp#L8 ,5113
18827,how to freeze feature extractor and train only classifier in distributeddataparallel,Try with updated version 1.3,4140
18828,attention weights with multiple heads in nn multiheadattention,This seems to be because the attention weights are averaged across all of the heads.,7800
18829,about torch autograd function,use this link https: //github.com/pytorch/vision/blob/0013d9314cf1bd83eaf38c3ac6e0e9342fa99683/torchvision/csrc/ops/autograd/roi_align_kernel.cpp#L111-L127,8321
18830,how to initialize tensors such that memory is allocated,"Rand appears to require additional memory to produce random numbers, but then consumes memory similar to zeros.",3972
18831,will the same model input data twice retain the gradient information of the first input data,use total.backward(),8509
18832,jit tried to access nonexistent attribute or method forward of type tensor, Unable to call troch script with super method ,4144
18833,pytorch with cuda 11 compatibility,install  the latest CUDA11 ,1298
18834,unsupported format string passed to list format,Pass Array,3158
18835,increasing data set size slows loss backward even though batch size is constant,use requires_grad=False,3219
18836,torch logsumexp returning nan gradients when inputs are inf,Simply mention gradients to 0,3909
18837,multiple calls to autograd grad with same graph increases memory usage,remove backwards hook which was added and it will solve memory issuses,4615
18838,pass all parameters to optimizer instead of only nonfrozen parameters,Gradient values are empty because .grad is None,2917
18839,how to calculate gradients correctly without in place operations for custom unpooling layer,"You don't have to stay away from them. It's just that autograd doesn't support every combination of them, and if you do, you'll get an error.So, if your code runs without errors, autograd is capable of handling this situation.The only problem I have with such an implementation is the nested loops' slowness. However, this has nothing to do with gradient accuracy.",9897
18840,grad fn get whole graph in dot,use this link   https: //github.com/szagoruyko/pytorchviz,2931
18841,gradient computation when using forward hooks,"The simplest way to comprehend what will happen here is to remember that the autograd lives beneath torch.nn and has no idea what torch.nn does.In this situation, the Tensor you offer to the remainder of the network is the one that gets gradients (it does not matter if it comes from a hook or not).Because A hooked is dependent on A, the gradients will flow back from A hooked to A in this situation.",2922
18842,dataset for cnn regression,"You can consider object counting datasets, the idea is that object counting can be formulated as a regression problem. ",2838
18843,is it better to set batch size as a integer power of 2 for torch utils data dataloader,"In all dimensions, such as channel count, spatial size, and so on, powers of two may be preferred.However, as previously described, internally padding could be used to avoid a performance cliff, and you should thus profile your workloads.",4885
18844,runtimeerror function addbackward0 returned an invalid gradient at index 1 expected type torch floattensor but got torch cuda floattensor,"One variable with the name,processed, which I was initialising within the loss function, was not being put on cuda.The important thing to remember for these issues is that some variables are not deployed on the GPU or CPU, whichever device you are using. As a result, a shortcut is to call the variable.to(device) function to assign every variable to the GPU or CPU, depending on which device you're using.",3206
18845,where should i look to solve running mean error in resnet transfer learning,"use self.conv1 = nn.Conv2d(1,64, kernel_size=(7,7), bias=False); self.inplanes = 128 ",7212
18846,save output image of cnn model,"either change the output shape of your third conv2d or add another layer with the input channel of your previous layer and your desired output dimension. However, depending on the loss function you are using, you may need to adjust it as well",3949
18847,need feature maps of resnet50,Use forward hooks ,3300
18848,tensors are at different cuda devices,CUDA_VISIBLE_DEVICES=1,5954
18849,simple rnn stuck around the mean,use this link https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html,1269
18850,pad packed sequence export to onnx,Use your own method of   packing and unpacking   urong export ,3303
18851,a very strange phenomenon i met in training machine translation,Learning rate could be problem,3947
18852,pytorch chatbot loss function with ignore index instead of targets padding mask,Calluate the loss properly ,3080
18853,do we need to set a fixed input sentence length when we use padding packing with rnn,"It does not depend on the sequences lengths. Sure, the processing takes more time for longer sequences",6943
18854,pytorch c api missing headers,Update to the lastet version ,3156
18855,about torchvision for c frontend,"use Opencv , torchvision c++ is not matching python support currently ",3839
18856,why gpu memory allocations are associated with the cuda stream,"The CUDA memory API takes a different approach: The cudaFree call synchronises all streams – the CPU waits until all streams have completed all outstanding work before completing the cudaFree call. This ensures that subsequent uses of the memory will take place after the previous ones have finished. CudaFree, on the other hand, is a relatively expensive call. The caching allocator's primary goal is to avoid this type of synchronisation.",1278
18857,c time sequence prediction py slow,Error in the test code,2263
18858,model weights are not moved to the gpu,Register layers with register_module(),3047
18859,unable to install torchvision,Use updated version ,4108
18860,libtorch glog doesnt print, AAdd add_definitions(-DC10_USE_GLOG,4228
18861,runtimeerror stop waiting response is expected,Reinstall  NCCL using this link  https://github.com/NVIDIA/nccl.git,7044
18862,torch nn parallel data parallel for distributed training backward pass model update,ocking is bultin function and weights will be updated  accordingly ,8485
18863,why is float tensor addition on cpu slower for avx2 than the default aten cpu capability,Due to memory allocation on CPU ,5166
18864,pytorch in place operator issue in numpy conversion,see .storage().data_ptr() ,9571
18865,customdataset give me error,check init ,6808
18866,is the sgd in pytorch a real sgd,It shoul dbe SGDStepper,4596
18867,runtimeerror number of dims dont match in permute,"This is the problem: the mask is two-dimensional, but you've given it three arguments. permute().I'm assuming you're converting the image from h x w x c to c x h x w. However, it appears that the mask is only in h x w format.",8845
18868,in pytorch is there pdf logpdf function for distribution,use this link  https: //pytorch.org/docs/master/distributions.html?highlight=distributions#module-torch.distributions,9546
18869,how to create computational graphs for updated parameters,There are sepearte library to do that ,2936
18870,neat way of temporarily disabling grads for a model,"I don't believe there has been an update. The for loop is straightforward and the most efficient option here.That would be tricky, especially with your special logic of things already not requiring gradients.To make it a little cleaner, you can add a method to your q model module yourself.",2921
18871,why autograd will accumate gradients,Accumulating the gradients gives you the ability to scale them manually afterwards without enforcing any assumptions on your use case.,8615
18872,complex functions exp does not support automatic differentiation for outputs with complex dtype,use the  latest version 0f 1.7 ,2923
18873,optimizing parameters of function generating convolution kernel instead of raw weights,use  nn.functional.conv2d ,8654
18874,where is the actual code for layernorm torch nn functional layer norm,use this link for  CPU C++ implementation https://github.com/pytorch/pytorch/blob/392abde8e64b0d91b7d52aecee8dce9aff8d0b2f/aten/src/ATen/native/layer_norm.cpp ,8579
18875,how can i apply l2 l1 loss with 3d voxels,You can directly apply both mentioned losses ,8573
18876,different init for training ensembles,"Setting the seed at the start of the script would cause the pseudorandom number generator to produce deterministic “random” values. Because the sequence of the random number generation is defined by the seed, creating multiple models in the same script would result in different parameters, but the values would not be the same.",5875
18877,torch lstsq output size incorrect,use this link https://github.com/pytorch/pytorch/issues/56833,8869
18878,on the fly image rotation cpu bottleneck,try Albumentatuins ,2760
18879,my program stops at loss backward without any prompt in cmd,Due to different OS ,2979
18880,creating input for the model from the raw text,load data with torchtext abstraction ,4692
18881,model before after loading weight totally different,use amp.scale_loss.' ,677
18882,cudaextension for multiple gpu architectures,update to new version or remove the cache of old version ,1153
18883,edge case with register hook,"The problem is that your hook actually waits on the other thread to finish because it is dependent on it.The problem is that because the hook is blocked while waiting for this, another thread cannot run backward (this current thread can though).So you'll either want to run this other thread backwards in the same thread as the hook, or you'll want to run it forwards in a different thread. Or, alternatively, do not block the hook while waiting on that backward.",2947
18884,network in q learning is predicting the same q values for all states,Scale   to 0-1 instead of -1 to 1,4340
18885,how do i map joblibs parallel function to pytorchs distributeddataparallel,use  torch.multiprocessing.pool,11308
18886,how to get the batch dimension right in the forward path of a custom layer,use pytorch .dot function ,10521
18887,unexpected key in state dict bn1 num batches tracked,use this link https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/2,3162
18888,allow size mis match in autograd forward vs backward,"use this x, z_g) → M_AND_PADDING → z_p",8006
18889,gradients exist but weights not updating, Try changing the weight,2935
18890,variables are not updated after loss backward and optimizer step,"In fact, the computational graph was broken into two different places, due to two wrong operations.",2435
18891,the second order derivative of a function with respective to the input,The problem is that your function is linear. So the first gradient is constant and the second order gradient is independent of the input.,2930
18892,loss backward time increases for each batch,Check Memory ,1920
18893,weight of layer as a result of dot operation,Use the functionals instead of the convolutional module. Functionals takes weights as inputs.,8086
18894,runtimeerror mat1 dim 1 must match mat2 dim 0 cnn,"use Flattenand reshape (batch_size,-1)",3963
18895,error on torch load pytorchstreamreader failed,Save the file size properly,4600
18896,the code that was working previously gets stuck at loading the checkpoint file that is cached on system,Reboot the machine,9519
18897,nn transformerencoderlayer 3d mask doesnt match the broadcast shape,Upgrde the version ,5955
18898,solved runtimeerror expected object of device type cuda but got device type cpu for argument 2 mat2 in call to th mm,use this self.attn = Attn(hidden_size) ,4613
18899,my implementation of self attention,queies are outputted from w_v,3031
18900,resume training validation loss going up increased,use same datasets ,6804
18901,lstm text generator repeats same words over and over,check on each batch outputs,4614
18902,what is the exactly implementation of torch embedding,It should eventually call into this method for the forward pass,3981
18903,how would i do load state dict in c,it is inpython and it will be same for  cpp also ,7019
18904,compiler c not compatible with the compiler pytorch was built, ArchLinux is a independent linux distribution,499
18905,unable to access my nets parameters,use  class ConvNet : public torch: :nn: :Module,11270
18906,converting simple rnn model from python to c, use this link https: //github.com/prabhuomkar/pytorch-cpp/tree/master/tutorials/intermediate/recurrent_neural_network ,5981
18907,using nn module list in c api,use this link https://github.com/pytorch/pytorch/blob/cd0724f9f1b57dae12be2c3fc6be1bd41210ee88/torch/csrc/api/include/torch/nn/modules/container/modulelist.h#L11 ,477
18908,gradient clipping in pytorch c libtorch, Same eay implement in test,7347
18909,futex wait hang,OMP and autograd  does it by default ,2899
18910,can we split a large pytorch built in nn module to multiple gpu, you can simply create 8 different Linear that each take a subset of the input and split the input yourself and call each of these Linears and then add all the results,2894
18911,sharing model between processes automatically allocates new memory,"The only way around it is dedicating one process to hold the pytorch module and act with the other processes in a producer-consumers pattern, which is a real headache when it comes to scalability and much more for RT application",3986
18912,how to split a pretrained model for model parallelism, use nn.sequential and .to to move output from one GPU to another ,17
18913,build pytorch gpu for different gpu archs,answered in the GitHub issue,4024
18914,confused about distributed data parallel behavior,use this torch.cuda.set_device() ,2918
18915,loss calculation within batch iteration,The two loss calualtion approaches are same ,7791
18916,best way to handle variable number of inputs,use *args and **kwargs,8414
18917,model to cpu does not release gpu memory allocated by registered buffer,"No, you cannot delete the CUDA context while the PyTorch process is still running and would have to shutdown the current process and use a new one for the downstream application.",11452
18918,implementing a custom convolution using conv2d input and conv2d weight,convert  into a loop function at conv2d_weight,2901
18919,why criterion cuda is not needed but model cuda is, Critertion don't have parameters but cuda has parameters,7113
18920,debugging memory allocations in torch autograd grad,enable anomly mode ,2903
18921,integrated gradients for rnns,use  the  technique on the output of the embedding layer,2937
18922,minibatch size by iteration,Divide the accumulated loss by the number of accumulation steps,8671
18923,getting cant export a trace that didnt finish running error with profiler,,5960
18924,pytorch lightning number of training and validation batches,total number of batches (training + validation),3263
18925,how to load imagenet, Shuffle the data  ,7348
18926,masking out locations in convolutional kernels,you can do some simple tests like running your layer on a ones tensor input and checking that the results are what you expect based on the mask,3054
18927,rewriting a crnn model with the same architecture gives different results than the original,,2453
18928,dynamically replacing the last linear layer,try running on CPU ,5983
18929,unsure of output dimension and loss type newbie,Check character RNN,8640
18930,improving nmt model outputs,Replacing numbers with  would also be very easy with a RegEx. but its probably too naive for the general case ,5095
18931,implement a keras model using pytorch doesnt learn,the data need to be shuffled in train loader,4901
18932,transformer mask doesnt do anything,Insert SOS and EOS tokens properly ,3079
18933,cant use from blob to construct tensor on gpu in c,use this link https: //github.com/pytorch/pytorch/issues/15426‚ ,583
18934,error in cmake while setting up libtorch,change cudnn.h to cudnn_version.h,9141
18935,how to mask tensor with boolean using c api how to achieve this python code with c api,use masked_scatter,11305
18936,libtorch ubuntu runtime error, use correct path address,746
18937,during deserialization torch load fails at debug while it works fine in release mode unhandled exception at 0x00007fff7de1a308 in test exe microsoft c exception c10 error at memory location 0x000000cdee5bd950 occurred,update to latest version of build ,7249
18938,per tensor channel quantization equivalents in pytorch caffe2,use this link https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py ,8052
18939,quantized squeeze block mobilenetv3,"use this link https://github.com/pytorch/pytorch/pull/30442, since the tensor iterator supports broadcast ",8556
18940,cannot quantize nn conv2d with dynamic quantization, for nn.Linear and nn.LSTM   Dynamic quantization is currently supported . use this link https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic ,2825
18941,when quantized max pool2d is used,use tis link https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Pooling.cpp#L128,8515
18942,assertionerror torch nn quantized relu does not support inplace,use this link https://github.com/pytorch/pytorch/pull/33105,10681
18943,conv2d unpack and conv2d prepack behavior,check this code https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp ,1442
18944,net in dataparallel make training aware quantization convert model acc error, follow the steps from this link https://gist.github.com/vkuzo/78b06c01f23f98ee2aaaeb37e55f8d40,7398
18945,construct quantized tensor from int repr,use this link https: //github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3862 ,11356
18946,tied conv1d and conv transpose1d not geting the same result as the input, reault should not be identical ,3220
18947,dynamic quantization error mixed serialization of script and non script modules is not supported, first quantize your net and then script it  ,3964
18948,pytorch1 5 0 win7 64bit didnt find engine for operation quantized conv2d prepack noqengine,use the VS 14.11  build ,8298
18949,did pytorch support int16 quantization,use fp16 dynamic quantization,8255
18950,dose static quantization support cuda,currently it works only on CPU ,4323
18951,quantized model consists of relu6, repalce relu6 with relu ,6885
18952,loading of quantized model,"Make sure you create the net using previous definition, and let the net go through process that was applied during quantization before (prepare_model, fuse_model, and convert), without rerun the calibration process.After that you can load the quantized state_dict in",2868
18953,problem in computing loss in multiple cpu distribution training,use PyTorch Distributed  Data Parallel module,8038
18954,how to link a custom nccl version,Check the build versions,8590
18955,training with ddp and syncbatchnorm hangs at the same training step on the first epoch, set the random seed that samples the probability of which alternating loss it will perform,8695
18956,dataparallel and conv2d,using types.MethodType to replace methods in a network is incompatible with DataParallel,7011
18957,how dose distributed sampler passes the value epoch to data loader,"the sampler is passed as an argument when initializing the DataLoader, so the train loader will have access to the sampler object.",7265
18958,the ddp seem to be disable to find the second node,use Distributed communication package - torch.distributed ‚ PyTorch 1.7.0 documentation,3375
18959,why the output of children part of a network has low resolution,I guess your model isn't able to create sharp images and you could check the literature for new architectures,1408
18960,build model from submodels,use ModuleList,3218
18961,how to install pytorch 1 3 0 or above with cuda 8,install using build package ,6801
18962,custom mean of tensor partitions,look into PyTorch scatter package ,3372
18963,functional linear may cause runtimeerror one of the variables needed for gradient computation has been modified by an inplace operation,Use model input ,2434
18964,how to remove the grad fn selectbackward in output array,The detach() in the no_grad block is not needed.,2926
18965,can i get gradients of network for each sample in the batch,Use simple NN,3584
18966,question about loading the model that was trained using 4gpu with distributed dataparallel to only 1 gpu job,"Create  the model   loading the state_dict, and push  the model to the single GPU",4025
18967,how to deploy different scripts on different gpus, call .to(device) ,8614
18968,distributeddataparralled not support cuda,"bucket_view.copy_(grad.view({-1}), /* non_blocking */ true); in mark_variable_ready_dense. ",2165
18969,using custom method in distributed model,original model <- scatter <- model replicas <- replica output <- gather <- final output.,8961
18970,unable to load waveglow checkpoint after training with multiple gpus,use rank == 0,7825
18971,strange behavior nn dataparallel,run model using NCCL_P2P_DISABLE=1,6857
18972,loss collection for outputs on multiple gpus,If you are using nn.DataParallel the model will be replicated to each GPU and each model will get a chunk of your input batch.,3558
18973,default collate fn sending data to cuda 0,set CUDA_VISIBLE_DEVICES env var befor launching the model  ,2761
18974,distributed gpu calculations and cuda extensions,"data = torch.randn(4, 100) chunks = data.chunk(4,) res = [] for idx, chunk in enumerate(chunks): res.append(my_fun(chunk.to('cuda: {}'.format(idx))).to('cuda: 0'))res = torch.stack(res)",8433
18975,question about torch distributed p2p communication, The message will directly send from  10.0.0.2 to 10.0.0.3.  and also use  torchpc ,2811
18976,behavior of dataloader when resuming training from the existing checkpoint,"Keep shuffle=True, but when you fetch a sample, run a tiny function that prints out the id that's being fetched/processed. Continue training in the same way as before (option 1), but instead of working with a single slice from shuffle=False, use the ids you've saved to slice out a portion of your dataset.",1275
18977,architecture of deeplabv3 resnet50,use this https: //github.com/szagoruyko/pytorchviz,4891
18978,create diagonal matrices from batch,"torch.diag_embed (torch.rand (size = (M, N))) ",2860
18979,how to display incorrect samples predicted by the model,Try img = img.squeeze() before calling ax.imshow(img),2751
18980,moving tensor to cuda,a = a.to(device='cuda'),3555
18981,efficient implementation of jacobian of softmax,use pytorch’s swiss army knife of tensormultiplication functions to construct a batch version of outer,2859
18982,pytorch can not move tensor to cuda,use updated version ,5932
18983,torch from numpy not support negative strides,"use torch.from_numpy(np.flip(x,axis=0).copy())",9529
18984,dropout argument input position 1 must be tensor not str,update the latest version ,2818
18985,discrepancy in matmul while batching,use this link http: //perso.ens-lyon.fr/jean-michel.muller/goldberg.pdf,2863
18986,iterabledataset never stops despite stopiteration being raised,set drop_last=True,4608
18987,bizarre cuda behavior with nn linear,update the latest version ,1926
18988,simultaneously multiple block indexing in a torch tensor,"Instead of using the for loop for the whole matrix, i just have to use it for the block_matrix size",9558
18989,gaussianblur transform causing error,use this link https://discuss.pytorch.org/t/quickstart-tutorial-array-takes-1-positional-argument-but-2-were-given/125550/2 ,7803
18990,dynamically set conv2d based on input channels,"    def reset_parameters(self, x): self.conv = nn.Conv2d(x.size(1), self.num_classes, kernel_size=1).to(x.device)self.bn = nn.BatchNorm2d(self.num_classes).to(x.device)",8547
18991,inferring shape via flatten operator,use torch.nn.Linear module,1310
18992,is it possible to execute two modules in parallel in pytorch,"If you call p1 and p2 after each other when running the code on a GPU, these calls will be queued on the device and executed asynchronously. Depending on p1's workload, p2 may begin while p1 is still running. If your system has numerous GPUs (and you aren't using data parallel), you could run p1 and p2 on each one and then concatenate the results on a single device.",3556
18993,detected that pytorch and torch scatter were compiled with,install the new package,5925
18994,typeerror tuple object is not callable,remove the trailing comma ,7342
18995,typeerror cant convert cuda 0 device type tensor to numpy use tensor cpu to copy the tensor to host memory first,use targets = targets.argmax(1).cpu().numpy(),1316
18996,expected tensors on same device,"X,y=X.to(device),y.to(device);",2757
18997,initialization of first hidden state in lstm and truncated bptt,"zero initial hiddenstate is standard so much so that it is the default in nn.LSTM if you don’t pass in a hidden state (rather than, e.g. throwing an error). Random initialization could also be used if zeros don’t work.",8538
18998,which copy is better,with torch.no_grad():x.copy_(k),2928
18999,dataloader parameter shuffle affects model accuracy,"This is to be expected given the calling model. eval() disables dropout layers and uses the running statistics of all batchnorm layers.If you leave the model in training mode, the batchnorm layers will normalise the inputs using the current batch stats and will also update the running stats. Thus, rearranging the dataset makes a difference, which is why you should call model. During validation and testing, use eval().",8925
19000,expected scalar type long but found float,"I'm not clear how the kernel size relates to the batchnorm layer's input shape.However, because batchnorm layers require to generate the stats from the input, you won't be able to utilise them in training mode if the input just contains a single value for each channel (as is the case here).You might eliminate these layers from the model because calculating the var from a single sample (which would result in NaNs) and subtracting the mean from a single value would result in a zero output.",3345
19001,smush multi channel tensor into one image channel with maximums,"preds = torch.argmax(output, dim=0) ",7888
19002,how to split dataset into test and validation sets,"torch.utils.data.random_split(dataset, lengths).",8603
19003,bcewithlogitsloss giving negative loss,"calculate loss using nn.CrossEntropyLoss,",8658
19004,how to backpropagate a transformer,nn.Transformer.forward method,1377
19005,learnable parameter as int not tensor,"consider a model with several weighted branches (where the weights are differentiable and learnable), where each branch has a different maxpool_1d configuration. The weights of each branch are updated during the architecture search training process and the branch(es) with the highest weight(s) are chosen.",7147
19006,how to unnormalize output of batch norm,model. train and model.eval()  in training mode normalizing the input batch with its own stats and updating the running stats and normalizing the input batch/sample with the running stats. no need to apply any running stats,7177
19007,understand dataset and dataloader,use torch.LongTensor([index]),9059
19008,ask about run my model with parallel device,DDP should be faster as it reduces the communication overhead in nn.DataParallel.,2978
19009,do loss backward and optimizer step with different frquency, scale the loss itself (divide with a constant) or use hooks to manipulate the .grad attributes of all parameters.,1412
19010,windowed tensor duplication,use tensor.unfold ,8621
19011,check the norm of gradients,for p in model.parameters(): param_norm = p.grad.data.norm(2) total_norm += param_norm.item() ** 2 total_norm = total_norm ** (1. / 2),929
19012,assertionerror torch not compiled with cuda enabled,Install NVIDIA driver ,8582
19013,batchnorm1d input shape,"batchnorm layer expects an input in [batch_size, features, temp. dim] ",8672
19014,why is pytorch ok with this wierd operation and not fail,For this part what I can tell you is that nn.CrossEntropyLoss() does take output as one-hot encoded and targets with indices that isn’t encoded into one-hot. Internally it converts it into one-hot encoding and computes the loss.,5882
19015,understanding time taken from moving data from gpu to cpu,"  synchronize the code before starting and stopping the timer usinguse torch.cuda.synchronize(), as CUDA operations are called asynchronously ",1506
19016,should i flatten before the linear layer,"Yes you. should have something shape of  , (batch_size, linear_in),  (batch_size, linear_out),, ",8536
19017,runtime error size mismatch m1 2048 x 1 m2 2048 x 1,"use self.out=nn.Linear(128,1)  and Conv1d wants (batch size x input_channels x seq_length) as input shape (with input_channels = ‘hidden_dim’ here)",8030
19018,neural net pytorch error multi target not supported,"use loss = loss_fn(outputs, labels.long())",7414
19019,help runtimeerror assertion cur target 0 cur target n classes failed,Use nn.BCELoss or nn.BCEWithLogitsLoss ,8084
19020,handling the hidden state with minibatches in a rnn for language modelling,"No, the sentences in a batch are processed independently and in parallel in one go.",4326
19021,expected object of scalar type long but got scalar type float for argument 2 mat2,change to torch.set_default_tensor_type('torch.cuda.LongTensor'),2758
19022,effect of vocabulary size on gpu memory slow training,"create a embedding matrix of size (vocab_size, embed_dim) , nothing should change when using pretrained embeddings",4221
19023,cuda runtimeerror cuda error device side assert triggered occuring only with some data,you can either run on the CPU or set CUDA_LAUNCH_BLOCKING=1 before launching you script on the GPU.,8591
19024,creating a sparse tensor from csr matrix,"Convert csr format matrix to coo , you can use spacy's coo format it looks similar to  pytroch sparse tensors ",8609
19025,clarification regarding the return of nn gru,"output.shape = (seq_len, batch, num_directions * hidden_size), so you get a (context) vector for each item in your sequence h_n.shape = (num_layers * num_directions, batch, hidden_size)",5883
19026,clarification on backpropagation for lstms,"No, I was only referring to LSTM. It encases an LSTMCell in order to support multiple layers, dropout, bidirectionality, and other features. An LSTMCell can only ever accept one word as input. LSTM is a full layer that can output entire sequences. It's just that no one is stopping you from giving it 1-length sequences. An LSTM with num layers=1, bidirectional=False, and dropout=0.0 that accepts one word at a time should behave similarly to an LSTMCell. Stick with LSTM for the time being, and consider LSTMCell if you require more control over the recursion.",5096
19027,when shuffle is true for dataloader what is the default sampler,sampler = RandomSampler(dataset),3457
19028,what is the most standard way to put data in batch,"In the first approach train_inputs and train_labels seem to be undefined,  he DataLoader loop is different, since you are unpacking the values in the first approach inside the loop",3049
19029,what is the most efficient way to get the closest pair of a set of vectors,"computational cost of your pdist solution is a suboptimal O(d * n^2). If you search on closest-pair problem, you will see that your problem can be solved in O(d * n * log n)",2849
19030,installing pytorch fails on macos,use this conda install -y pytorch torchvision torchaudio -c pytorch -c conda-forge,8047
19031,np pi equivalent in pytorch,use import torch torch.pi = torch.acos(torch.zeros(1)).item() * 2 ,4315
19032,an iterabledataset implementation for chunked data, Start a featurerequest on github ,6841
19033,return item in forward hook,the forward hook won't terminate the execution,4325
19034,saving torch distributions with state dict,"The main problem is because Distribution objects are immutable, which means you can't just push nn. In __init__, parameters are contained within them. Maintain parameters in modules, construct transient Distribution objects in forward, then save and train as usual.",8960
19035,training routine inside model definition class,Inherit torch.nn.Module and define missings things,8480
19036,updating the transform class variable of a dataset instance,"Overriding the dataloader's collate method is the modern solution for this (calling default collate and then optionally augmenting the dataset). You could also create entire replicas of your dataset. In the end, it's just a matter of running idx through a randperm generated via index indirection and tweaking the length.",8453
19037,check if tensor elements in a value list,"sum(a==i for i in vals).bool() tensor([[ True,  True, False],[ True,  True, False]])",8555
19038,typeerror new received an invalid combination of arguments got tensor int,ou are trying to pass the inputs directly to the initialization of the model,8661
19039,save torchsummary,"result, params_info = summary_string(model, input_size, batch_size, device, dtypes)",1339
19040,using for loop to train model multiple times produces vastly different results,"Based on the description, it appears that your overall training is insecure, and the program's success rate may be poor. You could retrain N models with different seeds for each run and see how many times the model converges correctly vs. how many times the model fails. You might try using different parameter initialization methods, for example, to stabilise it.",1406
19041,legacy autograd function with non static forward method is deprecated and will be removed in 1 3,mean = BinActive.apply(x),8025
19042,how to use netcdf data for dcgan,unsqueeze dim0 and dim1 as x should already contain the values to create the batch and channel dimension.,9803
19043,very slow training on gpu for lstm nlp multiclass classification,"Since you initialize h0 and c0 you don’t need to detach them as well. Maybe that’s even bad for the training itself,",2942
19044,valueerror expected input whentraining bert,"Input batch size should be (512,1",7125
19045,understanding input shape to pytorch conv1d," PyTorch by default uses a NCHW memory format, with tensor dimensions structured accordingly. In other words, tensor dimensions are (batch, channel, height, width).",3241
19046,unboundlocalerror local variable loss referenced before assignment,"get_batches might not return anything, so it might cause the error ",3110
19047,ulmfit fine tuning freeze," try this for name, params in your_learner_object.model.named_parameters(): if params.requires_grad: print(name)",8022
19048,typeerror cannot assign torch floattensor as parameter layer weights torch nn parameter or none expected," try this self.layer_weights = nn.Parameter(F.softmax(self.layer_weights,dim=0))",10662
19049,training loss is not changing at all while training lstm,Remove  F.softmax at the end of your model and pass the output of self.lin(3) ,10040
19050,runtimeerror size mismatch m1 1 x 30 m2 20 x 128 at c w 1 s windows pytorch aten src th generic thtensormath cpp 752," use  model=NGram(len(vocab),10,3)",8669
19051,runtimeerror expected object of scalar type long but got scalar type float for argument 2 mat2,"nn.Linear expects the input as a tensor of shape [batch_size, *, in_feautres].",1923
19052,rnn working on cpu shows loss but cannot send to gpu," use torch.randn(10, device='cuda')",8324
19053,retrieving hidden and cell states from lstm in a language model,"LSTM would return for you output, (h_n, c_n). In your case, (h_n, c_n) is named hidden. So by indexing hidden you can extract the h_n and c_n (i.e hidden[0] = h_n and hidden[1] = c_n)",2941
19054,randomly initialized embeddings for torchtext, use  torch.from_numpy(np.random.rand(...)).float() ,7478
19055,runtimeerror cuda out of memory after many epochs,use torch.cuda.memory_stats(),8683
19056,module list activation function, torch.nn.ReLU is a module ,2853
19057,rescale image data 0 255 to 1 1,"Given that torchvision.transform.ToTensor already standardize the data to [0, 1], you could just multiply that by 2 and subtract 1 on your input layer.",2721
19058,loading a tensor from file in batches,use numpy.memmap,1409
19059,a100 is slower than 1080ti with pytorch,install the latest release,8674
19060,how to get batch norms running stats such as running var and running mean in pytorch,my_net.bn.running_mean my_net.bn.running_var,8571
19061,parameters initialised by nn parameter not present in the model parameters,"use pytroch equivalents such as nn.ParameterList, nn.ModuleList, nn.ModuleDict ",8512
19062,encounter the runtimeerror one of the variables needed for gradient computation has been modified by an inplace operation,x += 1 or x[0] = 1  changes the data variable in place ,11426
19063,how to train a network with two outputs,"recon = model(img) # x, x2 loss = criterion(recon, img), loss = criterion(recon[0], img)",8679
19064,softmax cross entropy loss,"use this links for CrossEntropyLoss (https: //pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), and BCELoss(https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) ",4588
19065,how to change cudnn version in pytorch,use cudatoolkit=10.2,8619
19066,how to process variable length sequence of images with cnn,"pack_padded_sequence(images, image_seq_lens, batch_first=True, enforce_sorted=False) to produce packed_images.Run the CNN on packed_images.data to get packed_states_data.",6158
19067,python dictionary in the model not trained,use nn.ModuleList ,4842
19068,problem with dimension size lstm," if batch_first=True argument, the input should be [batch_size, seq, feature]",8676
19069,problem in making embedding layer for a cnn document classification,"nn.EmbeddingBag != nn.Embedding, use nn.Embedding  ",10052
19070,packing in rnns, use thi slink https: //towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e 1,31
19071,pack padded sequence with unsorted question length, use enforce_sorted=False,8503
19072,optimization of performance,"So you would have to change the last linear layer to nn.Linear(512,1) and use nn.Sigmoid",3587
19073,noob model with embedding bag not learning problem with my model, use nn.BCEWithLogitsLoss or F.binary_cross_entropy_with_logits. instead of sigmoid  and add a relu and another linear layer to it   ,4014
19074,multi head attention forward and batch dimension index,Linear consider anything that is not the last dimension as batch,2891
19075,lstm training loss does not decrease,The correct way to access loss is loss.item(),6780
19076,lstm layer dimensionality," input and output tensors are provided as (batch, seq, feature) if batch_first = True ",9579
19077,lstm input and output dimentions,"set batch_first=True,  expected dimensions of the input  to be in  (batch, seq_len, input_size) ",8622
19078,lstm hidden states issue," you can resolve the num_layers*num_directions with h = h.view(num_layers, num_directions, batch, hidden_size)",3566
19079,lstm error when using a saved model for prediction,"use this model = charGen(n_letters,512, hidden_dim=512).to(device)  # This is exactly how I had my model during training model.load_state_dict(loaded_model.state_dict())",3108
19080,lstm error in second epoch only on gpu, use inithidden() and use this link  https: //pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html,2178
19081,clarification on weightedrandomsampler,"If you want to have balanced batches, you should not use replacement=False.",4998
19082,loss function and lstm dimension issues,"use F.log_softmax(x, dim=1), and ",10810
19083,last hidden state in bidirectional stacked gru,"use this hidden = hidden.view(num_layers,2, batch, hidden_size) # 2 for bidirectionallast_hidden = hidden[-1] ",3589
19084,how to handle a memory expensive step in the model to support higher batch size,"If the intermediate tensors in vocab process require device memory and thus raise the OOM issue, your loop approach might work (assuming vocab process returns the correct output for sub-tensors). Because Python uses function scoping, you don't need to delete the intermediate tensors because they'll be freed when you exit vocab process. If you encounter an OOM before exiting the method, the del methods may still be useful.In-place operations do not allocate new memory, but instead directly manipulate the tensor.",3428
19085,how do i use nn crossentropyloss for seq2seq where my predication is of size bs seq len vocab size and truth of size bs seq len,"permute the dimensions in your predication tensor   to   [batch_size, nb_classes, seq_len]",8023
19086,error expected object of scalar type long but got scalar type float for argument 2 mat2,use this link https: //pytorch.org/docs/stable/nn.html#torch.nn.GRU,3377
19087,embeddings shows index out of range if padding id is 1,"use embedding = nn.Embedding(10,3, padding_idx=-1) ; tokens = torch.tensor([1,2]).long()",60
19088,efficient way to use torchtext tabulardataset on large dataset,use tokenize = lambda x:x.split() ,3590
19089,creat a tensor with random number of 1 and 1,"use a = torch.Tensor([-1,1]) idx = np.random.choice(2, size=your_shape, p=[r,1-r])",3376
19090,quantized cat running time is slower than fp32 model,us ethis link pytorch/pytorch/blob/f6c46df856d0588360db5b807960d1fc5e888c36/aten/src/ATen/native/quantized/cpu/qconcat.cpp#L61-L66,7630
19091,how to extract individual weights after per channel static quantization,"use layer[0].weight().int_repr().data[i,j,l,m ]",4558
19092,the parameters saved in the checkpoint are different from the ones in the fused model,"Pytorch developers could easily insert the actual exception args into this functions  but unhelpful message, allowing them to better debug the issue at hand.",6876
19093,loading a dynamically quantized transformers model,If you convert the model to quantized model and then load the quantized state_dict it should work.,8409
19094,post quantizing conv1d prelu layernorm layers can be done,"You can insert QuantStub, DequantStub blocks around the code that can be quantized. ",4666
19095,error in qat evaluate,use this link https: //discuss.pytorch.org/t/issue-with-quantization/67457,3237
19096,mobilenetv2 ssdlite quantization results in different model definition,"use lq_model = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)",11439
19097,quantization not decreasing model size static and qat,use this link https: //github.com/pytorch/pytorch/blob/530d48e93a3f04a5ec63a1b789c19a5f775bf497/torch/quantization/fuse_modules.py#L63,2857
19098,quantstub dequantstubs for qat confusion, use this model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm'),30
19099,implement selected sparse connected neural network,use self.networks = nn.ModuleList,7219
19100,how to know the memory that is allocated from your model on gpu,use print(torch.cuda.memory_summary()),8534
19101,how to take three tensor elements one by one,"new_tensor = [] for i, j, k in zip(a,b,c): new_tensor.append([i.item(),j.item(),k.item() ])",4226
19102,how to use a self argument in a function define outside the class attributeerror function object has no attribute data,use args as a place holder,8501
19103,efficiency of dataloader and collate for large array like datasets,you should not use DataLoader in the default random read style.,3708
19104,difference between seq len and input size in lstm,  input_size derives from the size of your word or character embeddings,8504
19105,convert lstm from keras to pytorch,"If that's what you want, one thing you'll need to do is pass batch first to the LSTM instantiation.While using the last timestep (as with lstm out[: ,-1,:]) is certainly a common way to set up sequence-to-one problems (assuming your inputs are all the same length), I would not call it a size adjustment. It is stated that the LSTM should memorise the pertinent information. Other options are available in a hurry. In ai's (Howard and Ruder) ULMFiT, they recommend concatenating the last timestep with the average and maximum over time (of course, the linear would have 3 * hidden size inputs).",5945
19106,combine train and test set torchtext but concatdataset object has no attribute get vocab,random_split will return Subsets,10554
19107,cant find inplace operation that messep up backwards,"use dist = M_2 - self.distance_function(x_in, prototypes[0].unsqueeze(0))",6847
19108,batchnorm1d for evaluation, define a Permute layer and use it inside your nn.Sequential container,3487
19109,batch examples in document,use this class class EvalIterator(data.Iterator): ,3203
19110,batch dimension for calculating loss,"use output.transpose(0,1)",19
19111,average of the gru lstm outputs for variable length sequences,use this out = padded.sum(dim=1).div(lengths.float().unsqueeze(dim=1)),8616
19112,attention implemented using encoder outputs rather than hidden states,"output, (h, c) = lstm(inputs, hidden)",7042
19113,accessing the lstm properties,"model = nn.LSTM(10,10,2,0.3, bidirectional=True)",8572
19114,proper distributeddataparallel usage,https: //pytorch.org/tutorials/intermediate/ddp_tutorial.html,5045
19115,distributed machine learning on multiple cores,use this tutorial https://pytorch.org/tutorials/intermediate/ddp_tutorial.html,3364
19116,multiple gpu model getting runtimeerror caught runtimeerror in replica 0 on device 0,try DistributedDataParallel ,4147
19117,distributed training creates multiple processes in gpu0, Set the right GPU context while calling clear_cache,2596
19118,is it expected for distributeddataparallel to use more memory on 1 gpu in a 1gpu 1process setup,torch.cuda.set_device(rank)),3885
19119,how to divide the dataset when it is distributed,"train_data, val_data = torch.utils.data.random_split(train_data, (num_train, num_val))",8459
19120,rpc with raw irecv reduce distributed primitives,use this link ,8526
19121,,,8583
19122,fine tuning a retinaface model. I'm having trouble mapping Pytorch's Resnet18 tutorial instructions to the Retinaface architecture.,"The in_features used in the tutorial represent the number of input features to the last linear layer.
Based on your code snippet it seems that the last activations are calculated in:
        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)
        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)
        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)

so you would most likely want to either replace these modules or the last layer(s) inside each of these modules.",7205
19123,"cgan image labels to linear layer returns nan ,The problem is that after a certain number of steps, <code>self.head</code> returns NaN values while none of the inputs contain NaN values.",The problem was not in the Discriminator but rather in an unstable module of the generator ,7231
19124,concatante k linear layers results to single output tensor,"torch.stack(res, dim = 1)",8611
19125,tensor shape for rnn batch training,"x.permute(1,0,2)",11427
19126,gru works even with incorrect initial hidden state,"This is a bug, and has been fixed on the master branch of pytorch",7594
19127,passing text as input to a cnn,the embedding is applied individually to each word of the input.,7300
19128,"runtimeerror inconsistent tensor size while using pre trained weights,RuntimeError: inconsistent tensor size, expected tensor [426 x 50] and src [114044 x 50] to have the same number of elements, but got 21300 and 5702200 elements respectively at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorCopy.c:121</code>","A vocabulary of 114044 distinct words was used to train the pretrained embedding.However, your embedding layer was set up with a vocabulary of 426 words.There are three options that come to mind...Create an embedding layer with a vocabulary of 114044 words and ensure that the 426 words in your new dataset use the 114044 word vocabulary's word indices.From the 114044 w, extract the embedding data for the 426 words.",7225
19129,how can i build an rnn without using nn rnn,CrossEntropyLoss basically combines a log_softmax with NLLLoss,6161
19130,"how to padding sequence of variable length in nlp task,",Try pad_sequence.,8011
19131,loading saved model with hacky weight sharing resulting in gibberish,"logits = torch.matmul(logits, self.embeds.weight.t())",3078
19132,my custom embeddding layer is returning only zeros,"outputs = F.embedding(inputs, self.weights * (self.dmodel ** 0.5), self.pad_idx, None,2, False, False)",1918
19133,"error loading bidirectional lstm model,When I loaded it using load_state_dict(torch.load()), it gives me an error says Unexpected key(s) in state_dict: ","Maybe the layer1.lstm in your trained model had two or more layers (l1 is the first), but the model you're loading only has one?",4229
19134,"rnn usage in pytorch, I see that people often use RNN in a loop and set sequence length to 1. I would like to ask if there is any different if RNN is run on the entire sequence without the loop. In addition, if RNN can be used that way, how is it different from RNNCell?","Many fascinating RNN designs use things other than what the vanilla RNN/GRU/LSTM can do - the classic is when one timestep's result affects the following timestep's input - one noteworthy example is attentiveness.You can no longer use multi-timesteps after you've got something like that. People may still choose them because they are familiar with them or because they provide a speed advantage, although the majority of applications are fairly comparable to the *Cell variants.",4206
19135,"masking recurrent layers,How can we mask our input sequences in RNNs?","For rnn, a simple working example of using packing for variable-length sequence inputs.",3268
19136,"what is nn embedding exactly doing,For example is a pre-trained embedding being used to project the word tokens to its hypothetical space? Is there a distance measure being used? Or is it embedding models like word2vec?","It's merely a table that converts indices to vectors. You can manually initialise them to work2vec weights, for example.",3936
19137,same model on scikit learn does well but fails in pytorch,Your loss feature CrossEntropyLoss raw logs and F.softmax applies to the end of your forward function. You either should not make the softmax or simply return tag space to a log softmax and change your loss function to NLLoss.,8678
19138,gradient of hidden state in lstm,use hx.retain_grad() ,2583
19139,embedding class in nlp tutorial,"The nn.Embedding class is a lookup  table that maps a vector to every index. It is actually a matrix, the weight matrix of the module of incorporation.When the vectors change slightly, the following calculation will change in a small quantity and eventually the loss (usually cross-entropy). Thus the vectors get a reverse gradient.You could use nn.Embedding to replace a single vocab size* dimensional parameter, using a [idxes] indexing, to get the vectors mathematically equivalent, without using fancy features such as gradient reversal.",7199
19140,does feeding input one by one affect gru output,It turned out that when I recreated the identical code in a more straightforward manner. The issue has been resolved.,3985
19141,"4 dimension input to lstm,My input is of shape (batch_size * num_sentences * sentence * embeddings). Any help on how to pass this 4-d input to an LSTM ?","Depending on the limits you impose on sentence dependencies, you could end up withSentences that are independent of one another are modelled separately. (num sentences, sentence, embeddings) (batch size*num sentences) (batch size*num sentences) (batch size*numParagraphs with distinct models: Each paragraph (sequence of sentences) is modelled separately. (num sentences*sentence, embeddings) (batch size, num sentences*sentence) (batch size, num sentences*sentence) (bAlternatively, you might use two layers of LSTM, with the first modelling and embedding individual sentences and the second using these sentence embeddings as input to model paragraphs.",2112
19142,why does code like fairseq override the default initialization of the nn embedding layer,"class Embedding(nn.Embedding): def __init__(self, num_embeddings, embedding_dim, padding_idx): super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx) nn.init.uniform_(self.weight,-0.1,0.1) nn.init.constant_(self.weight[padding_idx,0)",3366
19143,modifying a model without using nn sequential,"num_classes_in_new_model = 5 new_model = copy.deepcopy(old_model) in_features_last_layer = new_model.__getattr__('fc2').in_features new_model.__setattr__('fc2', nn.Linear(in_features_last_layer, num_classes_in_new_model))",8680
19144,model to device not identified error in c frontend,"nstead of m.to(device),we should use m-;to(device)",2848
19145,expected tensor not variable for argument 2 mat2, use  at: :Tensor for your input  as well as in forward instead of torch: :Tensor.,3112
19146,sparse tensor creation in the c api,I think this is the function you are looking for,3262
19147,"torchscript c extension list type,What is the type that torch considers as the proper list type for C++ when going from Torchscript to C++?",It’s a std::vector<std::string>,11275
19148,dcgan tutorial where is the output image size of the generator set,I found the answer myself,3098
19149,how to convert tensor entry to a float or double,"torch: :Tensor tensor = torch: :randn({3,4});",4107
19150,pytorch random number generation algorithm,Use curandStatePhilox now,1410
19151,why didnt this cuda kernel implement,use const torch::OptionalDeviceGuard device_guard(a.device()),8650
19152,parameters not changed after optimized,"If you allow two unsolicited comments:Many people claim that using std is a bad habit. Given that many people will read your code on the forums, I'd advise against doing this. Just 10 iterations would be fully sufficient so see the loss doesn't change. ",8660
19153,advanced tensor slicing in c,"The C++ equivalent of [...] indexing is torch: :index(self, indices) where indices is a vector of Tensors (which can be undefined)",6910
19154,terminate called after throwing an instance of c10 error what istensor for lstm,"output, hidden = model(examples.to(cpu, hidden)and at: :Tensor output = module-&gt;forward({indata1, tuple  }).toTensor();",1623
19155,vector of torch tensors to torch tensor,use torch::cat to concatenate along a single axis and torch::stack to create a new axis,4030
19156,initializing a tensor from a tensor c,sourceTensor.clone().detach().requires_grad_(),3760
19157,error when initializing registered tensors,,3135
19158,if i use scripting to convert my model how can i call other methods in c," use this at: :Tensor output = module->forward(inputs).toTensor(); with torch: :Tensor output = module -> run_method(weighted_kernel_sum, input).toTensor();",8623
19159,loss does not improve on training,"The learning rate was definitely set to 1.0, which is a little high but not ridiculous. Switching the learning rate back to 0.01 seems to be insufficient, as I'm still getting my nans.While I'm still having difficulties getting a decent standard deviation to operate, scaling the data by merely subtracting the mean seems to help. In the few tests I've run with the current settings, network loss is lowering and the network is converging. torch: :std(data,0)",6877
19160,how to replace specific values in a tensor,"auto a = t.accessor&lt;float, 1&gt;(); for(int i=0; i&lt;a.size(0); i++) if(a[i] == 0.0)a[i] = 1.0;",3045
19161,deleting tensors in context save for backward, Replace the .grad attributes with None to free these tensors as well instead of zeroing them out,8524
19162,check if model is eval or train, using module.training   ,9933
19163,backpropagation in a multi branched cnn rnn network,"Make sure you understand how pytorch works with a tensor's require grad attribute. The weights, etc., of your model, as well as the tensors that rely on them, should normally have requires grad = True. Normally, requires grad = False would be the input to your CNN. Your CNN's output, on the other hand, will contain requires grad = True.",2855
19164,modulenotfounderror no module named engine," use references/detection/engine.py , references/detection/utils.py and references/detection/transforms.py ",2614
19165,division in batches of a 3d tensor," a.dim() = b.dim()import torch n, m, p = 100,5,2 ;a = torch.rand(n, m, p);b = torch.rand(n, p);b = b.unsqueeze(dim=1) # n x 1 x p ;z=a/b",3280
19166,efficient way to get ordered permuations,Use itertools.permutations to permute your list.,3115
19167,how to load model weights that are stored as an ordereddict,"model = MyModel() optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # change to whatever optimizer was used checkpoint = torch.load(path_to_checkpoint) model.load_state_dict(checkpoint['model'])optimizer.load_state_dict(checkpoint['opt'])",7312
19168,torch nn ctcloss returns infinity for some inputs,"Well, for the input to represent the same label twice in a row, there must be a blank in between those two, as that's how CTC work",2864
19169,why do we need gradient checking,"If you only use PyTorch's built-in methods, you're probably not interested in testing utilities.Yes, because PyTorch is an open source framework with a large community of contributors, things can go wrong, which is why testing is necessary.",7819
19170,"store index of max values,How can find the top 3 max values in the tensor and how can I store the index values in one variable","This is a solution I found frame = sorted(range(len(mul_reward)), key=lambda i: mul_reward[i], reverse=True)[: 4] print(frame) con_frame = torch.Tensor(frame)",6779
19171,different time consumption of the same operation,"Yes, the nonzero operation will take some time to complete, but you are correct that your profiling is incorrect since it adds the model forward time to the nonzero operation, making it appear like the nonzero operation is more expensive than it actually is.As previously stated, this operation is synchronous and should therefore be avoided (in case you want to remove all syncs, if possible).I'd say it depends on the procedure in question, so you could profile it with synchronizations and compare runtimes if necessary.",8521
19172,skip weights in covolution,"We must use the functional form, torch.nn.functional.conv1d, because I assume the weight of a torch.nn.Conv1d must be a leaf tensor (). You may backpropagate through the creation of ker and appropriately optimise the two elements of k by utilising differentiable pytorch tensor operations to build ker from k.",2856
19173,what pytorch means by buffers,"Buffers are tensors that have been registered in the module and are thus contained within the state dict.Because these tensors don't require gradients, they aren't recorded as parameters.This is important for tracking the mean and standard deviation in batchnorm layers, for example, which should be stored and loaded using the module's state dict.",1468
19174,if i write a function using class name nn module and then call it 3 times will each call have its own parameters or will they share parameters,you instantiate the class WienerFilter() it will call the __init__ function,2920
19175,runtimeerror result type double cant be cast to the desired output type long,tgts.to(torch.float),4554
19176,concatenating images,you could use nn.BCELoss or nn.BCEWithLogitsLoss instead,3559
19177,cuda out of memory error during forward pass,"If you sum the losses (or store the references to those graphs in any other way), Autograd will save the computation graphs until a backward operation is performed.You may learn how to accumulate gradients by reading this post, which describes alternative ways, their calculation, and memory usage.",8492
19178,do pytorch containers come with cudnn installed,"PyTorch utilises the official cudnn release and dynamically or statically links to it.Because you're using the runtime container, you don't need to install nvcc:Use the devel container to develop applications inside the container:",32
19179,loading big dataset bigger than memory using pytorch,Use torch.utils.data  ,8595
19180,problem with backprop runtime error expected tensoroptions on cuda but got tensoroptions on cpu with function mulbackward0,"Could you please provide a simple executable code snippet that uses your classes to reproduce the problem?Also, what PyTorch version are you currently using? Could you perhaps update to the newest stable version (1.7.0) if you aren't already using it",1922
19181,log softmax and softmax,"Regardless of how you trained your model, you can do whatever you want in the test phase. (Just make sure the findings of the test phase imply what you want them to mean.)You mention CrossEntropyLoss. Raw-score logits should be the output of your model, and these will most likely be the output of a final Linear layer with no subsequent activation function. That's all right.If you wish to do something in your test phase with actual probabilities (rather than logits), use softmax() to convert the output of your model.",2767
19182,fourier transform and complex dtype restrictions,dim = 1,8641
19183,some additional output of forward function,Custom Function  MyFunction(torch.autograd.Function),3283
19184,how to add additional information for backward,"Because you return the original input inp (this is changed in 0.3, but before then, return inp.clone()), your reverse won't be called. Your backwards accepts much too many counter-arguments. Because the forward only returns a single argument, it should just take in grad out. Your reverse returns an excessive number of values. Because the forward only accepts a single argument, it should only return a single value.",8667
19185,torch model parameters by dictionary type,Use nn.ParameterDict  ,8104
19186,plot magnitude of gradient in sdm with training step,"for name, param in model.named_parameters(): print(name, param.grad)",8580
19187,gradient is none after applying custom mask,module1 should create valid gradients as long as mask2 isn't full of zeros ,9934
19188,backpropagation over summed hidden states in birnn,"If you don't need to use the output's.grad field afterwards, don't set requires grad=True on it.You also don't have to wrap everything with Variables.So, yeah, it should work properly.",2919
19189,where is the grad defined,"Modules lack a grad definition in favour of parameters or tensors.The attribute/property definition can be found here. Tensor.grad will be filled in after a backward operation or torch, for example. The autograd.grad function will return the gradient of X with respect to this tensor.",4260
19190,differentiate through sgd step,"Unfortunately there isn't as the  nn.Module is by design, not functional , and  refer this link https://github.com/facebookresearch/higher ",2934
19191,let custom autograd function know if create graph is enabled,"check if torch.is_grad_enabled() in the backward and if gd.requires_grad. Namely if grad mode is enabled and the input requires_grad then you should create the graph. Otherwise, it is not needed.",2902
19192,kernel size cant greater than actual input size,You must either reduce the kernel size to 1 or adjust your model before to this layer.,8677
19193,problem with autograd,"ds_loss.backward(inputs=(lr_list[j],), retrain_graph=True). Otherwise, you can do grad, = autograd.grad(ds_loss, lr_list[j], retain_graph=True",15
19194,"triplet loss backprop equations,However, I am still confused about the computation of backprop in this implementation. Specifically, as PyTorch accumulates the derivatives, the gradients of the triplet loss w.r.t. to the last linear layer (embedding) ","Correct, however these formulas only supply the gradients for the net's output, not the weights. And because the outputs are different Tensors, you never truly aggregate the gradient contribution of the outputs; just the weights are shared, and hence gradients are compounded.",5705
19195,reentering variable execution ending run backward twice error trying to backward through the graph a second time but the saved intermediate results have already been freed, surround  BERT generated embeddings with torch.nograd()  ,2815
19196,"torchvision totensor dont change channel order,When you use transforms.ToTensor() , by default it changes the input arrays from HWC to CHW order.Is it possible to have ToTensor not do this if my input data are already in the ordering I want?What can I replace this code with to make it keep the ordering I want and still convert to a torch tensor?","the reason that PyTorch is doing this is that, PyTorch uses CHW convention by default.",1317
19197,"torchvision totensor dont change channel order,When you use transforms.ToTensor() , by default it changes the input arrays from HWC to CHW order.Is it possible to have ToTensor not do this if my input data are already in the ordering I want?What can I replace this code with to make it keep the ordering I want and still convert to a torch tensor?","the reason that PyTorch is doing this is that, PyTorch uses CHW convention by default.",1317
19198,"runtime error expanded size error,in this case bias.shape = torch.Size([1104])  and result.shape = torch.Size([1,1104,15,20])I am getting the error in the line: result += bias.unsqueeze(0).expand_as(result)","Due to a discrepancy in the number of dimensions, the error is increased.New dimensions are added to the front of the expanded tensor, so that the missing last dimensions are unpressed:",7045
19199,"runtime error expanded size error,in this case bias.shape = torch.Size([1104])  and result.shape = torch.Size([1,1104,15,20])I am getting the error in the line: result += bias.unsqueeze(0).expand_as(result)","Due to a discrepancy in the number of dimensions, the error is increased.New dimensions are added to the front of the expanded tensor, so that the missing last dimensions are unpressed:",7045
19200,handling large 3d image dataset with dataloader,"The DataLoader num workers are executed with multiprocessing on the CPU and will not be displayed on the nvidia-smiIf you face bottlenecks in your code, for example data loading, GPU use may be low.See this post, which is good explanation and suggests certain improvements in performance, that's not right. The model uses the device and PyTorch won't use the CPU automatically based on DataLoader performance",10840
19201,handling large 3d image dataset with dataloader,"The DataLoader num workers are executed with multiprocessing on the CPU and will not be displayed on the nvidia-smiIf you face bottlenecks in your code, for example data loading, GPU use may be low.See this post, which is good explanation and suggests certain improvements in performance, that's not right. The model uses the device and PyTorch won't use the CPU automatically based on DataLoader performance",10840
19202,"data sampler to handle class imbalance,Is there a way in PyTorch to make such data loader and/or data sampler to handle aforementioned scenario?",use WeightedRandomSampler ,3585
19203,"data sampler to handle class imbalance,Is there a way in PyTorch to make such data loader and/or data sampler to handle aforementioned scenario?",use WeightedRandomSampler ,3585
19204,training process keep getting killed ,gdb --args python train.py --model....,9132
19205,training process keep getting killed ,gdb --args python train.py --model....,9132
19206,"how to reset gru, Should I reset hidden state of GRU to predict MOS value for another video file? because hidden state of GRU may affect different behavior?,Should I reset hidden state of GRU to predict MOS value for another video file?<br>because hidden state of GRU may affect different behavior?",nn.GRU module expects the intial hidden state,6909
19207,pytorch training network with cudnn inference network,etting CUDA_VISIBLE_DEVICES  or CUDA_LAUNCH_BLOCKING after creating the CUDA context wouldn’t have any effect.,8053
19208,"difference between adam and adamw implementation,What is the difference between the implementation of Adam(weight_decay=…) and AdamW(weight_decay=…)?", check the paper behind AdamW,10428
19209,"even though the seed is fixed i still get different results,I even add torch.manual_seed(seed) and torch.cuda.manual_seed_all(seed) in init() of every model or submodel. The trained models still got different performance on test dataset.",torch.use_deterministic_algorithms(True),2756
19210,typeerror new received an invalid combination of arguments got tensor int but expected one of torch device device didnt match because some of the arguments have invalid types tensor int,"model = ResNet18(num_classes)train(train_dataset,model,criterion1,criterion2,optimizer,epoch=2,result_directory=result_dir )",5948
19211,splitting unbalanced classes of images into training validation and testing sets,stratify=class_array[train_idx] ,8608
19212,,you need to call  optimizer.zero_grad()to reset the gradients at each step.,2861
19213,"combine several features,How can I use several features in my net, e.g. current token, next token, current pos-tag, next pos-tag?","Two embedding matrices, one for tokens and the other for pos tags, may be required. And the network looks like this: input token index -&gt; token embedding matrices -&gt; token embedding input pos-tag index -&gt; pos-tag embedding concatenate token embedding and its corresponding pos-tag embedding",8635
19214,"is there any comprehensive torchtext tutorial,, I was wondering that is there any comprehensive “official” tutorial for torchtext package?",A Comprehensive Introduction to Torchtext (Practical Torchtext part 1),3598
19215,"confused by pytorch embeding layer,I have a word index tensor, it was converted to python list type after I pass it to an embedding layer! What’s the problem?",Bug in the code,4020
19216,"what kind of attention mechanism used in seq2seq tutorial,",It uses only the Bahdanau   & Luong approach.,3087
19217,Convert int * into tensor,torch::from_blob ,8487
19218,compilation c extension using recentpytorch version and cuda 1 20 and 10 1,"Maybe it's because the version it's compiled with and the one it's running with aren't in sync?Unless you're using an internal API, I believe this is the most typical cause of error.",4223
19219,"source code for conv2d,Can someone point to the cpp code for conv2d. I would like to know how pytorch implements convolution internally.","Hi, We try to use high-performance libraries like cudnn and mkldnn as much as possible. In practise, their code is most likely to be the one that is executed when conv is called. There you can find the general implementation we have that works in all cases: for gpu and for cpu.",8292
19220,"how to create multiple distributeddataparallel tasks on a single node, I’m trying to start 2 training tasks on a single node and I want each task to occupy 2 GPUs respectively.",A general overview of the Env initialization method is the environment-variable-initialization. See also the aid of the launch utility running with assistance You will discover that you try to use the same port on localhost to find each other in a single task. Specify and it will work for a different port for each task.,2872
19221,"gradient scaling in federated learning,What should I do if I want to scale some workers’ gradient since their data are more valuable than others? ","Tensor.register hook(customHook) may work, but you must write customHook to change the tensor's grad. However, as far as I know, customHook should only be a function of grad. In your case, do you want customHook to be a function of both grad and workerRank?",10812
19222,accessing tensors present on different gpus,"Then, you can compute what you want and send the result back wrapped in a tensor via a <code>dist.scatter</code>. To make sure the tensors don’t change on the gpu before they are gathered, ensure all nodes call into <code>gather</code> at the same time when all nodes have the desired value. You could also use torch.distributed.barrier if you need additional synchronization. Check out the docs at https://pytorch.org/docs/stable/distributed.html",25
19223,"memory issue of using nn dataparallel, ’m currently using nn.DataParallel for mutli-gpu (8-gpu) training in a single node. However, if I put the data and model to devices[0], I found the memory on GPU 0 will be huge and make the program exits (cuda out of memory) at the begining of training.",We generally recommend using DDP,7497
19224,"loading of duplicated data in distributed train,I would like to pretrain BERT by using DDP","Did you keep the entire dataset in a single tensor? If that's the case, I believe you should load it once and store smaller chunks of data (and load only certain chunks in each process) or load the data lazily from the start.",2139
19225,change my username,"Sure, it should be good now",6159
19226,"gradient of output of network with respect to parameters,I need to calculate the gradient of output of network with respect to the parameters of network ( say with respect to first layer weights).",torch.autograd.functional.jacobian,10994
19227,"pytorch 1 8 and rtx 3070,Should we except any performance changes from pytorch 1.8 Cuda 11.1 in comparison to pytorch 1.7.1 with Cuda 11?",For the best performance on your 3070 you could build from source using the latest cudnn release,8659
19228,how to include gradient of nn wrt the input in the loss function,"I believe the error is caused by using retain graph=True and repeatedly updating parameters, which would be incorrect because the gradient would be stale.",3111
19229,what step backward and zero grad do," use them in the other order - opt.zero_grad(), loss.backward(), opt.step().",2956
19230,why loss backward didnt update parameters gradient,Use self.Bi_weight = Parameter(torch.Tensor(self.weight.shape).cuda()) or better yet just leave the .cuda() alone and do model.cuda() at the end,3754
19231,custom loss function runtimeerror tensor does not have a grad fn,"Remove Variable(output),Variable(target) torch.tensor(error_diff,requires_grad=True) loss = Variable(loss)",8546
19232,custom autograd function must it be static,"Option (1) is the traditional method of defining functions. This does not support gradients of gradients, and it may be phased out in the future (not sure when).The second option is the best option. Note that you can save arbitrary data in the ctx (in the same manner that you could save data in self in (1)), and the apply method that calls forward accepts any parameter, so you can just send whatever you passed to the __init__() function here. That is, instead of defining options globally, you may simply send them to the forward method.",2924
19233,"can gradient back propagate through circular padding,What about circular or reflect mode? In these two modes, the padding values are the sliced results of the previous layer, so that the gradient can back-propagate to previous layers","As a result, the gradients will flow back to the input corresponding to wherever it is used (including padding).",8484
19234,error running multiple models in torch 1 7 1 but works in torch 1 0,"Yes, if you're using stale gradients as indicated in the 1.5 release notes (explained in the torch), the inplace changes of parameters are now throwing an issue. Optimizers were updated to fix in-place tests for the optimizer section's changes).The reason for this is because the gradient calculation would be wrong. In your case, you'd use the model parameters in the starting state s0 to calculate loss1 and loss2. loss1.backward() calculates the gradients, while opt1.step() updates the parameters to state s1. loss2.backward() was calculated using the model in state s0, and so calculates the gradients of loss2 w.r.t. parameters s0, even though the model has already been updated to s1. These are the grades",8519
19235,pytorch jvp slow Are there any known issues on using JVP? Are there any methods to make it more efficient (maybe using other packages)?,"As noted in the function definition note, the function computes the jvp using the double backward trick,hence it is expected to be slower.We're working on a solution for 1.7.",9513
19236,cuda invalid configuration error on gpu only,Batch size of 1023 works but >= 1024 throwas error,6828
19237,different train result when just defining extra layers in code,"By running the pseudorandom number generator to initialise all parameters, you will affect all subsequent random calls. This is normal behaviour, and we've discussed it before on this site. ",3849
19238,tensor backward low level computation,"If you don't modify it, Autograd will record the dtype and the backward computation will be performed with the same precision as the forward component.This also means that your FP16 gradient updates could easily underflow, which is why we advocate using torch.cuda.amp's mixed-precision training tool.",8518
19239,control the training batch,"I assume you're shuffling the data in your training DataLoader, resulting in random samples across all batches.If you want to use the same image, you may access it by indexing the dataset with the following command: data, target = train loader.dataset[index].",3116
19240,how to get the gradients for both the input and intermediate variables,You'll get the gradient value for y after calling y.retain grad().,8673
19241,why does autograd track things like torch cat,"Yes, the torch.cat operation is tracked by default, thus based on your description, you'd append the model's outputs to the entire computation graph, then backpropagate via all of these outputs and the model.",8478
19242,confusing about autograd mechanism with pytorch1 8 0,"The problem is that older versions of Pytorch didn't properly treat the optimizer step as an inplace operation. As a result, despite computing incorrect gradients, this check was not performing properly and did not raise an error. This has been resolved in the newest master, and in this instance, the error is expected.The issue is that you require the parameter values to compute the backward, but optimizer.step() updates them in the middle of the computation. As a result, you'll have to wait until the entire reverse process is completed before proceeding to the next step. Alternatively, you can perform the forward after the step to use the updated weight values.",2929
19243,pytorch no computation graph mode,"Setting with torch to prevent the autograd from saving anything. no grad(): is sufficient.The other difficulty you may encounter is how you assess GPU memory, as Pytorch uses a proprietary allocator, and so memory provided by the OS is not always accurate.Out = relu(out) also computes the relu first, allocating a temporary output in the process. The original content of out is then discarded in favour of the temporary one. As a result, I'm expecting a memory blip here.It's worth noting that most activation functions provide an inplace flag that forces them to update in place, avoiding this problem.",2933
19244,one of the differentiated tensors appears to not have been used in the graph,"By skimming through the models, I am unable to identify any glaring flaws.Try adding print statements to all models' forward procedures and using print(x.grad fn) to see if the activations have a valid.grad fn.The statement should yield None at some point in the model, and the prior operation should remove the tensor from the graph. Also, double-check that you haven't disabled the gradient calculation worldwide, such as using torch.set grad enabled (False).",3034
19245,error grad can be implicitly created only for scalar outputs,"Your loss is very certainly a multi-dimensional tensor, resulting in this inaccuracy.You might apply a reduction or pass a gradient that has the same shape as loss.",1279
19246,when should we set torch backends cudnn enabled to false especially for lstm,"Thus if you call model.eval(), you won't be able to calculate the gradients using the cudnn RNN implementation. As described before, calling:model.eval() model.rnn_layer.train()",3996
19247,difficulty using multiprocessing num workers,torch.set_default_tensor_type('torch.cuda.FloatTensor'),3114
19248,how to calculate integral of function,"You can only approximate the integral as a sum, as you guessed, because the integral you're dealing with is also an expectation, so your entire formula can be viewed as the expectation over X,Y, and delta. In my opinion, you can create a loader over your original dataset that returns the perturbed X' in addition to X and Y. Then you can continue with the rest of the training as usual. Another oddity in your case is that your model $pi omega$ appears to be non-deterministic. However, I believe that in practise, this will be a neural network, so the second expectation on X' can be ignored, and you can simply compute the loss using the output of $pi omega$.",1323
19249,what are the main reasons for receiving runtimeerror stack expects a non empty tensorlist error for torch stack, Based on the error message a None object will be returned which then fails in the <code>torch.stack</code> operationCould you try to load the mentioned images using the image lib and check what the output would be?,4022
19250,how to free gpu memory changing architectures while training,"We have a custom allocator so you won't see it on nvidia-smi, even if the memory is released, but can use it in pytorch.only if you no longer refer to the memory is realised. You may want to pack your inner loop into a function to remove (and release) all intermediate results between loop iterations.",2862
19251,how to solve resnet overfitting,"I got better results after switching from Adam Optimizer to SGD. However, validation accuracy and loss saturate quickly.Curves of learning (before early stopping)",2731
19252,how shift an image vertically or horizontally,"shift = transforms.RandomAffine(degrees = 0, translate = (0.2,0.2))",8651
19253,"transfer learning with mixed precision,I am currently trying to figure out how to facilitate mixed-precision training when using transfer learning",replace model.classifier = nn.Identity()  and just call the model directly,5933
19254,"resnet 50 takes 10 13gb to run with batch size of 96, I have been working on using a ResNet-50 and have images of shape (3, 256, 256) and I’m trying to run it in batch size of 96, but I keep getting an error stating that it ran out of CUDA memory.","If you don’t want to train the model and would like to save memory by not storing the intermediates, wrap the forward pass into with torch.no_grad(),",812
19255,torchvision cifar 10 and imagefolder cifar 10 has different behaviors in torchvision preset networks,shuffling the CIFAR-10 dataset in the dataloader.,7292
19256,"failedpreconditionerror tagger master8 train is a directory,while running a training session of semantic role labeling. I`m using python 2.7 (anaconda) with TensorFlow 1.12 on Ubuntu 18.04.",This was due to the directory structure being improperly organised...,4902
19257,"where did gru implement,I want to know how GRU is implemented in pytorch. ",There also are CuDNN bindings…,7367
19258,"i want to serve my chatbot, How can I do it? ","There is now a tutorial on writing an API around your model using Flask, which can be found at https://pytorch.org/tutorials/intermediate/flask rest api tutorial.html. Your react-node app would then simply call it. Keeping in mind that there is probably a bit more you want to do before deploying to production, such as serving the Flask app with gunicorn or something similar, another approach would be to look into ONNX - I don't know much about that. I usually deploy models with Django or Flask.",4016
19259,how to get the minimal subset of libtorch for inferencing in c,"To the best of my knowledge, this is not currently possible. This is primarily due to the fact that we have a dynamic dispatcher, and thus which part of the library will be used is unknown at the time of compilation. That being said, I believe there is progress being made in this direction, as it would be very useful for the mobile build as well. However, I do not believe there is a solution at this time.",10925
19260,"range of a tensor in c,I’m currently writing some C++ code with torch (to eventually make it a PyTorch extension).",Subtensor add and division in libtorch,10654
19261,how can i cuda code to support mixed precision,Look at nvidia/apex for some use cases ,8336
19262,"using torch grid sampler 2d from c api,I can’t find the values for the interpolation mode and border mode, neither in the docs nor in the code and this doesn’t compile:<br>torch: :grid_sampler_2d(img, flow, torch: :kBilinear, torch: :kBorder, true)", I use grid_sample now and it works as expected,6800
19263,loss goes down on gpu0 but up on other 3 when using distributeddataparallel,"DDP should have kept all model replicas in sync, i.e., all model replicas should have the same parameter values. Could you please check if this is true in your use case, say by using all_gather to collect all model parameters into one rank and compare?",3039
19264,"how to share cpu memory in distributed training,Is there any method that I can train the model with DDP mode?","One possibility is to use a torch. multiprocessing. As the shared memory, use Queue 24. The main process can prepare multiple queues before passing one to each DDP process. The main process reads from the file and sends data items to the queue, while DDP processes wait for that data item on their own queue.Another possibility is to divide the tar file into smaller pieces and have each DDP process read a different one.Yes, but there is one caveat: those input data splits must produce the same number of DDP input batches. If rank 0 processes three batches and rank 1 processes four, rank 1 will hang on the last batch.",4657
19265,"distributed pagerank with pytorch,I am trying to implement Pagerank with libtorch.","It can be possible using APIs like send/recv APIs, collective communication APIs and RPC APIs",3980
19266,"change rank of machines manually,I am trying to deploy a crash-resilient distributed deployment in PyTorch. ", torchelastic is built for this purpose,9581
19267,"intersection between 2d tensors , is there a way for PyTorch to calculate the intersection of two 2D tensors?","Check (a==b) *a, result.nonzeor()",1562
19268,can not execute backward after expand,The temporary fix here is to remove the print of b between the no_grad block and the line where you use it for the sum.,2925
19269,"determinism of gradient accumulation,Am I correct in assuming that adding CPU operations like above makes gradient accumulation (for tensor shared_ctx) non-deterministic?","Yes, you are correct that the accumulation is not forced to run in a specific order when using multiple devices. A simple solution is to use a custom function that copies to the various devices in the forward direction and accumulates in the backward direction (in a fixed order in your custom backward). Is that something you'd be interested in?",8491
19270,"is there any way of accessing the output from intermediate layers,is there any way of somehow accessing the intermediate results in the intervening layers, from passing the batch through those layers, to get the output?","To save memory, we make every effort not to save all of these no. As a result, you cannot guarantee that they are saved.You could make use of global nn.Module forward hooks are used to force the saving of some results during the forward pass so that they can be accessed later",7913
19271,why do the generated samples have require grad false,You can use torch.normal instead of numpy.,10526
19272,cuda error an illegal memory access when fine tuning groupnorm,use CUDA_LAUNCH_BLOCKING=1 python script.py args,1927
19273,sharing a parameter between multiple loss functions graphs,"This solution would pass the same param mat to all optimizers, who would update it with its gradient, and I believe that's the use case you're after.",7461
19274,"step activation function,Is there a step activation function in pytorch?","You are free to create your own activation function. I believe there is a great tutorial on the Pytorch website. However, I believe your function is not differentiable, so you may need to be cautious when using this function.",8633
19275,function which outputs tensor by reference,"You don't use detach or no grad, therefore your convmatrix2d function will track the gradients correctly",10534
19276,whats wrong about the grad of softmax when i use just some of the inputs to do softmax,"Because F.log softmax uses the log-sum-exp method to boost stability, torch.log(torch.softmax(...)) is quantitatively less stable than F.log softmax.",7302
19277,incorrect hook being used in register hook implementation,"Use h = module.weight.register_hook(lambda grad, gradient_mask=gradient_mask: grad.mul_(gradient_mask))",6846
19278,simple categorical cross entropy model not learning,"out = torch.tensor(X, requires_grad=True).matmul(w0).add(b0).relu().matmul(w1).add(b1) out = torch.nn.functional.log_softmax(out, dim=1)",6790
19279,inplace parameter updation without torch no grad,"It would be prohibited because the weight update would be done in a differentiable manner.So, when computing the gradients in the next iteration, it will loop back over numerous iterations of your training loop, which is most certainly not what you want!",2898
19280,"how to get the bounding boxes in feature map space,Since width and height have been reduced to 1/16 can I interpolate and estimate the coordinates of the objects in the feature map?","There are 2 main operations for this: ROI-pooling and ROI-aligning. Basically, each bounding box is a certain region of interest (ROI), which is first projected onto the feature map. They differ in the way features are computed for a certain ROI.",3302
19281,how to downsample an image before feeding to generator,you can use  pix2pix models and chec  some implementations and reuse the logic for the generator,11468
19282,why no dropout in last layer of rnn,"As a result, the dropout in the final layer would be based on the RNN output. This means you can do it yourself on the output if necessary, which is not an option for the inner layers.It should be noted that the dropout implemented by the RNN is not the dropout that uses a single random draw for all timesteps.",5947
19283,"singular value decomposition svd,Computing Singular Value Decomposition (SVD) in Pytorch always give me this error"," inputs contain nan, thats why I got an invalid svd.",3235
19284,using torchtext for a multiclass classification problem,LABEL = data.LabelField(),9371
19285,pretrained bert package for windows in anaconda,use State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch,6069
19286,"convert array to tensor in c, I have to give a matrix as input to my network, which is a standard C++ 2d array, and I can’t find a method to transform my data to a tensor.",use torch::from_blob() function to read an array and turn it to a tensor,2599
19287,"libtorch statics library vs dynamicss library, What is the current state of statics libtorch statics library is it as stable as the dynamics one ?, It is advisable to use statics library instead of dynamics library?,Why there is no option to download the statics library from download menu at pytorch.org website ",Static linking libtorch doesn’t work well ,6072
19288,"converting vector of double to tensor issues,I am trying to convert an std::vector to torch:Tensor with the following code:","The module parameters interacting with your input tensor should probably be of the same type as the input tensor. I believe torch: : would be easier to work with. Because the kFloat tensor is used, you should convert your data to float before using from blob to create the input.",7179
19289,"initializing a model instance before loading,would there be a way to pull that information from a saved file prior to loading so I could “dynamically” initialize a UNet object that will have a valid structure for storing the data from the saved file ?",You could use some of the utility functions from this topic or directly load your jitted model (which seems to be the recommended way),8620
19290,using autograd grad function in c,use latest version,4023
19291,"cuda extension install without rebuilding,How do I move a CUDA extension after it has been build?","If you want, you can specify the build directory and share it among your Docker images.",8593
19292,"changing specific elements in tensor,I’m looking for some function or any efficient way to change specific elements in a tensor.<b","torch.where(x &gt; 5, x+100, x)",9580
19293,"multi gpu training on single node with distributeddataparallel,When I train with DistributedDataParallel do I get the functionality of DataParallel, meaning can I assume that on a single node if there is more than one GPU then all GPUs will be utilized on that node?","Yes, DistributedDataParallel (DDP) can use multiple GPUs on the same node, but it does so in a different way than DataParallel (DP). DDP employs multiple processes, one for each GPU, whereas DP employs a single multi-threaded process. See this page for a comparison of the two, and this for more information on getting started with DDP.",8473
19294,"how to balance gpu memories in ddp,When training a model with DDP, GPU for rank 0 consumes much higher memory than others.<br>Because of that GPU, I cannot increase batch-size for training.",I solved the issue by setting torch.cuda.set_device(args.local_rank) which works the same as setting CUDA_VISIBLE_DEVICES.,3146
19295,strange behavior in pytorch,i moved the data from HDD to SSD and it solved my problem,8316
19296,initialize tensor in autograd function,Keep in mind that the gradient for the l1 loss can be 1 or -1 depending on the sign of the difference! This will ensure that you do not continue in the same path indefinitely.,26
19297,custom nn conv2d,"Input.size(2) and input.size(3), respectively, should probably be defined for h and w.You're now giving both the same input value. They can, however, be fully deleted because you are giving information directly.As previously stated, you are passing input, therefore I'm not sure what image you should use.You're also moving the weight forward, which eliminates the need for filt. Additionally, you are now re-initializing filt in each forward pass, which will not save the previously learnt weights, so instead use weight for your operations.",7421
19298,"accessing update values,Is there a way to access the update values computed by the optimizer?","I'm afraid that, in most cases, there isn't another way to get that. As an aside, some optimizers, such as lbfgs, may actually perform the step as multiple in which case checking before and after is really the only option.",4021
19299,propagation trough 2 identical networks but do not accumulate gradients w r t the second pass,"Even if you compute dDistanceLoss(E(x hat),z.detach()) / dx hat for your second loss, it could conflict with the reconstruction loss. I believe that using non-detached z for the second loss is more promising... In any case, the above-mentioned manipulations may accomplish your goals or temporarily disable. For all nn.Parameters, requires grad is required.",2943
19300,grad attribute of a non leaf tensor being accessed,"This warning only signifies that you're trying to access a Tensor's.grad field, which pytorch will never populate.You can execute your code with python -W error your script.py to have Python throw an error when the warning occurs, allowing you to see exactly where it occurs.The main issue here is that only leaf Tensors will have their.grad field filled in. So if you get this warning, it means you mistook anything for a leaf when it isn't. This typically occurs when you do operations on a Tensor that require gradients, such as foo = torch.rand(10, requires grad=True) (device). Because foo is the result of the.to() operation, it will not be a leaf in this case.",2932
19301,how to train domain adaptive model,"Both approaches should be effective. The first would use less memory and more compute because you'd be making two backward calls (more compute), but Autograd will be able to delete the intermediate tensors from the first forward pass because they're no longer needed (gradients were already computed using them)The latter approach would store both computation graphs, requiring more memory.",1455
19302,"how to preprocess input for pre trained networks, when using the pretrained networks of torchvision.models module, what preprocessing should be done on the input images we give them ?","All pretrained torchvision models go through the same preprocessing, which is to normalise using the mean/std values .",1091
19303,very small class weights,"If you suspect rounding errors or other numerical issues, you could multiply the weights. Because the weights are used in a relative manner, you should be able to add a constant offset to the tensor.",8612
19304,error loading state dict gpt2 finetuning,There masked_bias was introduced. ,9734
19305,how to plot saliency map from rnn model in nlp task,just set the drop out layer to eval by adding model.dropout.eval() ,7429
19306,pytorch cuda speed,"Following the creation of the extension, the next step is to create a custom CUDA kernel and reduce the kernel launch overheads, as described in the tutorial. Rewriting parts in C++ (or CUDA) and fusing specific groups of operations is thus a definite way of speeding things up. Fusing is the process of combining the implementations of multiple functions into a single function, which benefits from fewer kernel launches as well as other optimizations we can perform with increased visibility of the global data flow.",1296
19307,use of lossclosure in optimizers,"auto step = [&](OptimizerClass& optimizer, Sequential model, torch::Tensor inputs, torch::Tensor labels) ",7323
19308,"torch from blob memory types,My question now is how does the situation differ if the memory is not linear but instead a volume as a Cuda Array.  This type of structure doesn’t really have a pointer in the same sense and under normal circumstances one would have to use surf3Dwrite() and surf3Dread() in Cuda kernels.  If the input is in this format, what is the recommended way to run inference on it?","Regrettably, from blob can only accept a raw memory pointer (along with the size and stride information). As a result, you won't be able to use a torch Tensor to wrap complex data types.However, if only one copy is acceptable, you can dump it into a new contiguous memory buffer and use it.",8049
19309,"link libtorch with make instead of cmake,For a project I am forced to link LibTorch with a Makefile, I can’t use cmake. Now looking into the LibTorch","You can do it, but keep in mind that the pytroch code changes frequently, and symbols may change. As an example, we may want to split some of the. so as to reduce the size of those.soWe strongly advise you to use cmake.",8575
19310,memory leaks in libtorch,"Thanks for the analysis, and I believe it would be worthwhile (as with your previous check) to create an issue on GitHub to track it.",6842
19311,c10 macros cmake macros h not exists,I have a CPP android project. I’m compiling it using ndk-build (ndk21 and C++14).I have a file named “main.cpp” and I’m trying to #include “torch/script.h” .,3092
19312,"does distributeddataparallel split data at the batch dimension,How the batch will be split? Will the batch first split into two and thus each node will get a batch of data size 32, and finally, each node will split the data among the four GPUs, thus each GPU will get a batch of data size 8? is this the way the will be split in DistributedDataParallel mode?","Because each node has four GPUs, each node will launch four processes, each with its own dataloader and DDP instance. As a result, each node will have four data loaders. If you want to run 64 batch sizes across two nodes (8 gpus), each data loader should load 64 / 8 = 8 data sizes.",3795
19313,error occurs in rpc pipeline code in tutorial.AttributeError: ‘torch.distributed.rpc.TensorPipeRpcBackendOptions’ object has no attribute ‘num_send_recv_threads’Could anyone please tell me what possible reasons could be?,,3197
19314,"distributed data parallel over the internet,how to ensure before running the training script on each remote node that the 2 remote nodes have access to each other?","To run the script, you don't need to ssh from node-1 to other nodes. I only mentioned ping/ssh to see what IP would work. You can set a node as master if you confirm that the IP of that node is accessible to all other nodes. This is only for rendezvous, and all nodes will automatically discover each other using the rendezvous process.",4328
19315,"why are many batches loaded on each gpu,When I use DDP package to train imagenet, there are always OOM problem.<br>I check the GPU utilization and I found there are many processes on each GPU ？<br>What is the reason and how can I avoid this problem ?","This should not occur, and each process should use a single GPU, resulting in a single CUDA context.Is the script calling CUDA operations on all devices, or did you write device-agnostic code that only uses one GPU?",7812
19316,modifying resnet18 architecture,"Yes, you may use any valid shape as long as it does not break anything. Because powers of two are often friendly for memory access patterns and so on, you may experience performance plateaus or cliffs. Yes, as in 1. I'm not intimately familiar with the package, but it appears that multiple inputs are supported.",8533
19317,freeze part of model parameters cause cuda error,"When you enable anomaly detection, the NaNs will trigger an error. If you are not concerned about it, you could disable anomaly detection and handle the NaN values separately.The idea of using this utility is to get a runtime error in order to debug the issue.",9278
19318,how to use zero grad with gans,use loss_G.backward()optimizer_G.step(),5143
19319,warning nan or inf found in input tensor but input tensors do not contain nan or inf,def check_nan(array):tmp = np.sum(array) if np.isnan(tmp) or np.isinf(tmp):print('Warning: NaN or Inf found in input tensor.') return array,3104
19320,running mean and running stats,"The running stats are always updated when using batchnorm in training mode. To use these stats, you should use the eval mode and stop updating them when evaluating.",8385
19321,how could you manually stop backpropagation,"use autograd.grad(loss, input) and it will return gradients for that input and only perform the necessary computations to get that value.If you prefer for the .grad fields to be populated, you can use loss.backward(inputs=input)",8602
19322,how to do forward on part of my model,out = model.middle_layer1(middle_activation)out = model.middle_layer2(out),3229
19323,how torch calculates the grads for the scalar and non scalar tensors,"It always performs a vector Jacobian product, according to the theory. When the output is scalar, it is 1D, so the vector is of size 1 and can be replaced with just the value 1. This will provide you with the complete Jacobian (and thus gradients)",7108
19324,torchvision faster r cnn model input arguments,"The training only uses boxes and labels and ignores any other keys.  the image_id, area and iscrowd are used in evaluation",7343
19325,typeerror tensor argument device must be torch device not type,"test_iter = Iterator(tst, batch_size=64, device=torch.device, sort=False, sort_within_batch=False, repeat=False)",6810
19326,remove padding dimension from output layer,"The workout uses only boxes and labels and does not use other keys. The image id, area and iscrowd are used to assess, as the tutorial states (but not during training).",18
19327,"tensor to c vector , Is there a way to turn a 2d libtorch torch::Tensor into vector","This may not be general enough because it really depends on the storage of contiguous data: It is easy to step through the memory and build row vectors and vectors from those as column vectors when you have a knowledge of the type and size of the tensor and the source (which can be accessed via data ptr(). (I could be backward, but I could have had a procedure of that kind to work for me.)",7721
19328,"std vector stores inherited torch module objects,I have created several modules derived from torch::nn::Module. How can I use std::vector to store them? ","If you have any particular module vector requirements, I'm not sure.",2993
19329,current status of automatic quantization support,"Yes, graph mode quantization will be ready for testing",8502
19330,check if grad is enabled,"You may check it out by going to torch. The requires grad attribute of a Tensor returns True if it is necessary to calculate gradient on that Tensor. It's worth noting that it's contagious; if A.requires grad=True for some Tensor A, then all Tensors computed from A have the requires grad property set to True.",8570
19331,non gradient trackable convolutions,"You could either use the functional API with a kernel specified as a plain tensor (not an nn.Parameter):kernel = torch.randn(...) out = F.conv2d(input, kernel, ...)or set the requires_grad attribute of the weight (and bias) in the conv layer to False",8516
19332,best way to encode latent variable with cnn,"I think you might need a flatten in 1, too.",3281
19333,custom color mapping in data loader for unet image segmentation,"The GIMP fill tool I was using had anti-aliasing, which added extra pixel values to the mask boundary to smooth the edges.",2414
19334,Support multiple simultaneous LR schedulers,This issue has been fixed,845
19335,Support multiple simultaneous LR schedulers,This issue has been fixed,845
19336,Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/,You need to build from source,8307
19337,Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/,You need to build from source,8307
19338,Crash when reading pandas parquet file after importing pyTorch,You need to uninstall the pyarrow installed by pip and then reinstall with conda,11282
19339,Crash when reading pandas parquet file after importing pyTorch,You need to uninstall the pyarrow installed by pip and then reinstall with conda,11282
19340,"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict",You need to change the output of your model from dict to list.,5959
19341,"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict",You need to change the output of your model from dict to list.,5959
19342,dataparallel not working on nvidia gpus and amd cpus,It works fine on threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.,3006
19343,dataparallel not working on nvidia gpus and amd cpus,It works fine on threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.,3006
19344,Matrix multiplication operator,This issue has been fixed,10899
19345,Matrix multiplication operator,This issue has been fixed,10899
19346,Remove dampening from SGD,This issue has been fixed,4193
19347,Remove dampening from SGD,This issue has been fixed,4193
19348,ImportError: No module named _C,You need to open torch in any directory other than repo's root,713
19349,ImportError: No module named _C,You need to open torch in any directory other than repo's root,713
19350,"Install Error, OSX 10.11.6, fresh miniconda install",The error is because you have CC and CXX set,9564
19351,"Install Error, OSX 10.11.6, fresh miniconda install",The error is because you have CC and CXX set,9564
19352,MaxUnpool2d segfaults for some configurations,This issue has been fixed,4898
19353,MaxUnpool2d segfaults for some configurations,This issue has been fixed,4898
19354,[legacy-nn] losses need to return tensors and not numbers,legacy-nn should not be modified.,9556
19355,[legacy-nn] losses need to return tensors and not numbers,legacy-nn should not be modified.,9556
19356,Optim API: per-layer learning rates etc.,this can be solved by gradient rescale function,10893
19357,Optim API: per-layer learning rates etc.,this can be solved by gradient rescale function,10893
19358,Add logical AND/OR/NOT/XOR operations,This issue has been fixed,3673
19359,Add logical AND/OR/NOT/XOR operations,This issue has been fixed,3673
19360,Printing tensors is sometimes very slow,This issue has been fixed,3359
19361,Printing tensors is sometimes very slow,This issue has been fixed,3359
19362,numpy.__config__.show() for torch,This issue has been fixed,7582
19363,numpy.__config__.show() for torch,This issue has been fixed,7582
19364,Deterministic cudnn algorithms,Parallel data loader is now deterministic.,4725
19365,Deterministic cudnn algorithms,Parallel data loader is now deterministic.,4725
19366,torch dot function consistent with numpy,This issue has been fixed,7581
19367,torch dot function consistent with numpy,This issue has been fixed,7581
19368,automatically assign attributes that are variable as parameters?,Variables are not saved as attributes since they will be kept around and they won't free the graph nodes preceding it.,594
19369,automatically assign attributes that are variable as parameters?,Variables are not saved as attributes since they will be kept around and they won't free the graph nodes preceding it.,594
19370,embeddings layer with IntTensor / cuda.IntTensor inputs,This issue has been fixed,7828
19371,embeddings layer with IntTensor / cuda.IntTensor inputs,This issue has been fixed,7828
19372,requirements.txt: cffi >= v1.4.0,This issue has been fixed,2463
19373,requirements.txt: cffi >= v1.4.0,This issue has been fixed,2463
19374,More optimizers in torch.optim,They will be added soon,8523
19375,More optimizers in torch.optim,They will be added soon,8523
19376,"error: ‘float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)’ is deprecated caused by outdated MKL",You need to update your MKL.,7672
19377,"error: ‘float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)’ is deprecated caused by outdated MKL",You need to update your MKL.,7672
19378,How can i convert ‘at::Tensor’ to ‘const float*’ in libtorch?,This can be done by `.data<float>()`,8706
19379,How can i convert ‘at::Tensor’ to ‘const float*’ in libtorch?,This can be done by `.data<float>()`,8706
19380,Bug in CosineAnnealingLR (division by zero),This issue has been fixed,3348
19381,Bug in CosineAnnealingLR (division by zero),This issue has been fixed,3348
19382,Incorrect behaviour of min() and argmin(),"The behavior is consistent for CPU, but it maybe different for multi-GPUs",497
19383,Incorrect behaviour of min() and argmin(),"The behavior is consistent for CPU, but it maybe different for multi-GPUs",497
19384,[autodiff] Fix owning model used in `differentiate`,This issue cannot be resolved,8287
19385,[autodiff] Fix owning model used in `differentiate`,This issue cannot be resolved,8287
19386,Conv3d fail after curtain batch size,This issue has been fixed.,3019
19387,Conv3d fail after curtain batch size,This issue has been fixed.,3019
19388,[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57,This error can be avoided by using reshape instead of contiguous and view. ,521
19389,[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57,This error can be avoided by using reshape instead of contiguous and view. ,521
19390,librosa tests on Windows don't work,Please use `conda install numba`,3342
19391,librosa tests on Windows don't work,Please use `conda install numba`,3342
19392,Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators,This issue has been fixed.,7713
19393,Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators,This issue has been fixed.,7713
19394,[jit] support general buffer mutation,This issue can be resolved by `self.vector = u.data` ,4612
19395,[jit] support general buffer mutation,This issue can be resolved by `self.vector = u.data` ,4612
19396,libtorch C++ library does not compile properly,Please set the variable `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir),490
19397,libtorch C++ library does not compile properly,Please set the variable `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir),490
19398,Excessive call to cudaGetDevice and cudaSetDevice,Please use the hotfix here https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3,661
19399,Excessive call to cudaGetDevice and cudaSetDevice,Please use the hotfix here https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3,661
19400,[Tutorial]: Wrong example of Our Own Ring-Allreduce,"The following example is better \r\n```python\r\nsend_req = dist.isend(send_buff, right)\r\ndist.recv(recv_buff, left) # recv is a blocking operation.\r\naccum[:] += recv_buff[:]\r\nsend_buff[:] = recv_buff[:]\r\n```",685
19401,[Tutorial]: Wrong example of Our Own Ring-Allreduce,"The following example is better \r\n```python\r\nsend_req = dist.isend(send_buff, right)\r\ndist.recv(recv_buff, left) # recv is a blocking operation.\r\naccum[:] += recv_buff[:]\r\nsend_buff[:] = recv_buff[:]\r\n```",685
19402,the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON,This issue could be solved by using the `/INCLUDE` switch,3152
19403,the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON,This issue could be solved by using the `/INCLUDE` switch,3152
19404,PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available),Switching to older checkpoints can fix the problem,3744
19405,PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available),Switching to older checkpoints can fix the problem,3744
19406,test_conv_transposed_large_cuda failed on Windows,The failure is seen in CI.,7056
19407,test_conv_transposed_large_cuda failed on Windows,The failure is seen in CI.,7056
19408,[docs] torch.onnx.export docs contains two descriptions for example_outputs arg,This issue has been fixed.,2464
19409,[docs] torch.onnx.export docs contains two descriptions for example_outputs arg,This issue has been fixed.,2464
19410,The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch,This issue has been fixed.,1604
19411,The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch,This issue has been fixed.,1604
19412,Pip torch_nightly on macOS installs wrong build,This issue has been fixed,3009
19413,Pip torch_nightly on macOS installs wrong build,This issue has been fixed,3009
19414,how to set cuda stream by call Aten function,"1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \r\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits.",96
19415,how to set cuda stream by call Aten function,"1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \r\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits.",96
19416,`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function,This error is because we don't restore the status in `autograd.grad` properly.,6867
19417,`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function,This error is because we don't restore the status in `autograd.grad` properly.,6867
19418,dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache,You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.,2570
19419,dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache,You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.,2570
19420,Distributed Using Gloo on Multiple Nodes Does not Work,"Your ranks should be 0, 1, 2, 3 for world size 4.",3239
19421,Distributed Using Gloo on Multiple Nodes Does not Work,"Your ranks should be 0, 1, 2, 3 for world size 4.",3239
19422,The guidelines for loading a PyTorch model in C++ do not work on Windows,You'll need to copy the PDBs along with your executable.,8663
19423,The guidelines for loading a PyTorch model in C++ do not work on Windows,You'll need to copy the PDBs along with your executable.,8663
19424,Fix 1.3.1 branch submodule fbjni dependency,This issue has been fixed.,4536
19425,Fix 1.3.1 branch submodule fbjni dependency,This issue has been fixed.,4536
19426,Connection closed by peer when using L-BFGS and distributed computing (gloo),"It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary.",2343
19427,Connection closed by peer when using L-BFGS and distributed computing (gloo),"It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary.",2343
19428,"`torch.Size` is tranfered to`torch.Tensor`, values don't equal",`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.,8744
19429,"`torch.Size` is tranfered to`torch.Tensor`, values don't equal",`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.,8744
19430,"Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?","You need to specify the gpu device, before calling to `empty_cache`.",668
19431,"Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?","You need to specify the gpu device, before calling to `empty_cache`.",668
19432,Ruby Library,Users can execute :\r\n\r\n```sh\r\nbrew install libtorch\r\n```,1101
19433,Ruby Library,Users can execute :\r\n\r\n```sh\r\nbrew install libtorch\r\n```,1101
19434,[jit] Spurious error when type comments are found in the body of a function.,This issue has been fixed,4589
19435,[jit] Spurious error when type comments are found in the body of a function.,This issue has been fixed,4589
19436,[jit] Cannot create a `Tuple[List[T]]`,"This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length.",7641
19437,[jit] Cannot create a `Tuple[List[T]]`,"This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length.",7641
19438,RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED,This issue has been fixed,1596
19439,RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED,This issue has been fixed,1596
19440,failed to convert torch.jit.ScriptModule to ONNX (crash),"Please specify the parameter `torch.onnx.export(..., opset_version=11).`",630
19441,failed to convert torch.jit.ScriptModule to ONNX (crash),"Please specify the parameter `torch.onnx.export(..., opset_version=11).`",630
19442,[Bug report] RuntimeError: backward_input can only be called in training mode,The mode i.e. train/eval needs to be consistent throughout the training loop.,693
19443,[Bug report] RuntimeError: backward_input can only be called in training mode,The mode i.e. train/eval needs to be consistent throughout the training loop.,693
19444,[Caffe2] Windows build errors in generated file caffe2.pb.h,It's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\r\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\r\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\r\n\r\nSo...the solution is to patch protobuf...,3186
19445,[Caffe2] Windows build errors in generated file caffe2.pb.h,It's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\r\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\r\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\r\n\r\nSo...the solution is to patch protobuf...,3186
19446,Segmentation fault on importing torch,You have to make sure both are compiled with the same compiler version. \n Either compile PyTorch from source or install  GCC 4.9,3000
19447,Segmentation fault on importing torch,You have to make sure both are compiled with the same compiler version. \n Either compile PyTorch from source or install  GCC 4.9,3000
19448,[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times,"Use the following \r\n`x[..., None, None]` and `x[..., 0,0]` in your example",8415
19449,[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times,"Use the following \r\n`x[..., None, None]` and `x[..., 0,0]` in your example",8415
19450,np.repeat vs torch.repeat,The two APIs have a different behavior.,2246
19451,np.repeat vs torch.repeat,The two APIs have a different behavior.,2246
19452,[JIT]torch._C._infer_size throws an exception when traced,You need to pass tensors with the correct shape in the torch.trace annotation,2732
19453,[JIT]torch._C._infer_size throws an exception when traced,You need to pass tensors with the correct shape in the torch.trace annotation,2732
19454,question: where (if) are the caffe2 libraries?,"To install the Caffe2 libraries, use  'conda install -c caffe2 caffe2' ",1513
19455,question: where (if) are the caffe2 libraries?,"To install the Caffe2 libraries, use  'conda install -c caffe2 caffe2' ",1513
19456,failed to move parameters to GPU,"To fix this, use `self.conv = nn.ModuleList()`",10325
19457,failed to move parameters to GPU,"To fix this, use `self.conv = nn.ModuleList()`",10325
19458,"Inconsistent behavior of F.conv2d(...,padding) and F.pad",You need to set `torch.backends.cudnn.deterministic=True`,2734
19459,"Inconsistent behavior of F.conv2d(...,padding) and F.pad",You need to set `torch.backends.cudnn.deterministic=True`,2734
19460,"[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64",You'll need to do a clean build from scratch,8664
19461,"[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64",You'll need to do a clean build from scratch,8664
19462,"[feature request] Clarify document to avoid \""Error Importing cuda extension\""","You need to import torch first, as this will resolve some symbols that the dynamic linker must see:",4647
19463,"[feature request] Clarify document to avoid \""Error Importing cuda extension\""","You need to import torch first, as this will resolve some symbols that the dynamic linker must see:",4647
19464,RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70,Please ensure that there isn't any path in LD_LIBRARY_PATH that might override general RPATH,9708
19465,RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70,Please ensure that there isn't any path in LD_LIBRARY_PATH that might override general RPATH,9708
19466,[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack),"The helper function `torch.distributions.utils.broadcast_all()` that should allow you to\r\n```py\r\ntorch.stack(broadcast_all(a, b))\r\n```",8273
19467,[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack),"The helper function `torch.distributions.utils.broadcast_all()` that should allow you to\r\n```py\r\ntorch.stack(broadcast_all(a, b))\r\n```",8273
19468,Python interpreter died without Traceback when CPU Tensor divided by zero.,This error is the result of a leaky abstraction.,1318
19469,Python interpreter died without Traceback when CPU Tensor divided by zero.,This error is the result of a leaky abstraction.,1318
19470,Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True,This is an expected behavior and not a bug.,27
19471,Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True,This is an expected behavior and not a bug.,27
19472,AttributeError: module 'torch._C' has no attribute '_TensorBase',Try to reinstall and make sure that you have binary pytorch installed.,1530
19473,AttributeError: module 'torch._C' has no attribute '_TensorBase',Try to reinstall and make sure that you have binary pytorch installed.,1530
19474,Feature request: Object detection model zoo,Torchvision now has models for object detection and semantic segmentation officially supported,7968
19475,Feature request: Object detection model zoo,Torchvision now has models for object detection and semantic segmentation officially supported,7968
19476,"dynamically change tensor with requires_grad=False by \""+=\"" cause error but \""+\"" doesn't","`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place.",8752
19477,"dynamically change tensor with requires_grad=False by \""+=\"" cause error but \""+\"" doesn't","`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place.",8752
19478,python setup.py install failed. undefined references,You need to run :\r\n\r\n```\r\nconda remove mkl mkl-include\r\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\r\n```\r\n\r\nbefore the installation to fix the issue.,8270
19479,python setup.py install failed. undefined references,You need to run :\r\n\r\n```\r\nconda remove mkl mkl-include\r\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\r\n```\r\n\r\nbefore the installation to fix the issue.,8270
19480,THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error,Restarting the machine can fix this issue.,10717
19481,THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error,Restarting the machine can fix this issue.,10717
19482,torchfile.T7ReaderException: unknown object type / typeidx: -1112529805,"You need to append the parameter long_size like this `load_lua(....., long_size=8)`.",4859
19483,torchfile.T7ReaderException: unknown object type / typeidx: -1112529805,"You need to append the parameter long_size like this `load_lua(....., long_size=8)`.",4859
19484,RuntimeError: CUDA error: unknown error,You need to install the right pytorch version for 10.1 CUDA with:\r\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\r\ninstead of installing it with the installtion snippet generated by pytorch website:\r\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n,3119
19485,RuntimeError: CUDA error: unknown error,You need to install the right pytorch version for 10.1 CUDA with:\r\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\r\ninstead of installing it with the installtion snippet generated by pytorch website:\r\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n,3119
19486,optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum,It can be fixed by indenting the second and third to last lines in the init,3279
19487,optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum,It can be fixed by indenting the second and third to last lines in the init,3279
19488,Different deterministic behavior between CPU and CUDA for orthogonal initialization,This is expected. It is not guaranteed that the random sequences generated on different devices will look the same.,7647
19489,Different deterministic behavior between CPU and CUDA for orthogonal initialization,This is expected. It is not guaranteed that the random sequences generated on different devices will look the same.,7647
19490,C++ torch::Tensor serialization,"Using  the latest macOS libtorch, one can both save and load torch::Tensor and std::vector<torch::Tensor>",1915
19491,C++ torch::Tensor serialization,"Using  the latest macOS libtorch, one can both save and load torch::Tensor and std::vector<torch::Tensor>",1915
19492,Compiling from master yields std::runtime_error,"You need to compile PyTorch again with a git clean -fdx in between, ",672
19493,Compiling from master yields std::runtime_error,"You need to compile PyTorch again with a git clean -fdx in between, ",672
19494,Improved documentation of distributed launch utility,We are working on this.,6838
19495,Improved documentation of distributed launch utility,We are working on this.,6838
19496,test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS,This is an issue related to Python 3.8 on Mac,1954
19497,test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS,This is an issue related to Python 3.8 on Mac,1954
19498,[package] error in colab tutorial #60189,This error is intentional—you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.,7498
19499,[package] error in colab tutorial #60189,This error is intentional—you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.,7498
19500,USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory,You need to update the submodules:\r\n```\r\ngit submodule sync\r\ngit submodule update --init --recursive\r\n```,3105
19501,USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory,You need to update the submodules:\r\n```\r\ngit submodule sync\r\ngit submodule update --init --recursive\r\n```,3105
19502,Can I train AI If AI model is located in the another model’s forward?,You may want to add model2 to an attribute of model1 in \\_\\_init\\_\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\r\n\r\n,8637
19503,Can I train AI If AI model is located in the another model’s forward?,You may want to add model2 to an attribute of model1 in \\_\\_init\\_\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\r\n\r\n,8637
19504,M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`,This will be fixed in the next nightly build.,8416
19505,M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`,This will be fixed in the next nightly build.,8416
19506,it seems n_heads is not handled correctly in nn.MultiheadAttention,You can pass embed_dim * num_heads as the embed_dim,1287
19507,it seems n_heads is not handled correctly in nn.MultiheadAttention,You can pass embed_dim * num_heads as the embed_dim,1287
19508,`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky,You need to diable them for XLA.,607
19509,`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky,You need to diable them for XLA.,607
19510,Does the NCCL operation use the default stream as other computations?,"If you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option.",7192
19511,Does the NCCL operation use the default stream as other computations?,"If you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option.",7192
19512,Add container for recurrent nets,"No, definitely not to `Module`. We can consider adding a base class for RNNs",4321
19513,Add container for recurrent nets,"No, definitely not to `Module`. We can consider adding a base class for RNNs",4321
19514,Support for einsum notation,Support for einsum notation has been added,8463
19515,Support for einsum notation,Support for einsum notation has been added,8463
19516,GPU usage extremely in-balance for segmentation task,"Leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs",6988
19517,GPU usage extremely in-balance for segmentation task,"Leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs",6988
19518,How to select GPU programmatically in code,"```\r\nimport os\r\nos.environ[\""CUDA_DEVICE_ORDER\""]=\""PCI_BUS_ID\"" \r\nos.environ[\""CUDA_VISIBLE_DEVICES\""]=\""1\""\r\n```\r\n",8720
19519,How to select GPU programmatically in code,"```\r\nimport os\r\nos.environ[\""CUDA_DEVICE_ORDER\""]=\""PCI_BUS_ID\"" \r\nos.environ[\""CUDA_VISIBLE_DEVICES\""]=\""1\""\r\n```\r\n",8720
19520,Pooling throws an exception in Tegra TX1,"You might have to set `CUDNN_INCLUDE_DIR` and `CUDNN_LIB_DIR` environment variables.
",10890
19521,Pooling throws an exception in Tegra TX1,"You might have to set `CUDNN_INCLUDE_DIR` and `CUDNN_LIB_DIR` environment variables.
",10890
19522,[Feature Request] Cyclical Learning Rates,This can be easily implemented using class `LambdaLR`,2940
19523,[Feature Request] Cyclical Learning Rates,This can be easily implemented using class `LambdaLR`,2940
19524,Get a single batch from DataLoader without iterating,Use `next(iter(data_loader))` ,8733
19525,Get a single batch from DataLoader without iterating,Use `next(iter(data_loader))` ,8733
19526,Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid,The behavior is same if `binary_cross_entropy` and `binary_cross_entropy_with_logits` the input and target should have the same shape,6830
19527,Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid,The behavior is same if `binary_cross_entropy` and `binary_cross_entropy_with_logits` the input and target should have the same shape,6830
19528,[pylint] E1101:Module 'torch' has no 'squeeze' member,"You need to change the settings for your editor. On VS code, you need to add ""python.linting.enabled"": false to the settings file.",4621
19529,[pylint] E1101:Module 'torch' has no 'squeeze' member,"You need to change the settings for your editor. On VS code, you need to add ""python.linting.enabled"": false to the settings file.",4621
19530,[Feature Request] Layer Normalization,"This code segment can acheive layer normalization:\r\n\r\n```python\r\nclass LayerNorm(nn.Module):\r\n\r\n def __init__(self, features, eps=1e-6):\r\n super().__init__()\r\n self.gamma = nn.Parameter(torch.ones(features))\r\n self.beta = nn.Parameter(torch.zeros(features))\r\n self.eps = eps\r\n\r\n def forward(self, x):\r\n mean = x.mean(-1, keepdim=True)\r\n std = x.std(-1, keepdim=True)\r\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n```",3297
19531,[Feature Request] Layer Normalization,"This code segment can acheive layer normalization:\r\n\r\n```python\r\nclass LayerNorm(nn.Module):\r\n\r\n def __init__(self, features, eps=1e-6):\r\n super().__init__()\r\n self.gamma = nn.Parameter(torch.ones(features))\r\n self.beta = nn.Parameter(torch.zeros(features))\r\n self.eps = eps\r\n\r\n def forward(self, x):\r\n mean = x.mean(-1, keepdim=True)\r\n std = x.std(-1, keepdim=True)\r\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n```",3297
19532,Import fails after Conda install,You need to install the latest build via `conda install cudatoolkit`,11461
19533,Import fails after Conda install,You need to install the latest build via `conda install cudatoolkit`,11461
19534,ReduceOps are breaking Pyro test,This issue has been fixed.,3100
19535,ReduceOps are breaking Pyro test,This issue has been fixed.,3100
19536,Linux CPU build script fails as MKL header files not found,"To install the header files, use `pip install mkl-devel`",3553
19537,Linux CPU build script fails as MKL header files not found,"To install the header files, use `pip install mkl-devel`",3553
19538,RuntimeError: value cannot be converted to type uint8_t without overflow: 10000,You can use this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```,11438
19539,RuntimeError: value cannot be converted to type uint8_t without overflow: 10000,You can use this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```,11438
19540,CUDNN_STATUS_NOT_INITIALIZED when built from source,"You need to install the latest versions of Nvidia driver, CUDA, and CuDNN",989
19541,CUDNN_STATUS_NOT_INITIALIZED when built from source,"You need to install the latest versions of Nvidia driver, CUDA, and CuDNN",989
19542,I cannot initialize Tensor. (They will become torch.autograd.variable.Variable),This issue has been fixed.,5778
19543,I cannot initialize Tensor. (They will become torch.autograd.variable.Variable),This issue has been fixed.,5778
19544,[feature request] Add underscore to nn.init functions,Non-underscore functions have been retained for backward compatibility. ,6888
19545,[feature request] Add underscore to nn.init functions,Non-underscore functions have been retained for backward compatibility. ,6888
19546,How can I access the model's attribution created during forward pass when using dataparallel?,"You can use the following code:\r\n\r\n```python\r\n\r\nclass NormalModel(nn.Module):\r\n pass\r\n\r\nclass ExtraOutputWrapper(nn.Module):\r\n def __init__(self, *args, **kw):\r\n self.wrapped = NormalModel(*args, **kw)\r\n\r\n def forward(self, input):\r\n normal_output = self.wrapped(input)\r\n extra_output = self.wrapped.layer_of_interest.property\r\n return normal_output, extra_output\r\n```\r\n\r\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest.",5207
19547,How can I access the model's attribution created during forward pass when using dataparallel?,"You can use the following code:\r\n\r\n```python\r\n\r\nclass NormalModel(nn.Module):\r\n pass\r\n\r\nclass ExtraOutputWrapper(nn.Module):\r\n def __init__(self, *args, **kw):\r\n self.wrapped = NormalModel(*args, **kw)\r\n\r\n def forward(self, input):\r\n normal_output = self.wrapped(input)\r\n extra_output = self.wrapped.layer_of_interest.property\r\n return normal_output, extra_output\r\n```\r\n\r\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest.",5207
19548,Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object,Install the latest version of cpuinfo to fix this.,7190
19549,Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object,Install the latest version of cpuinfo to fix this.,7190
19550,fatal error: torch/torch.h: No such file or directory,"There are three options:\r\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\r\n2. `python setup.py build install`\r\n3. `python run_test.py --exclude cpp_extensions`\r\n\r\n(1) is probably best",5707
19551,fatal error: torch/torch.h: No such file or directory,"There are three options:\r\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\r\n2. `python setup.py build install`\r\n3. `python run_test.py --exclude cpp_extensions`\r\n\r\n(1) is probably best",5707
19552,pytorch installation error on macOS 10.13.3,This error can be resolved by reinstalling TBB.,8701
19553,pytorch installation error on macOS 10.13.3,This error can be resolved by reinstalling TBB.,8701
19554,Pretrained Model Loading Error,Add the `encoding=latin1` argument to `pickle.load`,3588
19555,Pretrained Model Loading Error,Add the `encoding=latin1` argument to `pickle.load`,3588
19556,Bazel - Pybind - Pytorch - Undefined Symbol,You need to add 'linkshared=True' and compile it as a binary,5170
19557,Bazel - Pybind - Pytorch - Undefined Symbol,You need to add 'linkshared=True' and compile it as a binary,5170
19558,"TypeError: __init__() should return None, not 'int' in validation Dataset",This is an expected behavior.,1613
19559,"TypeError: __init__() should return None, not 'int' in validation Dataset",This is an expected behavior.,1613
19560,torch.linalg.cholesky fails for some PSD matrices,Using double precision will not give any errors.,6875
19561,torch.linalg.cholesky fails for some PSD matrices,Using double precision will not give any errors.,6875
19562,Scribe stats reporting is broken in GHA due to secrets access from fork PRs,This issue has been fixed.,8130
19563,Scribe stats reporting is broken in GHA due to secrets access from fork PRs,This issue has been fixed.,8130
19564,HTTP Error 403 for torch.hub ResNet,"Please add the following line before making any \""torch.hub\"" calls:\r\n```\r\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n```",1290
19565,HTTP Error 403 for torch.hub ResNet,"Please add the following line before making any \""torch.hub\"" calls:\r\n```\r\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n```",1290
19566,torch.permute missing in docs,This issue has been fixed.,7577
19567,torch.permute missing in docs,This issue has been fixed.,7577
19568,"Conv2d triggers assertion in mkl-dnn when padding=(n, 3)",Installing MKLDNN v0.21.1 can fix this issue.,687
19569,"Conv2d triggers assertion in mkl-dnn when padding=(n, 3)",Installing MKLDNN v0.21.1 can fix this issue.,687
19570,PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES,Use the following command \r\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`,4261
19571,PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES,Use the following command \r\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`,4261
19572,Allow parallel sending to device in DataLoader,The best way is to send to GPU in a data transform.,4314
19573,Allow parallel sending to device in DataLoader,The best way is to send to GPU in a data transform.,4314
19574,distributed all_reduce deadlocks in v1.1,Installing nccl 2.4.6 can fix this issue.,5016
19575,distributed all_reduce deadlocks in v1.1,Installing nccl 2.4.6 can fix this issue.,5016
19576,Build error with MSVC (aten\\src\\ATen\\native\\quantized\\Copy.cpp),"Moving the sentence \""float* src_data = src.data<float>();\"" into the function of \""AT_DISPATCH_QINT_TYPES ...\"" can solve this problem.",9935
19577,Build error with MSVC (aten\\src\\ATen\\native\\quantized\\Copy.cpp),"Moving the sentence \""float* src_data = src.data<float>();\"" into the function of \""AT_DISPATCH_QINT_TYPES ...\"" can solve this problem.",9935
19578,RuntimeError: ONNX export failed: Couldn't export operator aten::softmax,This works fine for the nightly version.,3149
19579,RuntimeError: ONNX export failed: Couldn't export operator aten::softmax,This works fine for the nightly version.,3149
19580,MultiheadAttention is not scriptable,This issue has been fixed.,6820
19581,MultiheadAttention is not scriptable,This issue has been fixed.,6820
19582,Building libtorch-dependent project with CMake,"You can use new cmake 'targets' feature, e.g.\r\n\r\n```\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_library(a)\r\ntarget_link_libraries(a PRIVATE Torch)\r\n```",3050
19583,Building libtorch-dependent project with CMake,"You can use new cmake 'targets' feature, e.g.\r\n\r\n```\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_library(a)\r\ntarget_link_libraries(a PRIVATE Torch)\r\n```",3050
19584,Parameter not registering if .to(device) is used,"You need to use:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```",10902
19585,Parameter not registering if .to(device) is used,"You need to use:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```",10902
19586,Windows CPU debug build fails at linking stage,Build will succeed without setting BUILD_TEST=OFF\r\n\,2938
19587,Windows CPU debug build fails at linking stage,Build will succeed without setting BUILD_TEST=OFF\r\n\,2938
19588,Conda did not install cudnn for pytorch,"Even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`.",2939
19589,Conda did not install cudnn for pytorch,"Even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`.",2939
19590,Tensor unfold backward is slow,This issue has been fixed.,7830
19591,Tensor unfold backward is slow,This issue has been fixed.,7830
19592,cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer,Use the latest versions of CUDA10 and CuDNN,1502
19593,cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer,Use the latest versions of CUDA10 and CuDNN,1502
19594,PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`,Enclose operations that do not require the backward computations within the  `with torch.no_grad():` block,5980
19595,PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`,Enclose operations that do not require the backward computations within the  `with torch.no_grad():` block,5980
19596,Is it possible to integrate jax into pytorch ?,Jax and PyTorch are both frontends and cannot be integrated.,101
19597,Is it possible to integrate jax into pytorch ?,Jax and PyTorch are both frontends and cannot be integrated.,101
19598,"ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5",This issue has been fixed.,4146
19599,"ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5",This issue has been fixed.,4146
19600,Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing,This issue has been fixed.,707
19601,Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing,This issue has been fixed.,707
19602,Why torch wheel is so huge (582MB)?,You can specify the GPU architectures during installation to reduce the size.,7201
19603,Why torch wheel is so huge (582MB)?,You can specify the GPU architectures during installation to reduce the size.,7201
19604,"We should not mark non-floating point Tensors as requirering gradients, ever","Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense.",2736
19605,"We should not mark non-floating point Tensors as requirering gradients, ever","Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense.",2736
19606,Pickling of _VariableFunctions no longer works in 1.5,Installing the latest version of PyTorch can fix the issue.,3396
19607,Pickling of _VariableFunctions no longer works in 1.5,Installing the latest version of PyTorch can fix the issue.,3396
19608,Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions,include_dirs now supports both absolute and relative paths.,9606
19609,Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions,include_dirs now supports both absolute and relative paths.,9606
19610,Empty GPU memory cache after Jupyter notebook interrupted,This issue is not related to PyTorch.,9754
19611,Empty GPU memory cache after Jupyter notebook interrupted,This issue is not related to PyTorch.,9754
19612,torch.utils.checkpoint.checkpoint + torch.cuda.amp fails,"Running `scaler.scale(loss).backward()` inside the `autocast` block, should work",6827
19613,torch.utils.checkpoint.checkpoint + torch.cuda.amp fails,"Running `scaler.scale(loss).backward()` inside the `autocast` block, should work",6827
19614,Windows 10 Libtorch installation issue.,"You need to add this line in CMakeLists.txt file\r\n``` set(CMAKE_PREFIX_PATH \""libtorch/share/cmake/Torch\"") ```",4617
19615,Windows 10 Libtorch installation issue.,"You need to add this line in CMakeLists.txt file\r\n``` set(CMAKE_PREFIX_PATH \""libtorch/share/cmake/Torch\"") ```",4617
19616,torch.remainder gives a remainder larger than the divisor,This is due to numerical precision issues. Please use `torch.fmod` instead of `torch.remainder`,6864
19617,torch.remainder gives a remainder larger than the divisor,This is due to numerical precision issues. Please use `torch.fmod` instead of `torch.remainder`,6864
19618,torch.cdist [Cuda out of memory Tried to allocate 108GB of memory],Building environment from scracth will fix the issue.,3331
19619,torch.cdist [Cuda out of memory Tried to allocate 108GB of memory],Building environment from scracth will fix the issue.,3331
19620,C++ API for Transformer model in libtorch 1.5.0,This is available in the recent versions of PyTorch.,7627
19621,C++ API for Transformer model in libtorch 1.5.0,This is available in the recent versions of PyTorch.,7627
19622,RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py),`LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.,480
19623,RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py),`LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.,480
19624,Better testing on CPUs without AVX capabilities,You can use a machine emulator such as QEMU.,3756
19625,Better testing on CPUs without AVX capabilities,You can use a machine emulator such as QEMU.,3756
19626,[BatchNorm] Unexpected behaviour with track_running_stats,This issue has been fixed.,1593
19627,[BatchNorm] Unexpected behaviour with track_running_stats,This issue has been fixed.,1593
19628,`torch.log10` with float32 produces different results on different CPU,Using `float64` can resolve this issue.,654
19629,`torch.log10` with float32 produces different results on different CPU,Using `float64` can resolve this issue.,654
19630,Falling to turn shape into Tensor,"This can be done via \r\n```python\r\na = torch.ones(10, 10)\r\nb = torch.tensor(a.shape)\r\nprint(b)\r\n# tensor([10, 10])\r\n```",2180
19631,Falling to turn shape into Tensor,"This can be done via \r\n```python\r\na = torch.ones(10, 10)\r\nb = torch.tensor(a.shape)\r\nprint(b)\r\n# tensor([10, 10])\r\n```",2180
19632,[pytorch] [feature request] Entropy function,This can ve calculated via `distributions.Categorical(probs=p).entropy()`,11448
19633,[pytorch] [feature request] Entropy function,This can ve calculated via `distributions.Categorical(probs=p).entropy()`,11448
19634,mse_loss reduction='none' is ignored when required_grads is True,This issue has been fixed.,4209
19635,mse_loss reduction='none' is ignored when required_grads is True,This issue has been fixed.,4209
19636,3x regression in JIT LSTM speeds,This issue arises because of memory leaks,8486
19637,3x regression in JIT LSTM speeds,This issue arises because of memory leaks,8486
19638,windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device,You need to uninstall and reinstall PyTorch.,7196
19639,Quadro m2000m not able to get pytorch working with gpu,You need to uninstall and reinstall PyTorch.,7196
19640,windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device,You need to uninstall and reinstall PyTorch.,7196
19641,Quadro m2000m not able to get pytorch working with gpu,You need to uninstall and reinstall PyTorch.,7196
19642,"Wrong Warning \""compiler (c++) may be ABI-incompatible with PyTorch!\""",The error is not encountered if compiled from source.,616
19643,"Wrong Warning \""compiler (c++) may be ABI-incompatible with PyTorch!\""",The error is not encountered if compiled from source.,616
19644,[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.,Inference time is the same if `set_flush_denorm` is used with CPU. ,8476
19645,[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.,Inference time is the same if `set_flush_denorm` is used with CPU. ,8476
19646,CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++),Adding `filter` as external input for the model works.,3150
19647,CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++),Adding `filter` as external input for the model works.,3150
19648,MSELoss wrongly sums instead of averages when reduction='elementwise_mean',This issue has been fixed.,8479
19649,MSELoss wrongly sums instead of averages when reduction='elementwise_mean',This issue has been fixed.,8479
19650,Backward engine computes unnecessary dependencies,This issue has been fixed.,2466
19651,Backward engine computes unnecessary dependencies,This issue has been fixed.,2466
19652,F.relu(inplace) followed by F.dropout(inplace) breaks backward pass,Removing the `inplace` flag should make it work.,8467
19653,F.relu(inplace) followed by F.dropout(inplace) breaks backward pass,Removing the `inplace` flag should make it work.,8467
19654,"torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",`torch.range` needs to be deprecated,3276
19655,"torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",`torch.range` needs to be deprecated,3276
19656,LSTM forget gate bias initialization,"The ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value.",8522
19657,LSTM forget gate bias initialization,"The ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value.",8522
19658,Error while saving my network,Using the correct arguments will not give this error.,10889
19659,Error while saving my network,Using the correct arguments will not give this error.,10889
19660,Build fails on Ubuntu 14.04 + conda latest,Installing the latest stable release fixes the issue.,1503
19661,Build fails on Ubuntu 14.04 + conda latest,Installing the latest stable release fixes the issue.,1503
19662,view() after transpose() raises non contiguous error,"This is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue",8525
19663,view() after transpose() raises non contiguous error,"This is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue",8525
19664,Errors import torch installed form source on macOS,You need to open torch in any directory other than repo's root,5720
19665,Errors import torch installed form source on macOS,You need to open torch in any directory other than repo's root,5720
19666,Add support for variable length sequences in cuDNN RNNs,CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality,593
19667,Add support for variable length sequences in cuDNN RNNs,CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality,593
19668,Maxout Layer,"The Maxout layer can be implemented as follows \r\n\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n def __init__(self, d_in, d_out, pool_size):\r\n super().__init__()\r\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n self.lin = nn.Linear(d_in, d_out * pool_size)\r\n\r\n\r\n def forward(self, inputs):\r\n shape = list(inputs.size())\r\n shape[-1] = self.d_out\r\n shape.append(self.pool_size)\r\n max_dim = len(shape) - 1\r\n out = self.lin(inputs)\r\n m, i = out.view(*shape).max(max_dim)\r\n return m\r\n```\r\n",2569
19669,Maxout Layer,"The Maxout layer can be implemented as follows \r\n\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n def __init__(self, d_in, d_out, pool_size):\r\n super().__init__()\r\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n self.lin = nn.Linear(d_in, d_out * pool_size)\r\n\r\n\r\n def forward(self, inputs):\r\n shape = list(inputs.size())\r\n shape[-1] = self.d_out\r\n shape.append(self.pool_size)\r\n max_dim = len(shape) - 1\r\n out = self.lin(inputs)\r\n m, i = out.view(*shape).max(max_dim)\r\n return m\r\n```\r\n",2569
19670,Unefined reference to C10::Error::Error when linking against libTorch,You need to use the build flags `QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0`,3286
19671,Unefined reference to C10::Error::Error when linking against libTorch,You need to use the build flags `QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0`,3286
19672,can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.,"`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor.",8707
19673,can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.,"`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor.",8707
19674,GPU Memery leak,"To free up GPU memory, you need to detach the predictions.",9557
19675,GPU Memery leak,"To free up GPU memory, you need to detach the predictions.",9557
19676,[Build error] libnccl.so: error adding symbols: File in wrong format,You need to install the correct version for your platform,9749
19677,[Build error] libnccl.so: error adding symbols: File in wrong format,You need to install the correct version for your platform,9749
19678,Can cuda10 use pytorch-0.4.1?,"Not, it cannot.",10229
19679,Can cuda10 use pytorch-0.4.1?,"Not, it cannot.",10229
19680,cosine_similarity function produces results more than 1.0,This error can be fixed by reordering the computations to improve numerical precisions.,4895
19681,cosine_similarity function produces results more than 1.0,This error can be fixed by reordering the computations to improve numerical precisions.,4895
19682,Is mkl-dnn enabled in the latest binary distribution v1.0.1?,Yes.,8539
19683,Finding the source function,Yes.,8539
19684,Is mkl-dnn enabled in the latest binary distribution v1.0.1?,Yes.,8539
19685,nn.LSTM gives nondeterministic results with dropout and multiple layers,This issue has been fixed.,1591
19686,nn.LSTM gives nondeterministic results with dropout and multiple layers,This issue has been fixed.,1591
19687,Add support for mixture models in torch.distributions,"`MixtureSameFamily` can be implemented as:\r\n```py\r\ncomponent = pyro.sample(\""component\"", dist.Categorical(probs),\r\n                        infer={\""enumerate\"": \""parallel\""})\r\nassert component.reshape(-1).shape == probs.shape[-1:]\r\nvalue = pyro.sample(\""mixture\"", MyDistribution(my_params[component]))\r\n```",8711
19688,Add support for mixture models in torch.distributions,"`MixtureSameFamily` can be implemented as:\r\n```py\r\ncomponent = pyro.sample(\""component\"", dist.Categorical(probs),\r\n                        infer={\""enumerate\"": \""parallel\""})\r\nassert component.reshape(-1).shape == probs.shape[-1:]\r\nvalue = pyro.sample(\""mixture\"", MyDistribution(my_params[component]))\r\n```",8711
19689,IndexError while trying to save torchscript,This issue has been fixed.,1610
19690,IndexError while trying to save torchscript,This issue has been fixed.,1610
19691,[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED,This issue has been fixed.,1601
19692,[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED,This issue has been fixed.,1601
19693,[CPU] several inplace functions fail since 1.0.1 on certain hw,Setting `OMP_NUM_THREADS=1` will not raise an error.,3028
19694,[CPU] several inplace functions fail since 1.0.1 on certain hw,Setting `OMP_NUM_THREADS=1` will not raise an error.,3028
19695,module' object has no attribute '_dl',You need to uninstall and reinstall PyTorch.,9743
19696,module' object has no attribute '_dl',You need to uninstall and reinstall PyTorch.,9743
19697,IndyLSTM & IndyGRU in PyTorch,This feature will not be implemented.,9745
19698,IndyLSTM & IndyGRU in PyTorch,This feature will not be implemented.,9745
19699,Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight',Model and data need to be on the same GPU.,11463
19700,Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight',Model and data need to be on the same GPU.,11463
19701,CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1,You need to downgrade your version of CUDA.,3106
19702,CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1,You need to downgrade your version of CUDA.,3106
19703,error executing torch_shm_manager in cifar10_tutorial.py,Setting `num_workers=0` will not raise an error.,2348
19704,error executing torch_shm_manager in cifar10_tutorial.py,Setting `num_workers=0` will not raise an error.,2348
19705,"\""module has no attribute 'downsample'\"" when scripting torchvision's resnet",You need to install the latest version of PyTorch.,3012
19706,"\""module has no attribute 'downsample'\"" when scripting torchvision's resnet",You need to install the latest version of PyTorch.,3012
19707,"Failed to load model on mobile for device type \""c10::DeviceType::CUDA\""",You need to move the model to `.cpu()`,632
19708,"Failed to load model on mobile for device type \""c10::DeviceType::CUDA\""",You need to move the model to `.cpu()`,632
19709,Didn't find kernel to dispatch to for operator 'quantized::conv2d',Passing a quantized vector to the operator will not raise an error.,2847
19710,Didn't find kernel to dispatch to for operator 'quantized::conv2d',Passing a quantized vector to the operator will not raise an error.,2847
19711,"Why t.arange(0,3) create an int type Tensor?",You need to specify the datatype as an argument.,10986
19712,"Why t.arange(0,3) create an int type Tensor?",You need to specify the datatype as an argument.,10986
19713,Loading opencv image to pytorch tensor,The order of dimensions is different in OpenCV and PyTorch.,3094
19714,Loading opencv image to pytorch tensor,The order of dimensions is different in OpenCV and PyTorch.,3094
19715,Bug in transferring model from pytorch --> caffe2,Remove the `pixel_shuffle` part and composed the network only with ReLU and conv layers will not give an error.,3170
19716,Bug in transferring model from pytorch --> caffe2,Remove the `pixel_shuffle` part and composed the network only with ReLU and conv layers will not give an error.,3170
19717,Using net.cuda crashes the kernel,You need to overwrite the reference with a new CUDA tensor:\r\n```python\r\ninput = input.cuda()\r\nmodel(Variable(input))\r\n```,3568
19718,Using net.cuda crashes the kernel,You need to overwrite the reference with a new CUDA tensor:\r\n```python\r\ninput = input.cuda()\r\nmodel(Variable(input))\r\n```,3568
19719,Gumbel noise,"The code is as follows:\r\n```python\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef sampler(input, tau, temperature):\r\n noise = torch.rand(input.size())\r\n noise.add_(1e-9).log_().neg_()\r\n noise.add_(1e-9).log_().neg_()\r\n noise = Variable(noise)\r\n x = (input + noise) / tau + temperature\r\n x = F.softmax(x.view(input.size(0), -1))\r\n return x.view_as(input)\r\n```",2794
19720,Gumbel noise,"The code is as follows:\r\n```python\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef sampler(input, tau, temperature):\r\n noise = torch.rand(input.size())\r\n noise.add_(1e-9).log_().neg_()\r\n noise.add_(1e-9).log_().neg_()\r\n noise = Variable(noise)\r\n x = (input + noise) / tau + temperature\r\n x = F.softmax(x.view(input.size(0), -1))\r\n return x.view_as(input)\r\n```",2794
19721,GOMP_4.0 not found,Importing OpenCV before PyTorch fixes the issue.,3088
19722,GOMP_4.0 not found,Importing OpenCV before PyTorch fixes the issue.,3088
19723,Cannot install `torchvision 0.1.7` by using conda,Installing torchvision through pip fixes this issue.,9562
19724,Cannot install `torchvision 0.1.7` by using conda,Installing torchvision through pip fixes this issue.,9562
19725,nn.Module not importing parameters contained in lists,This behaviour is expected.,7468
19726,nn.Module not importing parameters contained in lists,This behaviour is expected.,7468
19727,libTH doesn't recognize Intel MKL in its default location,Running the command `conda install mkl` fixes this issue.,3072
19728,libTH doesn't recognize Intel MKL in its default location,Running the command `conda install mkl` fixes this issue.,3072
19729,[build/nccl] failed to build libnccl on Debian unstable,You need to exprt the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```,541
19730,[build/nccl] failed to build libnccl on Debian unstable,You need to exprt the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```,541
19731,Allow optimizers to skip nn.Parameters that have requires_grad=False,"This can be done via ```pytorch\r\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\r\n```",5939
19732,Allow optimizers to skip nn.Parameters that have requires_grad=False,"This can be done via ```pytorch\r\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\r\n```",5939
19733,cuda runtime error (8) : invalid device function - adding cuda tensors,You need to use a more recent GPU for this to work.,528
19734,cuda runtime error (8) : invalid device function - adding cuda tensors,You need to use a more recent GPU for this to work.,528
19735,"\""Symbol not found\"" when \""import torch\"" on Mac OS",Install torchtext from pip instead of source to fix this issue.,1289
19736,"\""Symbol not found\"" when \""import torch\"" on Mac OS",Install torchtext from pip instead of source to fix this issue.,1289
19737,How to use cudnn in pytorch？,To verify that pytorch uses cudnn:\r\n```\r\n>>> torch.backends.cudnn.version()```,7933
19738,How to use cudnn in pytorch？,To verify that pytorch uses cudnn:\r\n```\r\n>>> torch.backends.cudnn.version()```,7933
19739,[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member,"You need to set the following flag `""python.linting.enabled\"": false` ",4622
19740,[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member,"You need to set the following flag `""python.linting.enabled\"": false` ",4622
19741,ImportError: No module named 'tools.setup_helpers',Install using instructions from PyTorch's website.,701
19742,ImportError: No module named 'tools.setup_helpers',Install using instructions from PyTorch's website.,701
19743,"from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",You need to install pydot.,3951
19744,"from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",You need to install pydot.,3951
19745,ImportError: No module named future.utils,You need to install the package 'future',10780
19746,ImportError: No module named future.utils,You need to install the package 'future',10780
19747,Raise correct error type when passing invalid covariance matrix to MultivariateNormal,The solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.,1307
19748,Raise correct error type when passing invalid covariance matrix to MultivariateNormal,The solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.,1307
19749,Install only caffe2,It is no longer possible to only install CAFFE2.,9738
19750,Install only caffe2,It is no longer possible to only install CAFFE2.,9738
19751,Illegal instruction (core dumped) on Debug CPU build,Setting `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code` will fix this issue.,1925
19752,Illegal instruction (core dumped) on Debug CPU build,Setting `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code` will fix this issue.,1925
19753,OMP: Warning #190 because of fork not waiting for parallel region to end,The warning does not appear if the argument `pin_memory=False` is given.,2988
19754,OMP: Warning #190 because of fork not waiting for parallel region to end,The warning does not appear if the argument `pin_memory=False` is given.,2988
19755,torch.jit.trace returns unwrapped C type,Installing PyTorch from source fixes the error.,502
19756,torch.jit.trace returns unwrapped C type,Installing PyTorch from source fixes the error.,502
19757,"I can't import PyTorch, libomp.dylib can't be loaded.","To solve the problem, run `brew install libomp`.",8724
19758,"I can't import PyTorch, libomp.dylib can't be loaded.","To solve the problem, run `brew install libomp`.",8724
19759,Slow distributed training,Setting the flag `OMP_NUM_THREADS=1` makes it faster.,990
19760,Slow distributed training,Setting the flag `OMP_NUM_THREADS=1` makes it faster.,990
19761,Unable to compile an older version of PyTorch,Run the following: `git submodule sync` and then `git submodule update --init`,8014
19762,Unable to compile an older version of PyTorch,Run the following: `git submodule sync` and then `git submodule update --init`,8014
19763,torch.arange always generate constant result in tracing,This issue has been fixed.,2834
19764,torch.arange always generate constant result in tracing,This issue has been fixed.,2834
19765,Unable to import 1.1 when installing with pip,"To solve the problem, run `brew install libomp`.",4897
19766,Unable to import 1.1 when installing with pip,"To solve the problem, run `brew install libomp`.",4897
19767,Is the `device=` parameter required in torch.FloatTensor and similar ones,"Please use  `torch.empty(..., device='cuda')` instead of `torch.cuda.FloatTensor` or `torch.FloatTensor`. ",8743
19768,Is the `device=` parameter required in torch.FloatTensor and similar ones,"Please use  `torch.empty(..., device='cuda')` instead of `torch.cuda.FloatTensor` or `torch.FloatTensor`. ",8743
19769,NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training,You need to set `n_proc=1` and `find_unused_parameters=True` to `DistributedDataParallel`.,3181
19770,NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training,You need to set `n_proc=1` and `find_unused_parameters=True` to `DistributedDataParallel`.,3181
19771,[FR] make IncompatibleKeys print nicer when there is no error,This error can be ignored.,566
19772,[FR] make IncompatibleKeys print nicer when there is no error,This error can be ignored.,566
19773,GRUcell has a wrong formula,The formula is correct.,9243
19774,GRUcell has a wrong formula,The formula is correct.,9243
19775,TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above,You need to install the latest version of Tensorboard.,3234
19776,TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above,You need to install the latest version of Tensorboard.,3234
19777,Building from source error: command 'gcc' failed with exit status 1,You need to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\r\n* torch/csrc/Storage.cpp\r\n* torch/csrc/Tensor.cpp\r\n* torch/csrc/cuda/Storage.cpp\r\n* torch/csrc/cuda/Tensor.cpp,3143
19778,Building from source error: command 'gcc' failed with exit status 1,You need to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\r\n* torch/csrc/Storage.cpp\r\n* torch/csrc/Tensor.cpp\r\n* torch/csrc/cuda/Storage.cpp\r\n* torch/csrc/cuda/Tensor.cpp,3143
19779,RuntimeError when using DistributedDataParallel,The problem can be solved by deleting a ``nn.Linear`` that is assigned to an attribute during the ``__init__`` of a custom ``nn.Module``,3075
19780,RuntimeError when using DistributedDataParallel,The problem can be solved by deleting a ``nn.Linear`` that is assigned to an attribute during the ``__init__`` of a custom ``nn.Module``,3075
19781,GPU Memory Leak at Master Branch,This issue has been fixed.,1605
19782,GPU Memory Leak at Master Branch,This issue has been fixed.,1605
19783,Support view() on batch dimensions for non-contiguous tensors?,This feature will not be implemented.,7427
19784,Support view() on batch dimensions for non-contiguous tensors?,This feature will not be implemented.,7427
19785,Slight memory leak for LSTM,This issue has been fixed.,6817
19786,Slight memory leak for LSTM,This issue has been fixed.,6817
19787,Error in nll_loss - multi-target not supported,You need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.,2044
19788,Error in nll_loss - multi-target not supported,You need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.,2044
19789,softmax doesn't support negative dimensions,This feature will not be implemented.,9573
19790,softmax doesn't support negative dimensions,This feature will not be implemented.,9573
19791,torch.load() requires model module in the same folder,You need to add the directory with your module to `sys.path`.,4963
19792,torch.load() requires model module in the same folder,You need to add the directory with your module to `sys.path`.,4963
19793,Pytorch AssertionError: Torch not compiled with CUDA enabled,PyTorch does not support GPU on OSX,11447
19794,Pytorch AssertionError: Torch not compiled with CUDA enabled,PyTorch does not support GPU on OSX,11447
19795,Variable methods which need to change before we combine Variable and Tensor,These methods are already implemented.,8275
19796,Variable methods which need to change before we combine Variable and Tensor,These methods are already implemented.,8275
19797,"using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__","This issue is not related to PyTorch, but to Python.",7479
19798,"using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__","This issue is not related to PyTorch, but to Python.",7479
19799,Windows source build fails with 'error LNK2019' at linking stage,Unset the flag `BUILD_TEST=0` to fix the error.,3103
19800,Windows source build fails with 'error LNK2019' at linking stage,Unset the flag `BUILD_TEST=0` to fix the error.,3103
19801,Too few arguments to vulkanOptimizeForMobile(),This issue has been fixed.,7158
19802,Too few arguments to vulkanOptimizeForMobile(),This issue has been fixed.,7158
19803,CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0,Building PyTorch binary using `devtoolset-7` can fix this issue.,1528
19804,CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0,Building PyTorch binary using `devtoolset-7` can fix this issue.,1528
19805,nn.Module.script,Please use  TorchText's `Vocab` instead of `ScriptVocab`.,4938
19806,nn.Module.script,Please use  TorchText's `Vocab` instead of `ScriptVocab`.,4938
19807,Add RMSE loss function,This feature will not be implemented.,934
19808,Add RMSE loss function,This feature will not be implemented.,934
19809,Is there a typo in ReLU6 declaration ?,"No, there isn't.",10261
19810,Is there a typo in ReLU6 declaration ?,"No, there isn't.",10261
19811,Ellipsis support for view and reshape functions,This feature will not be implemented.,3329
19812,Ellipsis support for view and reshape functions,This feature will not be implemented.,3329
19813,Static Quantized model accuracy varies greatly with Calibration data,This issue can be fixed using numeric suite.,8112
19814,Static Quantized model accuracy varies greatly with Calibration data,This issue can be fixed using numeric suite.,8112
19815,test_nn.py returns inconsistent result in different test setup,This issue does not arise while using GPU.,2796
19816,test_nn.py returns inconsistent result in different test setup,This issue does not arise while using GPU.,2796
19817,Nightly builds for cp38 missing for non-windows targets,This issue has been fixed.,4155
19818,Nightly builds for cp38 missing for non-windows targets,This issue has been fixed.,4155
19819,Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor,This is an expected behavior.,7643
19820,Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor,This is an expected behavior.,7643
19821,Building wheel for torch (setup.py) ... error - While running pip install torch,Install using instructions from PyTorch's website.,8613
19822,Building wheel for torch (setup.py) ... error - While running pip install torch,Install using instructions from PyTorch's website.,8613
19823,load_state_dict_from_url error with weights downloaded from Google Drive ?,"Please use a direct download URL, something like `https://sites.google.com/site/gdocs2direct/`",6883
19824,load_state_dict_from_url error with weights downloaded from Google Drive ?,"Please use a direct download URL, something like `https://sites.google.com/site/gdocs2direct/`",6883
19825,Support arbitrary types in jit,These methods are already implemented.,8277
19826,Support arbitrary types in jit,These methods are already implemented.,8277
19827,[C++ API] Support for CIFAR10 and CIFAR100 Datasets,They will be added soon,4351
19828,[C++ API] Support for CIFAR10 and CIFAR100 Datasets,They will be added soon,4351
19829,Cannot use setup.py install,You need to install the package 'future',237
19830,Cannot use setup.py install,You need to install the package 'future',237
19831,torch.cat is moving Tensors across devices silently,This is an expected behavior.,4655
19832,torch.cat is moving Tensors across devices silently,This is an expected behavior.,4655
19833,Compilation error on aarch64,You need to set the flag `USE_SYSTEM_SLEEF=ON`.,3176
19834,Compilation error on aarch64,You need to set the flag `USE_SYSTEM_SLEEF=ON`.,3176
19835,[feature request] Allow `torch.unsqueeze` to insert multiple new dims,"You can use `t_b = t[(..., ) + (None, ) * 3]`",724
19836,[feature request] Allow `torch.unsqueeze` to insert multiple new dims,"You can use `t_b = t[(..., ) + (None, ) * 3]`",724
19837,[feature request] add `torch.find` to find the indices of values,"You can use \r\n```python\r\ndef find(tensor, values):\r\n return torch.nonzero(tensor[..., None] == values)\r\n```",828
19838,[feature request] add `torch.find` to find the indices of values,"You can use \r\n```python\r\ndef find(tensor, values):\r\n return torch.nonzero(tensor[..., None] == values)\r\n```",828
19839,Segmentation Fault using dist.broadcast() with openmpi,Installing the latest version of openmpi fixes this issue.,927
19840,Segmentation Fault using dist.broadcast() with openmpi,Installing the latest version of openmpi fixes this issue.,927
19841,.topk() returns incorrect values + indeces on non-contiguous tensors (CUDA),You need to pass contiguous inputs to `topk`,7248
19842,.topk() returns incorrect values + indeces on non-contiguous tensors (CUDA),You need to pass contiguous inputs to `topk`,7248
19843,OOM when using Adam optimizer compared to SGD when using same batch size.,"Adam is more stateful than SGD, so it is expected that it uses more memory.",938
19844,OOM when using Adam optimizer compared to SGD when using same batch size.,"Adam is more stateful than SGD, so it is expected that it uses more memory.",938
19845,log_prob returns positive values for small cov,The values of the `pdf` can be arbitrarily large but never negative. ,7356
19846,log_prob returns positive values for small cov,The values of the `pdf` can be arbitrarily large but never negative. ,7356
19847,"\""ImportError: No module named tools.setup_helpers.env\"" when \""python setup.py egg_info\""",Please install using the flag `FULL_CAFFE2=1`,2185
19848,"\""ImportError: No module named tools.setup_helpers.env\"" when \""python setup.py egg_info\""",Please install using the flag `FULL_CAFFE2=1`,2185
19849,[bug] Multiplication of tensor with numpy scalar does not always work,"The problem is that the left operand is the default operand to execute the `__mul__`.\r\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\r\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.",7229
19850,[bug] Multiplication of tensor with numpy scalar does not always work,"The problem is that the left operand is the default operand to execute the `__mul__`.\r\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\r\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.",7229
19851,git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.,Installing Caffe2 from source fixes the error.,5988
19852,git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.,Installing Caffe2 from source fixes the error.,5988
19853,[jit] support at::optional,This is an expected behavior.,9044
19854,[jit] support at::optional,This is an expected behavior.,9044
19855,[distributions] dirichlet pathwise gradient does not work well with .expand,This issue has been fixed.,3266
19856,[distributions] dirichlet pathwise gradient does not work well with .expand,This issue has been fixed.,3266
19857,[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`,This feature will not be implemented.,976
19858,[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`,This feature will not be implemented.,976
19859,"RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True",Use `sudo python` to fix this.,9892
19860,"RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True",Use `sudo python` to fix this.,9892
19861,Add python 3.7 to binary install page,"python 3.7 binaries are live on PyPI, conda and on https://pytorch.org",10520
19862,Add python 3.7 to binary install page,"python 3.7 binaries are live on PyPI, conda and on https://pytorch.org",10520
19863,Caffe2 Train your own image,"Please refer to the \""CIFAR10_Part1\"" and \""CIFAR10_Part2\"" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. ",545
19864,Caffe2 Train your own image,"Please refer to the \""CIFAR10_Part1\"" and \""CIFAR10_Part2\"" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. ",545
19865,Conda Install: PackageNotFoundError,This can be fixed by updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda`.,674
19866,Conda Install: PackageNotFoundError,This can be fixed by updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda`.,674
19867,"Always get error \""ConnectionResetError: [Errno 104] Connection reset by peer\""",Setting `num_workers=0` will fix this problem,9375
19868,"Always get error \""ConnectionResetError: [Errno 104] Connection reset by peer\""",Setting `num_workers=0` will fix this problem,9375
19869,[Bug] Segmentation fault when importing fastText (with v0.4.0),To fix:\r\n - install gcc-4.9 in conda\r\n - install pytorch with `conda install` r\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone,2581
19870,[Bug] Segmentation fault when importing fastText (with v0.4.0),To fix:\r\n - install gcc-4.9 in conda\r\n - install pytorch with `conda install` r\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone,2581
19871,How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source,This issue has been fixed.,518
19872,How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source,This issue has been fixed.,518
19873,"[feature request] torch.isinf, torch.isfinite",`torch.isinf` has been merged; `torch.isfinite` has not been implemented yet,8747
19874,"[feature request] torch.isinf, torch.isfinite",`torch.isinf` has been merged; `torch.isfinite` has not been implemented yet,8747
19875,cannot reload on CPU model saved on GPU,"When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`.",8391
19876,cannot reload on CPU model saved on GPU,"When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`.",8391
19877,setting CUDA_VISIBLE_DEVICES just has no effect,You need to do that before import pytorch.,8645
19878,setting CUDA_VISIBLE_DEVICES just has no effect,You need to do that before import pytorch.,8645
19879,[feature request] nn.Identity,You can use nn.Sequential() to simulate identity,8605
19880,[feature request] nn.Identity,You can use nn.Sequential() to simulate identity,8605
19881,Big drop in performance for larger batch size for otherwise same training script,"You should tune different hyparams (e.g., lr) to accommodate different batch size. Larger batch size requires larger lr too.",8657
19882,Big drop in performance for larger batch size for otherwise same training script,"You should tune different hyparams (e.g., lr) to accommodate different batch size. Larger batch size requires larger lr too.",8657
19883,torch.save() and nn.DataParallel(),Use the following :\r\n```\r\ntry:\r\n state_dict = model.module.state_dict()\r\nexcept AttributeError:\r\n state_dict = model.state_dict()\r\n```,3298
19884,torch.save() and nn.DataParallel(),Use the following :\r\n```\r\ntry:\r\n state_dict = model.module.state_dict()\r\nexcept AttributeError:\r\n state_dict = model.state_dict()\r\n```,3298
19885,torch.Tensor.__repr__ is slow,This issue has been fixed.,7576
19886,torch.Tensor.__repr__ is slow,This issue has been fixed.,7576
19887,"[feature request] Convert \""indices\"" variable in \""torch.utils.data.dataset.random_split\"" to list",You can add a `.tolist()` after the code.,3273
19888,"[feature request] Convert \""indices\"" variable in \""torch.utils.data.dataset.random_split\"" to list",You can add a `.tolist()` after the code.,3273
19889,problem building with ROCm,Please set the environment variable `USE_ROCM=1`,7425
19890,problem building with ROCm,Please set the environment variable `USE_ROCM=1`,7425
19891,RNN weights are not Xavier-initialized,You cant try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.,1109
19892,RNN weights are not Xavier-initialized,You cant try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.,1109
19893,"Sequential does not allow muti-output modules such as RNN, LSTM",This is an expected behavior.,1280
19894,"Sequential does not allow muti-output modules such as RNN, LSTM",This is an expected behavior.,1280
19895,Weights won't update during backpropogation,Your learning rate has to be increased.,11464
19896,Weights won't update during backpropogation,Your learning rate has to be increased.,11464
19897,"error: identifier \""__half_as_ushort\"" is undefined",This error can be resolved by reinstalling CUDA.,3984
19898,"error: identifier \""__half_as_ushort\"" is undefined",This error can be resolved by reinstalling CUDA.,3984
19899,"When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.",Install the latest version of PyTorch to fix this.,3166
19900,"When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.",Install the latest version of PyTorch to fix this.,3166
19901,Memory leak from Function.save_for_backward() when looping over batch,Use static methods to fix this issue.,10841
19902,Memory leak from Function.save_for_backward() when looping over batch,Use static methods to fix this issue.,10841
19903,"Segfault in neg, introduced in 3e6e81d",This issue has been fixed.,2465
19904,"Segfault in neg, introduced in 3e6e81d",This issue has been fixed.,2465
19905,`torch.utils.data.DataLoader`,This is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe. To actually allow concurrent reads for a file you have to use SWMR feature of HDF5.,10887
19906,`torch.utils.data.DataLoader`,This is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe. To actually allow concurrent reads for a file you have to use SWMR feature of HDF5.,10887
19907,inconsistent behavior of max,This issue has been fixed.,10915
19908,inconsistent behavior of max,This issue has been fixed.,10915
19909,Kaiming/Xavier initializer cannot deal with bias term,This can be achieved by `layer.bias.data.zero_()`,1917
19910,Kaiming/Xavier initializer cannot deal with bias term,This can be achieved by `layer.bias.data.zero_()`,1917
19911,Implementation Discussion: Native CTC,This feature has been implemented.,8662
19912,Implementation Discussion: Native CTC,This feature has been implemented.,8662
19913,Can I plz has determinant function?,This feature has been implemented.,9350
19914,Can I plz has determinant function?,This feature has been implemented.,9350
19915,Recent bug in torch.cat() on Variables?,This issue has been fixed.,2747
19916,Recent bug in torch.cat() on Variables?,This issue has been fixed.,2747
19917,cuda out of memory error when GPU0 memory is fully utilized,You need to set `CUDA_VISIBLE_DEVICES` outside the python process,519
19918,cuda out of memory error when GPU0 memory is fully utilized,You need to set `CUDA_VISIBLE_DEVICES` outside the python process,519
19919,RuntimeError: CUDA error (3): initialization error,"Insert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(\""spawn\"")\r\n```",8899
19920,RuntimeError: CUDA error (3): initialization error,"Insert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(\""spawn\"")\r\n```",8899
19921,RuntimeError: context has already been set(multiprocessing),"You can call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`.",2839
19922,RuntimeError: context has already been set(multiprocessing),"You can call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`.",2839
19923,Gradient Ascent Cross Entropy Loss,Use this: `(-loss).backward(); optimizer.step()`,553
19924,Gradient Ascent Cross Entropy Loss,Use this: `(-loss).backward(); optimizer.step()`,553
19925,ParameterList and ModuleList with named modules or parameters,This issue has been fixed.,9352
19926,ParameterList and ModuleList with named modules or parameters,This issue has been fixed.,9352
19927,PyTorch Implementation of Michael Jordan’s lab's Perturbed SGD?,"A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.",3227
19928,PyTorch Implementation of Michael Jordan’s lab's Perturbed SGD?,"A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.",3227
19929,NameError: name 'logging' is not defined,This issue has been fixed.,3255
19930,NameError: name 'logging' is not defined,This issue has been fixed.,3255
19931,Builing for a specific SM number,"You can do this via:\r\n```\r\nTORCH_CUDA_ARCH_LIST=\""5.2;6.1;7.0\"" python setup.py install\r\n```",8541
19932,Builing for a specific SM number,"You can do this via:\r\n```\r\nTORCH_CUDA_ARCH_LIST=\""5.2;6.1;7.0\"" python setup.py install\r\n```",8541
19933,[jit] torch.empty_like is different from eager mode,"This is not an issue with `empty_like` in particular, but with all optional arguments.",3214
19934,[jit] torch.empty_like is different from eager mode,"This is not an issue with `empty_like` in particular, but with all optional arguments.",3214
19935,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0,Use this command:\r\n```\r\npip3 install torch torchvision\r\n```,8024
19936,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0,Use this command:\r\n```\r\npip3 install torch torchvision\r\n```,8024
19937,CI failure points to nonexistent code...,This issue has been fixed.,9351
19938,CI failure points to nonexistent code...,This issue has been fixed.,9351
19939,torch.onnx._export does not support tensor sum with multiple dims,Install the latest version of PyTorch to fix this.,747
19940,torch.onnx._export does not support tensor sum with multiple dims,Install the latest version of PyTorch to fix this.,747
19941,issue with ONNX and PyTorch,You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation.,544
19942,issue with ONNX and PyTorch,You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation.,544
19943,Wrong distribution sampled by torch.multinomial on CUDA,This issue has been fixed.,1592
19944,Wrong distribution sampled by torch.multinomial on CUDA,This issue has been fixed.,1592
19945,Inplace error if DistributedDataParallel module that contains a buffer is called twice,"To fix this , disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor.",6984
19946,Inplace error if DistributedDataParallel module that contains a buffer is called twice,"To fix this , disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor.",6984
19947,LR scheduler design bug !,This issue has been fixed.,4003
19948,LR scheduler design bug !,This issue has been fixed.,4003
19949,Segmentation fault Autograd,This issue has been fixed.,692
19950,Segmentation fault Autograd,This issue has been fixed.,692
19951,Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?,"In the current code, sigmoid is also deprecated in nn, but not relu.",932
19952,Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?,"In the current code, sigmoid is also deprecated in nn, but not relu.",932
19953,Building from source failed. Multiple errors in the printout,"To fix this, export the following environment variable:\r\n\r\n`export TORCH_CUDA_ARCH_LIST=\""7.0\""`",8577
19954,Building from source failed. Multiple errors in the printout,"To fix this, export the following environment variable:\r\n\r\n`export TORCH_CUDA_ARCH_LIST=\""7.0\""`",8577
19955,[dataloader] Add a context= argument for multiprocessing,"This is expected, because thed spawned workers does not see the dataset def.\n The proper way to solve this is to add a `context=` argument to data loader.",7644
19956,[dataloader] Add a context= argument for multiprocessing,"This is expected, because thed spawned workers does not see the dataset def.\n The proper way to solve this is to add a `context=` argument to data loader.",7644
19957,"Torch crashes when calling torch.rand(2,3)",Your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. You have to install from source.,11466
19958,"Torch crashes when calling torch.rand(2,3)",Your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. You have to install from source.,11466
19959,No method to set the timeout for distributed Gloo backend,This can be fixed by changing the default timeout and recompiling PyTorch,575
19960,No method to set the timeout for distributed Gloo backend,This can be fixed by changing the default timeout and recompiling PyTorch,575
19961,[PyTorch] Build error (NCCL) on Ubuntu 16.04,A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. ,841
19962,[PyTorch] Build error (NCCL) on Ubuntu 16.04,A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. ,841
19963,"Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue",You need to uninstall and reinstall Caffe2.,41
19964,"Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue",You need to uninstall and reinstall Caffe2.,41
19965,"BatchNorm2d when batch size 1 works, what is it doing?",It is normalizing `[B x C x *]` over the dimensions `[*]`,6886
19966,"BatchNorm2d when batch size 1 works, what is it doing?",It is normalizing `[B x C x *]` over the dimensions `[*]`,6886
19967,can't rebuild with NO_CUDA=1 after clean,Please run :\r\n`\r\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\r\n`,2194
19968,can't rebuild with NO_CUDA=1 after clean,Please run :\r\n`\r\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\r\n`,2194
19969,Where is the Caffe2 website?,It is at `https://github.com/caffe2/caffe2.github.io`.,9534
19970,Where is the Caffe2 website?,It is at `https://github.com/caffe2/caffe2.github.io`.,9534
19971,Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed,Please run \r\n```\r\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\r\n```,3128
19972,Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed,Please run \r\n```\r\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\r\n```,3128
19973,Add `torch.pi` like `numpy.pi` and possibly other constants,They exist in `math`,1277
19974,Add `torch.pi` like `numpy.pi` and possibly other constants,They exist in `math`,1277
19975,"torch.irfft produces \""cuFFT error: CUFFT_ALLOC_FAILED\"" when called after torch.rfft",Having fewer temp tensors along the way can help.,4230
19976,"torch.irfft produces \""cuFFT error: CUFFT_ALLOC_FAILED\"" when called after torch.rfft",Having fewer temp tensors along the way can help.,4230
19977,[feature request] Complex multiplication,Add a kwarg `complex = True` argument to `torch.mul`.,4661
19978,[feature request] Complex multiplication,Add a kwarg `complex = True` argument to `torch.mul`.,4661
19979,cuda runtime error (48): no kernel image is available for execution on the device,You can build from source to use some functionality. Some operations require a more recent GPU.,2916
19980,cuda runtime error (48): no kernel image is available for execution on the device,You can build from source to use some functionality. Some operations require a more recent GPU.,2916
19981,Stop using undefined tensors to represent zero gradients in engine,This issue has been fixed.,9480
19982,Stop using undefined tensors to represent zero gradients in engine,This issue has been fixed.,9480
19983,Proposal: rename upsample to resample,Use the `interpolate` function instead.,7530
19984,Proposal: rename upsample to resample,Use the `interpolate` function instead.,7530
19985,RuntimeError for indexing with high dimensional tensor only when using cuda,This issue has been fixed.,1302
19986,RuntimeError for indexing with high dimensional tensor only when using cuda,This issue has been fixed.,1302
19987,RuntimeError: reduce failed to synchronize: unspecified launch failure,Make sure your layer has values that make sense to the BCELoss.,3308
19988,RuntimeError: reduce failed to synchronize: unspecified launch failure,Make sure your layer has values that make sense to the BCELoss.,3308
19989,Zombie process when use GPU,Use the following command:\r\n`fuser -k /dev/nvidia*`\r\nor\r\n`kill $(lsof -t /dev/nvidia*)`,2504
19990,Zombie process when use GPU,Use the following command:\r\n`fuser -k /dev/nvidia*`\r\nor\r\n`kill $(lsof -t /dev/nvidia*)`,2504
19991,torch.HalfTensor' object has no attribute 'mean',Please convert tensor to cuda or CPU Float.,11357
19992,torch.HalfTensor' object has no attribute 'mean',Please convert tensor to cuda or CPU Float.,11357
19993,import torch; libcublas.so.9.0 error,Installing from pip fixes this error.,3120
19994,import torch; libcublas.so.9.0 error,Installing from pip fixes this error.,3120
19995,Expose find Dangling Impls to Python,"In an open source build can do `python test/test_dispatch.py` 
 There are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding",212
19996,Expose find Dangling Impls to Python,"In an open source build can do `python test/test_dispatch.py` 
 There are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding",212
19997,Inconsistent CUDA errors using PyTorch Docker image,"Run the example on another machine, and didn't encounter any issues. The issue lies with the specific configuration of the original machine",8299
19998,Inconsistent CUDA errors using PyTorch Docker image,"Run the example on another machine, and didn't encounter any issues. The issue lies with the specific configuration of the original machine",8299
19999,Parametrization goes into infinite recursion when trying to print a module,"Proposed approach - One can make it work modifying just the internal tensor storage via the function set_ (https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html). This should work in the case when, for example, you are sharing the mask between a number of layers. You would then set a new mask as:
 
 model.linear.mask.set_(torch.zeros(model.linear.weight.shape))
 A simplified implementation could then be:
 
 import torch
 import torch.nn as nn
 import torch.nn.utils.parametrize as parametrize
 
 class WeightMaskParametrization(nn.Module):
  def __init__(self, mask):
  super().__init__()
  self.mask = mask
 
  def forward(self, w):
  return self.mask * w
 
 model = nn.Linear(3, 4)
 model.register_buffer('mask', torch.ones_like(model.weight))
 parametrize.register_parametrization(model, 'weight', WeightMaskParametrization(model.mask))
 
 print(model.weight) # print original weight
 model.mask.set_(torch.zeros_like(model.weight))
 print(model.weight) # print zeros
 In this simplified example, it would be even better to have the mask live in the class WeightMaskParametrization rather than in nn.Linear, and thus, be managed directly by this class, but I guess that the real example motivating this issue may not allow for that.
 
 Another thing to point out is that this WeightMaskParametrization is just pruning in disguise. We will eventually move all the pruning methods into parametrizations, and the base parametrization will certainly take the form of WeightMaskParametrization, with a few more bells and whistles.",4493
20000,Parametrization goes into infinite recursion when trying to print a module,"Proposed approach - One can make it work modifying just the internal tensor storage via the function set_ (https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html). This should work in the case when, for example, you are sharing the mask between a number of layers. You would then set a new mask as:
 
 model.linear.mask.set_(torch.zeros(model.linear.weight.shape))
 A simplified implementation could then be:
 
 import torch
 import torch.nn as nn
 import torch.nn.utils.parametrize as parametrize
 
 class WeightMaskParametrization(nn.Module):
  def __init__(self, mask):
  super().__init__()
  self.mask = mask
 
  def forward(self, w):
  return self.mask * w
 
 model = nn.Linear(3, 4)
 model.register_buffer('mask', torch.ones_like(model.weight))
 parametrize.register_parametrization(model, 'weight', WeightMaskParametrization(model.mask))
 
 print(model.weight) # print original weight
 model.mask.set_(torch.zeros_like(model.weight))
 print(model.weight) # print zeros
 In this simplified example, it would be even better to have the mask live in the class WeightMaskParametrization rather than in nn.Linear, and thus, be managed directly by this class, but I guess that the real example motivating this issue may not allow for that.
 
 Another thing to point out is that this WeightMaskParametrization is just pruning in disguise. We will eventually move all the pruning methods into parametrizations, and the base parametrization will certainly take the form of WeightMaskParametrization, with a few more bells and whistles.",4493
20001,Something like nn.Dropout2d which does channel dropout but for 1d data. Could be called nn.Dropout1d,"Contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:
 
 ```python
 >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))
 tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],
  [ 0.2068, 1.2131, 1.2506, 2.1023],
  [ 0.0000, -0.0000, 0.0000, -0.0000]],
 
  [[-2.2049, -4.3484, 0.4871, 1.2764],
  [-0.0000, 0.0000, 0.0000, 0.0000],
  [-1.0111, -0.5624, 0.7527, -0.0970]]])
 ```
 
 Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well.",2807
20002,Something like nn.Dropout2d which does channel dropout but for 1d data. Could be called nn.Dropout1d,"Contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:
 
 ```python
 >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))
 tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],
  [ 0.2068, 1.2131, 1.2506, 2.1023],
  [ 0.0000, -0.0000, 0.0000, -0.0000]],
 
  [[-2.2049, -4.3484, 0.4871, 1.2764],
  [-0.0000, 0.0000, 0.0000, 0.0000],
  [-1.0111, -0.5624, 0.7527, -0.0970]]])
 ```
 
 Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well.",2807
20003,Issue in executing the script module in C++,"Call result.toTuple() instead of result.toTensor():
 ```C++
 auto result = model({inputs});
 for (auto& t : result.toTuple())
 {
  // t is a Tensor now
 }
 ```
 
 Recommend to push all tensors to a vector. You can also interogate the result to find what type it is.",8644
20004,Issue in executing the script module in C++,"Call result.toTuple() instead of result.toTensor():
 ```C++
 auto result = model({inputs});
 for (auto& t : result.toTuple())
 {
  // t is a Tensor now
 }
 ```
 
 Recommend to push all tensors to a vector. You can also interogate the result to find what type it is.",8644
20005,Deterministic indexing operation fails in indices size check / missing broadcast,"Old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886).",7938
20006,Deterministic indexing operation fails in indices size check / missing broadcast,"Old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886).",7938
20007,How to get in touch about a security issue?,"If you believe you have found a security vulnerability in PyTorch, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.
 
 Please refer to the following page for our responsible disclosure policy, reward guidelines, and those things that should not be reported:
 
 https://www.facebook.com/whitehat",5101
20008,How to get in touch about a security issue?,"If you believe you have found a security vulnerability in PyTorch, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.
 
 Please refer to the following page for our responsible disclosure policy, reward guidelines, and those things that should not be reported:
 
 https://www.facebook.com/whitehat",5101
20009,"Help, nn.functional.interpolate make no sense at image resizing",Use torchvision.transforms.Resize to resize images,3210
20010,"Help, nn.functional.interpolate make no sense at image resizing",Use torchvision.transforms.Resize to resize images,3210
20011,TensorPipe: build error: 'CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL' was not declared in this scope,Upgrade CUDA,5944
20012,TensorPipe: build error: 'CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL' was not declared in this scope,Upgrade CUDA,5944
20013,"BN+ReLU cause ""RuntimeError: MALFORMED INPUT: bad dtype in CompareSelect"" error in fp16, traced module","Smaller reproducer that doesn't require CUDA 
 
 import torch
 import torch._C._te as te
 
 input_str = """"""
 graph(%x : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu),
  %weight : Half(3, strides=[1], requires_grad=0, device=cpu),
  %bias : Half(3, strides=[1], requires_grad=0, device=cpu),
  %running_mean : Half(3, strides=[1], requires_grad=0, device=cpu),
  %running_var : Half(3, strides=[1], requires_grad=0, device=cpu)):
  %5 : bool = prim::Constant[value=1]()
  %6 : float = prim::Constant[value=0.001]()
  %7 : float = prim::Constant[value=0.10000000000000001]()
  %8 : bool = prim::Constant[value=0]()
  %input.1 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::batch_norm(%x, %weight, %bias, %running_mean, %running_var, %8, %7, %6, %5)
  %10 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::relu(%input.1)
  return (%10)
 """"""
 
 
 class kernel_arena_scope(object):
  def __enter__(self):
  self.scope = torch._C._te.KernelScope()
 
  def __exit__(self, typ, val, traceback):
  self.scope = None
 
 with kernel_arena_scope():
  graph = torch._C.parse_ir(input_str)
  print(graph)
  kernel = te.TensorExprKernel(graph) # Fails
  print(kernel.get_code_text(""asm""))",7714
20014,"BN+ReLU cause ""RuntimeError: MALFORMED INPUT: bad dtype in CompareSelect"" error in fp16, traced module","Smaller reproducer that doesn't require CUDA 
 
 import torch
 import torch._C._te as te
 
 input_str = """"""
 graph(%x : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu),
  %weight : Half(3, strides=[1], requires_grad=0, device=cpu),
  %bias : Half(3, strides=[1], requires_grad=0, device=cpu),
  %running_mean : Half(3, strides=[1], requires_grad=0, device=cpu),
  %running_var : Half(3, strides=[1], requires_grad=0, device=cpu)):
  %5 : bool = prim::Constant[value=1]()
  %6 : float = prim::Constant[value=0.001]()
  %7 : float = prim::Constant[value=0.10000000000000001]()
  %8 : bool = prim::Constant[value=0]()
  %input.1 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::batch_norm(%x, %weight, %bias, %running_mean, %running_var, %8, %7, %6, %5)
  %10 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::relu(%input.1)
  return (%10)
 """"""
 
 
 class kernel_arena_scope(object):
  def __enter__(self):
  self.scope = torch._C._te.KernelScope()
 
  def __exit__(self, typ, val, traceback):
  self.scope = None
 
 with kernel_arena_scope():
  graph = torch._C.parse_ir(input_str)
  print(graph)
  kernel = te.TensorExprKernel(graph) # Fails
  print(kernel.get_code_text(""asm""))",7714
20015,mean' reduction result in CrossEntropyLoss mismatches with manually computing mean,"This isn't a bug, look at the documentation for NLLLoss.
 
 See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss.",7699
20016,mean' reduction result in CrossEntropyLoss mismatches with manually computing mean,"This isn't a bug, look at the documentation for NLLLoss.
 
 See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss.",7699
20017,forward compatibility was attempted on non supported HW,"Reboot or reset nvidia drivers.
 <https://stackoverflow.com/a/45319156/1391392>",4031
20018,forward compatibility was attempted on non supported HW,"Reboot or reset nvidia drivers.
 <https://stackoverflow.com/a/45319156/1391392>",4031
20019,torch.cuda.is_available() is False,"`torch.distributed.is_available()` is always `False` on Windows, it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068).",233
20020,torch.cuda.is_available() is False,"`torch.distributed.is_available()` is always `False` on Windows, it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068).",233
20021,Using all_gather() in the forward pass in DDP throws RuntimeError,"To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you.",2805
20022,Using all_gather() in the forward pass in DDP throws RuntimeError,"To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you.",2805
20023,Implement autograd functions for c10d communication operations,implemented this in chainermn,6893
20024,Implement autograd functions for c10d communication operations,implemented this in chainermn,6893
20025,pca_lowrank memory allocation,"matmul doesn't support broadcasting the singleton dimension 
 Define M = C.as_strided(A.shape, (0, 1)) and then use _svd_lowrank(A, q, niter=niter, M=M).",206
20026,pca_lowrank memory allocation,"matmul doesn't support broadcasting the singleton dimension 
 Define M = C.as_strided(A.shape, (0, 1)) and then use _svd_lowrank(A, q, niter=niter, M=M).",206
20027,How to do matrix multiplication between two 2D sparse directly and quickly,"Look for similar function in pytorch 
 tf.sparse_matmul(
  a,
  b,
  transpose_a=False,
  transpose_b=False,
  a_is_sparse=False,
  b_is_sparse=False,
  name=None
 )",3097
20028,How to do matrix multiplication between two 2D sparse directly and quickly,"Look for similar function in pytorch 
 tf.sparse_matmul(
  a,
  b,
  transpose_a=False,
  transpose_b=False,
  a_is_sparse=False,
  b_is_sparse=False,
  name=None
 )",3097
20029,Unable to use Pytorch with CUDA,add `torch.cuda.current_device()` after `import torch` and it should fix the issue,2220
20030,Unable to use Pytorch with CUDA,add `torch.cuda.current_device()` after `import torch` and it should fix the issue,2220
20031,CPU torch.norm gives strange results for LargeTensor on Colab,"PyTorch 0.4 did the accumulation using double https://github.com/pytorch/pytorch/blob/v0.4.1/aten/src/TH/generic/THTensorMath.cpp#L4307
 
 Now it's using float accumulation:
 https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L57
 
 CUDA uses float accumulation, but is saved because the necessary parallelism forces a form of pairwise summation. We should probably do the same thing for CPU",7609
20032,CPU torch.norm gives strange results for LargeTensor on Colab,"PyTorch 0.4 did the accumulation using double https://github.com/pytorch/pytorch/blob/v0.4.1/aten/src/TH/generic/THTensorMath.cpp#L4307
 
 Now it's using float accumulation:
 https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L57
 
 CUDA uses float accumulation, but is saved because the necessary parallelism forces a form of pairwise summation. We should probably do the same thing for CPU",7609
20033,pytorch won't install from wheel,"workaround works, but it's very portable...
 wget https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl
 pip install torch-1.6.0+cpu-cp37-cp37m-linux_x86_64.whl 
 Or use ```torch==1.7.1+cu110```.",3333
20034,pytorch won't install from wheel,"workaround works, but it's very portable...
 wget https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl
 pip install torch-1.6.0+cpu-cp37-cp37m-linux_x86_64.whl 
 Or use ```torch==1.7.1+cu110```.",3333
20035,cannot import name 'ngrams_iterator',report to https://github.com/pytorch/tex,3267
20036,cannot import name 'ngrams_iterator',report to https://github.com/pytorch/tex,3267
20037,PyPI is slow please create git releases,"Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started
 
 
 
 Example:
 
 
 
 ```
 
 pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch
 
 ```",5142
20038,PyPI is slow please create git releases,"Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started
 
 
 
 Example:
 
 
 
 ```
 
 pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch
 
 ```",5142
20039,Tensor print format issue on Nvidia Jetson devices,disable neon for Jetson,8000
20040,Tensor print format issue on Nvidia Jetson devices,disable neon for Jetson,8000
20041,Roll-up: remaining TH functions,LegacyFunctionsCPU is no longer available,4089
20042,Roll-up: remaining TH functions,LegacyFunctionsCPU is no longer available,4089
20043,type_as() method change device too,`type()` is like `torch.cuda.FloatTensor` which contains both the device and dtype.,1421
20044,type_as() method change device too,`type()` is like `torch.cuda.FloatTensor` which contains both the device and dtype.,1421
20045,Error instaling using PIP and CUDA on windows 10,Please use 64-bit Python instead.,4862
20046,Error instaling using PIP and CUDA on windows 10,Please use 64-bit Python instead.,4862
20047,torch.tril_indices returns a float tensor on master,specify `dtype` to make it work,8483
20048,torch.tril_indices returns a float tensor on master,specify `dtype` to make it work,8483
20049,Failed to build pytorch 1.4.0 on Ubuntu 18.04.4 LTS,The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine.,7157
20050,Failed to build pytorch 1.4.0 on Ubuntu 18.04.4 LTS,The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine.,7157
20051,Complex number printing inconsistent with float,"In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:
 1. `, ` should line up across multiple lines of output.
 2. `+/-` should line up across multiple lines of output.
 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor.",539
20052,Complex number printing inconsistent with float,"In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:
 1. `, ` should line up across multiple lines of output.
 2. `+/-` should line up across multiple lines of output.
 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor.",539
20053,[Complex] Incorrect Complex Tensor inference,"There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.
 
 ``` Python
 j = 10
 c_num = eval(""1/2j"") # c_num = -0.5j (j is the suffix of 2)
 c_num = eval(""(1/2)*j"") #c_num = 5 (j is a variable = 10)
 ```
 
 Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:
 
 ``` Python
 j = 10
 c_num = eval(""1j/2"") # c_num = 0.5j (j is the suffix of 1)
 ```
 
 It's confusing, but this method allows people to use a variable named `j`.",3222
20054,[Complex] Incorrect Complex Tensor inference,"There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.
 
 ``` Python
 j = 10
 c_num = eval(""1/2j"") # c_num = -0.5j (j is the suffix of 2)
 c_num = eval(""(1/2)*j"") #c_num = 5 (j is a variable = 10)
 ```
 
 Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:
 
 ``` Python
 j = 10
 c_num = eval(""1j/2"") # c_num = 0.5j (j is the suffix of 1)
 ```
 
 It's confusing, but this method allows people to use a variable named `j`.",3222
20055,[feature request] random integer generator,read the blogs on pytorch.org to understand the building system and the `ATEN` document.,501
20056,[feature request] random integer generator,read the blogs on pytorch.org to understand the building system and the `ATEN` document.,501
20057,Error in Function backward when forward output is in-place of forward input and input is not leaf,"use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents",7396
20058,Error in Function backward when forward output is in-place of forward input and input is not leaf,"use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents",7396
20059,[BUG?]The same code behaves differently on pytorch based on py2 and py3 whose version are both 0.3.0.post_4,"from a rough scan, this looks like that it might be the culprit:
 
 ```
 
  scale = math.sqrt(3 / fan_in)
 
 ```
 
 in py2, `int / int` is an `int`; in py3, it is a `float`.",9397
20060,[BUG?]The same code behaves differently on pytorch based on py2 and py3 whose version are both 0.3.0.post_4,"from a rough scan, this looks like that it might be the culprit:
 
 ```
 
  scale = math.sqrt(3 / fan_in)
 
 ```
 
 in py2, `int / int` is an `int`; in py3, it is a `float`.",9397
20061,"""torch.sum()"" over a ByteTensor gives incorrect result when the ""dim"" is specified",This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed),7575
20062,"""torch.sum()"" over a ByteTensor gives incorrect result when the ""dim"" is specified",This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed),7575
20063,Support F.normalize on 1-dim tensors without explicit dim,"`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case.",8734
20064,Support F.normalize on 1-dim tensors without explicit dim,"`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case.",8734
20065,Add support similar to `tf.images.resize_images`.,We now have nn.functional.interpolate that does the same thing except for bicubic method. https://pytorch.org/docs/master/nn.html?highlight=interpolate#torch.nn.functional.interpolate,10912
20066,Add support similar to `tf.images.resize_images`.,We now have nn.functional.interpolate that does the same thing except for bicubic method. https://pytorch.org/docs/master/nn.html?highlight=interpolate#torch.nn.functional.interpolate,10912
20067,memory leaky on DataLoader,"loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)",7247
20068,memory leaky on DataLoader,"loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)",7247
20069,Distributed communication never frees target memory,"suggest switching to a different backend (e.g. gloo or MPI), because TCP is really nearly a debug-mode thing and will likely not get anywhere close to peak performance.",2909
20070,Distributed communication never frees target memory,"suggest switching to a different backend (e.g. gloo or MPI), because TCP is really nearly a debug-mode thing and will likely not get anywhere close to peak performance.",2909
20071,KLDivLoss behaves differently on CPU/GPU,Bug Fixed,6866
20072,KLDivLoss behaves differently on CPU/GPU,Bug Fixed,6866
20073,nccl2 backend distributed package,get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64,8629
20074,nccl2 backend distributed package,get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64,8629
20075,torch.poisson does not work with FloatTensor on master,Bug Fixed,3217
20076,torch.poisson does not work with FloatTensor on master,Bug Fixed,3217
20077,[feature request] same behavior in multi-gpu DataParallel vs single GPU,"Criiterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:
 
 ```python
 import torch
 import torch.nn as nn
 from torch.autograd import Variable
 input = Variable(torch.randn(8, 10).cuda())
 target = Variable(torch.ones(8).long().cuda())
 criterion = nn.CrossEntropyLoss().cuda()
 
 linear = nn.Linear(10, 10).cuda()
 dp = nn.DataParallel(linear, [0, 1])
 
 criterion(linear(input), target).backward()
 print(linear.weight.grad)
 
 dp.zero_grad()
 criterion(dp(input), target).backward()
 print(linear.weight.grad)
 ```",8675
20078,[feature request] same behavior in multi-gpu DataParallel vs single GPU,"Criiterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:
 
 ```python
 import torch
 import torch.nn as nn
 from torch.autograd import Variable
 input = Variable(torch.randn(8, 10).cuda())
 target = Variable(torch.ones(8).long().cuda())
 criterion = nn.CrossEntropyLoss().cuda()
 
 linear = nn.Linear(10, 10).cuda()
 dp = nn.DataParallel(linear, [0, 1])
 
 criterion(linear(input), target).backward()
 print(linear.weight.grad)
 
 dp.zero_grad()
 criterion(dp(input), target).backward()
 print(linear.weight.grad)
 ```",8675
20079,Assigning to an index of a sparse tensor is a no-op,"scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:
 ```
 >>> m = scipy.sparse.rand(3, 4, density=0.25)
 >>> m
 <3x4 sparse matrix of type '<class 'numpy.float64'>'
 with 3 stored elements in COOrdinate format>
 >>> m[0,0] = 100
 Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
 TypeError: 'coo_matrix' object does not support item assignment
 
 >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')
 >>> m[0,0] = 100
 .../python3.8/site-packages/scipy/sparse/_index.py:82: \
  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \
  lil_matrix is more efficient.
  self._set_intXint(row, col, x.flat[0])
 
 >>> m.todense()
 matrix([[100. , 0. , 0. , 0.2128684 ],
  [ 0. , 0. , 0. , 0.93232551],
  [ 0. , 0.98662986, 0. , 0. ]])
 ```",2371
20080,Assigning to an index of a sparse tensor is a no-op,"scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:
 ```
 >>> m = scipy.sparse.rand(3, 4, density=0.25)
 >>> m
 <3x4 sparse matrix of type '<class 'numpy.float64'>'
 with 3 stored elements in COOrdinate format>
 >>> m[0,0] = 100
 Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
 TypeError: 'coo_matrix' object does not support item assignment
 
 >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')
 >>> m[0,0] = 100
 .../python3.8/site-packages/scipy/sparse/_index.py:82: \
  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \
  lil_matrix is more efficient.
  self._set_intXint(row, col, x.flat[0])
 
 >>> m.todense()
 matrix([[100. , 0. , 0. , 0.2128684 ],
  [ 0. , 0. , 0. , 0.93232551],
  [ 0. , 0.98662986, 0. , 0. ]])
 ```",2371
20081,@torch.jit.script causes compilation error on Ampere architecture (RTX 3090),"A look at the documentation of nvrtc included with CUDA Toolkit 11.0 reveals that it indeed does not support the Ampere architecture (compute_80). This support is introduced in CUDA Toolkit 11.1.
 
 Unfortunately, the conda repositories do not contain builds of PyTorch against CUDA 11.1.",1501
20082,@torch.jit.script causes compilation error on Ampere architecture (RTX 3090),"A look at the documentation of nvrtc included with CUDA Toolkit 11.0 reveals that it indeed does not support the Ampere architecture (compute_80). This support is introduced in CUDA Toolkit 11.1.
 
 Unfortunately, the conda repositories do not contain builds of PyTorch against CUDA 11.1.",1501
20083,Errors in Eigen library when building pytorch mobile for Android,The problem is in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works.,3083
20084,Errors in Eigen library when building pytorch mobile for Android,The problem is in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works.,3083
20085,nn.utils.spectral_norm() does not perform normalization of the weight matrix.,"It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:
 
 ```py
 
 In [1]: import torch
 
  ...: print(torch.__version__)
 
  ...:
 
  ...: linear = torch.nn.Linear(3, 4)
 
  ...: norm_layer = torch.nn.utils.spectral_norm(linear)
 
  ...:
 
  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)
 
  ...: print('Original weight matrix: ', norm_layer.weight_orig)
 
  ...:
 
  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))
 
  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)
 
 1.7.0a0+33e2665
 
 Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]])
 
 Original weight matrix: Parameter containing:
 
 tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)
 
 Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],
 
  [ 2.1241, -2.7887, 1.7012],
 
  [ 1.6401, -3.0922, 1.5386],
 
  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)
 
 
 
 In [2]: norm_layer(torch.randn((3)))
 
 Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)
 
 
 
 In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)
 
  ...: print('Original weight matrix: ', norm_layer.weight_orig)
 
 Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],
 
  [-0.3385, 0.4444, -0.2711],
 
  [-0.2614, 0.4928, -0.2452],
 
  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)
 
 Original weight matrix: Parameter containing:
 
 tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)
 
 
 
 In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))
 
  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)
 
 Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],
 
  [-0.3385, 0.4444, -0.2711],
 
  [-0.2614, 0.4928, -0.2452],
 
  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)
 
 
 
 ```",3910
20086,nn.utils.spectral_norm() does not perform normalization of the weight matrix.,"It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:
 
 ```py
 
 In [1]: import torch
 
  ...: print(torch.__version__)
 
  ...:
 
  ...: linear = torch.nn.Linear(3, 4)
 
  ...: norm_layer = torch.nn.utils.spectral_norm(linear)
 
  ...:
 
  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)
 
  ...: print('Original weight matrix: ', norm_layer.weight_orig)
 
  ...:
 
  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))
 
  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)
 
 1.7.0a0+33e2665
 
 Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]])
 
 Original weight matrix: Parameter containing:
 
 tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)
 
 Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],
 
  [ 2.1241, -2.7887, 1.7012],
 
  [ 1.6401, -3.0922, 1.5386],
 
  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)
 
 
 
 In [2]: norm_layer(torch.randn((3)))
 
 Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)
 
 
 
 In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)
 
  ...: print('Original weight matrix: ', norm_layer.weight_orig)
 
 Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],
 
  [-0.3385, 0.4444, -0.2711],
 
  [-0.2614, 0.4928, -0.2452],
 
  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)
 
 Original weight matrix: Parameter containing:
 
 tensor([[ 0.4658, 0.3948, -0.1485],
 
  [-0.3340, 0.4385, -0.2675],
 
  [-0.2579, 0.4863, -0.2420],
 
  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)
 
 
 
 In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))
 
  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)
 
 Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],
 
  [-0.3385, 0.4444, -0.2711],
 
  [-0.2614, 0.4928, -0.2452],
 
  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)
 
 
 
 ```",3910
20087,Enable PyTorch compilation on Apple Silicon,"I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~
 
 The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).
 
 ### 1. Building Guide:
 
 1. fix deps:
 
 `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`
 I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.
 
 
 2. clone pytorch repo and build:
 
 First:
  ```shell
  git clone --recursive https://github.com/pytorch/pytorch
  cd pytorch
  ```
  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:
  (1) `option(USE_OPENMP ""Use OpenMP for parallel code"" ON)` to `option(USE_OPENMP ""Use OpenMP for parallel code"" OFF)`
  (2) `USE_MKLDNN ""Use MKLDNN. Only available on x86 and x86_64."" ON` to `USE_MKLDNN ""Use MKLDNN. Only available on x86 and x86_64."" OFF` 
 
 Then:
 ```shell
 export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
 MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install
  ```
 make sure here `python` is pointed to the version installed by conda-forge. 
 
 ### 2. Install from whl file
 
 I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!
 
 ### 3. Speed test
 
 From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:
 
 > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:
 > 
 > ```
 > from tqdm import tqdm
 > import torch
 > 
 > @torch.jit.script
 > def foo():
 > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)
 > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)
 > z = x + y
 > return z
 > 
 > 
 > if __name__ == '__main__':
 > z0 = None
 > for _ in tqdm(range(10000000000)):
 > zz = foo()
 > if z0 is None:
 > z0 = zz
 > else:
 > z0 += zz
 > ```
 > 
 > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!
 > Also the 3900x cpu shows just 21it/s.
 > 
 > Hope gets GPU and neural engine support soon.
 
 On my macbook air, the speed is around 44~45 it/s. 
 
 Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.
 Update2: Speed on TITAN XP GPU: ~320 it/s",2816
20088,Enable PyTorch compilation on Apple Silicon,"I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~
 
 The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).
 
 ### 1. Building Guide:
 
 1. fix deps:
 
 `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`
 I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.
 
 
 2. clone pytorch repo and build:
 
 First:
  ```shell
  git clone --recursive https://github.com/pytorch/pytorch
  cd pytorch
  ```
  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:
  (1) `option(USE_OPENMP ""Use OpenMP for parallel code"" ON)` to `option(USE_OPENMP ""Use OpenMP for parallel code"" OFF)`
  (2) `USE_MKLDNN ""Use MKLDNN. Only available on x86 and x86_64."" ON` to `USE_MKLDNN ""Use MKLDNN. Only available on x86 and x86_64."" OFF` 
 
 Then:
 ```shell
 export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
 MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install
  ```
 make sure here `python` is pointed to the version installed by conda-forge. 
 
 ### 2. Install from whl file
 
 I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!
 
 ### 3. Speed test
 
 From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:
 
 > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:
 > 
 > ```
 > from tqdm import tqdm
 > import torch
 > 
 > @torch.jit.script
 > def foo():
 > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)
 > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)
 > z = x + y
 > return z
 > 
 > 
 > if __name__ == '__main__':
 > z0 = None
 > for _ in tqdm(range(10000000000)):
 > zz = foo()
 > if z0 is None:
 > z0 = zz
 > else:
 > z0 += zz
 > ```
 > 
 > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!
 > Also the 3900x cpu shows just 21it/s.
 > 
 > Hope gets GPU and neural engine support soon.
 
 On my macbook air, the speed is around 44~45 it/s. 
 
 Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.
 Update2: Speed on TITAN XP GPU: ~320 it/s",2816
20089,The `normalize` function failed in the TorchScript interpreter with AMP enabled.,"Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:
 
 ""autocast interaction with jit scripting is very much WIP"".
 
 
 
 > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.
 
 
 
 Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:
 
 ```python
 
 def my_norm(x: Tensor):
 
  with autocast():
 
  return F.normalize(x, p=2.)
 
 
 
 device = torch.device('cuda:0')
 
 scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))
 
 scripted_norm(torch.rand((32, 32), device=device))
 
 ```
 
 Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility).",1405
20090,The `normalize` function failed in the TorchScript interpreter with AMP enabled.,"Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:
 
 ""autocast interaction with jit scripting is very much WIP"".
 
 
 
 > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.
 
 
 
 Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:
 
 ```python
 
 def my_norm(x: Tensor):
 
  with autocast():
 
  return F.normalize(x, p=2.)
 
 
 
 device = torch.device('cuda:0')
 
 scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))
 
 scripted_norm(torch.rand((32, 32), device=device))
 
 ```
 
 Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility).",1405
20091,[feature request] consistent default values for leaky_relu,1e-2 is 0.01,8708
20092,[feature request] consistent default values for leaky_relu,1e-2 is 0.01,8708
20093,Segmentation fault while training,"FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:
 
 ```
 
 FROM nvidia/cuda:9.0-devel-ubuntu16.04
 
 ENV PYTORCH_VERSION=0.4.1
 
 ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0
 
 ENV NCCL_VERSION=2.3.5-2+cuda9.0
 
 ENV HOROVOD_VERSION=0.15.2
 
 ```
 
 with `openmpi-3.1.2`. 
 
 
 
 Moving to 
 
 ```
 
 ENV PYTORCH_VERSION=1.0.0
 
 ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0
 
 ```
 
 seems to have fixed this issue.",2395
20094,Segmentation fault while training,"FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:
 
 ```
 
 FROM nvidia/cuda:9.0-devel-ubuntu16.04
 
 ENV PYTORCH_VERSION=0.4.1
 
 ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0
 
 ENV NCCL_VERSION=2.3.5-2+cuda9.0
 
 ENV HOROVOD_VERSION=0.15.2
 
 ```
 
 with `openmpi-3.1.2`. 
 
 
 
 Moving to 
 
 ```
 
 ENV PYTORCH_VERSION=1.0.0
 
 ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0
 
 ```
 
 seems to have fixed this issue.",2395
20095,Windows builds with CUDA 9.2,CUDA 9.2 is added into our Windows AMI,8066
20096,Windows builds with CUDA 9.2,CUDA 9.2 is added into our Windows AMI,8066
20097,[Feature Request] Inverse Hyperbolic Functions,"A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` 
 
 
 
 From https://www.plunk.org/~hatch/rightway.php",818
20098,[Feature Request] Inverse Hyperbolic Functions,"A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` 
 
 
 
 From https://www.plunk.org/~hatch/rightway.php",818
20099,cuda runtime error(59): device-side assert when running torch.topk,"define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing ""model_ft.fc = nn.Linear(num_ftrs, 2)"" to ""model_ft.fc = nn.Linear(num_ftrs, 3)",3157
20100,cuda runtime error(59): device-side assert when running torch.topk,"define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing ""model_ft.fc = nn.Linear(num_ftrs, 2)"" to ""model_ft.fc = nn.Linear(num_ftrs, 3)",3157
20101,"Disabling MPI fails, unable to build on Centos7","USE_MPI is not well defined for `python setup.py install`.
 
 
 
 Here's what you need:
 
 
 
 `USE_DISTRIBUTED=0 python setup.py install`",8046
20102,"Disabling MPI fails, unable to build on Centos7","USE_MPI is not well defined for `python setup.py install`.
 
 
 
 Here's what you need:
 
 
 
 `USE_DISTRIBUTED=0 python setup.py install`",8046
20103,[feature request] Matrix rank,We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`.,731
20104,[feature request] Matrix rank,We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`.,731
20105,"Initialization can be surprisingly important, and under-rated",See https://github.com/pytorch/pytorch/pull/9038,4042
20106,"Initialization can be surprisingly important, and under-rated",See https://github.com/pytorch/pytorch/pull/9038,4042
20107,libtorch.so.1: undefined symbol:,export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64,9294
20108,libtorch.so.1: undefined symbol:,export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64,9294
20109,a strange torch.no_grad behaviour when used with lazy_property from distributions,"Fixed in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1.",185
20110,a strange torch.no_grad behaviour when used with lazy_property from distributions,"Fixed in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1.",185
20111,[feature request] Treat tensor as tuple of tensors in torch.cat,"We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).
 I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension.",3248
20112,[feature request] Treat tensor as tuple of tensors in torch.cat,"We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).
 I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension.",3248
20113,Differences between .data and .detach,"Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..
 
 
 
 ```
 
 >>> a = torch.tensor([1,2,3.], requires_grad = True)
 
 >>> out = a.sigmoid()
 
 >>> c = out.detach()
 
 >>> c.zero_() 
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out # modified by c.zero_() !!
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()
 
 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
 
 ```
 
 
 
 As opposed to using `.data`:
 
 ```
 
 >>> a = torch.tensor([1,2,3.], requires_grad = True)
 
 >>> out = a.sigmoid()
 
 >>> c = out.data
 
 >>> c.zero_()
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out # out was modified by c.zero_()
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out.sum().backward()
 
 >>> a.grad # The result is very, very wrong because `out` changed!
 
 tensor([ 0., 0., 0.])
 
 ```
 
 
 
 I'll leave this issue open: we should add an example to the migration guide and clarify that section.",2798
20114,Differences between .data and .detach,"Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..
 
 
 
 ```
 
 >>> a = torch.tensor([1,2,3.], requires_grad = True)
 
 >>> out = a.sigmoid()
 
 >>> c = out.detach()
 
 >>> c.zero_() 
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out # modified by c.zero_() !!
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()
 
 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
 
 ```
 
 
 
 As opposed to using `.data`:
 
 ```
 
 >>> a = torch.tensor([1,2,3.], requires_grad = True)
 
 >>> out = a.sigmoid()
 
 >>> c = out.data
 
 >>> c.zero_()
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out # out was modified by c.zero_()
 
 tensor([ 0., 0., 0.])
 
 
 
 >>> out.sum().backward()
 
 >>> a.grad # The result is very, very wrong because `out` changed!
 
 tensor([ 0., 0., 0.])
 
 ```
 
 
 
 I'll leave this issue open: we should add an example to the migration guide and clarify that section.",2798
20115,Segmentation fault with cpp_extensions example,"It seems that you are using GCC 4.8
 you would have get the following message from `cpp_extensions`?
 ```
 Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!
 Please use a compiler that is ABI-compatible with GCC 4.9 and above.
 See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.
 
 See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6
 for instructions on how to install GCC 4.9 or higher.
 ```
 try updating gcc to 4.9 or higher",3975
20116,Segmentation fault with cpp_extensions example,"It seems that you are using GCC 4.8
 you would have get the following message from `cpp_extensions`?
 ```
 Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!
 Please use a compiler that is ABI-compatible with GCC 4.9 and above.
 See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.
 
 See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6
 for instructions on how to install GCC 4.9 or higher.
 ```
 try updating gcc to 4.9 or higher",3975
20117,RecursionError when using torch.utils.checkpoint,increased the max recursion depth like this `sys.setrecursionlimit(3000)`,3073
20118,RecursionError when using torch.utils.checkpoint,increased the max recursion depth like this `sys.setrecursionlimit(3000)`,3073
20119,It seems can't compile pytorch 0.4.0 from source on macOS.,Use cmake version 3.9.4. You can see it via `cmake --version`,8335
20120,It seems can't compile pytorch 0.4.0 from source on macOS.,Use cmake version 3.9.4. You can see it via `cmake --version`,8335
20121,pip install torch error,`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org.,8738
20122,pip install torch error,`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org.,8738
20123,torch.std() returns nan for single item tensors.,"np default is ddof=0, but pytorch has default unbiased=True.",7660
20124,torch.std() returns nan for single item tensors.,"np default is ddof=0, but pytorch has default unbiased=True.",7660
20125,Indexing with dtype torch.uint8,modify `dtype` in this function,3882
20126,Indexing with dtype torch.uint8,modify `dtype` in this function,3882
20127,windows release of pytorch 1.3 is missing several required cuda libraries,The suffix of DLLs is changed to `_10.dll` instead of `_101.dll`.,8322
20128,windows release of pytorch 1.3 is missing several required cuda libraries,The suffix of DLLs is changed to `_10.dll` instead of `_101.dll`.,8322
20129,[jit] `torch.isfinite` is broken,"we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.
  A workaround is to call `torch.functional.isfinite(x)`",222
20130,[jit] `torch.isfinite` is broken,"we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.
  A workaround is to call `torch.functional.isfinite(x)`",222
20131,RuntimeError: Didn't find engine for operation quantized::conv_prepack NoQEngine,try setting `torch.backends.quantized.engine = 'qnnpack'`,9043
20132,RuntimeError: Didn't find engine for operation quantized::conv_prepack NoQEngine,try setting `torch.backends.quantized.engine = 'qnnpack'`,9043
20133,Multiprocess data loader crash/hang in v1.3,"This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c
 
 1.3.0.post2 or 1.3.1 both have the fix, please upgrade.",7827
20134,Multiprocess data loader crash/hang in v1.3,"This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c
 
 1.3.0.post2 or 1.3.1 both have the fix, please upgrade.",7827
20135,Using tensor cores,NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf,7418
20136,Using tensor cores,NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf,7418
20137,"""no module named torch"". But installed pytorch 1.3.0 with conda in Ubuntu 18.04.02 Server Edition",Activate or deactive one of the enviornment,8631
20138,"""no module named torch"". But installed pytorch 1.3.0 with conda in Ubuntu 18.04.02 Server Edition",Activate or deactive one of the enviornment,8631
20139,Torch from sourc and Torchvision from pip,`pip install --user --no-dependencies torchvision`,8737
20140,Torch from sourc and Torchvision from pip,`pip install --user --no-dependencies torchvision`,8737
20141,Cannot compile CPP extensions for benchmarking,"`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment.",8727
20142,Cannot compile CPP extensions for benchmarking,"`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment.",8727
20143,"""undefined symbol: PySlice_Unpack"" of pytorch 1.0.0 on Ubuntu 14.04",use Python 3.6.3.,732
20144,"""undefined symbol: PySlice_Unpack"" of pytorch 1.0.0 on Ubuntu 14.04",use Python 3.6.3.,732
20145,Can not build pytorch tag v1.0.0 from source,downgrade cudnn from 7.4.1 to 7.3.0,3069
20146,Can not build pytorch tag v1.0.0 from source,downgrade cudnn from 7.4.1 to 7.3.0,3069
20147,Wrong recursive module::load,Fixed,6889
20148,Wrong recursive module::load,Fixed,6889
20149,torch.jit.trace incorrect for function outputting tuple of size 1,Fixed,11344
20150,torch.jit.trace incorrect for function outputting tuple of size 1,Fixed,11344
20151,what is the algorithm theory of torch.nn.AdaptiveMaxPool2d?,"You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder",3173
20152,what is the algorithm theory of torch.nn.AdaptiveMaxPool2d?,"You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder",3173
20153,RuntimeError: std::exception for conv2d with groups > 1,creating a new user + installing Anaconda + installing PyTorch,4290
20154,RuntimeError: std::exception for conv2d with groups > 1,creating a new user + installing Anaconda + installing PyTorch,4290
20155,torch::jit::trace C++ equivalent of torch.jit.trace,"It's not possible to trace scalar values in C++
 
 plan to merge script::Module and nn::Module in a future release",1139
20156,torch::jit::trace C++ equivalent of torch.jit.trace,"It's not possible to trace scalar values in C++
 
 plan to merge script::Module and nn::Module in a future release",1139
20157,libcudart.so not found when compiling with NO_DISTRIBUTED=1,"The short-term fix is quite simple, to switch to linking against libcudart.so.
 
 The long-term fix is to figure out what ctypes' problem is",7245
20158,libcudart.so not found when compiling with NO_DISTRIBUTED=1,"The short-term fix is quite simple, to switch to linking against libcudart.so.
 
 The long-term fix is to figure out what ctypes' problem is",7245
20159,When will the pytorch 1.0 stable version release?,"7th Dec,2018",9396
20160,When will the pytorch 1.0 stable version release?,"7th Dec,2018",9396
20161,"DataLoader error with multiple workers - ""RuntimeError: unable to open shared memory object XXX in read-write mode""","increasing your open files limit (try ulimit)
 trying another multiprocessing sharing strategy (https://pytorch.org/docs/master/multiprocessing.html#sharing-strategies)",5784
20162,"DataLoader error with multiple workers - ""RuntimeError: unable to open shared memory object XXX in read-write mode""","increasing your open files limit (try ulimit)
 trying another multiprocessing sharing strategy (https://pytorch.org/docs/master/multiprocessing.html#sharing-strategies)",5784
20163,"The sort order for tensor.unique() defaults to ""unsorted"" but in practice is ascending on gpu and descending on cpu",Fixed soerted = True,7619
20164,"The sort order for tensor.unique() defaults to ""unsorted"" but in practice is ascending on gpu and descending on cpu",Fixed soerted = True,7619
20165,issues to run minimal pytorch example,"Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`.",2471
20166,issues to run minimal pytorch example,"Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`.",2471
20167,"v1.7 requirement ""dataclasses"" is not compatible with python > 3.6","patch the wheel to remove the requirement in the metadata.
 Or install it in two steps from the latest requirements (or manually) : 
 ```
 pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt
 pip install <torch_wheel> --no-deps
 ```",629
20168,"v1.7 requirement ""dataclasses"" is not compatible with python > 3.6","patch the wheel to remove the requirement in the metadata.
 Or install it in two steps from the latest requirements (or manually) : 
 ```
 pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt
 pip install <torch_wheel> --no-deps
 ```",629
20169,"[ONNX] error with pytorch ""RuntimeError: Unexpected node type: onnx::Cast""",Use the latest version of PyTorch (1.7),662
20170,"[ONNX] error with pytorch ""RuntimeError: Unexpected node type: onnx::Cast""",Use the latest version of PyTorch (1.7),662
20171,clip_grad_norm_ silently passes when not finite,argument error_if_nonfinite=True,954
20172,clip_grad_norm_ silently passes when not finite,argument error_if_nonfinite=True,954
20173,x.shape return tuple instead of torch.Size,Bug Fixed,3007
20174,x.shape return tuple instead of torch.Size,Bug Fixed,3007
20175,"new_ones() missing 1 required positional arguments: ""size""","new_zeros does work:
 
 from torch import Tensor
 
 class MyTensor(Tensor):
  pass
 
 a = MyTensor([1,2,3])
 a.new_zeros(320).shape",8545
20176,"new_ones() missing 1 required positional arguments: ""size""","new_zeros does work:
 
 from torch import Tensor
 
 class MyTensor(Tensor):
  pass
 
 a = MyTensor([1,2,3])
 a.new_zeros(320).shape",8545
20177,batch_norm const running_mean/_var modified in-place,By design,561
20178,batch_norm const running_mean/_var modified in-place,By design,561
20179,torch.distributed.broadcast_object_list doesn't broadcast custom class object,"printing local dummy() object, not the object broadcast from rank 0. 
 
 Try to revise codes like this: 
 
  if dist.get_rank() == 0:
  objects = [dummy()] 
  else:
  objects = [None]
  torch.distributed.broadcast_object_list(objects, src=0)
 
  print(objects)",11446
20180,torch.distributed.broadcast_object_list doesn't broadcast custom class object,"printing local dummy() object, not the object broadcast from rank 0. 
 
 Try to revise codes like this: 
 
  if dist.get_rank() == 0:
  objects = [dummy()] 
  else:
  objects = [None]
  torch.distributed.broadcast_object_list(objects, src=0)
 
  print(objects)",11446
20181,Tensorboard: ValueError: Duplicate plugins for name projector, this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda,483
20182,Tensorboard: ValueError: Duplicate plugins for name projector, this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda,483
20183,Tensorboard: ValueError: Duplicate plugins for name projector, this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda,483
20184,Tensorboard: ValueError: Duplicate plugins for name projector, this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda,483
20185,"Data Loader does not work with Hdf5 file, when num_worker >1","**Solution**
 
 
 
 This issue could be solved and the solution is simple: 
 
 1. Do not open hdf5 inside `__init__`
 
 2. Open the hdf5 at the first data iteration.
 
 
 
 Here is an illustration:
 
 ```python
 
 class LXRTDataLoader(torch.utils.data.Dataset):
 
  def __init__(self):
 
  """"""do not open hdf5 here!!""""""
 
 
 
  def open_hdf5(self):
 
  self.img_hdf5 = h5py.File('img.hdf5', 'r')
 
  self.dataset = self.img_hdf5['dataset'] # if you want dataset.
 
 
 
  def __getitem__(self, item: int):
 
  if not hasattr(self, 'img_hdf5'):
 
  self.open_hdf5()
 
  img0 = self.img_hdf5['dataset'][0] # Do loading here
 
  img1 = self.dataset[1]
 
  return img0, img1
 
 ```
 
 Then the dataloader with `num_workers` > 1 could just be normally used.
 
 ```python
 
 train_loader = torch.utils.data.DataLoader(
 
  dataset=train_tset,
 
  batch_size=32,
 
  num_workers=4
 
  )
 
 ```
 
 
 
 
 
 
 
 **Explanation**
 
 The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):
 
 https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720
 
 In short, it would create multiple processes which ""copy"" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. 
 
 
 
 If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:
 
 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)
 
 2. The state of the hdfs is copied, which might not faithfully indicate the current state. 
 
 
 
 In the previous way, we bypass this two issues.",73
20186,"Data Loader does not work with Hdf5 file, when num_worker >1","**Solution**
 
 
 
 This issue could be solved and the solution is simple: 
 
 1. Do not open hdf5 inside `__init__`
 
 2. Open the hdf5 at the first data iteration.
 
 
 
 Here is an illustration:
 
 ```python
 
 class LXRTDataLoader(torch.utils.data.Dataset):
 
  def __init__(self):
 
  """"""do not open hdf5 here!!""""""
 
 
 
  def open_hdf5(self):
 
  self.img_hdf5 = h5py.File('img.hdf5', 'r')
 
  self.dataset = self.img_hdf5['dataset'] # if you want dataset.
 
 
 
  def __getitem__(self, item: int):
 
  if not hasattr(self, 'img_hdf5'):
 
  self.open_hdf5()
 
  img0 = self.img_hdf5['dataset'][0] # Do loading here
 
  img1 = self.dataset[1]
 
  return img0, img1
 
 ```
 
 Then the dataloader with `num_workers` > 1 could just be normally used.
 
 ```python
 
 train_loader = torch.utils.data.DataLoader(
 
  dataset=train_tset,
 
  batch_size=32,
 
  num_workers=4
 
  )
 
 ```
 
 
 
 
 
 
 
 **Explanation**
 
 The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):
 
 https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720
 
 In short, it would create multiple processes which ""copy"" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. 
 
 
 
 If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:
 
 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)
 
 2. The state of the hdfs is copied, which might not faithfully indicate the current state. 
 
 
 
 In the previous way, we bypass this two issues.",73
20187,"DataLoader, when num_worker is great than 1, the data loaded are wrong, or a error a reported!","h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...
 
 (i) pip uninstall h5py
 
 (ii) pip install mpi4py
 
 (iii) pip install h5py.",9499
20188,"DataLoader, when num_worker is great than 1, the data loaded are wrong, or a error a reported!","h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...
 
 (i) pip uninstall h5py
 
 (ii) pip install mpi4py
 
 (iii) pip install h5py.",9499
20189,DataParallel with Multi-GPU module (eg. 2 model replica on 4 GPU),"The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. 
 
 And wrap the data transfer in the `forward` function.
 
 Here is a working example. Hopefully, someone finds this useful. 
 
 ``` 
 class ConvBlck(nn.Module):
 
  def __init__(self, in_channel, out_channel, kernel_size, stride=1):
 
  super(ConvBlck, self).__init__()
 
  self.blck = nn.Sequential( OrderedDict([
 
  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),
 
  ('relu', nn.ReLU() )]) )
 
  def forward(self, x):
 
  return self.blck(x)
 
 
 
 class SixConv(nn.Module):
 
  def __init__(self):
 
  super(SixConv, self).__init__()
 
  tmp = 5
 
  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )
 
  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )
 
 
 
  def forward(self, x):
 
  x = self.conv1(x)
 
  next_index = x.device.index + 1
 
  x = x.cuda(next_index)
 
  x = self.conv2(x)
 
  return x
 
 model = SixConv()
 
 model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);
 
 model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);
 
 crite = torch.nn.CrossEntropyLoss()
 
 optim = torch.optim.Adam( model.parameters() , lr=0.001)
 
 model.conv1.to( 'cuda:0' )
 
 model.conv2.to( 'cuda:1' )
 
 
 
 ```",7362
20190,DataParallel with Multi-GPU module (eg. 2 model replica on 4 GPU),"The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. 
 
 And wrap the data transfer in the `forward` function.
 
 Here is a working example. Hopefully, someone finds this useful. 
 
 ``` 
 class ConvBlck(nn.Module):
 
  def __init__(self, in_channel, out_channel, kernel_size, stride=1):
 
  super(ConvBlck, self).__init__()
 
  self.blck = nn.Sequential( OrderedDict([
 
  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),
 
  ('relu', nn.ReLU() )]) )
 
  def forward(self, x):
 
  return self.blck(x)
 
 
 
 class SixConv(nn.Module):
 
  def __init__(self):
 
  super(SixConv, self).__init__()
 
  tmp = 5
 
  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )
 
  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )
 
 
 
  def forward(self, x):
 
  x = self.conv1(x)
 
  next_index = x.device.index + 1
 
  x = x.cuda(next_index)
 
  x = self.conv2(x)
 
  return x
 
 model = SixConv()
 
 model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);
 
 model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);
 
 crite = torch.nn.CrossEntropyLoss()
 
 optim = torch.optim.Adam( model.parameters() , lr=0.001)
 
 model.conv1.to( 'cuda:0' )
 
 model.conv2.to( 'cuda:1' )
 
 
 
 ```",7362
20191,index_put_ has unreasonable checks,pass the `indices` in the right format,8550
20192,index_put_ has unreasonable checks,pass the `indices` in the right format,8550
20193,upgrade MKL-DNN 0.20.1 to DNNL 1.1,The claim from Intel is that with DNNL 1.1 it will be faster,3786
20194,upgrade MKL-DNN 0.20.1 to DNNL 1.1,The claim from Intel is that with DNNL 1.1 it will be faster,3786
20195,Future callbacks in RPC should capture and restore autograd context id,"ad-hoc fix will still leave the profiler broken after RPC thread switch,  we should consider leveraging ThreadLocalState.cpp",2399
20196,Future callbacks in RPC should capture and restore autograd context id,"ad-hoc fix will still leave the profiler broken after RPC thread switch,  we should consider leveraging ThreadLocalState.cpp",2399
20197,"[Feature] View a floating point tensor as complex tensor, and vice versa","When implementing `view_as_floating` , `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free.",208
20198,"[Feature] View a floating point tensor as complex tensor, and vice versa","When implementing `view_as_floating` , `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free.",208
20199,"Should torch.real, torch.imag be implemented as a view?",tensor.imag is disabled for real tensors,5709
20200,"Should torch.real, torch.imag be implemented as a view?",tensor.imag is disabled for real tensors,5709
20201,"Can you add an overloaded operator &,|,^ to a libtorch tensor","We can Overload bitwise NOT, AND, OR, XOR operators",4700
20202,"Can you add an overloaded operator &,|,^ to a libtorch tensor","We can Overload bitwise NOT, AND, OR, XOR operators",4700
20203,torch._C._cuda_getDriverVersion() reporting CUDA version instead of NVIDIA driver version,Returns the latest version of CUDA supported by the driver,9737
20204,torch._C._cuda_getDriverVersion() reporting CUDA version instead of NVIDIA driver version,Returns the latest version of CUDA supported by the driver,9737
20205,gradcheck does not work for self-overlapping inputs,This is an error in gradcheck not in gradient computation Add a check for stride==0 in gradcheck ,7623
20206,gradcheck does not work for self-overlapping inputs,This is an error in gradcheck not in gradient computation Add a check for stride==0 in gradcheck ,7623
20207,libtorch 1.5.0 libiomp5.dylib contains erroneous link to /DLC/torch/libiomp5.dylib instead of using @rpath,"Use ```shell
install_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib
```",3123
20208,libtorch 1.5.0 libiomp5.dylib contains erroneous link to /DLC/torch/libiomp5.dylib instead of using @rpath,"Use ```shell
install_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib
```",3123
20209,RuntimeError: CUDA error: an illegal memory access was encountered,"there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception.",229
20210,RuntimeError: CUDA error: an illegal memory access was encountered,"there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception.",229
20211,Will the next version of libtorch provide modules instead of header files in the c++20 standard?,In order to support the old toolchains and old CUDA we need C++ headers.,8638
20212,Will the next version of libtorch provide modules instead of header files in the c++20 standard?,In order to support the old toolchains and old CUDA we need C++ headers.,8638
20213,Some operations crash autograd if parameter size is changed.,"the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does ```cpp { at::AutoNonVariableTypeMode non_var_type_mode(true); self_.set_(args); } ``` to call the Tensor version of `set_` that will be routed to the different implementations in aten.",3284
20214,Some operations crash autograd if parameter size is changed.,"the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does ```cpp { at::AutoNonVariableTypeMode non_var_type_mode(true); self_.set_(args); } ``` to call the Tensor version of `set_` that will be routed to the different implementations in aten.",3284
20215,Make StorageImpl untyped for non-POD types,"for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.So in this case I wouldn't try to remove the resize functions, as we really do need two different versions",211
20216,Make StorageImpl untyped for non-POD types,"for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.So in this case I wouldn't try to remove the resize functions, as we really do need two different versions",211
20217,Gradient checkpointing fails to backprop in some cases,"There could be multiple reasons - The first one raise an error because, since you checkpoint all the way to the end, second one works because the first module is not checkpointed and it's output requires grad, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error.",7640
20218,Gradient checkpointing fails to backprop in some cases,"There could be multiple reasons - The first one raise an error because, since you checkpoint all the way to the end, second one works because the first module is not checkpointed and it's output requires grad, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error.",7640
20219,max_pool2d doesn't check if input is contiguous: max_pool2d_with_indices_out_cuda_frame failed with error code 0 with specific inputs,use `problem_tensor = problem_tensor.contiguous()`,6859
20220,max_pool2d doesn't check if input is contiguous: max_pool2d_with_indices_out_cuda_frame failed with error code 0 with specific inputs,use `problem_tensor = problem_tensor.contiguous()`,6859
20221,LocalFileSystem' object has no attribute 'makedirs',"You can try to 
- Uninstall tensorflow, 
- Reinstall Tensorboard. 
Restart Kernel and tensorboard.",7841
20222,LocalFileSystem' object has no attribute 'makedirs',"You can try to 
- Uninstall tensorflow, 
- Reinstall Tensorboard. 
Restart Kernel and tensorboard.",7841
20223,A minor bug in unused grad_input check of run_backward(),Bux is fixed in [autograd] fix allow_unused checking for C++ API,8697
20224,A minor bug in unused grad_input check of run_backward(),Bux is fixed in [autograd] fix allow_unused checking for C++ API,8697
20225,Can't compile pytorch from source,"gcc don't support MOV V8.4s, V9.4s change MOV V8.4s, V9.4s to MOV V8.16b, V9.16b works",9543
20226,Can't compile pytorch from source,"gcc don't support MOV V8.4s, V9.4s change MOV V8.4s, V9.4s to MOV V8.16b, V9.16b works",9543
20227,Runtime Error when using DistributedDataParallel with torch.no_grad(),using torch1.1.0 nightly this can be solved,4348
20228,Runtime Error when using DistributedDataParallel with torch.no_grad(),using torch1.1.0 nightly this can be solved,4348
20229,Export to ONNX doesn't support basic operations,"torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type, it is not an ONNX error",7499
20230,Export to ONNX doesn't support basic operations,"torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type, it is not an ONNX error",7499
20231,promotion of float with int never increases precision of float,"`torch.double` can be perfectly represented as a `torch.complex128`; it ""is a"" `complex128`. A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double`",269
20232,promotion of float with int never increases precision of float,"`torch.double` can be perfectly represented as a `torch.complex128`; it ""is a"" `complex128`. A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double`",269
20233,Compiling error with tag v1.4.0,``` # you need to update submodule after checkout git submodule sync git submodule update --init --recursive <env variables...> python3 ./setup.py bdist_wheel ```,4004
20234,Compiling error with tag v1.4.0,``` # you need to update submodule after checkout git submodule sync git submodule update --init --recursive <env variables...> python3 ./setup.py bdist_wheel ```,4004
20235,[Bug] Weird bug when using ATen __rshift__() on cuda tensors,This issue has been fixed,5982
20236,[Bug] Weird bug when using ATen __rshift__() on cuda tensors,This issue has been fixed,5982
20237,MacOS conda caffe2 package incorrect protobuf versions.,All pytorch packages are actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers.,1090
20238,MacOS conda caffe2 package incorrect protobuf versions.,All pytorch packages are actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers.,1090
20239,LSTM segmentation fault in docker,you can build a docker image from source,8569
20240,LSTM segmentation fault in docker,you can build a docker image from source,8569
20241,Caffe2 fails to build with TensorRT on the Jetson TX2,Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF`,7661
20242,Caffe2 fails to build with TensorRT on the Jetson TX2,Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF`,7661
20243,GRUCELL crashes python for more than 2 layers,"conda create -n pyto python=3.6 -y
activate pyto

conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y
pip install cufflinks 
pip install sklearn 
conda install tbb -y

cd C:\Users\Gabi\Downloads
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch

set USER_LDFLAGS=/LIBPATH:C:\ProgramData\Miniconda3\envs\pyto\Library\lib
set ""VS150COMNTOOLS=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Auxiliary\Build""
set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1
call ""%VS150COMNTOOLS%\vcvarsall.bat"" x64 -vcvars_ver=14.14
set CMAKE_INCLUDE_PATH=C:\ProgramData\Miniconda3\Library\include
set LIB=C:\ProgramData\Miniconda3\Library\lib;C:\ProgramData\Miniconda3\envs\pyto\Library\lib;%LIB%
python setup.py install",9112
20244,GRUCELL crashes python for more than 2 layers,"conda create -n pyto python=3.6 -y
activate pyto

conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y
pip install cufflinks 
pip install sklearn 
conda install tbb -y

cd C:\Users\Gabi\Downloads
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch

set USER_LDFLAGS=/LIBPATH:C:\ProgramData\Miniconda3\envs\pyto\Library\lib
set ""VS150COMNTOOLS=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Auxiliary\Build""
set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1
call ""%VS150COMNTOOLS%\vcvarsall.bat"" x64 -vcvars_ver=14.14
set CMAKE_INCLUDE_PATH=C:\ProgramData\Miniconda3\Library\include
set LIB=C:\ProgramData\Miniconda3\Library\lib;C:\ProgramData\Miniconda3\envs\pyto\Library\lib;%LIB%
python setup.py install",9112
20245,[Feature request] Equivalent of softmax_cross_entropy_with_logits,"They are not equivalent CrossEntropyLoss expects class indices for the targets, and softmax_cross_entropy_with_logits works with probability distributions.",1478
20246,[Feature request] Equivalent of softmax_cross_entropy_with_logits,"They are not equivalent CrossEntropyLoss expects class indices for the targets, and softmax_cross_entropy_with_logits works with probability distributions.",1478
20247,No module named caffe2.python,Make sure you are not in pytorch dir,8553
20248,No module named caffe2.python,Make sure you are not in pytorch dir,8553
20249,cuda runtime error (77) : an illegal memory access at pytorch\aten\src\thcunn\generic/SpatialClassNLLCriterion.cu,`masks` are having out-of-bounds memory accesses,11462
20250,cuda runtime error (77) : an illegal memory access at pytorch\aten\src\thcunn\generic/SpatialClassNLLCriterion.cu,`masks` are having out-of-bounds memory accesses,11462
20251,public/cuda.cmake uses cuda_select_nvcc_arch_flags, Modules_CUDA_fix to public/cuda.cmake installs the corresponding files,7458
20252,public/cuda.cmake uses cuda_select_nvcc_arch_flags, Modules_CUDA_fix to public/cuda.cmake installs the corresponding files,7458
20253,TypeError: __init__() got an unexpected keyword argument 'target_tensor',"there is no keyword called target_tensor, you can pass variable length argument to the constructor ```python
x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)
dataset = Data.TensorDataset(x, y)
```",2612
20254,TypeError: __init__() got an unexpected keyword argument 'target_tensor',"there is no keyword called target_tensor, you can pass variable length argument to the constructor ```python
x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)
dataset = Data.TensorDataset(x, y)
```",2612
20255,Does torch.Tensor work well with share memory list?,NumPy arrays don't get moved to shared memory by default,1419
20256,Does torch.Tensor work well with share memory list?,NumPy arrays don't get moved to shared memory by default,1419
20257,Loaded network with load_state_dict has different shape but works anyway," fixed in pr `if input_param.shape != param.shape:
                    # local shape should match the one in checkpoint
                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `",9060
20258,Loaded network with load_state_dict has different shape but works anyway," fixed in pr `if input_param.shape != param.shape:
                    # local shape should match the one in checkpoint
                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `",9060
20259,Batchnorm gives different results depending on whether cudnn is enabled,the issue might be related to fp arithmetic being inexact. ,3310
20260,Batchnorm gives different results depending on whether cudnn is enabled,the issue might be related to fp arithmetic being inexact. ,3310
20261,Error in variance/stdv calculations, use `unbiased` option in the `var()` and `std()` function to match the answers.,7413
20262,Error in variance/stdv calculations, use `unbiased` option in the `var()` and `std()` function to match the answers.,7413
20263,[Bug] Failure to acquire gradient with requires_grad and backward(gradient) in cuda (with 0.4.0)," The code is creating a ""leaf"" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors. If you want gradients to accumulate in `x`, you should change code to
``` 
x = torch.randn(2, 2, requires_grad=True, device='cuda:0')
```
",276
20264,[Bug] Failure to acquire gradient with requires_grad and backward(gradient) in cuda (with 0.4.0)," The code is creating a ""leaf"" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors. If you want gradients to accumulate in `x`, you should change code to
``` 
x = torch.randn(2, 2, requires_grad=True, device='cuda:0')
```
",276
20265,Possible bug in KL divergence,"the `input`  has to be a set of log-probabilities,As with NLLLoss, the input given is expected to contain log-probabilities",10850
20266,Possible bug in KL divergence,"the `input`  has to be a set of log-probabilities,As with NLLLoss, the input given is expected to contain log-probabilities",10850
20267,[Bug] Memory leak on Convnet on CPU,"The dimensions of the input image are too big for the kernel size, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB",6908
20268,[Bug] Memory leak on Convnet on CPU,"The dimensions of the input image are too big for the kernel size, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB",6908
20269,[Bug] Model restoration on non cuda computer,"Load the model with 
```python
   torch.load(file, map_location='cpu')
```",921
20270,[Bug] Model restoration on non cuda computer,"Load the model with 
```python
   torch.load(file, map_location='cpu')
```",921
20271,Runtime Error with DataLoader: exited unexpectedly,"For GPU mode we can use -

dataset = UP_Dataset()
train_loader = DataLoader(dataset=dataset,
                          batch_size=100,
                          shuffle=True,
                          num_workers=0)                              # change num_workers=0
",3688
20272,Runtime Error with DataLoader: exited unexpectedly,"For GPU mode we can use -

dataset = UP_Dataset()
train_loader = DataLoader(dataset=dataset,
                          batch_size=100,
                          shuffle=True,
                          num_workers=0)                              # change num_workers=0
",3688
20273,ONNX support for AdaptiveMax/AvgPool ?,"use this module 
```
class MyAdaptiveMaxPool2d(nn.Module):
    def __init__(self, sz=None):
        super().__init__()
       

    def forward(self, x): 
        inp_size = x.size()
        return nn.functional.max_pool2d(input=x,
                  kernel_size= (inp_size[2], inp_size[3]))
```",2824
20274,ONNX support for AdaptiveMax/AvgPool ?,"use this module 
```
class MyAdaptiveMaxPool2d(nn.Module):
    def __init__(self, sz=None):
        super().__init__()
       

    def forward(self, x): 
        inp_size = x.size()
        return nn.functional.max_pool2d(input=x,
                  kernel_size= (inp_size[2], inp_size[3]))
```",2824
20275,"ONNX export does not support parallel model converted using nn.DataParallel(model) and will throw error message ""untraced buffer""","```
state_dict = torch.load('/path/to/your/.pth/model')
model.load_state_dict(state_dict)
model.eval()
dummy_input = Variable(torch.randn(B, C, H, W))
torch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)
```
This should work",8723
20276,"ONNX export does not support parallel model converted using nn.DataParallel(model) and will throw error message ""untraced buffer""","```
state_dict = torch.load('/path/to/your/.pth/model')
model.load_state_dict(state_dict)
model.eval()
dummy_input = Variable(torch.randn(B, C, H, W))
torch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)
```
This should work",8723
20277,Scalar variable's .data attribute returns a non-scalar,There are no scalar tensors,8531
20278,Scalar variable's .data attribute returns a non-scalar,There are no scalar tensors,8531
20279,Determining `requires_grad` automatically,Give inputs= kwarg for .backward(),7669
20280,Determining `requires_grad` automatically,Give inputs= kwarg for .backward(),7669
20281,`Interrupted system call` error appearing after updating install today,give num_workers=0  in dataloader,10280
20282,`Interrupted system call` error appearing after updating install today,give num_workers=0  in dataloader,10280
20283,DataLoader returning non-CUDA tensors,It reserves an area in CPU memory so pulling tensors in that area to GPU is fast,523
20284,DataLoader returning non-CUDA tensors,It reserves an area in CPU memory so pulling tensors in that area to GPU is fast,523
20285,torch.cat behaves differently on Tensor vs Variable (also a backward compatibility issue),"The cat on single Tensor can easily be done by view/reshape. Or by torch.cat(list(x), dim=1) .",2802
20286,torch.cat behaves differently on Tensor vs Variable (also a backward compatibility issue),"The cat on single Tensor can easily be done by view/reshape. Or by torch.cat(list(x), dim=1) .",2802
20287,Feature Request: More general learning-rate scheduling,```from torch.optim import lr_scheduler```,3327
20288,Feature Request: More general learning-rate scheduling,```from torch.optim import lr_scheduler```,3327
20289,torch.arange type,"We can return `long` tensors from `arange` by using the following constructor ```python idx = torch.arange(0, 10, out=torch.LongTensor()) ```",2054
20290,torch.arange type,"We can return `long` tensors from `arange` by using the following constructor ```python idx = torch.arange(0, 10, out=torch.LongTensor()) ```",2054
20291,torch.zeros_like() does not work,torch.zeros(a.size())  can be used as an alternative,3304
20292,torch.zeros_like() does not work,torch.zeros(a.size())  can be used as an alternative,3304
20293,nn.Linear requires 2D input,fixed in latest release,7155
20294,nn.Linear requires 2D input,fixed in latest release,7155
20295,Add get_lr in ReduceLROnPlateau,`get_lr` is designed to be more like an internal function for computing use `[ group['lr'] for group in optim.param_groups ]`,2896
20296,Add get_lr in ReduceLROnPlateau,`get_lr` is designed to be more like an internal function for computing use `[ group['lr'] for group in optim.param_groups ]`,2896
20297,optimizer load_state_dict() problem?,"move optimizer state to the GPU memory manually
```model = Model()
model.load_state_dict(checkpoint['model'])
model.cuda()
optimizer = optim.Adam(model.parameters())
optimizer.load_state_dict(checkpoint['optimizer'])
for state in optimizer.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor):
            state[k] = v.cuda()```",8017
20298,optimizer load_state_dict() problem?,"move optimizer state to the GPU memory manually
```model = Model()
model.load_state_dict(checkpoint['model'])
model.cuda()
optimizer = optim.Adam(model.parameters())
optimizer.load_state_dict(checkpoint['optimizer'])
for state in optimizer.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor):
            state[k] = v.cuda()```",8017
20299,register_hook-modified gradient cannot be applied to optimizer,"It is expected result, for instance Adam is scaling invariant.  It means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance.",8507
20300,register_hook-modified gradient cannot be applied to optimizer,"It is expected result, for instance Adam is scaling invariant.  It means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance.",8507
20301,Behavior of __setattr__ and __getattr__ not consistent," `__getattr__` is called when an attribute lookup has not found the attribute in the usual places
For example, this works fine:

```
import torch
from torch import nn
mod = nn.Module()
mod.val = 5
print(mod.val)
>>>5
getattr(mod, 'val')
>>>5
```

But this `getattr(mod, 'val2')` throws the error",3064
20302,Behavior of __setattr__ and __getattr__ not consistent," `__getattr__` is called when an attribute lookup has not found the attribute in the usual places
For example, this works fine:

```
import torch
from torch import nn
mod = nn.Module()
mod.val = 5
print(mod.val)
>>>5
getattr(mod, 'val')
>>>5
```

But this `getattr(mod, 'val2')` throws the error",3064
20303,DataParallel doesn't replicate module's member variables,"for consistent outputs use - 
`self.register_buffer('counter', torch.zeros(1))`

",8588
20304,DataParallel doesn't replicate module's member variables,"for consistent outputs use - 
`self.register_buffer('counter', torch.zeros(1))`

",8588
20305,CrossEntropyLoss is negative,"negative values exist in the latest version of nn.BCELoss().
workaround:
criterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))",9974
20306,CrossEntropyLoss is negative,"negative values exist in the latest version of nn.BCELoss().
workaround:
criterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))",9974
20307,[feature request] zeros_like and ones_like for Variables,For this use ATen bindings in Variable,8306
20308,[feature request] zeros_like and ones_like for Variables,For this use ATen bindings in Variable,8306
20309,Add unit tests that load jit models to protect against issues with API changes,The tests have been already added,1602
20310,Add unit tests that load jit models to protect against issues with API changes,The tests have been already added,1602
20311,ValueError: only one element tensors can be converted to Python scalars,Upgrad to pytorch 1.2,2750
20312,ValueError: only one element tensors can be converted to Python scalars,Upgrad to pytorch 1.2,2750
20313,Installing pytorch nightly with pip fails,"to install pytorch nightly use - 
pip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html",10419
20314,Installing pytorch nightly with pip fails,"to install pytorch nightly use - 
pip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html",10419
20315,Build Pytorch/Libtorch with TBB support is failing,upgrade to Cmake 3.13.3 ,1824
20316,Build Pytorch/Libtorch with TBB support is failing,upgrade to Cmake 3.13.3 ,1824
20317,Difference to numpy linspace with endpoint False,"it's simply different default for dtype:
```python
In [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]
Out[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)

In [4]: np.linspace(0, 1, 10, endpoint=False)
Out[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

In [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]
Out[5]: 
array([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,
       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,
       -1.19209289e-08,  2.38418579e-08])

In [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]
Out[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
```

The differences are simply of order `float32.eps`. ",7662
20318,Difference to numpy linspace with endpoint False,"it's simply different default for dtype:
```python
In [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]
Out[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)

In [4]: np.linspace(0, 1, 10, endpoint=False)
Out[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

In [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]
Out[5]: 
array([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,
       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,
       -1.19209289e-08,  2.38418579e-08])

In [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]
Out[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
```

The differences are simply of order `float32.eps`. ",7662
20319,Creating an nn.Module instance with a buffer inside the init function signature of another nn.Module and then moving the main module to CUDA results in a global CUDA device being set for internal nn.Module buffers for future module instances,"Don't use defaults in `def __init__(self, x, test1 = Test1()):`
Try:
```
t = Test2(torch.ones(5), Test1()).cuda()
t = Test2(torch.ones(5), Test1()).cuda()
```",2198
20320,Creating an nn.Module instance with a buffer inside the init function signature of another nn.Module and then moving the main module to CUDA results in a global CUDA device being set for internal nn.Module buffers for future module instances,"Don't use defaults in `def __init__(self, x, test1 = Test1()):`
Try:
```
t = Test2(torch.ones(5), Test1()).cuda()
t = Test2(torch.ones(5), Test1()).cuda()
```",2198
20321,libtorch: Error when using torch::from_blob,use float -> torch::kFloat32,11304
20322,libtorch: Error when using torch::from_blob,use float -> torch::kFloat32,11304
20323,Numerically inconsistent of division between numpy and torch,PyTorch's default datatype is float32 while NumPy's is float64. When performing the division with the same datatype the result will be same.,7599
20324,Numerically inconsistent of division between numpy and torch,PyTorch's default datatype is float32 while NumPy's is float64. When performing the division with the same datatype the result will be same.,7599
20325,unable to cast Python instance to C++ type in .backward()," convert the sparse format to tensor in the callback
example -
```python

    @staticmethod
    @rpc.functions.async_execution
    def average_gradient(
        ps_rref,
        received_batch_number,
        param_loc,
        gradient
    ):
        self = ps_rref.local_value()
        if type(gradient) is list:
            gradient = self.sparse_rpc_format_to_tensor(gradient)
        if not self.use_cuda_rpc:
            gradient = gradient.cuda(self.rank)
        fut = torch.futures.Future()
        with self.lock:
            if self.batch_number < received_batch_number:
                self.batch_number = received_batch_number
                self.clear_batch_state()
            self.process_gradient(gradient, param_loc)
            if param_loc not in self.futures:
                self.futures[param_loc] = []
            self.futures[param_loc].append(fut)
            if len(self.futures[param_loc]) == self.trainer_count:
                self.record_straggler_end(self.param_key(param_loc))
                param_loc_avg = self.average(param_loc)
                if not self.use_cuda_rpc:
                    param_loc_avg = param_loc_avg.cpu()
                if param_loc_avg.is_sparse:
                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)
                for cur_fut in self.futures[param_loc]:
                    cur_fut.set_result(param_loc_avg)
                self.record_batch_end(self.param_key(param_loc))
        return fut

    @staticmethod
    def process_bucket(state, bucket):
        cref = state.cref
        tensor = bucket.get_tensor()
        tensors_count = len(cref.bucket_to_parameters(bucket))
        sparse = tensor.is_sparse
        if not cref.use_cuda_rpc:
            tensor = tensor.cpu()
        if sparse:
            tensor = cref.sparse_tensor_to_rpc_format(tensor)
        ps = cref.ps
        ps_args = [
            cref.ps_rref,
            state.batch_number,
            state.param_loc,
            tensor
        ]
        fut = cref.send_async_request(
            state.get_key(),
            cref.ps_rref,
            ps.average_gradient,
            *ps_args
        )
        state.param_loc += tensors_count

        def callback(fut):
            tensor = fut.wait()
            if type(tensor) is list:
                tensor = cref.sparse_rpc_format_to_tensor(tensor)
            if not cref.use_cuda_rpc:
                tensor = tensor.cuda(cref.rank)
            return [tensor]

        return fut.then(callback)
```


",3101
20326,unable to cast Python instance to C++ type in .backward()," convert the sparse format to tensor in the callback
example -
```python

    @staticmethod
    @rpc.functions.async_execution
    def average_gradient(
        ps_rref,
        received_batch_number,
        param_loc,
        gradient
    ):
        self = ps_rref.local_value()
        if type(gradient) is list:
            gradient = self.sparse_rpc_format_to_tensor(gradient)
        if not self.use_cuda_rpc:
            gradient = gradient.cuda(self.rank)
        fut = torch.futures.Future()
        with self.lock:
            if self.batch_number < received_batch_number:
                self.batch_number = received_batch_number
                self.clear_batch_state()
            self.process_gradient(gradient, param_loc)
            if param_loc not in self.futures:
                self.futures[param_loc] = []
            self.futures[param_loc].append(fut)
            if len(self.futures[param_loc]) == self.trainer_count:
                self.record_straggler_end(self.param_key(param_loc))
                param_loc_avg = self.average(param_loc)
                if not self.use_cuda_rpc:
                    param_loc_avg = param_loc_avg.cpu()
                if param_loc_avg.is_sparse:
                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)
                for cur_fut in self.futures[param_loc]:
                    cur_fut.set_result(param_loc_avg)
                self.record_batch_end(self.param_key(param_loc))
        return fut

    @staticmethod
    def process_bucket(state, bucket):
        cref = state.cref
        tensor = bucket.get_tensor()
        tensors_count = len(cref.bucket_to_parameters(bucket))
        sparse = tensor.is_sparse
        if not cref.use_cuda_rpc:
            tensor = tensor.cpu()
        if sparse:
            tensor = cref.sparse_tensor_to_rpc_format(tensor)
        ps = cref.ps
        ps_args = [
            cref.ps_rref,
            state.batch_number,
            state.param_loc,
            tensor
        ]
        fut = cref.send_async_request(
            state.get_key(),
            cref.ps_rref,
            ps.average_gradient,
            *ps_args
        )
        state.param_loc += tensors_count

        def callback(fut):
            tensor = fut.wait()
            if type(tensor) is list:
                tensor = cref.sparse_rpc_format_to_tensor(tensor)
            if not cref.use_cuda_rpc:
                tensor = tensor.cuda(cref.rank)
            return [tensor]

        return fut.then(callback)
```


",3101
20327,Unclear documentation or unintended behaviour for Lazy modules,it's fixed in the nightly version,8465
20328,Unclear documentation or unintended behaviour for Lazy modules,it's fixed in the nightly version,8465
20329,PyTorch fails to detect Intel® oneAPI Math Kernel Library during build,"issue can be quick fixed by changing
https://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43

to :
```
  IF (EXISTS ""/opt/intel/oneapi/mkl"")
    SET(DEFAULT_INTEL_MKL_DIR ""/opt/intel/oneapi/mkl/latest"")
  ELSE()
    SET(DEFAULT_INTEL_MKL_DIR ""/opt/intel/mkl"")
  ENDIF()
```",8319
20330,PyTorch fails to detect Intel® oneAPI Math Kernel Library during build,"issue can be quick fixed by changing
https://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43

to :
```
  IF (EXISTS ""/opt/intel/oneapi/mkl"")
    SET(DEFAULT_INTEL_MKL_DIR ""/opt/intel/oneapi/mkl/latest"")
  ELSE()
    SET(DEFAULT_INTEL_MKL_DIR ""/opt/intel/mkl"")
  ENDIF()
```",8319
20331,"On master branch, `test_lkj_cholesky_log_prob` fails on MacOS 10.13 with `ATEN_CPU_CAPABILITY=default`", replacing `==` with `torch.allclose`,2946
20332,"On master branch, `test_lkj_cholesky_log_prob` fails on MacOS 10.13 with `ATEN_CPU_CAPABILITY=default`", replacing `==` with `torch.allclose`,2946
20333,Nightly PyTorch builds can not be installed using pip-21.1.1,"add the `--pre` flag
```
python3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre
```",2199
20334,Nightly PyTorch builds can not be installed using pip-21.1.1,"add the `--pre` flag
```
python3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre
```",2199
20335,Error when doing CUDA Conv2d with 1x1 kernel.,"instead of:

```python
x = Variable(torch.randn(1, 1, 100, 100))
x.cuda()  # This creates a copy on the GPU and immediately discards it. ""x"" is still on the CPU
```

try:

```python
x = Variable(torch.randn(1, 1, 100, 100).cuda())
```",7867
20336,Error when doing CUDA Conv2d with 1x1 kernel.,"instead of:

```python
x = Variable(torch.randn(1, 1, 100, 100))
x.cuda()  # This creates a copy on the GPU and immediately discards it. ""x"" is still on the CPU
```

try:

```python
x = Variable(torch.randn(1, 1, 100, 100).cuda())
```",7867
20337,Add CrossEntropyLoss2d," In the outer wrapper code the difference seems is an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d ",7906
20338,Add CrossEntropyLoss2d," In the outer wrapper code the difference seems is an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d ",7906
20339,Gradients are zero when run on GPU (x.cuda()),.cuda() is an immutable operation,3306
20340,Gradients are zero when run on GPU (x.cuda()),.cuda() is an immutable operation,3306
20341,CrossEntropyLoss masking,"simple gather can be very fast:

```python
logp = F.log_softmax(y_hat)
logpy = torch.gather(logp, 1, y).view(-1)
```",599
20342,CrossEntropyLoss masking,"simple gather can be very fast:

```python
logp = F.log_softmax(y_hat)
logpy = torch.gather(logp, 1, y).view(-1)
```",599
20343,rebuild pip wheels with manylinux,"run ""pip install --no-deps torchvision"" ",3127
20344,rebuild pip wheels with manylinux,"run ""pip install --no-deps torchvision"" ",3127
20345,ModuleNotFoundError: No module named 'torch._C',"problem is that you have a folder called `torch` in the same directory which is being picked up.
 Do this: `cd ..` (to change directory), and then start `python` and `import torch`,",670
20346,ModuleNotFoundError: No module named 'torch._C',"problem is that you have a folder called `torch` in the same directory which is being picked up.
 Do this: `cd ..` (to change directory), and then start `python` and `import torch`,",670
20347,MaxUnpool2d breaks for certain shaped inputs,"MaxPool's downsampling is ambiguous in input shape provide: `output_size` as the third optional argument to the call operato
```
import torch
from torch import nn
from torch.autograd import Variable

data = Variable(torch.rand(1, 3, 540, 960))

pool = nn.MaxPool2d(2, 2, return_indices=True)
unpool = nn.MaxUnpool2d(2, 2)

out, indices1 = pool(data)
size1 = out.size()
out, indices2 = pool(out)
size2 = out.size()
out, indices3 = pool(out)
size3 = out.size()

out = unpool(out, indices3, output_size=size2)
out = unpool(out, indices2, output_size=size1)
out = unpool(out, indices1)
```",10892
20348,MaxUnpool2d breaks for certain shaped inputs,"MaxPool's downsampling is ambiguous in input shape provide: `output_size` as the third optional argument to the call operato
```
import torch
from torch import nn
from torch.autograd import Variable

data = Variable(torch.rand(1, 3, 540, 960))

pool = nn.MaxPool2d(2, 2, return_indices=True)
unpool = nn.MaxUnpool2d(2, 2)

out, indices1 = pool(data)
size1 = out.size()
out, indices2 = pool(out)
size2 = out.size()
out, indices3 = pool(out)
size3 = out.size()

out = unpool(out, indices3, output_size=size2)
out = unpool(out, indices2, output_size=size1)
out = unpool(out, indices1)
```",10892
20349,Flag to check if a Module is on CUDA similar to is_cuda for Tensors,"You can check is using 
```python
next(model.parameters()).is_cuda
```",1111
20350,Flag to check if a Module is on CUDA similar to is_cuda for Tensors,"You can check is using 
```python
next(model.parameters()).is_cuda
```",1111
20351,CUDNN batchnorm backprop doesn't work properly in evaluation mode,cudnn does not support backprop in evaluate mode,9140
20352,CUDNN batchnorm backprop doesn't work properly in evaluation mode,cudnn does not support backprop in evaluate mode,9140
20353,1>E:\software\libtorch\include\torch/csrc/utils/variadic.h(195): error C2951: 模板 声明只能在全局、命名空间或类范围内使用,add `::` before the usage of `std`,4843
20354,1>E:\software\libtorch\include\torch/csrc/utils/variadic.h(195): error C2951: 模板 声明只能在全局、命名空间或类范围内使用,add `::` before the usage of `std`,4843
20355,OSError: [WinError 193] %1 is not a valid Win32 application,"there are two python environments involved here,make sure your `PATH` is clean and you can actually remove one of them first.",9854
20356,OSError: [WinError 193] %1 is not a valid Win32 application,"there are two python environments involved here,make sure your `PATH` is clean and you can actually remove one of them first.",9854
20357,PyTorch1.2 ONNX dynamic_axis is not working,Please use newer version PyTorch.,590
20358,PyTorch1.2 ONNX dynamic_axis is not working,Please use newer version PyTorch.,590
20359,Free Memory after CUDA out of memory error,free the memory by restarting the notebook / python command line.,225
20360,Free Memory after CUDA out of memory error,free the memory by restarting the notebook / python command line.,225
20361,There is a small wrong mistake in the pytorch official web file,"0.3.1 docs are frozen but it’s already been fixed in master,",89
20362,There is a small wrong mistake in the pytorch official web file,"0.3.1 docs are frozen but it’s already been fixed in master,",89
20363,gpu memory not released after run `sudo kill [pytorch process id]`, some process are still alive. You can find them by doing `lsof /dev/nvidia0`,7698
20364,gpu memory not released after run `sudo kill [pytorch process id]`, some process are still alive. You can find them by doing `lsof /dev/nvidia0`,7698
20365,Segmentation fault when cat list with all-empty cuda tensors,"`a` being an empty tensor (with shape (0,)), cat crashes",7426
20366,Segmentation fault when cat list with all-empty cuda tensors,"`a` being an empty tensor (with shape (0,)), cat crashes",7426
20367,ReLU(inplace=True) seems something wrong internal,"it's an in-place operation because `+=` is the in-place add
You avoid the in-place by doing
```python
out = out + residual
```",696
20368,ReLU(inplace=True) seems something wrong internal,"it's an in-place operation because `+=` is the in-place add
You avoid the in-place by doing
```python
out = out + residual
```",696
20369,Crash when dividing Variable(LongTensor) by a float,the float is being cast into a long,7677
20370,Crash when dividing Variable(LongTensor) by a float,the float is being cast into a long,7677
20371,nn.LSTM.cuda() leads to CuDNNError,try deleting ~/.nv if it exists,2833
20372,nn.LSTM.cuda() leads to CuDNNError,try deleting ~/.nv if it exists,2833
20373,torch.Size doesn't accept pytorch scalar,"everything should be called 'Tensor' now and nothing should be called 'Variable'
As a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` ",526
20374,torch.Size doesn't accept pytorch scalar,"everything should be called 'Tensor' now and nothing should be called 'Variable'
As a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` ",526
20375,BCELoss with weights for labels (like weighted_cross_entropy_with_logits in TF)," add the Weighted BCEloss, where the weights can be computed dynamically for each batch:
``` 
def weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):
    """"""
    Args:
        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.
        targets: true value, one-hot-like vector of size [N,C]
        pos_weight: Weight for postive sample
    """"""
    if not (targets.size() == sigmoid_x.size()):
        raise ValueError(""Target size ({}) must be the same as input size ({})"".format(targets.size(), sigmoid_x.size()))

    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()

    if weight is not None:
        loss = loss * weight

    if not reduce:
        return loss
    elif size_average:
        return loss.mean()
    else:
        return loss.sum()
```
``` 
class WeightedBCELoss(Module):
    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):
        """"""
        Args:
            pos_weight = Weight for postive samples. Size [1,C]
            weight = Weight for Each class. Size [1,C]
            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.
            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.
        """"""
        super().__init__()

        self.register_buffer('weight', weight)
        self.register_buffer('pos_weight', pos_weight)
        self.size_average = size_average
        self.reduce = reduce
        self.PosWeightIsDynamic = PosWeightIsDynamic

    def forward(self, input, target):
        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight
        if self.PosWeightIsDynamic:
            positive_counts = target.sum(dim=0)
            nBatch = len(target)
            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)

        if self.weight is not None:
            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight
            return weighted_binary_cross_entropy(input, target,
                                                 self.pos_weight,
                                                 weight=self.weight,
                                                 size_average=self.size_average,
                                                 reduce=self.reduce)
        else:
            return weighted_binary_cross_entropy(input, target,
                                                 self.pos_weight,
                                                 weight=None,
                                                 size_average=self.size_average,
                                                 reduce=self.reduce)
```",3090
20376,BCELoss with weights for labels (like weighted_cross_entropy_with_logits in TF)," add the Weighted BCEloss, where the weights can be computed dynamically for each batch:
``` 
def weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):
    """"""
    Args:
        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.
        targets: true value, one-hot-like vector of size [N,C]
        pos_weight: Weight for postive sample
    """"""
    if not (targets.size() == sigmoid_x.size()):
        raise ValueError(""Target size ({}) must be the same as input size ({})"".format(targets.size(), sigmoid_x.size()))

    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()

    if weight is not None:
        loss = loss * weight

    if not reduce:
        return loss
    elif size_average:
        return loss.mean()
    else:
        return loss.sum()
```
``` 
class WeightedBCELoss(Module):
    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):
        """"""
        Args:
            pos_weight = Weight for postive samples. Size [1,C]
            weight = Weight for Each class. Size [1,C]
            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.
            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.
        """"""
        super().__init__()

        self.register_buffer('weight', weight)
        self.register_buffer('pos_weight', pos_weight)
        self.size_average = size_average
        self.reduce = reduce
        self.PosWeightIsDynamic = PosWeightIsDynamic

    def forward(self, input, target):
        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight
        if self.PosWeightIsDynamic:
            positive_counts = target.sum(dim=0)
            nBatch = len(target)
            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)

        if self.weight is not None:
            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight
            return weighted_binary_cross_entropy(input, target,
                                                 self.pos_weight,
                                                 weight=self.weight,
                                                 size_average=self.size_average,
                                                 reduce=self.reduce)
        else:
            return weighted_binary_cross_entropy(input, target,
                                                 self.pos_weight,
                                                 weight=None,
                                                 size_average=self.size_average,
                                                 reduce=self.reduce)
```",3090
20377,Unable to build from latest master - nccl.h not found,This issue is fixed,9345
20378,Unable to build from latest master - nccl.h not found,This issue is fixed,9345
20379,ONNX export runtime error - tuple appears in op that does not forward tuples,This can be solved by removing removing `DataParallel`.,602
20380,ONNX export runtime error - tuple appears in op that does not forward tuples,This can be solved by removing removing `DataParallel`.,602
20381,Cannot convert certain empty tensors from numpy,you'll need to use a nightly build,270
20382,Cannot convert certain empty tensors from numpy,you'll need to use a nightly build,270
20383,Default chosen cuDNN convolution algorithm for V100 uses twice as much memory,Bug is fixed,7675
20384,Default chosen cuDNN convolution algorithm for V100 uses twice as much memory,Bug is fixed,7675
20385,When CUDA 10 support is planned?,It has been released,8308
20386,When CUDA 10 support is planned?,It has been released,8308
20387,Chaining Operations - Misunderstanding?,after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2),8842
20388,Chaining Operations - Misunderstanding?,after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2),8842
20389,Suppress Scientific Notation,Issue can be resolved using `torch.set_printoptions(sci_mode=True)`.,7702
20390,Suppress Scientific Notation,Issue can be resolved using `torch.set_printoptions(sci_mode=True)`.,7702
20391,torch.sigmoid behaves inconsistently for 32- and 64-bit NaN inputs,sigmoid() doesn't use avx_mathfun.h anymore.,10684
20392,torch.sigmoid behaves inconsistently for 32- and 64-bit NaN inputs,sigmoid() doesn't use avx_mathfun.h anymore.,10684
20393,What is the net *.pb file encoding?,read the file in binary mode,3081
20394,What is the net *.pb file encoding?,read the file in binary mode,3081
20395,"C++ API, IValue toTensor() didn't work",call `.toTuple()` `->elements()` `.toTensor()` ,213
20396,"C++ API, IValue toTensor() didn't work",call `.toTuple()` `->elements()` `.toTensor()` ,213
20397,Assigning a parameter to an indexed tensor that was produced by DDP no longer works in torch nightly (1.7),You should not modify the input to your net when you use DDP,2874
20398,Assigning a parameter to an indexed tensor that was produced by DDP no longer works in torch nightly (1.7),You should not modify the input to your net when you use DDP,2874
20399,libtorch_cpu.a is huge (1 GB) with build_android.sh, '-g0' flag solves the problem,6854
20400,libtorch_cpu.a is huge (1 GB) with build_android.sh, '-g0' flag solves the problem,6854
20401,`__torch_function__` does not get call for torch.Tensor's `__getitem__` syntax, `MyTensor` is not a Tensor so trying to use `as_subclass` isn't really viable,3773
20402,`__torch_function__` does not get call for torch.Tensor's `__getitem__` syntax, `MyTensor` is not a Tensor so trying to use `as_subclass` isn't really viable,3773
20403,I'm compiling the pytorch 1.6.0 stable version on CUDA 11.1 and CUDNN 8.04 with 3090 and failed.,"1.6 does not support CUDA 11, please use a newer version",195
20404,I'm compiling the pytorch 1.6.0 stable version on CUDA 11.1 and CUDNN 8.04 with 3090 and failed.,"1.6 does not support CUDA 11, please use a newer version",195
20405,How can I fix NAN loss (or very large MSE losses)?,"Instead of:
```
for step in range(1, len(train_loader) + 1):
    batch = next(iter(train_loader)) # <- issue here
    ...
```
try:
```
for step, batch in enumerate(train_loader):
    ...
```",4863
20406,How can I fix NAN loss (or very large MSE losses)?,"Instead of:
```
for step in range(1, len(train_loader) + 1):
    batch = next(iter(train_loader)) # <- issue here
    ...
```
try:
```
for step, batch in enumerate(train_loader):
    ...
```",4863
20407,Add `inputs` argument to `autograd.backward()`,feature added,5725
20408,Add `inputs` argument to `autograd.backward()`,feature added,5725
20409,JIT LibTorch: Profiling Mode Enabled causes memory leak,Bug is fixed,2911
20410,JIT LibTorch: Profiling Mode Enabled causes memory leak,Bug is fixed,2911
20411,Warnings during compiling: floating-point value does not fit in required integral type,compiler-warnings arise during compilation of the templated function uniform_int(),7433
20412,Warnings during compiling: floating-point value does not fit in required integral type,compiler-warnings arise during compilation of the templated function uniform_int(),7433
20413,Couldn't build multi scaled kernel nested model,The issue is shared on pytorch discuss,492
20414,Couldn't build multi scaled kernel nested model,The issue is shared on pytorch discuss,492
20415,torch.median returns the smaller element when the median value lies between two elements., need some special handling for the gradient if we'd like to take the mean of the two middle elements. ,1324
20416,torch.median returns the smaller element when the median value lies between two elements., need some special handling for the gradient if we'd like to take the mean of the two middle elements. ,1324
20417,Deadlock with shared CUDA tensors and multiprocessing (spawn),This issue is fixed,6819
20418,Deadlock with shared CUDA tensors and multiprocessing (spawn),This issue is fixed,6819
20419,"throw error when EmbeddingBag(..., sparse=True) and slice the embedding's weight"," `grad_out` is sparse, When the dense view receives a sparse gradient, it should be able to backpropagate through slice by offsetting the indices",530
20420,"throw error when EmbeddingBag(..., sparse=True) and slice the embedding's weight"," `grad_out` is sparse, When the dense view receives a sparse gradient, it should be able to backpropagate through slice by offsetting the indices",530
20421,torch.scatter_ returns incorrect result when running on CPU using int64 indexes,This issue is fixed,7654
20422,torch.scatter_ returns incorrect result when running on CPU using int64 indexes,This issue is fixed,7654
20423,Required some argument in dataloader for setting randomstate, you can create a custom `RandomSampler` class that uses a given RNG state,6942
20424,Required some argument in dataloader for setting randomstate, you can create a custom `RandomSampler` class that uses a given RNG state,6942
20425,masked_fill_ (and possibly others) produces a different output than masked_fill on cpu,You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`.,8554
20426,masked_fill_ (and possibly others) produces a different output than masked_fill on cpu,You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`.,8554
20427,The error of `torch.nn.SyncBatchNorm.convert_sync_batchnorm`,The bug is fixed,10718
20428,The error of `torch.nn.SyncBatchNorm.convert_sync_batchnorm`,The bug is fixed,10718
20429,"Training using “mp.spawn”, can not reproduce the training results","seed in worker function, not main",10657
20430,"Training using “mp.spawn”, can not reproduce the training results","seed in worker function, not main",10657
20431,nonzero function in C++ much slower than python(CUDA),"the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence",595
20432,nonzero function in C++ much slower than python(CUDA),"the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence",595
20433,[SSL: CERTIFICATE_VERIFY_FAILED] For Inception Resnet V2 on Google Colab,it's pretrained-models's hosting issue,4955
20434,[SSL: CERTIFICATE_VERIFY_FAILED] For Inception Resnet V2 on Google Colab,it's pretrained-models's hosting issue,4955
20435,Add custom request headers to torch.hub.download_url_to_file,Added to the pr,2840
20436,Add custom request headers to torch.hub.download_url_to_file,Added to the pr,2840
20437,Missing info in Tensorboard's add_graph(),Bug is fixed,2222
20438,Missing info in Tensorboard's add_graph(),Bug is fixed,2222
20439,torch.split(..) / torch.chunk(..) does not remove one dimension from tensor in some cases,Neither chunk nor split are expected to remove a dimension use torch.unbind. ,4302
20440,torch.split(..) / torch.chunk(..) does not remove one dimension from tensor in some cases,Neither chunk nor split are expected to remove a dimension use torch.unbind. ,4302
20441,Nightly build includes absolute path in cmake file,there are no unexpected cuda libraries that need to be linked to,3160
20442,Nightly build includes absolute path in cmake file,there are no unexpected cuda libraries that need to be linked to,3160
20443,torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order,Bug is fixed,9039
20444,torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order,Bug is fixed,9039
20445,l1loss different results based on arguments position,This issue is fixed in master,9349
20446,l1loss different results based on arguments position,This issue is fixed in master,9349
20447,How do you convert the tensor to a float,`x.data<float>()` ,8753
20448,How do you convert the tensor to a float,`x.data<float>()` ,8753
20449,the running time difference between the ATen library and pytorch ?,"You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`.",2373
20450,the running time difference between the ATen library and pytorch ?,"You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`.",2373
20451,Feature request:  'concat' for Variable,"Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`]",2195
20452,Feature request:  'concat' for Variable,"Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`]",2195
20453,torch.load() and torch.save() of big tensors is slow due to tar,rewritten `load` and `save` to no longer use tar. It's 5-10x faster,3360
20454,torch.load() and torch.save() of big tensors is slow due to tar,rewritten `load` and `save` to no longer use tar. It's 5-10x faster,3360
20455,LSTM output dimensions,"making each RNN layer of size 1, and using this after each BRNN solves this 
 ```python # (TxNxD*2) -> (TxNxD) by sum if self.bidirectional: x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) ```",7363
20456,LSTM output dimensions,"making each RNN layer of size 1, and using this after each BRNN solves this 
 ```python # (TxNxD*2) -> (TxNxD) by sum if self.bidirectional: x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) ```",7363
20457,Resize gradients computed by basic math Functions to match original input sizes,This issue is fixed,10914
20458,Resize gradients computed by basic math Functions to match original input sizes,This issue is fixed,10914
20459,Backprop issue I can't figure out,cuDNN problem,8689
20460,Backprop issue I can't figure out,cuDNN problem,8689
20461,torch.cat bug/unexpected behavior,bug is fixed,6160
20462,torch.cat bug/unexpected behavior,bug is fixed,6160
20463,[RFC] Join-based API to support uneven inputs in DDP, keeping current design in this proposal,3544
20464,[RFC] Join-based API to support uneven inputs in DDP, keeping current design in this proposal,3544
20465,"Scripted Model gave totally wrong result on iOS, but was correct on C++ frontend","The error between them can be almost ignored, the average error is about 10e-8. 

The error calculation is attached. 

- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++
- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS 
- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`
",2843
20466,"Scripted Model gave totally wrong result on iOS, but was correct on C++ frontend","The error between them can be almost ignored, the average error is about 10e-8. 

The error calculation is attached. 

- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++
- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS 
- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`
",2843
20467,"Can't create tensor from tensor list, but tensor from numpy arrays list works fine",Bug is fixed,1914
20468,"Can't create tensor from tensor list, but tensor from numpy arrays list works fine",Bug is fixed,1914
20469,ValueError: can't optimize a non-leaf Tensor,Solution can be found in pytorch discuss,4846
20470,ValueError: can't optimize a non-leaf Tensor,Solution can be found in pytorch discuss,4846
20471,internal assert failed bug due to argmax gradfn,works in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`,5811
20472,internal assert failed bug due to argmax gradfn,works in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`,5811
20473,Adding typing_extensions as a dependency of pytorch,Bug is fixed,4145
20474,Adding typing_extensions as a dependency of pytorch,Bug is fixed,4145
20475,Docs of KLDivLoss seems incorrect,"The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:
```
As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).
```",6957
20476,Docs of KLDivLoss seems incorrect,"The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:
```
As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).
```",6957
20477,Store TORCH_CUDA_ARCH_LIST in torch and use it for C++ extensions," one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting
```
$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u
sm_35
sm_37
sm_50
sm_60
sm_61
sm_70
sm_75
```",3779
20478,Store TORCH_CUDA_ARCH_LIST in torch and use it for C++ extensions," one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting
```
$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u
sm_35
sm_37
sm_50
sm_60
sm_61
sm_70
sm_75
```",3779
20479,Crash on exit in Python 3.9.0a6,destruction of GIL in autograd threads,5998
20480,Crash on exit in Python 3.9.0a6,destruction of GIL in autograd threads,5998
20481,AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05 :,This issue is fixed,7809
20482,AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05 :,This issue is fixed,7809
20483,Installation on windows python 3.8 32bits, `set USE_MKLDNN=0`,554
20484,Installation on windows python 3.8 32bits, `set USE_MKLDNN=0`,554
20485,jit much slower with pytorch 1.5 (on custom rnns),"```python
        torch._C._jit_set_profiling_executor(False)
        torch._C._jit_set_profiling_mode(False)
```",729
20486,jit much slower with pytorch 1.5 (on custom rnns),"```python
        torch._C._jit_set_profiling_executor(False)
        torch._C._jit_set_profiling_mode(False)
```",729
20487,"Implementing deg2rad, rad2deg","you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. 
",1913
20488,"Implementing deg2rad, rad2deg","you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. 
",1913
20489,Implementing HyperLSTM,The  feature will be added,3194
20490,Implementing HyperLSTM,The  feature will be added,3194
20491,Is grad attribute in Variable immutable?,You can assign it now and it works as expected,7673
20492,Is grad attribute in Variable immutable?,You can assign it now and it works as expected,7673
20493,CuDNN ConvTranspose1d issue with transposed cuda Variable (can't convert to contiguous),This has been fixed on master,7579
20494,CuDNN ConvTranspose1d issue with transposed cuda Variable (can't convert to contiguous),This has been fixed on master,7579
20495,Unable to use cat on torch.LongTensor,`torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well.,536
20496,Unable to use cat on torch.LongTensor,`torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well.,536
20497,Low performance issue on CPU-only machine,bug is fixed,514
20498,Low performance issue on CPU-only machine,bug is fixed,514
20499,indexing operator `[]` inconsistent,indexing with a list of indices will never return a view,259
20500,indexing operator `[]` inconsistent,indexing with a list of indices will never return a view,259
20501,"torch.svd() get error, w/ CUDA 9.0 w/o MAGMA",now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`,752
20502,"torch.svd() get error, w/ CUDA 9.0 w/o MAGMA",now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`,752
20503,Installing from source failing on Ubuntu 17.10,"1) Install CUDA and CUDNN from the respective .deb files.
2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.
3) ```sudo apt install g++-6 gcc-6```
3) Set $CC and $CXX to point to the right version of the tools.
4) Clone pytorch, install with ```python setup.py install```",727
20504,Installing from source failing on Ubuntu 17.10,"1) Install CUDA and CUDNN from the respective .deb files.
2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.
3) ```sudo apt install g++-6 gcc-6```
3) Set $CC and $CXX to point to the right version of the tools.
4) Clone pytorch, install with ```python setup.py install```",727
20505,CrossEntropyLoss for 3D and higher,feature added,8843
20506,CrossEntropyLoss for 3D and higher,feature added,8843
20507,undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9,Fixed by doing `conda uninstall cudnn` and recompiling.,7422
20508,undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9,Fixed by doing `conda uninstall cudnn` and recompiling.,7422
20509,"[Feature Request] Make ""forward"" handle large inputs in batches","there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line
```
torch.cat([model(x) for x in input.split(batch_size, 0)], 0)
```",3340
20510,"[Feature Request] Make ""forward"" handle large inputs in batches","there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line
```
torch.cat([model(x) for x in input.split(batch_size, 0)], 0)
```",3340
20511,Sort sequences internally in pack_padded_sequence,feature added,8432
20512,Sort sequences internally in pack_padded_sequence,feature added,8432
20513,"Documentation: Indexing output from bidirectional RNN (GRU,LSTM)","1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`
2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).
3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.
4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.

```python
# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.
# hidden_dim = 3, bidirectional=True, num_layers=1
In [525]: input_
Out[525]:

 1  3
 3  5
 3  2
 2  0
 1  0
[torch.LongTensor of size 5x2]

# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)
In [526]: print(hs[:, 1], ht[:, 1])

Variable containing:
   (( forward states ))                      (( backward states ))
-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408
-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796
-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440
 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000
 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000
[torch.FloatTensor of size 5x6]

 Variable containing:
-0.1935  0.0484 -0.4111
 0.3609 -0.4958  0.3408
[torch.FloatTensor of size 2x3]
```",2623
20514,"Documentation: Indexing output from bidirectional RNN (GRU,LSTM)","1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`
2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).
3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.
4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.

```python
# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.
# hidden_dim = 3, bidirectional=True, num_layers=1
In [525]: input_
Out[525]:

 1  3
 3  5
 3  2
 2  0
 1  0
[torch.LongTensor of size 5x2]

# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)
In [526]: print(hs[:, 1], ht[:, 1])

Variable containing:
   (( forward states ))                      (( backward states ))
-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408
-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796
-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440
 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000
 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000
[torch.FloatTensor of size 5x6]

 Variable containing:
-0.1935  0.0484 -0.4111
 0.3609 -0.4958  0.3408
[torch.FloatTensor of size 2x3]
```",2623
20515,Docker build fail,The bug is fixed,2962
20516,Docker build fail,The bug is fixed,2962
20517,Assert MKL conditions in THBlas.c,"They will not pass for things like expanded tensors:

```
x = torch.randn(1).expand(5, 5)
x.mm(x)
```",7450
20518,Assert MKL conditions in THBlas.c,"They will not pass for things like expanded tensors:

```
x = torch.randn(1).expand(5, 5)
x.mm(x)
```",7450
20519,binary cross entropy requires double tensor for target, the documentation is wrong,494
20520,binary cross entropy requires double tensor for target, the documentation is wrong,494
20521,pytorch distributed timeout when running with number processes > 16,"added `setTimeout()`, like follows:
```c++
  // NOTE: this function needs to be thread safe
  std::shared_ptr<context_type> createContext(
    const DataChannelGloo::Group& group,
    const std::string& prefix
  ) {
    auto context = std::make_shared<context_type>(
        group.mustGetGroupRank(_rank), group.size());
    prefix_store_type prefix_store(prefix, *group._store);
    context->setTimeout(std::chrono::minutes(15));
    context->connectFullMesh(prefix_store, _device);
    return context;
  }
```
 code with NCCL2 transport works flawlessly and initializes much faster.",543
20522,pytorch distributed timeout when running with number processes > 16,"added `setTimeout()`, like follows:
```c++
  // NOTE: this function needs to be thread safe
  std::shared_ptr<context_type> createContext(
    const DataChannelGloo::Group& group,
    const std::string& prefix
  ) {
    auto context = std::make_shared<context_type>(
        group.mustGetGroupRank(_rank), group.size());
    prefix_store_type prefix_store(prefix, *group._store);
    context->setTimeout(std::chrono::minutes(15));
    context->connectFullMesh(prefix_store, _device);
    return context;
  }
```
 code with NCCL2 transport works flawlessly and initializes much faster.",543
20523,off_t' undeclared with gcc 4.8.5 on Linux,adding CI against gcc 4.8,4095
20524,off_t' undeclared with gcc 4.8.5 on Linux,adding CI against gcc 4.8,4095
20525,conda install torch?,"torch is a separate product from pytorch, pytorch has no depedency on torch. ",698
20526,conda install torch?,"torch is a separate product from pytorch, pytorch has no depedency on torch. ",698
20527,DISABLED test_send_recv_any_source_autograd_profiler (__main__.TestMPIWithFork),bug is fixed,5890
20528,DISABLED test_send_recv_any_source_autograd_profiler (__main__.TestMPIWithFork),bug is fixed,5890
20529,Unable to trace RRef's when using torch.jit.script with remote functions.,"you need use `torch.jit.fork`, which returns a `Future`.",3067
20530,Unable to trace RRef's when using torch.jit.script with remote functions.,"you need use `torch.jit.fork`, which returns a `Future`.",3067
20531,Failed to build pytorch on mac os. Building pthreadpool,"`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solves the issue.

The final command to build:
```
DEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop
```",8740
20532,Failed to build pytorch on mac os. Building pthreadpool,"`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solves the issue.

The final command to build:
```
DEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop
```",8740
20533,Profiling within a callback function does not raise an error but fails to execute a print statement,"profiler should be invoked with

`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. 

Also we should add a `wait` to `return_future.then` which helps raise the error",537
20534,Profiling within a callback function does not raise an error but fails to execute a print statement,"profiler should be invoked with

`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. 

Also we should add a `wait` to `return_future.then` which helps raise the error",537
20535,"Nightly CUDA 11.1 Libtorch Builds failing, undefined symbol '__gmon_start__'", try kicking the build with option disabled,570
20536,"Nightly CUDA 11.1 Libtorch Builds failing, undefined symbol '__gmon_start__'", try kicking the build with option disabled,570
20537,"macOS conda nightlies failing when importing torch, 'Library not loaded: @rpath/libmkl_intel_lp64.1.dylib'",the issue is fixed,9536
20538,"macOS conda nightlies failing when importing torch, 'Library not loaded: @rpath/libmkl_intel_lp64.1.dylib'",the issue is fixed,9536
20539,PyTorch Installation for Windows using Conda gives unrecognized arguments:nvidia,this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688,8455
20540,PyTorch Installation for Windows using Conda gives unrecognized arguments:nvidia,this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688,8455
20541,`Warning: Leaking Caffe2 thread-pool after fork` when using `DataLoader` with `num_workers>0` and `pin_memory=True`,Bug is fixed,3174
20542,`Warning: Leaking Caffe2 thread-pool after fork` when using `DataLoader` with `num_workers>0` and `pin_memory=True`,Bug is fixed,3174
20543,layer identifier in powerSGD_hook,closed in in favor of posting on the forums,3270
20544,layer identifier in powerSGD_hook,closed in in favor of posting on the forums,3270
20545,COO to CSR tensor conversion is slow,use  `torch.searchsorted`,2958
20546,COO to CSR tensor conversion is slow,use  `torch.searchsorted`,2958
20547,`coalesce` creates overflowed indices in large sparse COO tensors,Bug is fixed,2748
20548,`coalesce` creates overflowed indices in large sparse COO tensors,Bug is fixed,2748
20549,test_eig_with_eigvec_cuda_float64 (TestTensorDeviceOpsCUDA) is failing intermittently on ROCm,The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284.,7325
20550,test_eig_with_eigvec_cuda_float64 (TestTensorDeviceOpsCUDA) is failing intermittently on ROCm,The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284.,7325
20551,"std: ambiguous in C++17.h, upgrading from LibTorch 1.6.0 to 1.8.1 with MSVC 2019 and c++17 standard ",append `::` before `std::function` calls in `C++17.h`.,601
20552,"std: ambiguous in C++17.h, upgrading from LibTorch 1.6.0 to 1.8.1 with MSVC 2019 and c++17 standard ",append `::` before `std::function` calls in `C++17.h`.,601
20553,custom collect_fn return None but collate_fn does not accept None,use filtering in Dataset to solve the problem,6874
20554,custom collect_fn return None but collate_fn does not accept None,use filtering in Dataset to solve the problem,6874
20555,"error: identifier ""cusparseScsrmm2"" is undefined",CUDA-11 was not yet available when PyTorch-1.3 was released,2187
20556,"error: identifier ""cusparseScsrmm2"" is undefined",CUDA-11 was not yet available when PyTorch-1.3 was released,2187
20557,torch.tensor with list of arrays orders of magnitude slower than np.array,Bug is fixed,6860
20558,torch.tensor with list of arrays orders of magnitude slower than np.array,Bug is fixed,6860
20559,Expected to have finished reduction in the prior iteration before starting a new one.,"pass the ""plugins=DDPPlugin(find_unused_parameters=True)"" into pl.trainer() ",6848
20560,Expected to have finished reduction in the prior iteration before starting a new one.,"pass the ""plugins=DDPPlugin(find_unused_parameters=True)"" into pl.trainer() ",6848
20561,Unskip CUDA grad/gradgrad checks or no longer mark as slow test,Bug is fixed,624
20562,Unskip CUDA grad/gradgrad checks or no longer mark as slow test,Bug is fixed,624
20563,torch.size for cfloat or cdouble,`x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward.,11363
20564,torch.size for cfloat or cdouble,`x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward.,11363
20565,Native Declared Functions Don't Support Full Aliasing Specification,work around by removing the annotations,3017
20566,Native Declared Functions Don't Support Full Aliasing Specification,work around by removing the annotations,3017
20567,Meshes not showing in tensorboard, raised it here PyTorch Forum,6881
20568,Meshes not showing in tensorboard, raised it here PyTorch Forum,6881
20569,cuDNN built against wrong CUDA version (10.0 instead of 9.0) when building from source -> CUDNN_STATUS_NOT_INITIALIZED,you can find PyTorch 1.3 or nightlies with CUDA 10.1 support ,5078
20570,cuDNN built against wrong CUDA version (10.0 instead of 9.0) when building from source -> CUDNN_STATUS_NOT_INITIALIZED,you can find PyTorch 1.3 or nightlies with CUDA 10.1 support ,5078
20571,clang 9 segfaults when trying to compile PyTorch, this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0.,4035
20572,clang 9 segfaults when trying to compile PyTorch, this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0.,4035
20573,Load data directly from GPU without copy to CPU,"Example for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)

```C++
void deleter(void *arg){};
    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)
    {
        bool isByte = (image.type() & 0xF) < 2;
        auto chans = image.channels();
        std::vector<int64_t> dims = {image.rows, image.cols, chans};
        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};
        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);
        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);

        if (isByte)
        {
            tensorImage = tensorImage.to(torch::kFloat);
        }

        return tensorImage;
    }```",3358
20574,Load data directly from GPU without copy to CPU,"Example for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)

```C++
void deleter(void *arg){};
    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)
    {
        bool isByte = (image.type() & 0xF) < 2;
        auto chans = image.channels();
        std::vector<int64_t> dims = {image.rows, image.cols, chans};
        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};
        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);
        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);

        if (isByte)
        {
            tensorImage = tensorImage.to(torch::kFloat);
        }

        return tensorImage;
    }```",3358
20575,The number of batches in epoch is affected by (num_workers + IterableDataset),Behaviour is by design,6781
20576,The number of batches in epoch is affected by (num_workers + IterableDataset),Behaviour is by design,6781
20577,Upgrade pytorch to use XNNPACK instead of NNPACK for android,Bug is fixed,2858
20578,Upgrade pytorch to use XNNPACK instead of NNPACK for android,Bug is fixed,2858
20579,Problem installation via pip pytorch 1.3.0 and 1.2.0 on windows cuda 10.0,"uda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0",1301
20580,Problem installation via pip pytorch 1.3.0 and 1.2.0 on windows cuda 10.0,"uda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0",1301
20581,Add high level autograd functions,This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients).,2880
20582,Add high level autograd functions,This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients).,2880
20583,Pytorch android Tensor.Shape() function not producing expected results.,"tensor.shape() simply returns a raw long[]
You should be able to use ""System.out.println(**Arrays.toString**(tensor.shape()));"" to print its content correctly.",2048
20584,Pytorch android Tensor.Shape() function not producing expected results.,"tensor.shape() simply returns a raw long[]
You should be able to use ""System.out.println(**Arrays.toString**(tensor.shape()));"" to print its content correctly.",2048
20585,How to convert Tensor back to BitMap or any image format in Android?,This issue is fixed,2851
20586,How to convert Tensor back to BitMap or any image format in Android?,This issue is fixed,2851
20587,AssertionError: Torch not compiled with CUDA enabled,"option to install using pip worked for me (inside a miniconda env, python 3.7.7)

From the pytorch website:

`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`",3126
20588,AssertionError: Torch not compiled with CUDA enabled,"option to install using pip worked for me (inside a miniconda env, python 3.7.7)

From the pytorch website:

`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`",3126
20589,Make it an option to have a different mask for each sequence in a batch for the Transformer,it's all the way to `nn.MultiheadAttention`,6858
20590,Make it an option to have a different mask for each sequence in a batch for the Transformer,it's all the way to `nn.MultiheadAttention`,6858
20591,"cuda_tensor.norm(dim=(X, Y)) is broken","pass `p=2`, then it calculates it as you expec",7428
20592,"cuda_tensor.norm(dim=(X, Y)) is broken","pass `p=2`, then it calculates it as you expec",7428
20593,Distributed Package asynchronous send/receive not working as expected (Gloo),"Bug is fixed
1) Update Gloo to allow non-blocking check for send/recv completion.
2) Update the bindings to call this function when `is_completed()` is called.",2870
20594,Distributed Package asynchronous send/receive not working as expected (Gloo),"Bug is fixed
1) Update Gloo to allow non-blocking check for send/recv completion.
2) Update the bindings to call this function when `is_completed()` is called.",2870
20595,conv1d fails in PyTorch 1.0,this issue is caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.,589
20596,conv1d fails in PyTorch 1.0,this issue is caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.,589
20597,v1.0.0 nn.utils.weight_norm seems to nullify gradients of unrelated parameters if wrapped in DataParallel,Bug is fixed ,651
20598,v1.0.0 nn.utils.weight_norm seems to nullify gradients of unrelated parameters if wrapped in DataParallel,Bug is fixed ,651
20599,[jit] CUDA fusion: a PTX JIT compilation failed,the issue is fixed,8712
20600,[jit] CUDA fusion: a PTX JIT compilation failed,the issue is fixed,8712
20601,Build command `python setup.py rebuild_libtorch` does not exist,this has been deprecated,8489
20602,Build command `python setup.py rebuild_libtorch` does not exist,this has been deprecated,8489
20603,better cmake checks on compiler version for libtorch binaries,"use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`",498
20604,better cmake checks on compiler version for libtorch binaries,"use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`",498
20605,PyTorch 1.0 fails to build with fbgemm enabled,This issue is fixed,9071
20606,PyTorch 1.0 fails to build with fbgemm enabled,This issue is fixed,9071
20607,Blocking: Do modules wait?,"every operation in CUDA in PyTorch is asynchronous, and thus non-blocking.",9279
20608,Blocking: Do modules wait?,"every operation in CUDA in PyTorch is asynchronous, and thus non-blocking.",9279
20609,[JIT] Trace->Script + Inplace causes shapes to be fixed where they should not,Bug is fixed now,3837
20610,[JIT] Trace->Script + Inplace causes shapes to be fixed where they should not,Bug is fixed now,3837
20611,error: unknown type name 'mkldnn_shuffle_desc_t',"```
cd third_party/ideep # this is on branch 
 grokmachine@Dendis-MacBook-Pro   ~/dev/facebook/pytorch/third_party/ideep     remotes/origin/mkldnn_0.17  git submodule update --init --recursive
 grokmachine@Dendis-MacBook-Pro   ~/dev/facebook/pytorch/third_party/ideep     remotes/origin/mkldnn_0.17  git checkout master
M        mkl-dnn
Previous HEAD position was d06f361 Fix klocwork issues
Switched to branch 'master'
Your branch is up to date with 'origin/master'.
```
then I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install`",4307
20612,model.cuda() doesn't automatically detect layers initialized in a list,Use `nn.ModuleList`,4002
20613,model.cuda() doesn't automatically detect layers initialized in a list,Use `nn.ModuleList`,4002
20614,[JIT] undefined symbol: cuCtxGetCurrent,fix for this is `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`.,803
20615,[JIT] undefined symbol: cuCtxGetCurrent,fix for this is `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`.,803
20616,[distributed] NCCL dist.barrier doesn't respect default device,"A workaround 
```
def barrier():
    t = torch.randn((), device='cuda')
    dist.all_reduce(t)
    torch.cuda.synchronize()

```",4231
20617,[distributed] NCCL dist.barrier doesn't respect default device,"A workaround 
```
def barrier():
    t = torch.randn((), device='cuda')
    dist.all_reduce(t)
    torch.cuda.synchronize()

```",4231
20618,Dilated conv in v1.0.0 is too slow.,"CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer.",7790
20619,Dilated conv in v1.0.0 is too slow.,"CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer.",7790
20620,libtorch hardcoded libculibos.a causes build errors for C++ programs on Arch Linux,This has been fixed,4609
20621,libtorch hardcoded libculibos.a causes build errors for C++ programs on Arch Linux,This has been fixed,4609
20622,Exporting model to onnx increases the model size,This is fixed,227
20623,Exporting model to onnx increases the model size,This is fixed,227
20624,Pytorch DataLoader freezes when num_workers > 0 in jupyter notebook(windows 10),"try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`.",644
20625,Pytorch DataLoader freezes when num_workers > 0 in jupyter notebook(windows 10),"try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`.",644
20626,autograd.grad with set_detect_anomaly(True) will cause memory leak,"``` #include <c10/cuda/CUDACachingAllocator.h> torch::autograd::DetectAnomalyGuard detect_anomaly; for(int i = 0; i < 10; i++) { auto x = torch::ones({10, 30000}).cuda().requires_grad_(); auto y = x.exp(); auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true); auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0); std::cout << ""allocated: "" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl; } ```",532
20627,autograd.grad with set_detect_anomaly(True) will cause memory leak,"``` #include <c10/cuda/CUDACachingAllocator.h> torch::autograd::DetectAnomalyGuard detect_anomaly; for(int i = 0; i < 10; i++) { auto x = torch::ones({10, 30000}).cuda().requires_grad_(); auto y = x.exp(); auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true); auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0); std::cout << ""allocated: "" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl; } ```",532
20628,Perf regression for YoloV3 CPU eager eval,Issue is fixed,7365
20629,Perf regression for YoloV3 CPU eager eval,Issue is fixed,7365
20630,Perf regression for background matting CUDA train,"dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct.",8844
20631,Perf regression for background matting CUDA train,"dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct.",8844
20632,DataLoader for Video Loading that utilizes the GPU,"use `pin_memory`.
You can also use `default_collate_fn` to transform batch into tensor",6899
20633,DataLoader for Video Loading that utilizes the GPU,"use `pin_memory`.
You can also use `default_collate_fn` to transform batch into tensor",6899
20634,[docs] Improve documentation for LayerNorm,Documentation is updated,7622
20635,[docs] Improve documentation for LayerNorm,Documentation is updated,7622
20636,CUDA error: an illegal memory access was encountered: on RTX3090 (using multiple GPUs),"disable ""Hardware-accelerated GPU scheduling"" in Windows settings",3311
20637,CUDA error: an illegal memory access was encountered: on RTX3090 (using multiple GPUs),"disable ""Hardware-accelerated GPU scheduling"" in Windows settings",3311
20638,Nans in matrix multiplication on ARM,it's due to the bug in implementation of missing NEON intrinsics,640
20639,Nans in matrix multiplication on ARM,it's due to the bug in implementation of missing NEON intrinsics,640
20640,"If a module passed to DistributedDataParallel has no parameter required gradient, expect_sparse_gradient[0] in _ddp_init_helper function will raise error.",bug is fixed,2255
20641,"If a module passed to DistributedDataParallel has no parameter required gradient, expect_sparse_gradient[0] in _ddp_init_helper function will raise error.",bug is fixed,2255
20642,Wrong crossentropy loss calculation,"you passed the weights, the mean should be a weighted mean.

Let's say...
`a = (-np.log(np.exp(0.5) / ∑exp(x[j])))`
`b = (-np.log(np.exp(0.2) / ∑exp(x[j])))`

Then...
**sum**
`6*a + 4*b`
**mean**
`(6*a + 4*b)/(6+4)`",5934
20643,Wrong crossentropy loss calculation,"you passed the weights, the mean should be a weighted mean.

Let's say...
`a = (-np.log(np.exp(0.5) / ∑exp(x[j])))`
`b = (-np.log(np.exp(0.2) / ∑exp(x[j])))`

Then...
**sum**
`6*a + 4*b`
**mean**
`(6*a + 4*b)/(6+4)`",5934
20644,assignment bug of advanced indexing,This is expected behaviour,1099
20645,assignment bug of advanced indexing,This is expected behaviour,1099
20646,Adding Mish Activation Function,want to include methods that the community uses as a standard,8264
20647,Adding Mish Activation Function,want to include methods that the community uses as a standard,8264
20648,automatic transition to 'cuda' if available,It's a breaking compatibility changes so we are not going to do this.,7318
20649,automatic transition to 'cuda' if available,It's a breaking compatibility changes so we are not going to do this.,7318
20650,"Replace ""NVIDIA driver"" with ""CUDA Toolkit"" in _check_driver() error messages","It returns ""Returns the latest version of CUDA supported by the driver."".

If you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`",9736
20651,"Replace ""NVIDIA driver"" with ""CUDA Toolkit"" in _check_driver() error messages","It returns ""Returns the latest version of CUDA supported by the driver."".

If you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`",9736
20652,Reference cycle in _LRScheduler,calls to `optim.step` may be missing a few cases,579
20653,Reference cycle in _LRScheduler,calls to `optim.step` may be missing a few cases,579
20654,Move QNNPACK micro kernels under ATen,This issue has been fixed,479
20655,Move QNNPACK micro kernels under ATen,This issue has been fixed,479
20656,FasterRCNN and MaskRCNN doesn't work with DataParallel or DistributedDataParallel,We don't need `args.local_rank` because of the `use_env` argument from launch,8263
20657,FasterRCNN and MaskRCNN doesn't work with DataParallel or DistributedDataParallel,We don't need `args.local_rank` because of the `use_env` argument from launch,8263
20658,Unable to  subscirpt self-defined class in torchscript function,BUg is fixed now,6861
20659,Unable to  subscirpt self-defined class in torchscript function,BUg is fixed now,6861
20660,Implement pep 503 Simple Repository API for deployment,Feature is added,8991
20661,Implement pep 503 Simple Repository API for deployment,Feature is added,8991
20662,Number of prefetch in DataLoader,"TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way.",6172
20663,Number of prefetch in DataLoader,"TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way.",6172
20664,"""Using PyTorch C++ Frontend"" TorchModule generator architecture is erroneous",This issue is fixed,615
20665,"""Using PyTorch C++ Frontend"" TorchModule generator architecture is erroneous",This issue is fixed,615
20666,Missing pip install wheels from the PyTorch official website,"use command line to download the wheels:

to see the content of the URL
`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`

e.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl 
`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`

See relative issue #25448 .",8604
20667,Missing pip install wheels from the PyTorch official website,"use command line to download the wheels:

to see the content of the URL
`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`

e.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl 
`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`

See relative issue #25448 .",8604
20668,Dropout behaves differently on different devices,we dont guarantee that the RNG outputs the same sequence across different GPU models.,11358
20669,Dropout behaves differently on different devices,we dont guarantee that the RNG outputs the same sequence across different GPU models.,11358
20670,No type hints on nn.Identity,This issue is fixed,572
20671,No type hints on nn.Identity,This issue is fixed,572
20672,PyTorch C++ API as a static lib: how to compile ?,"libtorch has download links on pytorch.org that look like this:

```
https://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip
```

You can replaced ""shared"" with ""static"" and get URLs like this:

```
https://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip
```",9790
20673,PyTorch C++ API as a static lib: how to compile ?,"libtorch has download links on pytorch.org that look like this:

```
https://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip
```

You can replaced ""shared"" with ""static"" and get URLs like this:

```
https://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip
```",9790
20674,toDense is misdocumented as a method on sparse tensors,This issue is fixed,504
20675,toDense is misdocumented as a method on sparse tensors,This issue is fixed,504
20676,backward_hook triggered despite RemovableHandle.remove(),"Use
```
import torch
import torch.nn as nn


def tensor_hook_adder(module, input, output):
    def tensor_backwards(grad):
        setattr(module, 'backprops', grad)
    output.register_hook(tensor_backwards)

layer = nn.Linear(2, 2, bias=False)
layer.weight.data.copy_(2*torch.eye(2))
model = layer
layer.register_forward_hook(tensor_hook_adder)

output = model(torch.tensor([1.,2]))
loss = output[0]**2+2*output[1]**2
loss.backward()
assert torch.allclose(layer.backprops, torch.tensor([4, 16.]))
```",6880
20677,backward_hook triggered despite RemovableHandle.remove(),"Use
```
import torch
import torch.nn as nn


def tensor_hook_adder(module, input, output):
    def tensor_backwards(grad):
        setattr(module, 'backprops', grad)
    output.register_hook(tensor_backwards)

layer = nn.Linear(2, 2, bias=False)
layer.weight.data.copy_(2*torch.eye(2))
model = layer
layer.register_forward_hook(tensor_hook_adder)

output = model(torch.tensor([1.,2]))
loss = output[0]**2+2*output[1]**2
loss.backward()
assert torch.allclose(layer.backprops, torch.tensor([4, 16.]))
```",6880
20678,[NGC Container]Runtime Error - Nvidia Nsight System,The issue is fixed,8552
20679,[NGC Container]Runtime Error - Nvidia Nsight System,The issue is fixed,8552
20680,[InstanceNorm] Unexpected behaviour with track_running_stats set to True in evaluation mode,Bug is fixed,7816
20681,[InstanceNorm] Unexpected behaviour with track_running_stats set to True in evaluation mode,Bug is fixed,7816
20682,"[Android] Latest nightly build causes error: library ""libpytorch_jni.so"" not found","To use lite interpreter, the model can be generated following the example:
```
import torch

model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)
model.eval()

scripted_module = torch.jit.script(model)
# Export full jit version model (not compatible lite interpreter), leave it here for comparison
scripted_module.save(""deeplabv3_scripted.pt"")
# Export lite interpreter version model (compatible with lite interpreter)
scripted_module._save_for_lite_interpreter(""deeplabv3_scripted.ptl"")
```

To load the model:
```
import org.pytorch.LiteModuleLoader
...
mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), ""deeplabv3_scripted.ptl""));
...",8237
20683,"[Android] Latest nightly build causes error: library ""libpytorch_jni.so"" not found","To use lite interpreter, the model can be generated following the example:
```
import torch

model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)
model.eval()

scripted_module = torch.jit.script(model)
# Export full jit version model (not compatible lite interpreter), leave it here for comparison
scripted_module.save(""deeplabv3_scripted.pt"")
# Export lite interpreter version model (compatible with lite interpreter)
scripted_module._save_for_lite_interpreter(""deeplabv3_scripted.ptl"")
```

To load the model:
```
import org.pytorch.LiteModuleLoader
...
mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), ""deeplabv3_scripted.ptl""));
...",8237
20684,NcclErrorHandlingTest.test_nccl_errors_blocking_abort frequently fails,The issue is already fixed,634
20685,NcclErrorHandlingTest.test_nccl_errors_blocking_abort frequently fails,The issue is already fixed,634
20686,Enabling AVX512 vectorization for `qadaptive_avg_pool2d_nhwc_kernel` & `qavg_pool2d_nhwc_kernel`,This issue is fixed,3962
20687,Enabling AVX512 vectorization for `qadaptive_avg_pool2d_nhwc_kernel` & `qavg_pool2d_nhwc_kernel`,This issue is fixed,3962
20688,Running some CI tests with `xlarge` `resource_class` VMs for testing AVX512 support,The feature has been added,9042
20689,Running some CI tests with `xlarge` `resource_class` VMs for testing AVX512 support,The feature has been added,9042
20690,"I got the error ""couldn't find ""libpytorch_jni.so"""" when using pytorch Android","To use lite interpreter, the model can be generated following the example:
```
import torch

model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)
model.eval()

scripted_module = torch.jit.script(model)
# Export full jit version model (not compatible lite interpreter), leave it here for comparison
scripted_module.save(""deeplabv3_scripted.pt"")
# Export lite interpreter version model (compatible with lite interpreter)
scripted_module._save_for_lite_interpreter(""deeplabv3_scripted.ptl"")
```

To load the model:
```
import org.pytorch.LiteModuleLoader
...
mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), ""deeplabv3_scripted.ptl""));
...",6798
20691,"I got the error ""couldn't find ""libpytorch_jni.so"""" when using pytorch Android","To use lite interpreter, the model can be generated following the example:
```
import torch

model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)
model.eval()

scripted_module = torch.jit.script(model)
# Export full jit version model (not compatible lite interpreter), leave it here for comparison
scripted_module.save(""deeplabv3_scripted.pt"")
# Export lite interpreter version model (compatible with lite interpreter)
scripted_module._save_for_lite_interpreter(""deeplabv3_scripted.ptl"")
```

To load the model:
```
import org.pytorch.LiteModuleLoader
...
mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), ""deeplabv3_scripted.ptl""));
...",6798
20692,Circular padding in Conv2d applies padding across the wrong dimension (regression from 1.4),The issue is fixed,9130
20693,Circular padding in Conv2d applies padding across the wrong dimension (regression from 1.4),The issue is fixed,9130
20694,Let future expose a then() API,Feature is added,2184
20695,Let future expose a then() API,Feature is added,2184
20696,convert_sync_batchnorm should respect device affinity,workaround is to move to cuda after converting to sync BN,7861
20697,convert_sync_batchnorm should respect device affinity,workaround is to move to cuda after converting to sync BN,7861
20698,TensorPipe dependency breaks build for non-X86; it expects x86intrin.h,This issue is fixed,6822
20699,TensorPipe dependency breaks build for non-X86; it expects x86intrin.h,This issue is fixed,6822
20700,hardsigmoid cuda_dispatch_ptr INTERNAL ASSERT FAILED,Bug is fixed,9091
20701,hardsigmoid cuda_dispatch_ptr INTERNAL ASSERT FAILED,Bug is fixed,9091
20702,DISABLED test_profiler_with_sync_rpc_udf (__main__.RpcTestWithSpawn),This has been fixed,7658
20703,DISABLED test_profiler_with_sync_rpc_udf (__main__.RpcTestWithSpawn),This has been fixed,7658
20704,Build documentation without emitting warnings,"only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:
```
torch.nn.intrinsic
torch.nn.instrinsic.qat
torch.nn.intrinsic.quantized
torch.nn.qat
torch.nn.quantized
torch.nn.quantized.dynamic
```

The remaining warnings are due to repeating names of some of the classes in those modules.",980
20705,Build documentation without emitting warnings,"only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:
```
torch.nn.intrinsic
torch.nn.instrinsic.qat
torch.nn.intrinsic.quantized
torch.nn.qat
torch.nn.quantized
torch.nn.quantized.dynamic
```

The remaining warnings are due to repeating names of some of the classes in those modules.",980
20706,CUDA debug build failed on Windows,This has been fixed,635
20707,CUDA debug build failed on Windows,This has been fixed,635
20708,DistributedDataSampler converts NamedTuple to regular tuple,Bug is fixed,9038
20709,DistributedDataSampler converts NamedTuple to regular tuple,Bug is fixed,9038
20710,quantization.test_quantize.TestGraphModePostTrainingStatic fails in tsan,issues with QNNPACK tests when ASAN/UBSAN is enabled. This is fixed now,3250
20711,quantization.test_quantize.TestGraphModePostTrainingStatic fails in tsan,issues with QNNPACK tests when ASAN/UBSAN is enabled. This is fixed now,3250
20712,[Feature] Elementwise operator complex_tensor.normalize(),this feature is added as `torch.sgn`,2812
20713,[Feature] Elementwise operator complex_tensor.normalize(),this feature is added as `torch.sgn`,2812
20714,Quantization: FakeQuant and Observers should sync enabled flags with DDP,the quantization methods should be created before DDP,9537
20715,Quantization: FakeQuant and Observers should sync enabled flags with DDP,the quantization methods should be created before DDP,9537
20716,pytorch-mobile memory leak or emptyCache api,"**module code**
```
import torch

print(torch.__version__)


# simple module

class SimpleModule(torch.nn.Module):
    def __init__(self):
        super(SimpleModule, self).__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(32, 32, 3, 2),
            torch.nn.ReLU()
        )
        
    def forward(self, chunk):
        chunk = torch.as_tensor(chunk).unsqueeze(0)
        y = chunk.unsqueeze(1)  
        y = self.conv(y)
        b, c, t, f = y.size()
        y.transpose(1, 2).contiguous().view(b, t, c * f)
        return y
    

simple_cell = SimpleModule()

chunk = torch.randn(100, 80)
print(simple_cell(chunk))

simple_torchscript = torch.jit.script(simple_cell)
simple_torchscript.save(""simple.pt"")
```

**C++ sample code：**
```
#include <iostream>
#include ""torch/csrc/api/include/torch/torch.h""
#include ""torch/csrc/api/include/torch/utils.h""
#include <caffe2/utils/threadpool/ThreadPool.h>
#include <caffe2/utils/threadpool/ThreadPoolMobile.h>
#include ""torch/script.h""
#include <utility>
#include <unistd.h>
#include <vector>
#include <pthread.h>

using std::cout;
using std::endl;
torch::jit::Module module_;
struct JITCallGuard {
  // AutoGrad is disabled for mobile by default.
  torch::autograd::AutoGradMode no_autograd_guard{false};
  // VariableType dispatch is not included in default mobile build. We need set
  // this guard globally to avoid dispatch error (only for dynamic dispatch).
  // Thanks to the unification of Variable class and Tensor class it's no longer
  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to
  // always set NonVariableTypeMode for inference only use case.
  torch::AutoNonVariableTypeMode non_var_guard{true};
  // Disable graph optimizer to ensure list of unused ops are not changed for
  // custom mobile build.
  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};
};

void *autoTest(void *args) {
  printf(""start load \n"");
  try {
    JITCallGuard guard;
    module_ = torch::jit::load(std::move(""./simple.pt""));
  } catch (const c10::Error &e) {
    printf(""load module resource failed."");
    return 0;
  }
  module_.eval();
  printf(""load over \n"");
  usleep(1000 * 1000);

  // run forward
  caffe2::mobile_threadpool()->setNumThreads(1);
  torch::jit::getProfilingMode() = false;
  torch::jit::getExecutorMode() = false;
  torch::jit::setGraphExecutorOptimize(false);
  printf(""start recycle \n"");
  int count = 3;
  while (count >= 0) {
    float *data = new float[8000];
    torch::Tensor datas = torch::from_blob(data, {100, 80});
    std::vector<torch::jit::IValue> inputs{};
    inputs.emplace_back(datas);
    JITCallGuard guard;
    module_.forward(inputs);
    delete[] data;
    usleep(500 * 1000);
    printf(""current count is %d .\n"", count);
    count--;
  }
  pthread_exit(NULL);
}


int main() {
  cout << ""Test TorchScript Memory Leak pytorch 1.5.0"" << endl;
  for (int i = 0; i < 200; i++) {
    cout << ""times is "" << i << endl;
    pthread_t tid;
    pthread_attr_t attr;
    void *status;
    pthread_attr_init(&attr);

    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);
    int ret = pthread_create(&tid, &attr, autoTest, NULL);
    if (ret != 0) {
      cout << ""pthread_create error: error_code="" << ret << endl;
      return 0;
    }
    pthread_attr_destroy(&attr);
    int rc = pthread_join(tid, &status);
    if (rc) {
      cout << ""Error:unable to join,"" << rc << endl;
      exit(-1);
    }
    cout << ""Main: completed thread id : ""
            ""exiting with status :"" << status << endl;

  }
  return 0;
}
```

get memory info by
```
pid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done
```
CMakeList
```
cmake_minimum_required(VERSION 3.6)

set(PLATFORM_ANDROID TRUE)
set(ANDROID_STL c++_static)
set(CMAKE_VERBOSE_MAKEFILE ON)
# NDK r20b
set(ANDROID_NDK ""path/to/your/ndk"")
set(CMAKE_TOOLCHAIN_FILE ""${ANDROID_NDK}/build/cmake/android.toolchain.cmake"")
set(ANDROID_ABI arm64-v8a)
set(ANDROID_NATIVE_API_LEVEL android-28)


project(MemoryLeak)

set(CMAKE_BUILD_TYPE Release)

if (CMAKE_BUILD_TYPE MATCHES ""Debug"" OR CMAKE_BUILD_TYPE MATCHES ""None"")
    message(STATUS ""CMAKE_BUILD_TYPE is Debug"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
elseif (CMAKE_BUILD_TYPE MATCHES ""Release"")
    message(STATUS ""CMAKE_BUILD_TYPE is Release"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O3"")
    set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -Wl,-s"")
elseif (CMAKE_BUILD_TYPE MATCHES ""RelWitchDebInfo"")
    message(STATUS ""CMAKE_BUILD_TYPE is RelWitchDebInfo"")
elseif (CMAKE_BUILD_TYPE MATCHES ""MinSizeRel"")
    message(STATUS ""CMAKE_BUILD_TYPE is MinSizeRel"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O3"")
else ()
    message(STATUS ""unknown CMAKE_BUILD_TYPE = "" ${CMAKE_BUILD_TYPE})
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
ENDif ()
# set STL C++ 14
set(CMAKE_CXX_STANDARD 14)

include_directories(
        ${PROJECT_SOURCE_DIR}/pytorch/include/
        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include
)

link_directories(
        ${PROJECT_SOURCE_DIR}/pytorch/lib/
)


add_executable(MemoryLeak main.cpp)
target_link_libraries(
        MemoryLeak
        -Wl,--gc-sections
        -Wl,--whole-archive
        torch.a
        torch_cpu.a
        -Wl,--no-whole-archive
        c10.a
        nnpack.a
        XNNPACK.a
        pytorch_qnnpack.a
        eigen_blas.a
        cpuinfo.a
        clog.a
        log
        m
        z
)
```
I found use pthread will come out memory leak.use main function will not, NOTE: in android environment",80
20717,pytorch-mobile memory leak or emptyCache api,"**module code**
```
import torch

print(torch.__version__)


# simple module

class SimpleModule(torch.nn.Module):
    def __init__(self):
        super(SimpleModule, self).__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(32, 32, 3, 2),
            torch.nn.ReLU()
        )
        
    def forward(self, chunk):
        chunk = torch.as_tensor(chunk).unsqueeze(0)
        y = chunk.unsqueeze(1)  
        y = self.conv(y)
        b, c, t, f = y.size()
        y.transpose(1, 2).contiguous().view(b, t, c * f)
        return y
    

simple_cell = SimpleModule()

chunk = torch.randn(100, 80)
print(simple_cell(chunk))

simple_torchscript = torch.jit.script(simple_cell)
simple_torchscript.save(""simple.pt"")
```

**C++ sample code：**
```
#include <iostream>
#include ""torch/csrc/api/include/torch/torch.h""
#include ""torch/csrc/api/include/torch/utils.h""
#include <caffe2/utils/threadpool/ThreadPool.h>
#include <caffe2/utils/threadpool/ThreadPoolMobile.h>
#include ""torch/script.h""
#include <utility>
#include <unistd.h>
#include <vector>
#include <pthread.h>

using std::cout;
using std::endl;
torch::jit::Module module_;
struct JITCallGuard {
  // AutoGrad is disabled for mobile by default.
  torch::autograd::AutoGradMode no_autograd_guard{false};
  // VariableType dispatch is not included in default mobile build. We need set
  // this guard globally to avoid dispatch error (only for dynamic dispatch).
  // Thanks to the unification of Variable class and Tensor class it's no longer
  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to
  // always set NonVariableTypeMode for inference only use case.
  torch::AutoNonVariableTypeMode non_var_guard{true};
  // Disable graph optimizer to ensure list of unused ops are not changed for
  // custom mobile build.
  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};
};

void *autoTest(void *args) {
  printf(""start load \n"");
  try {
    JITCallGuard guard;
    module_ = torch::jit::load(std::move(""./simple.pt""));
  } catch (const c10::Error &e) {
    printf(""load module resource failed."");
    return 0;
  }
  module_.eval();
  printf(""load over \n"");
  usleep(1000 * 1000);

  // run forward
  caffe2::mobile_threadpool()->setNumThreads(1);
  torch::jit::getProfilingMode() = false;
  torch::jit::getExecutorMode() = false;
  torch::jit::setGraphExecutorOptimize(false);
  printf(""start recycle \n"");
  int count = 3;
  while (count >= 0) {
    float *data = new float[8000];
    torch::Tensor datas = torch::from_blob(data, {100, 80});
    std::vector<torch::jit::IValue> inputs{};
    inputs.emplace_back(datas);
    JITCallGuard guard;
    module_.forward(inputs);
    delete[] data;
    usleep(500 * 1000);
    printf(""current count is %d .\n"", count);
    count--;
  }
  pthread_exit(NULL);
}


int main() {
  cout << ""Test TorchScript Memory Leak pytorch 1.5.0"" << endl;
  for (int i = 0; i < 200; i++) {
    cout << ""times is "" << i << endl;
    pthread_t tid;
    pthread_attr_t attr;
    void *status;
    pthread_attr_init(&attr);

    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);
    int ret = pthread_create(&tid, &attr, autoTest, NULL);
    if (ret != 0) {
      cout << ""pthread_create error: error_code="" << ret << endl;
      return 0;
    }
    pthread_attr_destroy(&attr);
    int rc = pthread_join(tid, &status);
    if (rc) {
      cout << ""Error:unable to join,"" << rc << endl;
      exit(-1);
    }
    cout << ""Main: completed thread id : ""
            ""exiting with status :"" << status << endl;

  }
  return 0;
}
```

get memory info by
```
pid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done
```
CMakeList
```
cmake_minimum_required(VERSION 3.6)

set(PLATFORM_ANDROID TRUE)
set(ANDROID_STL c++_static)
set(CMAKE_VERBOSE_MAKEFILE ON)
# NDK r20b
set(ANDROID_NDK ""path/to/your/ndk"")
set(CMAKE_TOOLCHAIN_FILE ""${ANDROID_NDK}/build/cmake/android.toolchain.cmake"")
set(ANDROID_ABI arm64-v8a)
set(ANDROID_NATIVE_API_LEVEL android-28)


project(MemoryLeak)

set(CMAKE_BUILD_TYPE Release)

if (CMAKE_BUILD_TYPE MATCHES ""Debug"" OR CMAKE_BUILD_TYPE MATCHES ""None"")
    message(STATUS ""CMAKE_BUILD_TYPE is Debug"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
elseif (CMAKE_BUILD_TYPE MATCHES ""Release"")
    message(STATUS ""CMAKE_BUILD_TYPE is Release"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O3"")
    set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -Wl,-s"")
elseif (CMAKE_BUILD_TYPE MATCHES ""RelWitchDebInfo"")
    message(STATUS ""CMAKE_BUILD_TYPE is RelWitchDebInfo"")
elseif (CMAKE_BUILD_TYPE MATCHES ""MinSizeRel"")
    message(STATUS ""CMAKE_BUILD_TYPE is MinSizeRel"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O3"")
else ()
    message(STATUS ""unknown CMAKE_BUILD_TYPE = "" ${CMAKE_BUILD_TYPE})
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb"")
    set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb"")
ENDif ()
# set STL C++ 14
set(CMAKE_CXX_STANDARD 14)

include_directories(
        ${PROJECT_SOURCE_DIR}/pytorch/include/
        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include
)

link_directories(
        ${PROJECT_SOURCE_DIR}/pytorch/lib/
)


add_executable(MemoryLeak main.cpp)
target_link_libraries(
        MemoryLeak
        -Wl,--gc-sections
        -Wl,--whole-archive
        torch.a
        torch_cpu.a
        -Wl,--no-whole-archive
        c10.a
        nnpack.a
        XNNPACK.a
        pytorch_qnnpack.a
        eigen_blas.a
        cpuinfo.a
        clog.a
        log
        m
        z
)
```
I found use pthread will come out memory leak.use main function will not, NOTE: in android environment",80
20718,Very slow for gradient penalty!,"Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown",2735
20719,Very slow for gradient penalty!,"Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown",2735
20720,error when installing Caffe2: undefined reference to `void caffe2::math::BiasCHW,update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag,3121
20721,error when installing Caffe2: undefined reference to `void caffe2::math::BiasCHW,update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag,3121
20722,Batchnorm1d cannot work with batch size == 1,"This only fails because you have a batch size of 1 and a single feature per channel.
Iuse `drop_last=False` on the DataLoader to avoid the issue.",7770
20723,Batchnorm1d cannot work with batch size == 1,"This only fails because you have a batch size of 1 and a single feature per channel.
Iuse `drop_last=False` on the DataLoader to avoid the issue.",7770
20724,in-place Arithmetic assignment operators gives wrong answers,Ideally pytorch throw an error or warn if the tensors involved in an in-place operation have overlapping storage,6865
20725,in-place Arithmetic assignment operators gives wrong answers,Ideally pytorch throw an error or warn if the tensors involved in an in-place operation have overlapping storage,6865
20726,ATen C++ tensor creation places tensors on devices inconsistently from torch.* Python calls,The issue is fixed,638
20727,ATen C++ tensor creation places tensors on devices inconsistently from torch.* Python calls,The issue is fixed,638
20728,better error if file does not support seek (torch.load/save),Better error message has been added,4286
20729,better error if file does not support seek (torch.load/save),Better error message has been added,4286
20730,[pytorch] [feature request] Flatten convenience method,Feature is added in pytorch,555
20731,[pytorch] [feature request] Flatten convenience method,Feature is added in pytorch,555
20732,torch.empty after construction puts a high load on CPU for a long time when the size is big,"the problem is ""Variable Inspector"" plugin.",5991
20733,torch.empty after construction puts a high load on CPU for a long time when the size is big,"the problem is ""Variable Inspector"" plugin.",5991
20734,[pytorch] [Feature Request] SoftArgMax Function for differentiable argmax,use softmax and dot product with indices,1565
20735,[pytorch] [Feature Request] SoftArgMax Function for differentiable argmax,use softmax and dot product with indices,1565
20736,Cannot compile Caffe2 on Mac,The bug is fixed,5970
20737,Cannot compile Caffe2 on Mac,The bug is fixed,5970
20738,Compiling C/C++ extension receive undefined symbol: _ZTIN5torch8autograd8Variable4ImplE on importing into python.,import torch before the module it works,2768
20739,Compiling C/C++ extension receive undefined symbol: _ZTIN5torch8autograd8Variable4ImplE on importing into python.,import torch before the module it works,2768
20740,Tag v0.4.0 eigen submodule no longer downloads,This has been fixed,3324
20741,Tag v0.4.0 eigen submodule no longer downloads,This has been fixed,3324
20742,torch.cdist gradients are NAN for p<1 and very small differences in a given dimension (0<delta<~e-45),The bug is fixed,7799
20743,torch.cdist gradients are NAN for p<1 and very small differences in a given dimension (0<delta<~e-45),The bug is fixed,7799
20744,"`F.logsigmoid(input, out=blah)` crashes",`log_softmax` and `log_sigmoid` used underscores consistently,66
20745,"`F.logsigmoid(input, out=blah)` crashes",`log_softmax` and `log_sigmoid` used underscores consistently,66
20746,INTERNAL ASSERT FAILED at mmdet/ops/nms/src/nms_cpu.cpp:7,The issue is fixed,645
20747,INTERNAL ASSERT FAILED at mmdet/ops/nms/src/nms_cpu.cpp:7,The issue is fixed,645
20748,Multiprocessing on Python 3.8 fails with cannot pickle '_io.TextIOWrapper',"this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac.",690
20749,Multiprocessing on Python 3.8 fails with cannot pickle '_io.TextIOWrapper',"this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac.",690
20750,"Load pytorch tensor created by torch.save(tensor_name, tensor_path) in c++ libtorch failed.",The feature is added,7657
20751,"Load pytorch tensor created by torch.save(tensor_name, tensor_path) in c++ libtorch failed.",The feature is added,7657
20752,Build fails with gcc 9.1 + CUDA-10.2,The issue is fixed,689
20753,Build fails with gcc 9.1 + CUDA-10.2,The issue is fixed,689
20754,The mean method results in different outputs," it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.

If you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision",210
20755,The mean method results in different outputs," it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.

If you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision",210
20756,torch.save does not use zipfile serialization,The issue is fixed,2720
20757,torch.save does not use zipfile serialization,The issue is fixed,2720
20758,Multi channel linear layer,"using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:

```
my_tensor = torch.randn(channel_size, batch_size, input_size)
weight = torch.randn(channel_size, input_size, output_size)
bias = torch.randn(channel_size, 1, output_size)

output = torch.bmm(my_tensor, weight) + bias
```",8293
20759,Multi channel linear layer,"using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:

```
my_tensor = torch.randn(channel_size, batch_size, input_size)
weight = torch.randn(channel_size, input_size, output_size)
bias = torch.randn(channel_size, 1, output_size)

output = torch.bmm(my_tensor, weight) + bias
```",8293
20760,Exception in find_cuda_windows_lib,These functions are removed,7437
20761,Exception in find_cuda_windows_lib,These functions are removed,7437
20762,Autograd view CreationMeta are not properly propagated when chaining views,The issue is fixed,3289
20763,Autograd view CreationMeta are not properly propagated when chaining views,The issue is fixed,3289
20764,Add TracedModule attribute for the constants table,The feature is added,4141
20765,Add TracedModule attribute for the constants table,The feature is added,4141
20766,THCUNN/BCECriterion.cu:42: Assertion `input >= 0. && input <= 1.` failed.,"this may happens when your network initiated with a bad startup.
usually the output is too big , eg , sigmoid(20.) == 1
so, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.

solution:
torch.nn.init.xavier_normal_(weight)
torch.nn.init.constant_(bias, 0)",9559
20767,THCUNN/BCECriterion.cu:42: Assertion `input >= 0. && input <= 1.` failed.,"this may happens when your network initiated with a bad startup.
usually the output is too big , eg , sigmoid(20.) == 1
so, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.

solution:
torch.nn.init.xavier_normal_(weight)
torch.nn.init.constant_(bias, 0)",9559
20768,Add torch.sgn to return complex sign,The feature is added,625
20769,Add torch.sgn to return complex sign,The feature is added,625
20770,torch.triangular_solver doesn't work on batched inputs,The issue is fixed now,8520
20771,torch.triangular_solver doesn't work on batched inputs,The issue is fixed now,8520
20772,Explicitly Define Gradient for Certain Computation,"You can implement your Function/Operator the way `MyRelu` is defined.
https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html",658
20773,Explicitly Define Gradient for Certain Computation,"You can implement your Function/Operator the way `MyRelu` is defined.
https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html",658
20774,Bug when an inplace op is done on the output of the forward of a autograd.Function and this output is a view of an intermediary result of the forward.,It has been fixed by modifying `CheckpointFunction`.,558
20775,Bug when an inplace op is done on the output of the forward of a autograd.Function and this output is a view of an intermediary result of the forward.,It has been fixed by modifying `CheckpointFunction`.,558
20776,Support matmul for scalar tensors,The feature is not added,7246
20777,Support matmul for scalar tensors,The feature is not added,7246
20778,Build failure (sleef.h not found) in some hard-to-understand situations,"To fix, 
`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`",7896
20779,Build failure (sleef.h not found) in some hard-to-understand situations,"To fix, 
`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`",7896
20780,AttributeError: module 'torch.jit' has no attribute 'unused',"`entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work",8728
20781,AttributeError: module 'torch.jit' has no attribute 'unused',"`entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work",8728
20782,BC check test was failing on Friday and Saturday,bug is fixed now,1143
20783,BC check test was failing on Friday and Saturday,bug is fixed now,1143
20784,Where did 1.2.0 go on the website?,This is fixed now,7653
20785,Where did 1.2.0 go on the website?,This is fixed now,7653
20786,[jit] `zip` and `enumerate` can't be used in a list comprehension,This has been fixed,5902
20787,[jit] `zip` and `enumerate` can't be used in a list comprehension,This has been fixed,5902
20788,MultiheadAttention and DDP incompatability,This issue is fixed now,649
20789,MultiheadAttention and DDP incompatability,This issue is fixed now,649
20790,"spectral_norm used in RNN causes ""parameter types mismatch"" in GPU",The issue is closed without any fix,3015
20791,"spectral_norm used in RNN causes ""parameter types mismatch"" in GPU",The issue is closed without any fix,3015
20792,[jit] `zeros_like` needs full Tensor options to work,"```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```",8719
20793,[jit] `zeros_like` needs full Tensor options to work,"```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```",8719
20794,[jit] Tensor `.tolist()` is not bound in TorchScript,"This isn't possible, because we need to type the resulting list, and `tolist()` returns an arbitrarily nested list",7701
20795,"""type id is VariableTensorId"" runtime error for some mobile models",The bug is fixed,48
20796,Crash on assigning cuda tensor to bool-indexed cpu tensor,bug is fixed ,10905
20797,Flakiness in TestAutogradDeviceTypeCUDA,"1-line addition of a future->waitNoThrow() call to the early exit path in mark_graph_task_complete().
    This will have the property of guaranteeing that the future is completed on exit ",4592
20798,Export to ONNX of nop-squeeze errors out in ONNXRT,The bug is fixed,6843
20799,Error with CUDA for quantization aware training,workground is to do `qat_model.to(device='cuda')` again before the loop. Maybe some tensors are not transferred to GPU after calling `torch.quantization.prepare_qat`.,8493
20800,Nondeterministic output from Optimizer.state_dict(),Bug is fixed,6163
20801,torch.assert operator / async assertion for cuda tensors,This feature is added,3988
20802,SyncBatchNorm size check,This feature is already added,7494
20803,"Max-over-dim is 2,000 times slower than max"," max() uses a better implementation than max over a dimension, especially when the output size is small , The short-term fix for the max performance may be
```
ii=a.argmax(0)
maxval = a.gather(0, ii.unsqueeze(0)).squeeze(0)
```",6840
20804,How does pytorch computes on GPU?,The question should be asked on pytorch discuss,4845
20805,Unexpected crash in autograd.grad," Make sure that returned gradient is not the same by making the backward clone it (this is what happens when you multiply by 1)
Fix the jacobian code to not reuse the original Tensor by doing `jac.append(grad_x.reshape(x.shape).clone())`. ",2878
20806,pytorch latest update(1.4) broke MultiStepLR: wrong LR after step from _get_closed_form_lr,The issue is fixed,730
20807,test_DistributedDataParallel fails with parameter replication error,This pickle error is fixed,4151
20808,MacOS install error: Library not loaded: @rpath/libc++.1.dylib,"from __future__ import print_function
import torch
x = torch.rand(5, 3)
print(x)

It devolved:

tensor([[0.3633, 0.7173, 0.6055],
        [0.3442, 0.6892, 0.2950],
        [0.3909, 0.4718, 0.0202],
        [0.1055, 0.8912, 0.0667],
        [0.3819, 0.4413, 0.1544]])",4599
20809,nightly nvcc fatal : Value 'c++14' is not defined for option 'std',The bug is fixed,11444
20810,libtorch.so breaks google test,build googletest with `-D_GLIBCXX_USE_CXX11_ABI=0` to force it to use old ABI,922
20811,Very Slow moving model to device with model.to(device),This issue is fixed,717
20812,Abort all nccl communicators explicitly when destroy process group,The issue is fixed,9034
20813,"cannot initialize type ""WorkerId""' crashing pytest in 1.4.0",make sure there is no  mishmash of both versions pytorch and conda,10834
20814,Hi i found a typo in the Documentation,The typo is fixed,2869
20815,Enable MKLDNN support  on Windows packages by default as well,This is a duplicate issue,2221
20816,Memory leak in MaxPool2d," this is not linked to `max_pool2d`. In the for loop, replacing it with `torch._C._nn.max_pool2d_with_indices(x, 1)`, `torch._C._nn.log_sigmoid(x)`, `torch.threshold(x, 0, 0)` or `torch.relu(x)` all give the same behavior",652
20817,Python 3.8 serialization error," if you're unpickling from a mmap object, it's far more efficient to do so directly from memory:
```python
data = pickle.loads(memoryview(mmap_object))
```
than to issue `read()` calls by treating the mmap as a regular file object:
```python
data = pickle.load(mmap_object)
```",1314
20818,PyTorch-1.4.0 doesn't encode numpy dependency,This issue is fixed,2461
20819,Pytorch 1.4.0 deadlock in multiprocessing,the problem is actually with `mp.spawn`,8072
20820,Second derivative fails if first derivative happens to be constant,This is expected behaviour,2876
20821,NCCL_ROOT_DIR in Cmake FindNCCL,"cmake_cache_vars['USE_SYSTEM_NCCL'] is by default pointing to /usr/local/cuda for UNIX

Given the above code, it will look for system NCCL using env vars `NCCL_LIBRARIES` and `NCCL_INCLUDE_DIRS`",239
20822,Inconsistent behaviour of `argmax` between `PyTorch` and `NumPy`,"This is expected behavior, due to parallelization we don't make any guarantees on which element is returned, so it's not even guaranteed to be the last.",5150
20823,Pytorch 1.4.0 weight drop - 'LSTM' object has no attribute 'weight_hh_l0',get nightly package to get a fix.,8051
20824,Gaps for making template-unboxing work for all operators,This issue is fixed,5749
20825,OneCycleLR mentions `verbose` as an argument on the doc page when it is not,This feature has been added,6835
20826,Build failed on Raspberry Pi: fatal error: gloo/algorithm.h: No such file or directory,"Please try commenting out this line:
https://github.com/pytorch/pytorch/blob/752f433a2484db25f076b2fc85c40ab191656bd9/test/cpp/rpc/CMakeLists.txt#L6",4860
20827,torch.cuda.device not working but torch.cuda.set_device works,"`torch.cuda.device()` is a context manager: ``` torch.cuda.set_device(0) # On device 0 with torch.cuda.device(1): print(""Inside device is 1"") # On device 1 print(""Outside is still 0"") # On device 0 ```",2882
20828,torch.cuda.device not working but torch.cuda.set_device works,"`torch.cuda.device()` is a context manager: ``` torch.cuda.set_device(0) # On device 0 with torch.cuda.device(1): print(""Inside device is 1"") # On device 1 print(""Outside is still 0"") # On device 0 ```",2882
20829,Have torch.manual_seed seed all GPUs as well,"Yes I think it's enough to add these two lines. It's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState",8477
20830,Have torch.manual_seed seed all GPUs as well,"Yes I think it's enough to add these two lines. It's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState",8477
20831,ImportError: libmkl_intel_lp64.so: cannot open shared object file,"Try a clean re-install. 
```bash
rm -rf build
rm -rf torch/lib/build
```",8015
20832,ImportError: libmkl_intel_lp64.so: cannot open shared object file,"Try a clean re-install. 
```bash
rm -rf build
rm -rf torch/lib/build
```",8015
20833,Variable.clone() does not clone to the same device,It's not expected. `clone()` should operate within a single device,4005
20834,Variable.clone() does not clone to the same device,It's not expected. `clone()` should operate within a single device,4005
20835,How to get raw pointer from tensors?,"for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel",516
20836,How to get raw pointer from tensors?,"for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel",516
20837,Pytorch Freezes System,"it's not typical at all, there's something weird going on, but it's not the GPU memory. Usually freezing indicates two things: - you are running out of CPU memory and you are hitting disk swap - a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly. I wonder if your case is either of them.",9744
20838,Pytorch Freezes System,"it's not typical at all, there's something weird going on, but it's not the GPU memory. Usually freezing indicates two things: - you are running out of CPU memory and you are hitting disk swap - a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly. I wonder if your case is either of them.",9744
20839,Ellipsis encoding fails when printing tensors,What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`.,3179
20840,Ellipsis encoding fails when printing tensors,What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`.,3179
20841,[JIT] jit.trace does not support parameter.requires_grad?,"Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design.",8498
20842,[JIT] jit.trace does not support parameter.requires_grad?,"Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design.",8498
20843,torch.meshgrid has no docstring if typing.TYPE_CHECKING is True,This is fixed in 1.8.0 and in master,194
20844,torch.meshgrid has no docstring if typing.TYPE_CHECKING is True,This is fixed in 1.8.0 and in master,194
20845,ManyLinux v1.8 release .whl for AArch64 (Arm) does not work on CentOS 8,"That works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box,",2837
20846,ManyLinux v1.8 release .whl for AArch64 (Arm) does not work on CentOS 8,"That works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box,",2837
20847,Add complex autograd support for torch.symeig,Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue,4026
20848,Add complex autograd support for torch.symeig,Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue,4026
20849,Pytorch install via Pip Error,verified this is fixed in 1.8.1 `pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1,11345
20850,Pytorch install via Pip Error,verified this is fixed in 1.8.1 `pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1,11345
20851,Exception in thread pool task: !completed() INTERNAL ASSERT FAILED,"It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling,",3960
20852,Exception in thread pool task: !completed() INTERNAL ASSERT FAILED,"It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling,",3960
20853,"[linear-algebra][discussion][proposal] Move linear algebra functions to torch.linalg, at::native::linalg","`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar).",8745
20854,"[linear-algebra][discussion][proposal] Move linear algebra functions to torch.linalg, at::native::linalg","`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar).",8745
20855,Distributed Training shut down on second epoch,This is fixed in 1.8.0 and in master,739
20856,Distributed Training shut down on second epoch,This is fixed in 1.8.0 and in master,739
20857,MultivariateNormal and potrf is slow on gpu and seems to have some memory leak,This is fixed in 1.8.0 and in master,6853
20858,MultivariateNormal and potrf is slow on gpu and seems to have some memory leak,This is fixed in 1.8.0 and in master,6853
20859,Quantization Model Support,This is fixed in 1.8.0 and in master,10885
20860,Quantization Model Support,This is fixed in 1.8.0 and in master,10885
20861,ImportError: cannot import name 'caffe2_pb2' (Ubuntu 18.04),"Don't start python from inside the pytorch directory, cd somewhere else and try again.",2196
20862,ImportError: cannot import name 'caffe2_pb2' (Ubuntu 18.04),"Don't start python from inside the pytorch directory, cd somewhere else and try again.",2196
20863,"RuntimeError ""sizes must be non-negative"" (0.4.1)",This is fixed in 1.8.0 and in master,8259
20864,"RuntimeError ""sizes must be non-negative"" (0.4.1)",This is fixed in 1.8.0 and in master,8259
20865,Unintuitive error message when loading state into incompatibly-sized model,This is fixed in 1.8.0 and in master,7580
20866,Unintuitive error message when loading state into incompatibly-sized model,This is fixed in 1.8.0 and in master,7580
20867,[JIT] Tracer throws runtime exception for torch.normal,This is fixed in 1.8.0 and in master,3334
20868,[JIT] Tracer throws runtime exception for torch.normal,This is fixed in 1.8.0 and in master,3334
20869,"RuntimeError: the derivative for 'target' is not implemented. When I use F.smooth_l1_loss(x, y, reduce=False)","You feed (GT, PRED) but it should be (PRED, GT)...",1272
20870,"RuntimeError: the derivative for 'target' is not implemented. When I use F.smooth_l1_loss(x, y, reduce=False)","You feed (GT, PRED) but it should be (PRED, GT)...",1272
20871,nn.BatchNorm1d failed on GPU,"The problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu: ``` import torch from torch.autograd import Variable a = Variable(torch.randn(2,5).cuda(), requires_grad=True) batchnorm = torch.nn.BatchNorm1d(5).cuda() y = batchnorm(a) ```",6762
20872,nn.BatchNorm1d failed on GPU,"The problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu: ``` import torch from torch.autograd import Variable a = Variable(torch.randn(2,5).cuda(), requires_grad=True) batchnorm = torch.nn.BatchNorm1d(5).cuda() y = batchnorm(a) ```",6762
20873,Aten compile error,A workaround is removing the old headers from system include path.,3564
20874,Aten compile error,A workaround is removing the old headers from system include path.,3564
20875,`backward` hangs in multiprocess after single-process,"workaround is to add this right after `import torch.multiprocessing as mp`: ```python if __name__ == ""__main__"": mp.set_start_method(""spawn"") ```",527
20876,`backward` hangs in multiprocess after single-process,"workaround is to add this right after `import torch.multiprocessing as mp`: ```python if __name__ == ""__main__"": mp.set_start_method(""spawn"") ```",527
20877,Out of memory with higher-order gradients involving batchnorm2d,This is fixed in 0.3.0 and in master,6879
20878,Out of memory with higher-order gradients involving batchnorm2d,This is fixed in 0.3.0 and in master,6879
20879,0.2_4 release notes inconsistent with documentation and actual behavior of reduce functions,This is fixed in 1.8.0 and in master,7252
20880,0.2_4 release notes inconsistent with documentation and actual behavior of reduce functions,This is fixed in 1.8.0 and in master,7252
20881,RuntimeError: cublas runtime error : library not initialized,`sudo rm -rf ~/.nv` works.,8741
20882,RuntimeError: cublas runtime error : library not initialized,`sudo rm -rf ~/.nv` works.,8741
20883,Feature request: expm1,This is fixed in master,8746
20884,Feature request: expm1,This is fixed in master,8746
20885,How to use conda to update pytorch to 0.4 version,The main website contains the informations to install 0.3 with conda.,2873
20886,How to use conda to update pytorch to 0.4 version,The main website contains the informations to install 0.3 with conda.,2873
20887,NVIDIA memory not deallocated after interupt,"You can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it.",2881
20888,NVIDIA memory not deallocated after interupt,"You can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it.",2881
20889,Error in building 0.3.0 in macOS High Sierra,"you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package.",6162
20890,Error in building 0.3.0 in macOS High Sierra,"you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package.",6162
20891,Test without backward the model will run out of memory,"Same things apply for 0.4. For `volatile`, use `torch.no_grad()`",5732
20892,Test without backward the model will run out of memory,"Same things apply for 0.4. For `volatile`, use `torch.no_grad()`",5732
20893,"FAILED Build: __AVX2__ is defined (via e.g. -mavx2) "" ""but CAFFE2_PERF_WITH_AVX2 is not defined.","It turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it",2595
20894,"FAILED Build: __AVX2__ is defined (via e.g. -mavx2) "" ""but CAFFE2_PERF_WITH_AVX2 is not defined.","It turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it",2595
20895,Segmentation fault (core dumped) in C++ API for centos,"try updating your GCC to be 4.9 or higher,",3287
20896,Segmentation fault (core dumped) in C++ API for centos,"try updating your GCC to be 4.9 or higher,",3287
20897,Non-deterministic behavior in pytorch even with seeds set,"I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this.",3032
20898,Non-deterministic behavior in pytorch even with seeds set,"I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this.",3032
20899,torch.nn.utils.rnn.pack_padded_sequence not working in multi-GPU environments,You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU,8647
20900,torch.nn.utils.rnn.pack_padded_sequence not working in multi-GPU environments,You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU,8647
20901,export onnx model and load by caffe2 error,"onnx_caffe2 is out of date. It is merged to caffe2. Also here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb",562
20902,export onnx model and load by caffe2 error,"onnx_caffe2 is out of date. It is merged to caffe2. Also here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb",562
20903,Memory Error in pip install of torch 1.2.0 on Linux,`pip install --no-cache-dir install torchvision` seems to have gotten around the issue. I guess there is a typo in your command. The command which worked for me: `pip --no-cache-dir install torchvision`,193
20904,Memory Error in pip install of torch 1.2.0 on Linux,`pip install --no-cache-dir install torchvision` seems to have gotten around the issue. I guess there is a typo in your command. The command which worked for me: `pip --no-cache-dir install torchvision`,193
20905,ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory,Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`,5989
20906,ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory,Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`,5989
20907,torch.load issue on loading file created by torch.save,This is fixed in 1.8.0 and in master,6850
20908,torch.load issue on loading file created by torch.save,This is fixed in 1.8.0 and in master,6850
20909,Trouble installing PyTorch for CUDA 9.0,"> How do I install it in this case?

1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser
2. Go into the source 
3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file
4. Install via `pip install [downloaded .whl file]`.",223
20910,Trouble installing PyTorch for CUDA 9.0,"> How do I install it in this case?

1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser
2. Go into the source 
3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file
4. Install via `pip install [downloaded .whl file]`.",223
20911,problem with mkldnn and march=native,This is fixed in 1.8.0 and in master,549
20912,problem with mkldnn and march=native,This is fixed in 1.8.0 and in master,549
20913,Unknown Error at torch/lib/THC/THCGeneral.c:66,"cuda error unknown happens for many weird reasons.
Here's one try to fix it:

```
$ sudo python
>>> import torch
>>> a = torch.randn().cuda()
```
Then exit.

This might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps.",9138
20914,Unknown Error at torch/lib/THC/THCGeneral.c:66,"cuda error unknown happens for many weird reasons.
Here's one try to fix it:

```
$ sudo python
>>> import torch
>>> a = torch.randn().cuda()
```
Then exit.

This might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps.",9138
20915,LSTM memory leak?,It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle,3989
20916,LSTM memory leak?,It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle,3989
20917,How to specify the cuda PATH in pytorch?,Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`,8019
20918,How to specify the cuda PATH in pytorch?,Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`,8019
20919,Confusing error msg when padding is set to float in nn.Conv1d,"I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.
```python
import torch
import torch.nn as nn
from torch.autograd import Variable
input = Variable(torch.randn(1, 1, 10))
output = nn.Conv1d(1, 1, 3, padding=1)(input) # fine
output = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error
```",7866
20920,Confusing error msg when padding is set to float in nn.Conv1d,"I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.
```python
import torch
import torch.nn as nn
from torch.autograd import Variable
input = Variable(torch.randn(1, 1, 10))
output = nn.Conv1d(1, 1, 3, padding=1)(input) # fine
output = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error
```",7866
20921,Implement toCFloat() for Variables and numel() = 1 Tensors,this is now implemented.,10900
20922,Implement toCFloat() for Variables and numel() = 1 Tensors,this is now implemented.,10900
20923,cuda 9.0 not found but detected 6.1,CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good.,1504
20924,cuda 9.0 not found but detected 6.1,CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good.,1504
20925,Why the Dropout2d and BatchNorm2d's model.eval() result is poor,decrease momentum value in BatchNorm layer to something small like 0.0001,9151
20926,Why the Dropout2d and BatchNorm2d's model.eval() result is poor,decrease momentum value in BatchNorm layer to something small like 0.0001,9151
20927,2 processes cannot use same GPU,"It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`.",3915
20928,2 processes cannot use same GPU,"It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`.",3915
20929,"how could i get old version of libtorch , thanks",https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip,2513
20930,"how could i get old version of libtorch , thanks",https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip,2513
20931,Export to ONNX fails using F.interpolate with mode='area',"area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter.",660
20932,Export to ONNX fails using F.interpolate with mode='area',"area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter.",660
20933,Performance regression for inference from pytorch 1.4.0 to >= 1.5.0,I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0,6814
20934,Performance regression for inference from pytorch 1.4.0 to >= 1.5.0,I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0,6814
20935,Error installing 0.3.0 from Anaconda on MacOS 10.13.1,Update conda first with `conda update conda` and try again,8061
20936,Error installing 0.3.0 from Anaconda on MacOS 10.13.1,Update conda first with `conda update conda` and try again,8061
20937,Segmentation Fault when importing Torch,Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable.,1156
20938,Segmentation Fault when importing Torch,Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable.,1156
20939,How to only padding the bottom when use the Conv2D ?,use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad),11306
20940,How to only padding the bottom when use the Conv2D ?,use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad),11306
20941,how should I cite PyTorch in the paper?,"For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:

```
@article{paszke2017automatic,
 title={Automatic differentiation in PyTorch},
 author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
 year={2017}
}
```

The paper is located here: https://openreview.net/forum?id=BJJsrmfCZ",2568
20942,how should I cite PyTorch in the paper?,"For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:

```
@article{paszke2017automatic,
 title={Automatic differentiation in PyTorch},
 author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
 year={2017}
}
```

The paper is located here: https://openreview.net/forum?id=BJJsrmfCZ",2568
20943,non-cudnn LSTM and GRU biases have wrong shapes,Fix now included in https://github.com/pytorch/pytorch/pull/1683,2454
20944,non-cudnn LSTM and GRU biases have wrong shapes,Fix now included in https://github.com/pytorch/pytorch/pull/1683,2454
20945,Is there any API to visualize the architecture of model?,There's (https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file,7424
20946,Is there any API to visualize the architecture of model?,There's (https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file,7424
20947,Add a torch.matmul function and document broadcast behavior of it and delegated functions,"Yes, `matmul` is present in master and in v0.2",8499
20948,Add a torch.matmul function and document broadcast behavior of it and delegated functions,"Yes, `matmul` is present in master and in v0.2",8499
20949,Add SELU (Scaled ELU),"It looks like it can be implemented somewhat efficiently in one line: ```python import torch.nn.functional as F def selu(x): alpha = 1.6732632423543772848170429916717 scale = 1.0507009873554804934193349852946 return scale * F.elu(x, alpha) ```",3957
20950,Add SELU (Scaled ELU),"It looks like it can be implemented somewhat efficiently in one line: ```python import torch.nn.functional as F def selu(x): alpha = 1.6732632423543772848170429916717 scale = 1.0507009873554804934193349852946 return scale * F.elu(x, alpha) ```",3957
20951,What to do if CUDA doesn't work,"where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine. This usually happens when the nvidia driver gets updated.",273
20952,What to do if CUDA doesn't work,"where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine. This usually happens when the nvidia driver gets updated.",273
20953,Missing bernoulli_ on torch.cuda.ByteTensor for nn.AlphaDropout,"we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)",5942
20954,Missing bernoulli_ on torch.cuda.ByteTensor for nn.AlphaDropout,"we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)",5942
20955,RuntimeError: invalid multinomial distribution (encountering probability entry < 0),"When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`",8381
20956,RuntimeError: invalid multinomial distribution (encountering probability entry < 0),"When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`",8381
20957,Exception using optimize_for_mobile on retinanet from torchvision,Fixed on master,2470
20958,Exception using optimize_for_mobile on retinanet from torchvision,Fixed on master,2470
20959,The return of torch.inverse contains nan sometime,You can still use multi-stream if you properly register all the tensors to the correct streams,8599
20960,The return of torch.inverse contains nan sometime,You can still use multi-stream if you properly register all the tensors to the correct streams,8599
20961,[jit] create dict or list from zip,Fixed on master,9353
20962,[jit] create dict or list from zip,Fixed on master,9353
20963,CUDAExtension: nvcc does not pick right gcc by default,Fixed on master,3973
20964,CUDAExtension: nvcc does not pick right gcc by default,Fixed on master,3973
20965,Update how to build PyTorch with CUDA Windows instructions,I think we should just say that we support cuda >= 10.1 and vs >= 2019.,655
20966,Update how to build PyTorch with CUDA Windows instructions,I think we should just say that we support cuda >= 10.1 and vs >= 2019.,655
20967,Unable to create TorchScript compatible CUDA extension,Fixed on master,736
20968,Unable to create TorchScript compatible CUDA extension,Fixed on master,736
20969,Migrate `set_` from the TH to Aten (CUDA),Fixed in Master,9542
20970,Migrate `set_` from the TH to Aten (CUDA),Fixed in Master,9542
20971,Support cast of single-element tensors to numbers,"Yup 

```python
>>> import numpy as np
>>> zero_dim = np.array(3)
>>> scalar = np.float(3)
>>> single_elem = np.array([[[3]]])
>>> float(zero_dim), float(scalar), float(single_elem)
(3.0, 3.0, 3.0)
>>> empty = np.array([], dtype=np.float32)
>>> float(empty)
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
TypeError: only length-1 arrays can be converted to Python scalars

```",8688
20972,Support cast of single-element tensors to numbers,"Yup 

```python
>>> import numpy as np
>>> zero_dim = np.array(3)
>>> scalar = np.float(3)
>>> single_elem = np.array([[[3]]])
>>> float(zero_dim), float(scalar), float(single_elem)
(3.0, 3.0, 3.0)
>>> empty = np.array([], dtype=np.float32)
>>> float(empty)
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
TypeError: only length-1 arrays can be converted to Python scalars

```",8688
20973,CUDNN_STATUS_NOT_INITIALIZED,pytorch ships it's own cudnn,10524
20974,CUDNN_STATUS_NOT_INITIALIZED,pytorch ships it's own cudnn,10524
20975,Tensor flattening on broadcast doesn't handle heterogeneous types,fixed in master,9348
20976,Tensor flattening on broadcast doesn't handle heterogeneous types,fixed in master,9348
20977,"Anaconda installation CUDA requested, cpuonly obtained","In case someone still runs into this: try setting `conda config --set channel_priority strict`.

A similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. 

This will install the `cuda10.1` package:
```
conda create -n pytorch14
conda activate pytorch14
conda install pytorch==1.4.0 -c pytorch
```

Starting again in a clean env:
```
conda create -n pytorch14
conda activate pytorch14
conda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version
conda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say ""no""

conda config --set channel_priority strict
conda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version
```",3705
20978,"Anaconda installation CUDA requested, cpuonly obtained","In case someone still runs into this: try setting `conda config --set channel_priority strict`.

A similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. 

This will install the `cuda10.1` package:
```
conda create -n pytorch14
conda activate pytorch14
conda install pytorch==1.4.0 -c pytorch
```

Starting again in a clean env:
```
conda create -n pytorch14
conda activate pytorch14
conda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version
conda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say ""no""

conda config --set channel_priority strict
conda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version
```",3705
20979,how to install pytorch on AMD GPU,`python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?,8325
20980,how to install pytorch on AMD GPU,`python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?,8325
20981,AttributeError: module 'torch.nn.utils' has no attribute 'prune',you need to `import torch.nn.utils.prune` not only `import torch`,11459
20982,AttributeError: module 'torch.nn.utils' has no attribute 'prune',you need to `import torch.nn.utils.prune` not only `import torch`,11459
20983,Flaky test test_cos_scalar_cpu TestAutogradDeviceTypeCPU on MacOS,"we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop.",534
20984,Flaky test test_cos_scalar_cpu TestAutogradDeviceTypeCPU on MacOS,"we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop.",534
20985,Numerical problems with torch.nn.functional.kl_div,"The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions... So, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal.",4603
20986,Numerical problems with torch.nn.functional.kl_div,"The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions... So, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal.",4603
20987,Instantiating `torch.distributions.Categorical` with all-zero long-dtype probabilities crashes Python,"It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes.",4010
20988,Instantiating `torch.distributions.Categorical` with all-zero long-dtype probabilities crashes Python,"It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes.",4010
20989,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input,"It happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big.",3916
20990,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input,"It happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big.",3916
20991,Input and output tensors to `torch._C.Node`,"You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet.",8601
20992,Input and output tensors to `torch._C.Node`,"You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet.",8601
20993,RuntimeError: CUDA out of memory. Tried to allocate 12.50 MiB (GPU 0; 10.92 GiB total capacity; 8.57 MiB already allocated; 9.28 GiB free; 4.68 MiB cached),"It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.",3931
20994,RuntimeError: CUDA out of memory. Tried to allocate 12.50 MiB (GPU 0; 10.92 GiB total capacity; 8.57 MiB already allocated; 9.28 GiB free; 4.68 MiB cached),"It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.",3931
20995,tensor multinomial dependent on shape for some reason,"For me, it was 
```
import torch
import numpy as np
foo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))
torch.multinomial(foo3, 10, True)
# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
```",2556
20996,tensor multinomial dependent on shape for some reason,"For me, it was 
```
import torch
import numpy as np
foo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))
torch.multinomial(foo3, 10, True)
# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
```",2556
20997,Implement target derivative for smooth L1 loss,"In the meantime, you can implement `smooth_l1_loss` via the following:
```python
def smooth_l1_loss(input, target, beta=1, size_average=True):
 """"""
 very similar to the smooth_l1_loss from pytorch, but with
 the extra beta parameter
 """"""
 n = torch.abs(input - target)
 cond = n < beta
 loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)
 if size_average:
 return loss.mean()
 return loss.sum()
```",3780
20998,Implement target derivative for smooth L1 loss,"In the meantime, you can implement `smooth_l1_loss` via the following:
```python
def smooth_l1_loss(input, target, beta=1, size_average=True):
 """"""
 very similar to the smooth_l1_loss from pytorch, but with
 the extra beta parameter
 """"""
 n = torch.abs(input - target)
 cond = n < beta
 loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)
 if size_average:
 return loss.mean()
 return loss.sum()
```",3780
20999,When can PyTorch support for RTX series GPU?,PyTorch works and ships with CUDA10.,9090
21000,When can PyTorch support for RTX series GPU?,PyTorch works and ships with CUDA10.,9090
21001,Naming conflict of `test_api` in Windows release builds for wheels,"@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.
```bash
git submodule sync
git submodule update --init --recursive
```",489
21002,Naming conflict of `test_api` in Windows release builds for wheels,"@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.
```bash
git submodule sync
git submodule update --init --recursive
```",489
21003,Lack of Square Function,"In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with ""redundant"" functions (and we get the same behavior with less typing by just doing `a ** 2`).",3785
21004,Lack of Square Function,"In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with ""redundant"" functions (and we get the same behavior with less typing by just doing `a ** 2`).",3785
21005,Combine Variable and Tensor APIs (Perform autograd directly on torch.Tensor),Fixed in Master,7632
21006,Combine Variable and Tensor APIs (Perform autograd directly on torch.Tensor),Fixed in Master,7632
21007,How to convert to older version of pytorch v0.1.12?,"`conda install pytorch=0.1.12 cuda80 -c soumith`
",8726
21008,How to convert to older version of pytorch v0.1.12?,"`conda install pytorch=0.1.12 cuda80 -c soumith`
",8726
21009,add documentation / links for old binaries,"you can find the linux wheels
- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)
- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)

and the mac wheel: 
- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl",11451
21010,add documentation / links for old binaries,"you can find the linux wheels
- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)
- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)

and the mac wheel: 
- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl",11451
21011,"datasets.ImageFolder error ""in __getattr__ raise AttributeError(name) AttributeError: __exit__""","I had the same issue, and upgrading the Pillow library resolved it.
Try this:
`pip install --upgrade Pillow `",3125
21012,"datasets.ImageFolder error ""in __getattr__ raise AttributeError(name) AttributeError: __exit__""","I had the same issue, and upgrading the Pillow library resolved it.
Try this:
`pip install --upgrade Pillow `",3125
21013,RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1502004572321/work/torch/lib/THC/generic/THCStorage.cu:66,"Restart the program with os.environ[""CUDA_VISIBLE_DEVICES""] = '0'",5171
21014,RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1502004572321/work/torch/lib/THC/generic/THCStorage.cu:66,"Restart the program with os.environ[""CUDA_VISIBLE_DEVICES""] = '0'",5171
21015,CondaError: CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/soumith/linux-64/pytorch-0.2.0-py35hb1547bd_4cu80.tar.bz2>,could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network.,9521
21016,CondaError: CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/soumith/linux-64/pytorch-0.2.0-py35hb1547bd_4cu80.tar.bz2>,could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network.,9521
21017,"Same ADD operation, Tensor has a different result from Numpy",This problem is solved by upgrading PyTorch version.,204
21018,"Same ADD operation, Tensor has a different result from Numpy",This problem is solved by upgrading PyTorch version.,204
21019,`@torch.jit.ignore` and `@property`,"`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export
`@unused` = cannot run in TorchScript nor Python and throws an error if called",8709
21020,`@torch.jit.ignore` and `@property`,"`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export
`@unused` = cannot run in TorchScript nor Python and throws an error if called",8709
21021,torch.linalg.svd out of memory,"The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added.",6989
21022,torch.linalg.svd out of memory,"The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added.",6989
21023,Please verify ONNX v1.10.0 Release Candidate,"Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.
I ran:
`python test/onnx/test_pytorch_onnx_onnxruntime.py`",6778
21024,Please verify ONNX v1.10.0 Release Candidate,"Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.
I ran:
`python test/onnx/test_pytorch_onnx_onnxruntime.py`",6778
21025,Unable to install 1.2,Pytorch 1.2 just went live on conda for Windows,5017
21026,Unable to install 1.2,Pytorch 1.2 just went live on conda for Windows,5017
21027,problem with torch.util.tensorboard add_graph(),"I too am getting a graph page that is empty. I did flush and close the SummaryWriter.
Attaching screenshot including Chrome's console which shows an error that may be related.

Note:
* I do see the textual graph being dumped to the command line console and it seems correct there.

**Configuration:**
* PyTorch 1.2.0
* TensoBoard 1.14.0
* Python 3.5.2

![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)",3290
21028,problem with torch.util.tensorboard add_graph(),"I too am getting a graph page that is empty. I did flush and close the SummaryWriter.
Attaching screenshot including Chrome's console which shows an error that may be related.

Note:
* I do see the textual graph being dumped to the command line console and it seems correct there.

**Configuration:**
* PyTorch 1.2.0
* TensoBoard 1.14.0
* Python 3.5.2

![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)",3290
21029,Implement torch.uniform,"
```python
a = torch.tensor([0., 10.])
b = torch.tensor([1., 11.])
torch.distributions.Uniform(a, b).sample()
>>> tensor([ 0.8583, 10.0226])
```",3251
21030,Implement torch.uniform,"
```python
a = torch.tensor([0., 10.])
b = torch.tensor([1., 11.])
torch.distributions.Uniform(a, b).sample()
>>> tensor([ 0.8583, 10.0226])
```",3251
21031,How to install torchaudio on Mac M1 ARM?,"Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573",2912
21032,How to install torchaudio on Mac M1 ARM?,"Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573",2912
21033,Pytorch 1.2.0 RuntimeError: code is too big,The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2).,7068
21034,Pytorch 1.2.0 RuntimeError: code is too big,The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2).,7068
21035,Very Slow Moving Tensor to CUDA device (CUDA 10.1 with PyTorch 1.3),"This issue is now fixed with newly updated binaries.
Uninstalling and reinstalling PyTorch from Anaconda will fix it.",7704
21036,Very Slow Moving Tensor to CUDA device (CUDA 10.1 with PyTorch 1.3),"This issue is now fixed with newly updated binaries.
Uninstalling and reinstalling PyTorch from Anaconda will fix it.",7704
21037,"Indexed, in-place multiplication segfaults/drops values",it has been fixed.,8506
21038,"Indexed, in-place multiplication segfaults/drops values",it has been fixed.,8506
21039,Install from source on Centos7 doesn't work,"Have you tried the following?
* `python setup.py clean --all` before compiling
* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`
* compiling without CUDA: `NO_CUDA=1 python setup.py install`
* making sure you are on the lastest master commit",2762
21040,Install from source on Centos7 doesn't work,"Have you tried the following?
* `python setup.py clean --all` before compiling
* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`
* compiling without CUDA: `NO_CUDA=1 python setup.py install`
* making sure you are on the lastest master commit",2762
21041,Cholesky Error for positive definite matrix,"Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite.",2397
21042,Cholesky Error for positive definite matrix,"Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite.",2397
21043,libtorch cannot find CUDA," if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc",3096
21044,libtorch cannot find CUDA," if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc",3096
21045,Does tensors got from torch.distributed.all_gather in order?,"yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat).",254
21046,Does tensors got from torch.distributed.all_gather in order?,"yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat).",254
21047,What is the torchvision version for pytorch-nightly? Use 0.3.0 to report errors,"It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently.",3978
21048,What is the torchvision version for pytorch-nightly? Use 0.3.0 to report errors,"It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently.",3978
21049,running into error installing from source,Try cudnn v6.,8330
21050,running into error installing from source,Try cudnn v6.,8330
21051,padding_idx doesn't work.,"Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))
From nn.Embedding source, you could see:
```
 if self.padding_idx is not None:
 self.weight.data[self.padding_idx].fill_(0)
```",5146
21052,padding_idx doesn't work.,"Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))
From nn.Embedding source, you could see:
```
 if self.padding_idx is not None:
 self.weight.data[self.padding_idx].fill_(0)
```",5146
21053,"Undefined symbol ""_state""","I started using `MACOSX_DEPLOYMENT_TARGET=10.11` instead of `10.9` because I realized that was the wrong version, but it didn't fix anything. Also, I ran `install_name_tool -add_rpath /usr/local/cuda/lib ~/Desktop/pytorch/torch/lib/tmp_install/lib/libTHPP.1.dylib` and this seemed to solve the error listed in my edit.",1274
21054,"Undefined symbol ""_state""",,1274
21055,batch matrix-vector multiplication (bmv),"according to broadcasting rules, batch2 wont auto-expand because the expansion is to not prepend dimensions but to append in this case.

I believe you can do:
`batch2 = batch2.unsqueeze(2)`",8787
21056,batch matrix-vector multiplication (bmv),,8787
21057,nonzero doesn't squeeze dimension,Current nonzero() output cannot be used for indexing e.g. `z[x.nonzero()] = 2.0` would be convenient. AFAIK it cannot be used with index_select or index_fill as well.,11495
21058,nonzero doesn't squeeze dimension,,11495
21059,numerical stability for logSigmoid,"Using function below seems to be a better choice. def log_sigmoid(x): return torch.clamp(x, max=0)-torch.log(torch.exp(-torch.abs(x))+1)",8111
21060,numerical stability for logSigmoid,,8111
21061,ModuleNotFoundError: No module named 'torch.autograd',pip install torchvision,10420
21062,ModuleNotFoundError: No module named 'torch.autograd',,10420
21063,torch.std giving incorrect results,They are correct. The difference lies in [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).,7448
21064,torch.std giving incorrect results,,7448
21065,Missing ‘get_trace_graph’ function in Torch1.4,"Yes, `get_trace_graph` was an internal function that was intended for use by ONNX export only. As an alternative, please get the graph like: ``` traced = torch.jit.trace(model, inputs) traced_graph = traced.graph ```",8497
21066,Missing ‘get_trace_graph’ function in Torch1.4,,8497
21067,Didn't find engine for operation quantized::conv_prepack NoQEngine (operator () at ..\aten\src\ATen\native\quantized\cpu\qconv_prepack.cpp:264) (no backtrace available),"If you are on Windows, then you'll need to update to 1.5.0.",725
21068,Didn't find engine for operation quantized::conv_prepack NoQEngine (operator () at ..\aten\src\ATen\native\quantized\cpu\qconv_prepack.cpp:264) (no backtrace available),,725
21069,cpu usage is too high on the main thread after pytorch version 1.1 (and 1.2) (not data loader workers ),"Pytorch 1.1 and above utilize more CPU threads than Pytorch 1.0.1. If you want to return to previous behavior (slower code, using less cores) run your code with `OMP_NUM_THREADS=1` or any other suitable value.",5015
21070,cpu usage is too high on the main thread after pytorch version 1.1 (and 1.2) (not data loader workers ),,5015
21071,Source of `torch.testing.assert_close` reaches a dead end,Fixed by https://github.com/pytorch/pytorch.github.io/commit/5a2028452f93fdc5e44a65e761e03ed658048336 and https://github.com/pytorch/pytorch.github.io/commit/82eaa25adc292d81414e2bd33e95466c52e78dea,2459
21072,Source of `torch.testing.assert_close` reaches a dead end,,2459
21073,fx: unable to symbolically trace BatchNorm2d due to control flow,"The use case we are working with is hitting this because a custom child of `nn.BatchNorm` is calling `super().forward(x)`: ``` class BatchNorm2d(torch.nn.BatchNorm2d): def forward(self, x): if x.numel() > 0: return super(BatchNorm2d, self).forward(x) # get output shape output_shape = x.shape return _NewEmptyTensorOp.apply(x, output_shape) ```",3193
21074,fx: unable to symbolically trace BatchNorm2d due to control flow,,3193
21075,[JIT] Support Dict comprehension,"instead of writing a forloop manually, one interesting workaround is to use list comprehension: `x = dict([(i,i) for i in range(2)])`",9684
21076,[JIT] Support Dict comprehension,,9684
21077,Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error,"successfully compile it on Jetson TX2 follow these steps: 1.Build PyTorch from source. `python3 setup.py build` 2.No need to download LibTorch from the official website, which is the x86_64 build. 3.build the example-cpp using compiled tmp_install directory: `cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch/torch/lib/tmp_install ..`",3165
21078,Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error,,3165
21079,ONNX export failure: Exporting the operator hardsigmoid to ONNX opset version 12 is not supported,Directly export to ONNX::Hardsigmoid for Hardsigmoid symbols.,2482
21080,ONNX export failure: Exporting the operator hardsigmoid to ONNX opset version 12 is not supported,Directly export to ONNX::Hardsigmoid for Hardsigmoid symbols.,2482
21081,ROCm CI is intermittently failing with std::out_of_range,"In the autograd engine queue lookup, add a range assert.",945
21082,ROCm CI is intermittently failing with std::out_of_range,"In the autograd engine queue lookup, add a range assert.",945
21083,Building with MSVC 2019 Community Edition fails,"Add the assignment operator for `torch::OrderedDict<Key, Value>::Item`.",5161
21084,Building with MSVC 2019 Community Edition fails,"Add the assignment operator for `torch::OrderedDict<Key, Value>::Item`.",5161
21085,torch.log() returns -inf/nan on exponential input,Use float datatype inputs or convert it to numpy.,3900
21086,torch.log() returns -inf/nan on exponential input,Use float datatype inputs or convert it to numpy.,3900
21087,__torch_function__ documentation for inheriting from tensor seems incomplete or incorrect,This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`,994
21088,__torch_function__ documentation for inheriting from tensor seems incomplete or incorrect,This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`,994
21089,`nn.Conv3d` throws incorrect error message,the in-place op is called on the output tensor (with the same dtype as the input),6813
21090,`nn.Conv3d` throws incorrect error message,the in-place op is called on the output tensor (with the same dtype as the input),6813
21091,Is there a way to remove all zero elements in one line?,remove an element from 1-d tensor,228
21092,Is there a way to remove all zero elements in one line?,remove an element from 1-d tensor,228
21093,Backprop second time on spectral_norm,there are some inplace ops in there that break double backward,4153
21094,Backprop second time on spectral_norm,there are some inplace ops in there that break double backward,4153
21095,quantize_per_tensor and quantize_per_channel should work on fp16 tensors,"The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies.",9125
21096,quantize_per_tensor and quantize_per_channel should work on fp16 tensors,"The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies.",9125
21097,Query on PyTorch threading model,"two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`.",7326
21098,Query on PyTorch threading model,"two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`.",7326
21099,torch.cat results does not have `requires_grad` if under an autograd function,Things run in `Function::forward` are without autograd graph tracking.,9520
21100,torch.cat results does not have `requires_grad` if under an autograd function,Things run in `Function::forward` are without autograd graph tracking.,9520
21101,How to access model embedded functions?,Use <model>.<function>,10430
21102,How to access model embedded functions?,Use <model>.<function>,10430
21103,It'd best to set persistent_workers default value as True on Windows,Default persistent_worker from False to True on Windows,6809
21104,It'd best to set persistent_workers default value as True on Windows,Default persistent_worker from False to True on Windows,6809
21105,torch.autograd.Function with multiple outputs returns outputs not requiring grad,"Returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.
 
 So effectively, in your custom function here, you actually return the Tensors ""unpacked"" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either.",8320
21106,torch.autograd.Function with multiple outputs returns outputs not requiring grad,"Returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.
 
 So effectively, in your custom function here, you actually return the Tensors ""unpacked"" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either.",8320
21107,OpInfo addmv port from method_tests errors,torch.addmv_() allows inplace operations to change the size of the tensor they're operating on,7324
21108,OpInfo addmv port from method_tests errors,torch.addmv_() allows inplace operations to change the size of the tensor they're operating on,7324
21109,RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.,"include the loss computation in the forward function and let the forward function directly return the loss tensors. Then by setting find_unused_parameters=True, DDP should be able to traverse the graph from the loss and identify unused ones.",3993
21110,RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.,"include the loss computation in the forward function and let the forward function directly return the loss tensors. Then by setting find_unused_parameters=True, DDP should be able to traverse the graph from the loss and identify unused ones.",3993
21111,`Softplus` forward and backward discrepancy,"The fix converts the binary TensorIterator used by softplus backwards to a ternary one, adding in the original input for comparison against beta * threshold.",6787
21112,`Softplus` forward and backward discrepancy,"The fix converts the binary TensorIterator used by softplus backwards to a ternary one, adding in the original input for comparison against beta * threshold.",6787
21113,addmv_() allows resizing the tensor it operates on and produces wrong results,It produces bogus values.,3970
21114,addmv_() allows resizing the tensor it operates on and produces wrong results,It produces bogus values.,3970
21115,Verify that attempting to resize a tensor with an inplace operation throws a runtime error,Add broadcasts_input and verifies the behaviour for inplace_variant.,4910
21116,Verify that attempting to resize a tensor with an inplace operation throws a runtime error,Add broadcasts_input and verifies the behaviour for inplace_variant.,4910
21117,Modification of tests for one method causes failures for some others,Port them to OpInfos.,231
21118,Modification of tests for one method causes failures for some others,Port them to OpInfos.,231
21119,"torch.linalg: unclear ""synchronizes that device with the CPU""","Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer.",6785
21120,"torch.linalg: unclear ""synchronizes that device with the CPU""","Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer.",6785
21121,test_zero_redundancy_optimizer.py fails when run more than 4 GPU setup,"I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP.",5994
21122,test_zero_redundancy_optimizer.py fails when run more than 4 GPU setup,"I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP.",5994
21123,CUDA error when used torch.mm() with gpu in pytorch1.8.0,torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations.,3026
21124,CUDA error when used torch.mm() with gpu in pytorch1.8.0,torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations.,3026
21125,Failed to compute shorthash for libnvrtc.so when compiling application with libtorch 1.8.0,"Problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined.",3335
21126,Failed to compute shorthash for libnvrtc.so when compiling application with libtorch 1.8.0,"Problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined.",3335
21127,AdamW variable referenced before assignment,The current error is that beta1 is referenced before assignment.,2915
21128,AdamW variable referenced before assignment,The current error is that beta1 is referenced before assignment.,2915
21129,Selection of test classes / methods through run_test.py is broken,enable test selection for default test handler.,4312
21130,Selection of test classes / methods through run_test.py is broken,enable test selection for default test handler.,4312
21131,Status of 64-bit Arm support in PyTorch 1.8,issue previously encountered with NumPy ManyLinux builds.,2836
21132,Status of 64-bit Arm support in PyTorch 1.8,issue previously encountered with NumPy ManyLinux builds.,2836
21133,Backward compatibility problem of LSTMs in v1.8.0,Users should be only saving state_dicts to avoid such issues.,10269
21134,Backward compatibility problem of LSTMs in v1.8.0,Users should be only saving state_dicts to avoid such issues.,10269
21135,[bug] test_autograd sometimes runs CUDA tests on CPU,"`det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values.",7860
21136,[bug] test_autograd sometimes runs CUDA tests on CPU,"`det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values.",7860
21137,Lazy Module Documentation suggestions,Lazy Modules Documentation Clarifications.,6855
21138,Lazy Module Documentation suggestions,Lazy Modules Documentation Clarifications.,6855
21139,Undocumented change of behavior for Embeddings in PyTorch 1.8,"always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value",3338
21140,Undocumented change of behavior for Embeddings in PyTorch 1.8,"always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value",3338
21141,DDP.no_sync() function + torch.cuda.synchronize(device=rank),"calling `backward()` on a loss tensor produced by DDP will always sync grads if not under no_sync() context manager. To manually sync grads, you can use a local model on all ranks and call `dist.all_reduce()` on gradients after backwards pass is completed, but we recommend DDP for performance reasons.",1603
21142,DDP.no_sync() function + torch.cuda.synchronize(device=rank),"calling `backward()` on a loss tensor produced by DDP will always sync grads if not under no_sync() context manager. To manually sync grads, you can use a local model on all ranks and call `dist.all_reduce()` on gradients after backwards pass is completed, but we recommend DDP for performance reasons.",1603
21143,ImportError: libtinfo.so.5,"install the `libncurses5` using:
 ```text
 
 sudo apt install libncurses5
 
 ```",3020
21144,ImportError: libtinfo.so.5,"install the `libncurses5` using:
 ```text
 
 sudo apt install libncurses5
 
 ```",3020
21145,SIGSEGV in torch.linalg.inv,one of the arguments to the LAPACK call was not correct for 0x0 matrices,7233
21146,SIGSEGV in torch.linalg.inv,one of the arguments to the LAPACK call was not correct for 0x0 matrices,7233
21147,"Library location assumption in test/jit/test_backends.py, test/jit/test_torchbind.py, and test/test_fx.py",installing the torch wheel package,52
21148,"Library location assumption in test/jit/test_backends.py, test/jit/test_torchbind.py, and test/test_fx.py",installing the torch wheel package,52
21149,PyTorch 1.8 CUDNN_STATUS_NOT_INITIALIZED,"was fine on 1080ti, but broke for 2080ti.",10639
21150,PyTorch 1.8 CUDNN_STATUS_NOT_INITIALIZED,"was fine on 1080ti, but broke for 2080ti.",10639
21151,libtorch 1.8.0 with CUDA 11.1: CUDA error: no kernel image is available for execution..,download the nighly build instead using CUDA 10.2,3070
21152,libtorch 1.8.0 with CUDA 11.1: CUDA error: no kernel image is available for execution..,download the nighly build instead using CUDA 10.2,3070
21153,The signature of `torch.tensordot` in Python should be compatible with its signature in TorchScript,`tensordot` should be added.,8742
21154,The signature of `torch.tensordot` in Python should be compatible with its signature in TorchScript,`tensordot` should be added.,8742
21155,OpInfo tests for inplace variants use the same sample inputs as the method,add a new field to SampleInput to notify if self gets broadcasted or not.,4894
21156,OpInfo tests for inplace variants use the same sample inputs as the method,add a new field to SampleInput to notify if self gets broadcasted or not.,4894
21157,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.,take 256 length inputs to the native backend.,3325
21158,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.,take 256 length inputs to the native backend.,3325
21159,Compiling release/1.5 broken on Ubuntu 18.04,"sync the submodules.
 
 ```bash
 
 git submodule sync
 
 git submodule update --init --recursive
 
 ```",8634
21160,Compiling release/1.5 broken on Ubuntu 18.04,"sync the submodules.
 
 ```bash
 
 git submodule sync
 
 git submodule update --init --recursive
 
 ```",8634
21161,Sums of expanded and repeated tensors are different,"```
 
 tensor(1.0100e+08) tensor(1.0100)
 
 tensor(1.0012e+08) tensor(1.0012)
 
 tensor(1.0100e+08) tensor(1.0100)
 
 ```",8420
21162,Sums of expanded and repeated tensors are different,"```
 
 tensor(1.0100e+08) tensor(1.0100)
 
 tensor(1.0012e+08) tensor(1.0012)
 
 tensor(1.0100e+08) tensor(1.0100)
 
 ```",8420
21163,Bad performance with python threads,enable original settings if user uses ATen/Parallel API,50
21164,Bad performance with python threads,enable original settings if user uses ATen/Parallel API,50
21165,Torch norm error on gpu tensors,`sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used,10290
21166,Torch norm error on gpu tensors,`sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used,10290
21167,torch.cuda.is_available() get false on cuda10,"pytorch version is available for download, however, it does not support cuda 10",243
21168,torch.cuda.is_available() get false on cuda10,"pytorch version is available for download, however, it does not support cuda 10",243
21169,copy.deepcopy() breaks when pruning is set on sequential,"Reparametrizing the network necessarily results in the creation of new parameters in terms of the leaf params.
 Now, I don't know exactly why in deepcopy in tensor.py we explicitly check for leaves-only, and whether that can be relaxed.
 
 Yes, workaround would be to call prune.remove before the deepcopy (but that might not be suitable if the masks or the original parameters are needed), or to copy first and then prune.",9037
21170,copy.deepcopy() breaks when pruning is set on sequential,"Reparametrizing the network necessarily results in the creation of new parameters in terms of the leaf params.
 Now, I don't know exactly why in deepcopy in tensor.py we explicitly check for leaves-only, and whether that can be relaxed.
 
 Yes, workaround would be to call prune.remove before the deepcopy (but that might not be suitable if the masks or the original parameters are needed), or to copy first and then prune.",9037
21171,torch.norm is numerically unstable at zero for multidim reductions,this issue does appear to be fixed with torch.linalg.norm.,3309
21172,torch.norm is numerically unstable at zero for multidim reductions,this issue does appear to be fixed with torch.linalg.norm.,3309
21173,Upgrade NCCL submodule,Update NCCL from 2.4.8 to 2.7.3.,8136
21174,Upgrade NCCL submodule,Update NCCL from 2.4.8 to 2.7.3.,8136
21175,Pytorch crashes while training a simple MNIST classification problem,fix the install commands,5162
21176,Pytorch crashes while training a simple MNIST classification problem,fix the install commands,5162
21177,"RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /tmp/pip-req-build-p5q91txh/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.","modify ""find_unused_parameters=True"" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)",8840
21178,"RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /tmp/pip-req-build-p5q91txh/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.","modify ""find_unused_parameters=True"" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)",8840
21179,In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported,The slope == 0.0 case is disabled for inplace backward calculation,7301
21180,In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported,The slope == 0.0 case is disabled for inplace backward calculation,7301
21181,Unexpected result on c10::ArrayRef,it's a general problem with view types in C++,8469
21182,Unexpected result on c10::ArrayRef,it's a general problem with view types in C++,8469
21183,Port torch/csrc/jit/runtime/register_prim_ops_c10.cpp to new operator registration API,New port for registrations in `register_distributed_ops.cpp`,6156
21184,Port torch/csrc/jit/runtime/register_prim_ops_c10.cpp to new operator registration API,New port for registrations in `register_distributed_ops.cpp`,6156
21185,Inconsistent behavior in model.parameters() in Pytorch 1.5.0,"`parameters()` is specified to return an iterator, and iterators are only iterable over once",3037
21186,Inconsistent behavior in model.parameters() in Pytorch 1.5.0,"`parameters()` is specified to return an iterator, and iterators are only iterable over once",3037
21187,Support custom gradient reduction algorithm in DDP,"if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`.",219
21188,Support custom gradient reduction algorithm in DDP,"if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`.",219
21189,torch.multinomial behaves abnormally with CUDA tensor,"Both with and withoutReplacement kernels don't properly advance rng state (they move it by 4, whereas each thread is likely generating much more than 4 numbers). For withoutReplacement, it happens when there are multiple distributions, but for withReplacement, all samples are generated in the single kernel, so in addition to ndistributions multiplier there's also nsamples/nwarps multiplier (nwarps = 4).",3043
21190,torch.multinomial behaves abnormally with CUDA tensor,"Both with and withoutReplacement kernels don't properly advance rng state (they move it by 4, whereas each thread is likely generating much more than 4 numbers). For withoutReplacement, it happens when there are multiple distributions, but for withReplacement, all samples are generated in the single kernel, so in addition to ndistributions multiplier there's also nsamples/nwarps multiplier (nwarps = 4).",3043
21191,torch.utils.tensorboard.writer.SummaryWriter.add_graph missing documentation,"`torch._C._log_api_usage_once(""tensorboard.logging.add_graph"")` should be below the docstring.",663
21192,torch.utils.tensorboard.writer.SummaryWriter.add_graph missing documentation,"`torch._C._log_api_usage_once(""tensorboard.logging.add_graph"")` should be below the docstring.",663
21193,torchfetch - detect hardware capabilities for PyTorch install,Already done by https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py,9539
21194,torchfetch - detect hardware capabilities for PyTorch install,Already done by https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py,9539
21195,module.h(483): error: a member with an in-class initializer must be const,CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`.,3213
21196,module.h(483): error: a member with an in-class initializer must be const,CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`.,3213
21197,CUDA out of memory in subprocesses spawned by unit tests in Windows,can be fixed by add 'poll()' after 'communicate()',7469
21198,CUDA out of memory in subprocesses spawned by unit tests in Windows,can be fixed by add 'poll()' after 'communicate()',7469
21199,"master Build failure : undefined reference to `void fbgemm::CodeGenBase<unsigned char, signed char, int, int>::storeCRegs<asmjit::x86::Zmm, 64>(asmjit::x86::Emitter*, int, int, asmjit::x86::Gp, asmjit::x86::Gp, bool)","Explicitly instantiate `storeCRegs<x86::Zmm, 64>` template",2223
21200,"master Build failure : undefined reference to `void fbgemm::CodeGenBase<unsigned char, signed char, int, int>::storeCRegs<asmjit::x86::Zmm, 64>(asmjit::x86::Emitter*, int, int, asmjit::x86::Gp, asmjit::x86::Gp, bool)","Explicitly instantiate `storeCRegs<x86::Zmm, 64>` template",2223
21201,NCCL Alltoall Process Group introducing time-out of other NCCL tests,torch_python should not be directly linked with nccl,11243
21202,NCCL Alltoall Process Group introducing time-out of other NCCL tests,torch_python should not be directly linked with nccl,11243
21203,NCCL operation fails with torch.int16 tensors,NCCL's data types don't have 16-bits integer,650
21204,NCCL operation fails with torch.int16 tensors,NCCL's data types don't have 16-bits integer,650
21205,inplace operation error when using nn.ReLU without setting inplace=True,"The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag.",260
21206,inplace operation error when using nn.ReLU without setting inplace=True,"The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag.",260
21207,torch.as_strided segfault when stride is empty tuple,It is gone in nightly version.,4027
21208,torch.as_strided segfault when stride is empty tuple,It is gone in nightly version.,4027
21209,"Caffe2 Error: more than one operator ""+"" matches these operands, windows and Cuda 11",Add CUDA 11 builds for Windows CI,4854
21210,"Caffe2 Error: more than one operator ""+"" matches these operands, windows and Cuda 11",Add CUDA 11 builds for Windows CI,4854
21211,Exception raised in backward() loses backtrace information,"Future API eats up the original error and only throws a basic std::exception with the original message.
 
 We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc).",2883
21212,Exception raised in backward() loses backtrace information,"Future API eats up the original error and only throws a basic std::exception with the original message.
 
 We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc).",2883
21213,View/Reinterpret Tensor as a different type w/o copying,Current special view_as_real / view_as_complex,5133
21214,View/Reinterpret Tensor as a different type w/o copying,Current special view_as_real / view_as_complex,5133
21215,LSTMCell and GRUCell need autocast patching,"The cudnn RNN API expects weights to occupy a flat buffer in memory with a particular layout. This PR implements a ""speed of light"" fix: _cudnn_rnn_cast_reflatten (the autocast wrapper assigned to _cudnn_rnn) copies weights to the right slices of a flat FP16 buffer with a single read/write per weight (as opposed to casting them to FP16 individually then reflattening the individual FP16 weights, which would require 2 read/writes per weight).",3190
21216,LSTMCell and GRUCell need autocast patching,"The cudnn RNN API expects weights to occupy a flat buffer in memory with a particular layout. This PR implements a ""speed of light"" fix: _cudnn_rnn_cast_reflatten (the autocast wrapper assigned to _cudnn_rnn) copies weights to the right slices of a flat FP16 buffer with a single read/write per weight (as opposed to casting them to FP16 individually then reflattening the individual FP16 weights, which would require 2 read/writes per weight).",3190
21217,pyinstaller exe generated fails to run Faster RCNN model due to runtime exception cannot find nms function,patch torchvision 0.2.2 or Keep recent torchvision & patch torch.jit in your entry point,3969
21218,pyinstaller exe generated fails to run Faster RCNN model due to runtime exception cannot find nms function,patch torchvision 0.2.2 or Keep recent torchvision & patch torch.jit in your entry point,3969
21219,"`torch.cholesky_solve` free(): invalid pointer, Aborted (core dumped)","It produces many kinds of error message with abort, and even segfault",3154
21220,"`torch.cholesky_solve` free(): invalid pointer, Aborted (core dumped)","It produces many kinds of error message with abort, and even segfault",3154
21221,Linking torch_cpu files if compiled in a path containing whitespace,"try to surround `$<TARGET_FILE:${SRC}>` with \"" here:
 pytorch/cmake/public/utils.cmake
 
 Line 55 in 40ac95d
 
 ` ${DST} INTERFACE -WHOLEARCHIVE:$<TARGET_FILE:${SRC}>) `",6760
21222,Linking torch_cpu files if compiled in a path containing whitespace,"try to surround `$<TARGET_FILE:${SRC}>` with \"" here:
 pytorch/cmake/public/utils.cmake
 
 Line 55 in 40ac95d
 
 ` ${DST} INTERFACE -WHOLEARCHIVE:$<TARGET_FILE:${SRC}>) `",6760
21223,"Failed to export an ONNX attribute 'onnx::Div', since it's not constant","In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX.",2820
21224,"Failed to export an ONNX attribute 'onnx::Div', since it's not constant","In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX.",2820
21225,PyTorch 1.5 failed to import c:\miniconda3-x64\envs\test\lib\site-packages\torch\lib\caffe2_nvrtc.dll,download additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copy them to `C:\Windows\System32`,4105
21226,PyTorch 1.5 failed to import c:\miniconda3-x64\envs\test\lib\site-packages\torch\lib\caffe2_nvrtc.dll,download additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copy them to `C:\Windows\System32`,4105
21227,Avoid keeping two copies of gradients (param.grad and buckets) in DDP,"The problem is that the view/inplace logic links the history of all these Tensors together. And so doing a .detach_() would be tricky. For example we allow it for the base even if it has views and this lead to the following weird behavior:
 import torch
 
 a = torch.rand(10, requires_grad=True).clone()
 b = a.narrow(0, 0, 2)
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 
 print(""Detaching a inplace"")
 a.detach_()
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 
 print(""Modifying b inplace"")
 b += 1
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 Giving
 
 a: True <CloneBackward object at 0x100e18350>
 b: True <SliceBackward object at 0x100e18350>
 Detaching a inplace
 a: False None
 b: True <SliceBackward object at 0x100e18350>
 Modifying b inplace
 a: True <CopySlices object at 0x100e18390>
 b: True <AsStridedBackward object at 0x100e18390>
 We do detach_ to try and keep references to the old Tensor valid yes. And it is also slightly more efficient if you don't need a new Tensor object to do this as you only set 2 fields.",3346
21228,Avoid keeping two copies of gradients (param.grad and buckets) in DDP,"The problem is that the view/inplace logic links the history of all these Tensors together. And so doing a .detach_() would be tricky. For example we allow it for the base even if it has views and this lead to the following weird behavior:
 import torch
 
 a = torch.rand(10, requires_grad=True).clone()
 b = a.narrow(0, 0, 2)
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 
 print(""Detaching a inplace"")
 a.detach_()
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 
 print(""Modifying b inplace"")
 b += 1
 
 print(""a: "", a.requires_grad, a.grad_fn)
 print(""b: "", b.requires_grad, b.grad_fn)
 Giving
 
 a: True <CloneBackward object at 0x100e18350>
 b: True <SliceBackward object at 0x100e18350>
 Detaching a inplace
 a: False None
 b: True <SliceBackward object at 0x100e18350>
 Modifying b inplace
 a: True <CopySlices object at 0x100e18390>
 b: True <AsStridedBackward object at 0x100e18390>
 We do detach_ to try and keep references to the old Tensor valid yes. And it is also slightly more efficient if you don't need a new Tensor object to do this as you only set 2 fields.",3346
21229,PyTorch C++ multithreading support on GPU,dispatch key is not correctly propagated,5027
21230,PyTorch C++ multithreading support on GPU,dispatch key is not correctly propagated,5027
21231,Segmentation fault when building LibTorch 1.5.0 on macOS Catalina,fix to SobolEngineOps.cpp,1525
21232,Segmentation fault when building LibTorch 1.5.0 on macOS Catalina,fix to SobolEngineOps.cpp,1525
21233,torch.sum() CPU is much slower for bool/uint8 tensors than int32/float32 tensors,"uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only.",8529
21234,torch.sum() CPU is much slower for bool/uint8 tensors than int32/float32 tensors,"uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only.",8529
21235,[Poll] Add Windows support to torch.distributed package,"The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore.",2866
21236,[Poll] Add Windows support to torch.distributed package,"The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore.",2866
21237,torch.isfinite() doesn't work for fp16 on CPU,addressed in the latest nighly build,10911
21238,torch.isfinite() doesn't work for fp16 on CPU,addressed in the latest nighly build,10911
21239,Nightly failed buiding on OSX with CUDA because of tuple undefined,cudnn 9.0 is no longer supported.,9139
21240,Nightly failed buiding on OSX with CUDA because of tuple undefined,cudnn 9.0 is no longer supported.,9139
21241,"Link error, Libtorch 1.5 on Windows","add `/INCLUDE:""?warp_size@cuda@at@@YAHXZ""` in Configuration Properties -> Linker -> Command Line -> Additional Options",2904
21242,"Link error, Libtorch 1.5 on Windows","add `/INCLUDE:""?warp_size@cuda@at@@YAHXZ""` in Configuration Properties -> Linker -> Command Line -> Additional Options",2904
21243,Incremental pruning reorders module hooks,The `Parametrization` framework will handle this issue correctly. Two Pruning methods will only be folded into one if they were applied on the same weight one after the other with no other Parametrizations between them.,9131
21244,Incremental pruning reorders module hooks,The `Parametrization` framework will handle this issue correctly. Two Pruning methods will only be folded into one if they were applied on the same weight one after the other with no other Parametrizations between them.,9131
21245,torch.distributions.utils.broadcast_all does not support Tensor-like objects,Fix broadcast_all crashing on Tensor-likes,2413
21246,torch.distributions.utils.broadcast_all does not support Tensor-like objects,Fix broadcast_all crashing on Tensor-likes,2413
21247,"torch.cdist produces nan gradients in Pytorch 1.5, but not Pytorch 1.4",Fix cdist backward calculation for p=2,817
21248,"torch.cdist produces nan gradients in Pytorch 1.5, but not Pytorch 1.4",Fix cdist backward calculation for p=2,817
21249,CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs,misconfigured cublas call or an internal cublas error,1349
21250,CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs,misconfigured cublas call or an internal cublas error,1349
21251,pytorch1.5 does not support CUDA,"Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False.",5721
21252,pytorch1.5 does not support CUDA,"Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False.",5721
21253,"Libtorch C++ model predict/forward propagation crashed on windows10, CUDA 10.0, VS 2017 15.7.6 ,RTX 2080, but libtorch C++ works with cpu successfully","```
 
 auto module = torch::jit::load(argv[1], torch::kCUDA);
 
 auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);
 
 auto output2 = module->forward({inputs});
 
 ```",2188
21254,"Libtorch C++ model predict/forward propagation crashed on windows10, CUDA 10.0, VS 2017 15.7.6 ,RTX 2080, but libtorch C++ works with cpu successfully","```
 
 auto module = torch::jit::load(argv[1], torch::kCUDA);
 
 auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);
 
 auto output2 = module->forward({inputs});
 
 ```",2188
21255,libtorch gpu efficiency,CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.,1505
21256,libtorch gpu efficiency,CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.,1505
21257,"[JIT] isinstance(m, nn.Linear) returns False in ScriptModules",add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right,11419
21258,"[JIT] isinstance(m, nn.Linear) returns False in ScriptModules",add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right,11419
21259,Linking torch libraries and yaml-cpp gives undefined reference to yaml-cpp libraries,"downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release).",3349
21260,Linking torch libraries and yaml-cpp gives undefined reference to yaml-cpp libraries,"downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release).",3349
21261,Provide a context manager to silence PyTorch exporter warning,"you might trip the assert when different input shapes are used, so be careful. Ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:
 
 
 
 ```python
 
 import warnings
 
 with warnings.catch_warnings():
 
  warnings.filterwarnings(
 
  action='ignore',
 
  category=torch.jit.TracerWarning,
 
  module=r'.*'
 
  )
 
  
 
  assert x.shape[0] > 2, ""checking""
 
 ```",1315
21262,Provide a context manager to silence PyTorch exporter warning,"you might trip the assert when different input shapes are used, so be careful. Ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:
 
 
 
 ```python
 
 import warnings
 
 with warnings.catch_warnings():
 
  warnings.filterwarnings(
 
  action='ignore',
 
  category=torch.jit.TracerWarning,
 
  module=r'.*'
 
  )
 
  
 
  assert x.shape[0] > 2, ""checking""
 
 ```",1315
21263,ONNX export regression: examples/fast_neural_style fails to export (pytorch_nightly),The issue here seems to be with upsample upon export.,3315
21264,ONNX export regression: examples/fast_neural_style fails to export (pytorch_nightly),The issue here seems to be with upsample upon export.,3315
21265,nn.Linear module weight initialization does not match the documentation,nn.linear module weight initialization fix,8254
21266,nn.Linear module weight initialization does not match the documentation,nn.linear module weight initialization fix,8254
21267,Sending small model to GPU leads to 2GB increase in RAM,It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a one off occurance.,4128
21268,Sending small model to GPU leads to 2GB increase in RAM,It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a one off occurance.,4128
21269,Headers use 'slots' keyword which conflict with QT code.,"text replacement, or:
 
 ```
 
 #undef slots
 
 #include ""torch/torch.h""
 
 #def slots Q_SLOTS
 
 ```",3561
21270,Headers use 'slots' keyword which conflict with QT code.,"text replacement, or:
 
 ```
 
 #undef slots
 
 #include ""torch/torch.h""
 
 #def slots Q_SLOTS
 
 ```",3561
21271,pip install does not work,get the same error if use pytorch or torch. with pip or pip3,2813
21272,pip install does not work,get the same error if use pytorch or torch. with pip or pip3,2813
21273,"Option to enable/disable autograd.profiler, move Chrome trace processing to C++",not easy to restructure the code so that you trace only one iteration.,8461
21274,"Option to enable/disable autograd.profiler, move Chrome trace processing to C++",not easy to restructure the code so that you trace only one iteration.,8461
21275,Document that autograd::Profiler::RecordFunction is available in Python,avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions,263
21276,Document that autograd::Profiler::RecordFunction is available in Python,avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions,263
21277,DistributedDataParallelTest .test_dist_broadcast_coalesced_gloo is flaky,Remove usages of TypeID,2597
21278,DistributedDataParallelTest .test_dist_broadcast_coalesced_gloo is flaky,Remove usages of TypeID,2597
21279,How install old version pytorch 0.4.1 from source?,"the nervanagpu submodule is optional. Here are steps to build:
 
 
 
 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`
 
 2. `cd pytorch-0.4.1`
 
 3. `git rm --cached third_party/nervanagpu`
 
 4. Delete these lines:
 
 
 
 https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21
 
 
 
 5. `git submodule update --init --recursive`
 
 6. `python setup.py install`",5987
21280,How install old version pytorch 0.4.1 from source?,"the nervanagpu submodule is optional. Here are steps to build:
 
 
 
 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`
 
 2. `cd pytorch-0.4.1`
 
 3. `git rm --cached third_party/nervanagpu`
 
 4. Delete these lines:
 
 
 
 https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21
 
 
 
 5. `git submodule update --init --recursive`
 
 6. `python setup.py install`",5987
21281,torch.potri example doesn't work because cholesky defaults to lower triangle and potri defaults to upper,rename `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions,6784
21282,torch.potri example doesn't work because cholesky defaults to lower triangle and potri defaults to upper,rename `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions,6784
21283,Sparse tensor creation ignores indices placement,`torch.sparse.FloatTensor',3085
21284,Sparse tensor creation ignores indices placement,`torch.sparse.FloatTensor',3085
21285,Boolean tensor transpose bug in 1.3.0,Compute correct strides after type promotion,1297
21286,Boolean tensor transpose bug in 1.3.0,Compute correct strides after type promotion,1297
21287,Inconsistent results from `mask == 0`,issue is with the `mask == 0` line,6157
21288,Inconsistent results from `mask == 0`,issue is with the `mask == 0` line,6157
21289,Scripting module with type annotations and torch.jit.ignore fails,Use real argument names for Python functions,7831
21290,Scripting module with type annotations and torch.jit.ignore fails,Use real argument names for Python functions,7831
21291,Conversion to .bfloat16() makes require_grad False,should add complex,8540
21292,Conversion to .bfloat16() makes require_grad False,should add complex,8540
21293,Unexpected behaviour of torch.BoolTensor on `not` operator,"Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory.",2884
21294,Unexpected behaviour of torch.BoolTensor on `not` operator,"Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory.",2884
21295,torch.mm seems to be broken. It doesn't enforce the dimensionality of out tensor,"The semantic of out= is definitely that it will resize if needed.
 The point of this api is to be able to reuse existing Tensors as buffers to reduce memory usage if needed.",5940
21296,torch.mm seems to be broken. It doesn't enforce the dimensionality of out tensor,"The semantic of out= is definitely that it will resize if needed.
 The point of this api is to be able to reuse existing Tensors as buffers to reduce memory usage if needed.",5940
21297,masked_scatter change elements even the the self and source tensor are of same values,"The mask operates on the `self` tensor, not on the given `source` tensor.",3288
21298,masked_scatter change elements even the the self and source tensor are of same values,"The mask operates on the `self` tensor, not on the given `source` tensor.",3288
21299,[ONNX] incorrect export of opset10 slice with dynamic_axis,"warning is at autograd level for slice indexing, not exporter directly",7826
21300,[ONNX] incorrect export of opset10 slice with dynamic_axis,"warning is at autograd level for slice indexing, not exporter directly",7826
21301,argmax for half datatype.,"``` if (iter.dtype(1) == kHalf) {
  argmax_kernel_cuda_impl<at::Half, float>(iter);```",7681
21302,argmax for half datatype.,"``` if (iter.dtype(1) == kHalf) {
  argmax_kernel_cuda_impl<at::Half, float>(iter);```",7681
21303,Python crashes after repeatedly calling `refine_names` to get named tensors,increase refcount for None after each refine_names call,10283
21304,Python crashes after repeatedly calling `refine_names` to get named tensors,increase refcount for None after each refine_names call,10283
21305,GPU Memory Leak when using lambdas in nn.Modules,when using gc.collect() and empty_cache the gpu memory is freed,6791
21306,GPU Memory Leak when using lambdas in nn.Modules,when using gc.collect() and empty_cache the gpu memory is freed,6791
21307,[type promotion] torch.exp does not work on integral types,NumPy already has this,3274
21308,[type promotion] torch.exp does not work on integral types,NumPy already has this,3274
21309,Error Exporting Quantized model to ONNX,We currently don't support ONNX for the quantized kernels.,1150
21310,Error Exporting Quantized model to ONNX,We currently don't support ONNX for the quantized kernels.,1150
21311,[JIT] `iterator expression` is expected to accept `Tensor`,iterator expression is expected to accept Tensor,3130
21312,[JIT] `iterator expression` is expected to accept `Tensor`,iterator expression is expected to accept Tensor,3130
21313,"pytorch 1.3, addition of different data typed tensors after permute giving incorrect results","fixed on master.
 ```
 >>> t1+m
 tensor([[2., 6.],
  [4., 8.]]) ```",258
21314,"pytorch 1.3, addition of different data typed tensors after permute giving incorrect results","fixed on master.
 ```
 >>> t1+m
 tensor([[2., 6.],
  [4., 8.]]) ```",258
21315,warn if user tries to build from source on windows & python 2.7,runtime warning if one runs on Windows under python 2.7,8290
21316,warn if user tries to build from source on windows & python 2.7,runtime warning if one runs on Windows under python 2.7,8290
21317,CUDA error: no kernel image is available for execution on the device Error from operator: output,Remove the quotation marks,5145
21318,CUDA error: no kernel image is available for execution on the device Error from operator: output,Remove the quotation marks,5145
21319,[pytorch] [feature request] torch.eye_like,"rather not include this function as a built-in because it's different enough from the other _like functions (which work across all shapes) to be misleading. If we were to support N-dimensional eye(s) then it would be okay, but I don't think we have any rank 2+ functions defined for which identity makes sense.",2995
21320,[pytorch] [feature request] torch.eye_like,"rather not include this function as a built-in because it's different enough from the other _like functions (which work across all shapes) to be misleading. If we were to support N-dimensional eye(s) then it would be okay, but I don't think we have any rank 2+ functions defined for which identity makes sense.",2995
21321,ModuleNotFoundError: No module named 'peachpy.x86_64.avx',Use `git submodule update --init --recursive` to update submodules.,1271
21322,ModuleNotFoundError: No module named 'peachpy.x86_64.avx',Use `git submodule update --init --recursive` to update submodules.,1271
21323,DLL load failed: The specified module could not be found. After installed using pip in python 3.5 windows.,problem might be visual studio related,2769
21324,DLL load failed: The specified module could not be found. After installed using pip in python 3.5 windows.,problem might be visual studio related,2769
21325,Got CUDNN_STATUS_NOT_INITIALIZED although PyTorch recognizes CUDA & CuDNN,driver upgrade solving the issue.,3318
21326,Got CUDNN_STATUS_NOT_INITIALIZED although PyTorch recognizes CUDA & CuDNN,driver upgrade solving the issue.,3318
21327,concat tensor,"``python
 
 a = torch.rand(3, 3)
 
 b = torch.rand(3, 6)
 
 c = torch.cat((a, b), dim=1)
 
 ```",8594
21328,concat tensor,"``python
 
 a = torch.rand(3, 3)
 
 b = torch.rand(3, 6)
 
 c = torch.cat((a, b), dim=1)
 
 ```",8594
21329,[feature request] Low-discrepancy quasi-random sampler (Sobol sequences),this can be ported into Aten,5984
21330,[feature request] Low-discrepancy quasi-random sampler (Sobol sequences),this can be ported into Aten,5984
21331,[feature request] Multivariate Gamma function,"`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise",8710
21332,[feature request] Multivariate Gamma function,"`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise",8710
21333,tensor.chunk returns wrong number of chunks,"Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method",3030
21334,tensor.chunk returns wrong number of chunks,"Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method",3030
21335,[pytorch] Intel MKL ERROR when doing torch.eig on a CUDA tensor,tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore,8988
21336,[pytorch] Intel MKL ERROR when doing torch.eig on a CUDA tensor,tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore,8988
21337,backward pass different behaviors with inplace operation,Dependant variables cause issue.,9600
21338,backward pass different behaviors with inplace operation,Dependant variables cause issue.,9600
21339,[pytorch] [feature request] Pairwise distances between all points in a set (a true pdist),"> ```
 
 > X = torch.from_numpy(np.random.normal(size=(B, N, D)))
 
 > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))
 
 > parwise_dist(X, Y) # Should be B x N x M
 
 > ```",242
21340,[pytorch] [feature request] Pairwise distances between all points in a set (a true pdist),"> ```
 
 > X = torch.from_numpy(np.random.normal(size=(B, N, D)))
 
 > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))
 
 > parwise_dist(X, Y) # Should be B x N x M
 
 > ```",242
21341,"[jit][script] support tensor.expand([-1,-1,10,-1,-1])",JIT should have better support for this,3982
21342,"[jit][script] support tensor.expand([-1,-1,10,-1,-1])",JIT should have better support for this,3982
21343,[jit] Can not pickle torch.futures.Future,Flaky test passed but error occurs.,10839
21344,[jit] Can not pickle torch.futures.Future,Flaky test passed but error occurs.,10839
21345,backward of torch.repeat slower than for torch.repeat_interleave,backward of `repeat` is inefficient,5123
21346,backward of torch.repeat slower than for torch.repeat_interleave,backward of `repeat` is inefficient,5123
21347,TorchScript sets requires_grad to True under some circumstances,no longer be on by default in the 1.7 release,7771
21348,TorchScript sets requires_grad to True under some circumstances,no longer be on by default in the 1.7 release,7771
21349,[JIT] Unable to cast Python instance to C++ type (compile in debug mode for details),"if you build Pytorch with ""DEBUG=1"" environment variable, you can use a c++ debugger to step into problematic code and get more info.",4149
21350,[JIT] Unable to cast Python instance to C++ type (compile in debug mode for details),"if you build Pytorch with ""DEBUG=1"" environment variable, you can use a c++ debugger to step into problematic code and get more info.",4149
21351,Link error in windows build,"In visual studio project settings, it's not necessary to add quota for the directory.",9045
21352,Link error in windows build,"In visual studio project settings, it's not necessary to add quota for the directory.",9045
21353,"F.mse_loss(a, b, reduction='elementwise_mean') value is incorrect and doesn't show deprecation warning when 2nd argument requires gradient",modify the MSELoss CriterionTests to verify that the target derivative is checked.,7573
21354,"F.mse_loss(a, b, reduction='elementwise_mean') value is incorrect and doesn't show deprecation warning when 2nd argument requires gradient",modify the MSELoss CriterionTests to verify that the target derivative is checked.,7573
21355,"ã€ERRORã€‘connection reset by peer, when using infiniband","set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL",3095
21356,ã€,"set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL",3095
21357,What dose _ctx in UniqueVoidPtr do?,"you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct.",3755
21358,What dose _ctx in UniqueVoidPtr do?,"you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct.",3755
21359,Add IsEmpty() to the at::Tensor in libtorch,`t.numel() == 0`,8413
21360,Add IsEmpty() to the at::Tensor in libtorch,`t.numel() == 0`,8413
21361,Neon intrinsic types issue,address the missing intrinsics,8765
21362,Neon intrinsic types issue,address the missing intrinsics,8765
21363,torch-1.6.0 contains AVX instruction in the default codepath,remove all of the inline methods from TensorIterator.,4896
21364,torch-1.6.0 contains AVX instruction in the default codepath,remove all of the inline methods from TensorIterator.,4896
21365,"Question, why Data moving to CUDA is slightly different",Because `model` is a 'container' for parameters/tensors.,1420
21366,"Question, why Data moving to CUDA is slightly different",Because `model` is a 'container' for parameters/tensors.,1420
21367,Proposal: Generic Triplet-Margin Loss,"work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way.",8062
21368,Proposal: Generic Triplet-Margin Loss,"work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way.",8062
21369,Possible return value bug when a function is registered in boxed form and called unboxed,"the ""inplace/outplace"" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the ""or more"" case.",10268
21370,Possible return value bug when a function is registered in boxed form and called unboxed,"the ""inplace/outplace"" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the ""or more"" case.",10268
21371,Query regarding support for RISC-V Vector ISA,try (cross-)compiling to RISC-V,3563
21372,Query regarding support for RISC-V Vector ISA,try (cross-)compiling to RISC-V,3563
21373,Autocompletion does not work in torch.nn module,fixed in master,3027
21374,Autocompletion does not work in torch.nn module,fixed in master,3027
21375,[Feature Request] Deformable Convolution,add deformable convolution to pytorch library,2186
21376,[Feature Request] Deformable Convolution,add deformable convolution to pytorch library,2186
21377,SSL Handshake Error when getting pretrained model,"``` import torchvision.models
 
  from torchvision.models.vgg import model_urls
 
  
 
  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')
 
  vgg16 = torchvision.models.vgg16(pretrained=True)
 
 ```",4522
21378,SSL Handshake Error when getting pretrained model,"``` import torchvision.models
 
  from torchvision.models.vgg import model_urls
 
  
 
  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')
 
  vgg16 = torchvision.models.vgg16(pretrained=True)
 
 ```",4522
21379,BCEWithLogitsLoss TypeError,No support for automatic type casting in pytorch at present.,8257
21380,BCEWithLogitsLoss TypeError,No support for automatic type casting in pytorch at present.,8257
21381,Model params change with 0 learning rate,have `weight_decay`.,2176
21382,Model params change with 0 learning rate,have `weight_decay`.,2176
21383,Import fails on 0.2 release installed with Conda,deactivate your current environment and try the import again,9566
21384,Import fails on 0.2 release installed with Conda,deactivate your current environment and try the import again,9566
21385,0.2 building from source,"while building ATen, it invokes ""python"" instead of which python was invoked in setup.py install.
 
 worth fixing via an env variable that's set in setup.py.",3238
21386,0.2 building from source,"while building ATen, it invokes ""python"" instead of which python was invoked in setup.py install.
 
 worth fixing via an env variable that's set in setup.py.",3238
21387,LSTM mask / remove zeros,Sum the time axis and divide it with length tensor.,4032
21388,LSTM mask / remove zeros,Sum the time axis and divide it with length tensor.,4032
21389,Errors on jupyter,"For ubuntu the solution is
 
 ```
 
 sudo apt-get install libtcmalloc-minimal4
 
 export LD_PRELOAD=""/usr/lib/libtcmalloc_minimal.so.4""
 
 ```",2865
21390,Errors on jupyter,"For ubuntu the solution is
 
 ```
 
 sudo apt-get install libtcmalloc-minimal4
 
 export LD_PRELOAD=""/usr/lib/libtcmalloc_minimal.so.4""
 
 ```",2865
21391,Variable' object has no attribute 'shape' [v0.2],add shape to pass-throughs,7584
21392,Variable' object has no attribute 'shape' [v0.2],add shape to pass-throughs,7584
21393,Advanced Indexing Error,"``` a[torch.LongTensor([0, 1])][:, 0]
 a[[0, 1], 0:2]
 ```",796
21394,Advanced Indexing Error,"``` a[torch.LongTensor([0, 1])][:, 0]
 a[[0, 1], 0:2]
 ```",796
21395,Unhandled CUDA Error (1),"try ""pip install""",2753
21396,Unhandled CUDA Error (1),"try ""pip install""",2753
21397,Python3 crashes when importing torch when referencing it.,"the problem occurs only if we install wheel files, and not via conda.",10660
21398,Python3 crashes when importing torch when referencing it.,"the problem occurs only if we install wheel files, and not via conda.",10660
21399,BrokenPipeError: [Errno 32] Broken pipe,set num_workers to 0 to see the actual error.,8592
21400,BrokenPipeError: [Errno 32] Broken pipe,set num_workers to 0 to see the actual error.,8592
21401,Does CosineEmbeddingLoss support CUDA tensors?,"```
 
 y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))
 
 ```",11272
21402,Does CosineEmbeddingLoss support CUDA tensors?,"```
 
 y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))
 
 ```",11272
21403,jupyter kernel died at division,Due to division by 0 due to conversion of 0.1 to 0.,7676
21404,jupyter kernel died at division,Due to division by 0 due to conversion of 0.1 to 0.,7676
21405,"I want to know how to use the select(int64_t dim, int64_t index) in at::Tensor?What is the definition of a parameter ?",Select a slice at an index of a dimension.,9747
21406,"I want to know how to use the select(int64_t dim, int64_t index) in at::Tensor?What is the definition of a parameter ?",Select a slice at an index of a dimension.,9747
21407,Wrong results when exporting a traced LSTM,No longer reproducible in `torch-nightly-1.0.0.dev20190304`.,3898
21408,Wrong results when exporting a traced LSTM,No longer reproducible in `torch-nightly-1.0.0.dev20190304`.,3898
21409,[Windows] subprocess.CalledProcessError (exit status 1) while installing pytorch by python-32,CMake / setup.py to error out if Python 32-bit version is used,207
21410,[Windows] subprocess.CalledProcessError (exit status 1) while installing pytorch by python-32,CMake / setup.py to error out if Python 32-bit version is used,207
21411,torch.arange dtype mismatch with and without jit,Fix dtype of arange in JIT and remove dtype of arange from TorchANI,10838
21412,torch.arange dtype mismatch with and without jit,Fix dtype of arange in JIT and remove dtype of arange from TorchANI,10838
21413,Can i use torch.nn.modules.pixelshuffle for commercial products?,Patent won't impede.,8236
21414,Can i use torch.nn.modules.pixelshuffle for commercial products?,Patent won't impede.,8236
21415,big difference between numpy.matmul and torch.matmul,"Expected behavior because float operations are inexact.
 
 The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot.",2885
21416,big difference between numpy.matmul and torch.matmul,"Expected behavior because float operations are inexact.
 
 The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot.",2885
21417,The function of torch.max?,Find max value in a tensor along a dimension.,268
21418,The function of torch.max?,Find max value in a tensor along a dimension.,268
21419,NNPack support for arm ( linux ),Building with NNPack and QNNPack works out of the box on both raspbian and arm64,4006
21420,NNPack support for arm ( linux ),Building with NNPack and QNNPack works out of the box on both raspbian and arm64,4006
21421,Assigning a slice of the same tensor is not consistent on GPU,"If you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard).",8717
21422,Assigning a slice of the same tensor is not consistent on GPU,"If you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard).",8717
21423,torch.max and torch.min inconsistent on cpu/gpu for tensors with zero elements.,"It should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.",3060
21424,torch.max and torch.min inconsistent on cpu/gpu for tensors with zero elements.,"It should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.",3060
21425,Device agnostic gradient reduction,"The mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel.",4344
21426,Device agnostic gradient reduction,"The mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel.",4344
21427,Default forward method for ModuleList and ModuleDict,Viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential),10851
21428,Default forward method for ModuleList and ModuleDict,Viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential),10851
21429,[CI] Is approval of reviewers reasonable while one of CI facilities is failure?,"1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming
 
 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for ""flaky"" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.
 
 3. ROCm is still a little unstable, and is often a culprit.",7397
21430,[CI] Is approval of reviewers reasonable while one of CI facilities is failure?,"1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming
 
 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for ""flaky"" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.
 
 3. ROCm is still a little unstable, and is often a culprit.",7397
21431,Binomial.log_prob returns -inf when actual probability is 1 if logit is large,Both n and p are required.,4285
21432,Binomial.log_prob returns -inf when actual probability is 1 if logit is large,Both n and p are required.,4285
21433,"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got NoneType","For add_graph, you need to pass through a sample bit of data in order to trace the graph.",6862
21434,"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got NoneType","For add_graph, you need to pass through a sample bit of data in order to trace the graph.",6862
21435,Tweak Java API before 1.3 release,Consolidate factory methods.,241
21436,Tweak Java API before 1.3 release,Consolidate factory methods.,241
21437,Python tests use torch.Doubletensor as their default tensor type,use the @dtypes decorator,7391
21438,Python tests use torch.Doubletensor as their default tensor type,use the @dtypes decorator,7391
21439,record_stream() on a shifted view tensor doesn't work,record_stream() for shifted view tensors,6829
21440,record_stream() on a shifted view tensor doesn't work,record_stream() for shifted view tensors,6829
21441,Check DataPtr's deleter to determine if it is allocated by CUDA in record_stream,Raise error if a block can not be found from a CUDA tensor.,7735
21442,Check DataPtr's deleter to determine if it is allocated by CUDA in record_stream,Raise error if a block can not be found from a CUDA tensor.,7735
21443,the list has inconsistent indentation.,Documentation now corrected.,4139
21444,the list has inconsistent indentation.,Documentation now corrected.,4139
21445,CTCLoss cuda backend computes wrong gradient when target (i.e. label) length is greater than 896 for double inputs or 1024 for float inputs,There is some looping missing when the number of threads is smaller than length,6789
21446,CTCLoss cuda backend computes wrong gradient when target (i.e. label) length is greater than 896 for double inputs or 1024 for float inputs,There is some looping missing when the number of threads is smaller than length,6789
21447,Issues with ConvReLU2d and LinearReLU doc issues,"""same as X"" becoming ""as X (same)"".",1524
21448,Issues with ConvReLU2d and LinearReLU doc issues,"""same as X"" becoming ""as X (same)"".",1524
21449,add_zero_attn in MultiheadAttention breaks causality,in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect.,3137
21450,add_zero_attn in MultiheadAttention breaks causality,in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect.,3137
21451,torch.histc segfaults if array has inf,"torch.histc added a finite range check to resolve segfaults if tensor has inf. also added checks for nan values, min>max",8418
21452,torch.histc segfaults if array has inf,"torch.histc added a finite range check to resolve segfaults if tensor has inf. also added checks for nan values, min>max",8418
21453,No module named _C,"happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`.",8618
21454,No module named _C,"happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`.",8618
21455,Jacobian-vector equation in autograd_tutorial font size is too small,"PyTorch tutorials are using MathJax library for the equation
 Change the setting of Mathjax
 Math render -> preview HTML instead of HTML-CSS",4555
21456,Jacobian-vector equation in autograd_tutorial font size is too small,"PyTorch tutorials are using MathJax library for the equation
 Change the setting of Mathjax
 Math render -> preview HTML instead of HTML-CSS",4555
21457,[JIT] builtin function attributes do not recursively compile,Doesn't work if it's a builtin function.,8318
21458,[JIT] builtin function attributes do not recursively compile,Doesn't work if it's a builtin function.,8318
21459,[ONNX] Export torch.meshgrid,Add support for meshgrid,7417
21460,[ONNX] Export torch.meshgrid,Add support for meshgrid,7417
21461,Raspberry Pi Zero W build fails,"run the command:
 git submodule update --remote third_party/protobuf",7423
21462,Raspberry Pi Zero W build fails,"run the command:
 git submodule update --remote third_party/protobuf",7423
21463,torch.split with tensor sizes fails in tracing,Issue with the tracer to handle traced Tensor inputs where int/int list is expected.,3252
21464,torch.split with tensor sizes fails in tracing,Issue with the tracer to handle traced Tensor inputs where int/int list is expected.,3252
21465,"How can I implement ""nn.unFold"" on 5D tensor?",Refer to the implementation in the old torch7 library,8639
21466,"How can I implement ""nn.unFold"" on 5D tensor?",Refer to the implementation in the old torch7 library,8639
21467,JIT profiling executor does not fuse Relu and Dropout for a GPU,Turn off profiling graph exec,3161
21468,JIT profiling executor does not fuse Relu and Dropout for a GPU,Turn off profiling graph exec,3161
21469,[quant] QuantizedCUDA,QuantizedCUDA implementation,5986
21470,[quant] QuantizedCUDA,QuantizedCUDA implementation,5986
21471,kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA,Scalar handing within ops,7674
21472,kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA,Scalar handing within ops,7674
21473,Regression on split operator benchmark after __torch_function__ merge,remove the `torch_function_dispatch` decorator from everything in `torch.functional`,4036
21474,Regression on split operator benchmark after __torch_function__ merge,remove the `torch_function_dispatch` decorator from everything in `torch.functional`,4036
21475,DISABLED test_max_pool2d (__main__.TestQuantizedOps),"Hypothesis deadline testing is now disabled altogether so we can reenable this test (if needed, on a plane and haven’t checked)",3893
21476,DISABLED test_max_pool2d (__main__.TestQuantizedOps),"Hypothesis deadline testing is now disabled altogether so we can reenable this test (if needed, on a plane and haven’t checked)",3893
21477,backward hooks on parameters don't work with distributed autograd.,Needed to mix and match RPC with DDP for combined model and data parallelism.,2372
21478,backward hooks on parameters don't work with distributed autograd.,Needed to mix and match RPC with DDP for combined model and data parallelism.,2372
21479,"error occures when trying to import torch, ""ImportError: cannot import name 'ClassType'""",`pip3 install torch==0.3.1`,3118
21480,"error occures when trying to import torch, ""ImportError: cannot import name 'ClassType'""",`pip3 install torch==0.3.1`,3118
21481,I can't build pytorch 1.3.1 from sources.,USE_NINJA=OFF.,3991
21482,I can't build pytorch 1.3.1 from sources.,USE_NINJA=OFF.,3991
21483,Windows Build fails,"See the C++ standard, [meta] / p4
 Unless otherwise specified, the behavior of a program that adds specializations for any of the templates.
 Specified in this subclause [meta] is undefined.",7690
21484,Windows Build fails,"See the C++ standard, [meta] / p4
 Unless otherwise specified, the behavior of a program that adds specializations for any of the templates.
 Specified in this subclause [meta] is undefined.",7690
21485,[jit] runtime error with backward of matmul and squeeze,Install the latest nightly and see if it reproduces in your environment,2944
21486,[jit] runtime error with backward of matmul and squeeze,Install the latest nightly and see if it reproduces in your environment,2944
21487,.sum() not return ideal result?,Try converting it to a larger type and doing the sum on that.,9746
21488,.sum() not return ideal result?,Try converting it to a larger type and doing the sum on that.,9746
21489,Compilation error: libATen.so.1: undefined reference to `convolve_5x5_sse',You probably have two versions of cudnn installed and are linking against the wrong one.,8681
21490,Compilation error: libATen.so.1: undefined reference to `convolve_5x5_sse',You probably have two versions of cudnn installed and are linking against the wrong one.,8681
21491,Runtime error(s) for Conv2d second gradient,Always define outputs of ConvBackwardBackward,9555
21492,Runtime error(s) for Conv2d second gradient,Always define outputs of ConvBackwardBackward,9555
21493,layer-by-layer profiling feature,Python package for layer by layer profiling that extends the autograd profiler.,3707
21494,layer-by-layer profiling feature,Python package for layer by layer profiling that extends the autograd profiler.,3707
21495,additional continuous builds,Windows CI has been enabled.,8419
21496,additional continuous builds,Windows CI has been enabled.,8419
21497,add torch.stft and torch.fft,"For proper fft, I think we should use existing wheels, e.g., cuFFT and MKL/Eigen.",8474
21498,add torch.stft and torch.fft,"For proper fft, I think we should use existing wheels, e.g., cuFFT and MKL/Eigen.",8474
21499,saved_tensors attribute for grad_fn,Export the model to ONNX and then use the visualization tool they provide to inspect the graph.,3236
21500,saved_tensors attribute for grad_fn,Export the model to ONNX and then use the visualization tool they provide to inspect the graph.,3236
21501,tensor.max(other) not documented,Document it or disable it.,4227
21502,tensor.max(other) not documented,Document it or disable it.,4227
21503,broadcasting behaves differently on CPU and GPU,Manually try to broadcast before calling the fused kernels.,6761
21504,broadcasting behaves differently on CPU and GPU,Manually try to broadcast before calling the fused kernels.,6761
21505,Problem when I load a state dictionary,"```
 
 import argparse
 
 import torch
 
 
 
 parser = argparse.ArgumentParser()
 
 parser.add_argument(""--source"", type=str, required=True)
 
 parser.add_argument(""--dest"", type=str, required=True)
 
 
 
 args = parser.parse_args()
 
 
 
 model_state = torch.load(args.source)
 
 new_model_state = {}
 
 
 
 for key in model_state.keys():
 
  new_model_state[key[7:]] = model_state[key]
 
 
 
 torch.save(new_model_state, args.dest)
 
 ```",5731
21506,Problem when I load a state dictionary,"```
 
 import argparse
 
 import torch
 
 
 
 parser = argparse.ArgumentParser()
 
 parser.add_argument(""--source"", type=str, required=True)
 
 parser.add_argument(""--dest"", type=str, required=True)
 
 
 
 args = parser.parse_args()
 
 
 
 model_state = torch.load(args.source)
 
 new_model_state = {}
 
 
 
 for key in model_state.keys():
 
  new_model_state[key[7:]] = model_state[key]
 
 
 
 torch.save(new_model_state, args.dest)
 
 ```",5731
21507,Commit breaks CUDA9 builds [NativeFunctions: support backend-specific dispatch],"```diff
 
 diff --git i/setup.py w/setup.py
 
 index e484692f..75e1836e 100644
 
 --- i/setup.py
 
 +++ w/setup.py
 
 @@ -90,7 +90,7 @@ import distutils.sysconfig
 
  cfg_vars = distutils.sysconfig.get_config_vars()
 
  for key, value in cfg_vars.items():
 
  if type(value) == str:
 
 - cfg_vars[key] = value.replace(""-Wstrict-prototypes"", """")
 
 + cfg_vars[key] = value.replace(""-Wstrict-prototypes"", """").replace(""-fno-plt"", """")
 
 
 
  ################################################################################
 
  # Custom build commands
 
 ```",1266
21508,Commit breaks CUDA9 builds [NativeFunctions: support backend-specific dispatch],"```diff
 
 diff --git i/setup.py w/setup.py
 
 index e484692f..75e1836e 100644
 
 --- i/setup.py
 
 +++ w/setup.py
 
 @@ -90,7 +90,7 @@ import distutils.sysconfig
 
  cfg_vars = distutils.sysconfig.get_config_vars()
 
  for key, value in cfg_vars.items():
 
  if type(value) == str:
 
 - cfg_vars[key] = value.replace(""-Wstrict-prototypes"", """")
 
 + cfg_vars[key] = value.replace(""-Wstrict-prototypes"", """").replace(""-fno-plt"", """")
 
 
 
  ################################################################################
 
  # Custom build commands
 
 ```",1266
21509,Calling float() in modules converts integer buffers or parameters to floating point,"Make integer types immune to .float(), .half() and .double() calls.",10906
21510,Calling float() in modules converts integer buffers or parameters to floating point,"Make integer types immune to .float(), .half() and .double() calls.",10906
21511,Scheduler.step() doesn't perform optimizer.step(),"Scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.",7480
21512,Scheduler.step() doesn't perform optimizer.step(),"Scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.",7480
21513,Building from source on master is broken,"```
 
 git clone --recursive https://github.com/pytorch/pytorch.git
 
 cd pytorch
 
 git pull origin pull/3831/head:build_fix
 
 git checkout build_fix
 
 export CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../"" # [anaconda root directory]
 
 python setup.py install | tee build.log # Writes the build logs to build.log
 
 
 
 ```",11450
21514,Building from source on master is broken,"```
 
 git clone --recursive https://github.com/pytorch/pytorch.git
 
 cd pytorch
 
 git pull origin pull/3831/head:build_fix
 
 git checkout build_fix
 
 export CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../"" # [anaconda root directory]
 
 python setup.py install | tee build.log # Writes the build logs to build.log
 
 
 
 ```",11450
21515,python3--no module named PIL,Uninstall and reinstall PIL,3317
21516,python3--no module named PIL,Uninstall and reinstall PIL,3317
21517,torch.tensor does not support list of complex tensor,Tensor objects do not have `__complex__`,3249
21518,torch.tensor does not support list of complex tensor,Tensor objects do not have `__complex__`,3249
21519,"segfault together with ""import cv2""",glibc bug triggered by some libs in opencv.,7818
21520,"segfault together with ""import cv2""",glibc bug triggered by some libs in opencv.,7818
21521,MultivariateNormal backprop performance issue related to broadcasting,Broadcasting tensors together and then slicing them after is a bad idea,3976
21522,MultivariateNormal backprop performance issue related to broadcasting,Broadcasting tensors together and then slicing them after is a bad idea,3976
21523,CrossEntropyLoss does not raise target out of bounds error on gpu,"Fixed in 1.5.1, 1.6 and master.",2467
21524,CrossEntropyLoss does not raise target out of bounds error on gpu,"Fixed in 1.5.1, 1.6 and master.",2467
21525,Missing `IndexError` when accessing elements outside of shape of CUDA tensor,"If you call `zz[0, 2]` it'll correctly report the dimension.",2982
21526,Missing `IndexError` when accessing elements outside of shape of CUDA tensor,"If you call `zz[0, 2]` it'll correctly report the dimension.",2982
21527,torch.cat is over 300x slower than torch.index_copy / numpy.concatenate on CPU,"Each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less.",475
21528,torch.cat is over 300x slower than torch.index_copy / numpy.concatenate on CPU,"Each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less.",475
21529,"C++ torch::min_values, torch::max_values no longer found in current libtorch",Latest version has torch::amin and torch::amax.,3778
21530,"C++ torch::min_values, torch::max_values no longer found in current libtorch",Latest version has torch::amin and torch::amax.,3778
21531,Documentation mention parameters `verbose` for torch.optim.lr_scheduler but it does not exist,Verbose param was added recently to the schedulers and it should be there in the nightly release and hence the docs are not synced properly with the source.,8129
21532,Documentation mention parameters `verbose` for torch.optim.lr_scheduler but it does not exist,Verbose param was added recently to the schedulers and it should be there in the nightly release and hence the docs are not synced properly with the source.,8129
21533,disable instruction set when build libtorch,Build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF.,4980
21534,disable instruction set when build libtorch,Build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF.,4980
21535,Integer overflow when doing 1x1 convolution on very large tensor,Integer overflow when doing 1x1 convolution on very large tensor.,1598
21536,Integer overflow when doing 1x1 convolution on very large tensor,Integer overflow when doing 1x1 convolution on very large tensor.,1598
21537,fx: unable to symbolically trace simple nn.Sequential model,Error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch).,7303
21538,fx: unable to symbolically trace simple nn.Sequential model,Error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch).,7303
21539,"Xcode 12 Build Error: LibTorch/install/lib/libpytorch_qnnpack.a , building for iOS, but linking in object file built for macOS","1. upload a bug fix version to cocoapods - 1.6.1
 
 2. Build the static libs from source. Make sure you're using the latest CMake (3.18).",7159
21540,"Xcode 12 Build Error: LibTorch/install/lib/libpytorch_qnnpack.a , building for iOS, but linking in object file built for macOS","1. upload a bug fix version to cocoapods - 1.6.1
 
 2. Build the static libs from source. Make sure you're using the latest CMake (3.18).",7159
21541,Installation with non-root access,Try `python setup.py install --user`.,2759
21542,Installation with non-root access,Try `python setup.py install --user`.,2759
21543,Add sparse softmax/log_softmax functionality (ignore zero entries),Sparse softmax support (CUDA),5151
21544,Add sparse softmax/log_softmax functionality (ignore zero entries),Sparse softmax support (CUDA),5151
21545,"The expanded size of the tensor (13) must match the existing size (12) at non-singleton dimension 1. Target sizes: [3, 13]. Tensor sizes: [12]",It's very likely a problem with a downstream library.,4866
21546,"The expanded size of the tensor (13) must match the existing size (12) at non-singleton dimension 1. Target sizes: [3, 13]. Tensor sizes: [12]",It's very likely a problem with a downstream library.,4866
21547,Back propagation trough slicing with list breaks,You are backproping twice through the x->y graph.,3066
21548,Back propagation trough slicing with list breaks,You are backproping twice through the x->y graph.,3066
21549,Official and nightly wheel structure plan,Rename the `torch_nightly` wheels to `torch`.,8043
21550,Official and nightly wheel structure plan,Rename the `torch_nightly` wheels to `torch`.,8043
21551,Mention future package dependency for python 2.7 in documentation,Fix the generic install documentation as requirements.txt includes future.,7810
21552,Mention future package dependency for python 2.7 in documentation,Fix the generic install documentation as requirements.txt includes future.,7810
21553,Versions of TorchScript,"We've made some breaking serialization changes between versions (those will be detailed in the 1.2 release notes soon). We try to maintain backwards compatibility between versions, but we don't make any guarantees about forward compatibility at this point.",4313
21554,Versions of TorchScript,"We've made some breaking serialization changes between versions (those will be detailed in the 1.2 release notes soon). We try to maintain backwards compatibility between versions, but we don't make any guarantees about forward compatibility at this point.",4313
21555,Conv1D output changes,Set `bias=False.`,8548
21556,Conv1D output changes,Set `bias=False.`,8548
21557,Documentation for Tensor.record_stream(),Tensor.record_stream can simplify this sort of use-case.,6620
21558,Documentation for Tensor.record_stream(),Tensor.record_stream can simplify this sort of use-case.,6620
21559,Jetson TX2 crashes when running simple nn on GPU using libtorch,Update the jetson and try with the newer version.,2990
21560,Jetson TX2 crashes when running simple nn on GPU using libtorch,Update the jetson and try with the newer version.,2990
21561,Dedicated channel for PyTorch nightlies; no more munging package names,Move nightlies to the new channel for Windows.,2981
21562,Dedicated channel for PyTorch nightlies; no more munging package names,Move nightlies to the new channel for Windows.,2981
21563,Torch.Quantization: functions for determining supported modules/functionals/methods for quantized tensors,`dir(torch.nn.quantized)`,11422
21564,Torch.Quantization: functions for determining supported modules/functionals/methods for quantized tensors,`dir(torch.nn.quantized)`,11422
21565,Add Type Promotion for Bool types,Currently one should explicitly cast to float() before doing mean().,3998
21566,Add Type Promotion for Bool types,Currently one should explicitly cast to float() before doing mean().,3998
21567,conda installs CPU only version of pytorch-nightly during new package uploads (CPU and win64 are uploaded first),The cpu-only package was renamed to cpuonly.,7197
21568,conda installs CPU only version of pytorch-nightly during new package uploads (CPU and win64 are uploaded first),The cpu-only package was renamed to cpuonly.,7197
21569,Drop Python 2 support,We're on track to drop support in 2020.,8317
21570,Drop Python 2 support,We're on track to drop support in 2020.,8317
21571,Difference in the implementation of rmsprop with Tensorflow,Variant of RMSProp that tries to stay true to the TF version.,5922
21572,Difference in the implementation of rmsprop with Tensorflow,Variant of RMSProp that tries to stay true to the TF version.,5922
21573,Weird result for inplace operation for a tensor with itself,This is expected behavior and will not be fixed.,9226
21574,Weird result for inplace operation for a tensor with itself,This is expected behavior and will not be fixed.,9226
21575,pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store,"In the Windows Registry, within HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem you have to set LongPathsEnabled to 1 and reboot.",4616
21576,pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store,"In the Windows Registry, within HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem you have to set LongPathsEnabled to 1 and reboot.",4616
21577,[MKLDNN] Corrupted malloc metadata in mkldnn_convolution_backward_input,"This problem has disappeared after using jemalloc by runing:
 `export LD_PRELOAD=jemalloc/lib/libjemalloc.so`
 So this problem is the alloctor's problem used by mkldnn.",8301
21578,[MKLDNN] Corrupted malloc metadata in mkldnn_convolution_backward_input,"This problem has disappeared after using jemalloc by runing:
 `export LD_PRELOAD=jemalloc/lib/libjemalloc.so`
 So this problem is the alloctor's problem used by mkldnn.",8301
21579,[JIT] Support mixed int/float math in python,"Add ops between float & int, and change list equality output to be a boolean.",1597
21580,[JIT] Support mixed int/float math in python,"Add ops between float & int, and change list equality output to be a boolean.",1597
21581,[JIT] Don't support varargs in script,"We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).
 
 
 
 I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code.",8406
21582,[JIT] Don't support varargs in script,"We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).
 
 
 
 I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code.",8406
21583,[JIT] Don't support pass tuple in a PythonOp,This is supported now for as long as the Python Op contains annotations.,7682
21584,[JIT] Don't support pass tuple in a PythonOp,This is supported now for as long as the Python Op contains annotations.,7682
21585,"[CAffe2] Issue compiling with Cuda 9.2, YellowFinOp error",Compile with gcc 6 instead of 7. Some of the syntax isn't compatible with gcc 7.,3199
21586,"[CAffe2] Issue compiling with Cuda 9.2, YellowFinOp error",Compile with gcc 6 instead of 7. Some of the syntax isn't compatible with gcc 7.,3199
21587,Fatal error: THC/THC.h: no such file or directory,Try `export CUDA_NVCC_EXECUTABLE=$(which nvcc)`,3221
21588,Fatal error: THC/THC.h: no such file or directory,Try `export CUDA_NVCC_EXECUTABLE=$(which nvcc)`,3221
21589,Problem with terminating dataloader workers,It is an expected behavior in pytorch <= 0.4 and not a bug.,6807
21590,Problem with terminating dataloader workers,It is an expected behavior in pytorch <= 0.4 and not a bug.,6807
21591,[JIT] Don't support torch.tensor/Tensor/FloatTensor in Script,torch.tensor() is already supported in JIT now.,3010
21592,[JIT] Don't support torch.tensor/Tensor/FloatTensor in Script,torch.tensor() is already supported in JIT now.,3010
21593,[bug report] ConvTranspose is not padding output with zero,It is used to resolve the ambiguity when one have `stride > 1`.,3912
21594,[bug report] ConvTranspose is not padding output with zero,It is used to resolve the ambiguity when one have `stride > 1`.,3912
21595,Argmax performance slower than numpy,Benchmark that by breaking it into two lines and by using a cpp benchmarking.,8600
21596,Argmax performance slower than numpy,Benchmark that by breaking it into two lines and by using a cpp benchmarking.,8600
21597,Pytorch 0.4.0: Model behavior changes heavily after save and load weights,"Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs.",2905
21598,Pytorch 0.4.0: Model behavior changes heavily after save and load weights,"Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs.",2905
21599,output_padding constraint,"When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition.",3195
21600,output_padding constraint,"When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition.",3195
21601,Build CUDA extension in windows 10,C++ extensions are well supported on Windows.,1496
21602,Build CUDA extension in windows 10,C++ extensions are well supported on Windows.,1496
21603,Runtime Error thrown when using Optimizer in a Pytorch Function: element 0 of tensors does not require grad and does not have a grad_fn,Wrap the call of the function (in the autograd.Function forward) and return something more reasonable than `0`.,3228
21604,Runtime Error thrown when using Optimizer in a Pytorch Function: element 0 of tensors does not require grad and does not have a grad_fn,Wrap the call of the function (in the autograd.Function forward) and return something more reasonable than `0`.,3228
21605,torch.cuda.sparse.FloatTensor is not enabled.,"It was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround.",4858
21606,torch.cuda.sparse.FloatTensor is not enabled.,"It was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround.",4858
21607,NaN loss when using half precision,Change eps=1e-4,7840
21608,NaN loss when using half precision,Change eps=1e-4,7840
21609,import torch: DLL load failed: The operating system cannot run %1. (pip installation),Replace the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl`,5160
21610,import torch: DLL load failed: The operating system cannot run %1. (pip installation),Replace the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl`,5160
21611,[print] improve ModuleList print format,"Shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead.",8471
21612,[print] improve ModuleList print format,"Shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead.",8471
21613,[complex] torch.pow : Incorrect output,"Modify x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, If not, modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic.",11460
21614,[complex] torch.pow : Incorrect output,"Modify x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, If not, modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic.",11460
21615,The RandomSampler is not be set as expected.,Use `generator` rather than `self.generator`.,2578
21616,The RandomSampler is not be set as expected.,Use `generator` rather than `self.generator`.,2578
21617,[JIT] nn.Sequential of nn.Module with input type List[torch.Tensor] inferred to torch.Tensor,Subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors.,7066
21618,[JIT] nn.Sequential of nn.Module with input type List[torch.Tensor] inferred to torch.Tensor,Subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors.,7066
21619,Why isn't torch._aminmax() present in the docs?,"Functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API.",3689
21620,Why isn't torch._aminmax() present in the docs?,"Functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API.",3689
21621,"Replace deprecated AT_ERROR with `TORCH_CHECK(false,` in `c10`","AT_ERROR should be replaced with TORCH_CHECK(false, ...) for user-facing errors, but TORCH_INTERNAL_ASSERT(...) for errors caused or encountered by the system that are independent of the user.",3301
21622,"Replace deprecated AT_ERROR with `TORCH_CHECK(false,` in `c10`","AT_ERROR should be replaced with TORCH_CHECK(false, ...) for user-facing errors, but TORCH_INTERNAL_ASSERT(...) for errors caused or encountered by the system that are independent of the user.",3301
21623,can't run dictionary with string key as input to jit model in libtorch,"Missing the double underscores on `__init___` and there is incorrect spacing. Missing the underscores means defining a custom method called `init`, so Python generates a default implementation of `__init__` .",2819
21624,can't run dictionary with string key as input to jit model in libtorch,"Missing the double underscores on `__init___` and there is incorrect spacing. Missing the underscores means defining a custom method called `init`, so Python generates a default implementation of `__init__` .",2819
21625,conda installation from nighlty causes package conflicts and fails to install PyTorch,"Official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0.",3113
21626,conda installation from nighlty causes package conflicts and fails to install PyTorch,"Official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0.",3113
21627,torch.norm behavior for negative p when reducing over empty dimension,A reduction should have a proper mathematical and not just a numerical identity.,3337
21628,torch.norm behavior for negative p when reducing over empty dimension,A reduction should have a proper mathematical and not just a numerical identity.,3337
21629,Incorrect type annotation for DataLoader,"`mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files.",5996
21630,Incorrect type annotation for DataLoader,"`mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files.",5996
21631,"Import Torch Error. TypeError: function() argument 1 must be code, not str",tqdm dependency is optional so you can uninstall the package to workaround the issue.,3758
21632,"Import Torch Error. TypeError: function() argument 1 must be code, not str",tqdm dependency is optional so you can uninstall the package to workaround the issue.,3758
21633,Illegal memory access in cuda max pooling for large inputs,Multiplication in int64_t is good.,2191
21634,Illegal memory access in cuda max pooling for large inputs,Multiplication in int64_t is good.,2191
21635,setup.py sdist does not include third party submodules,That portion should be easy to solve though with a `MANIFEST.in`.,192
21636,setup.py sdist does not include third party submodules,That portion should be easy to solve though with a `MANIFEST.in`.,192
21637,Make `aten::adaptive_avg_pool3d` work for quantized Tensor inputs,Quantized adaptive_avg_pool3d,4623
21638,Make `aten::adaptive_avg_pool3d` work for quantized Tensor inputs,Quantized adaptive_avg_pool3d,4623
21639,[dist_autograd] GPU continuations does not work in distributed autograd,Add basic GPU support to distributed autograd.,3958
21640,[dist_autograd] GPU continuations does not work in distributed autograd,Add basic GPU support to distributed autograd.,3958
21641,[JIT] Support `with torch.autograd.profiler.record_function`,TorchScript Context Manager Support.,5974
21642,[JIT] Support `with torch.autograd.profiler.record_function`,TorchScript Context Manager Support.,5974
21643,RuntimeError: Only Tensors of floating point dtype can require gradients,Never use `.data`.,2886
21644,RuntimeError: Only Tensors of floating point dtype can require gradients,Never use `.data`.,2886
21645,"Py pip installation error,zipfile.BadZipFile: Bad CRC-32 for file 'torch/lib/cudnn64_7.dll",Try the `no-cache` option.,11457
21646,"Py pip installation error,zipfile.BadZipFile: Bad CRC-32 for file 'torch/lib/cudnn64_7.dll",Try the `no-cache` option.,11457
21647,RuntimeError: _th_exp_out not supported on CUDAType for Long,BCEWithLogitsLoss needs both preds and labels to be floating point,1395
21648,RuntimeError: _th_exp_out not supported on CUDAType for Long,BCEWithLogitsLoss needs both preds and labels to be floating point,1395
21649,scatter_ throwing a RunTimeError,Allow index/src to have one dimension more than `self.dim` if their sizes are 1,3018
21650,scatter_ throwing a RunTimeError,Allow index/src to have one dimension more than `self.dim` if their sizes are 1,3018
21651,Exponential distribution invalid parameter,Set `validate_args` argument to True.,2166
21652,Exponential distribution invalid parameter,Set `validate_args` argument to True.,2166
21653,"""log_softmax_lastdim_kernel_impl"" not implemented for 'Long'",CrossEntropyLoss needs floating point inputs and long labels.,2043
21654,"""log_softmax_lastdim_kernel_impl"" not implemented for 'Long'",CrossEntropyLoss needs floating point inputs and long labels.,2043
21655,mypy doesn't recognize torch functions that start with `_`,Add __all__ to torch/_C/_VariableFunctions.pyi.,9465
21656,mypy doesn't recognize torch functions that start with `_`,Add __all__ to torch/_C/_VariableFunctions.pyi.,9465
21657,typing is missing for most optimizers,1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue.,6890
21658,typing is missing for most optimizers,1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue.,6890
21659,[v1.6.0] Release Tracker,Change to CUDAStream.h so that we can build 1.6 with cuda11,5777
21660,[v1.6.0] Release Tracker,Change to CUDAStream.h so that we can build 1.6 with cuda11,5777
21661,libcurand.so.8.0: cannot open shared object file,Install latest PyTorch version.,11455
21662,libcurand.so.8.0: cannot open shared object file,Install latest PyTorch version.,11455
21663,AttributeError: 'Conv2d' object has no attribute 'padding_mode' when loading model from pytorch 1.0 to 1.1,"Comment out the lines that use self.padding_mode in module.py, the model can then be imported, and saving the state_dictionary instead of the whole model allows loading into an unmodified version of pytorch 1.1.",7811
21664,AttributeError: 'Conv2d' object has no attribute 'padding_mode' when loading model from pytorch 1.0 to 1.1,"Comment out the lines that use self.padding_mode in module.py, the model can then be imported, and saving the state_dictionary instead of the whole model allows loading into an unmodified version of pytorch 1.1.",7811
21665,torch.hub does not close the resource before removing,"Add the ""cached_zipfile.close()"" call after the cached_zipfile.extractall(hub_dir) function in the _get_cache_or_reload function.",3761
21666,torch.hub does not close the resource before removing,"Add the ""cached_zipfile.close()"" call after the cached_zipfile.extractall(hub_dir) function in the _get_cache_or_reload function.",3761
21667,grid_sample is not aligned,"Convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.
 This would allow `F.grid_sample` to be agnostic to the size of the sampled image.",8542
21668,grid_sample is not aligned,"Convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.
 This would allow `F.grid_sample` to be agnostic to the size of the sampled image.",8542
21669,libtorch terminate called after throwing an instance of 'c10::Error',"```C++
 
 torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();
 
 ```
 
 
 
 The variable `outputs` now contains your bounding boxes and your confidence tensors.",7683
21670,libtorch terminate called after throwing an instance of 'c10::Error',"```C++
 
 torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();
 
 ```
 
 
 
 The variable `outputs` now contains your bounding boxes and your confidence tensors.",7683
21671,"pytorch1.1.0 windows than one operator "" "" matches these operands","Change `setup.py` to the following:
 
 ```python
 
 #!/usr/bin/env python3
 
 import os
 
 import torch
 
 
 
 from setuptools import setup, find_packages
 
 from torch.utils.cpp_extension import BuildExtension, CUDAExtension
 
 
 
 cxx_args = ['-std=c++11']
 
 
 
 nvcc_args = [
 
 '-gencode', 'arch=compute_50,code=sm_50',
 
 '-gencode', 'arch=compute_52,code=sm_52',
 
 '-gencode', 'arch=compute_60,code=sm_60',
 
 '-gencode', 'arch=compute_61,code=sm_61',
 
 '-gencode', 'arch=compute_70,code=sm_70',
 
 '-gencode', 'arch=compute_70,code=compute_70',
 
 '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line
 
 ]
 setup(
 
 name='correlation_cuda',
 
 ext_modules=[
 
 CUDAExtension('correlation_cuda', [
 
 'correlation_cuda.cc',
 
 'correlation_cuda_kernel.cu'
 
 ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})
 
 ],
 
 cmdclass={
 
 'build_ext': BuildExtension
 
 })
 
 ```",4327
21672,"pytorch1.1.0 windows than one operator "" "" matches these operands","Change `setup.py` to the following:
 
 ```python
 
 #!/usr/bin/env python3
 
 import os
 
 import torch
 
 
 
 from setuptools import setup, find_packages
 
 from torch.utils.cpp_extension import BuildExtension, CUDAExtension
 
 
 
 cxx_args = ['-std=c++11']
 
 
 
 nvcc_args = [
 
 '-gencode', 'arch=compute_50,code=sm_50',
 
 '-gencode', 'arch=compute_52,code=sm_52',
 
 '-gencode', 'arch=compute_60,code=sm_60',
 
 '-gencode', 'arch=compute_61,code=sm_61',
 
 '-gencode', 'arch=compute_70,code=sm_70',
 
 '-gencode', 'arch=compute_70,code=compute_70',
 
 '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line
 
 ]
 setup(
 
 name='correlation_cuda',
 
 ext_modules=[
 
 CUDAExtension('correlation_cuda', [
 
 'correlation_cuda.cc',
 
 'correlation_cuda_kernel.cu'
 
 ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})
 
 ],
 
 cmdclass={
 
 'build_ext': BuildExtension
 
 })
 
 ```",4327
21673,expected ) but found 'ident' here: quantized::fake_quantize_per_tensor_affine_forward,"Problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python.",2557
21674,expected ) but found 'ident' here: quantized::fake_quantize_per_tensor_affine_forward,"Problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python.",2557
21675,Stabilize gradient for unfolded Tensor,"If you do finite differences, the gradient check wont match I think, except if you change the behavior during forward to divide the pixels of the output image by the number of overlapping regions.",8551
21676,Stabilize gradient for unfolded Tensor,"If you do finite differences, the gradient check wont match I think, except if you change the behavior during forward to divide the pixels of the output image by the number of overlapping regions.",8551
21677,10% difference noticed between jit and python model,Remove parts of the transformer network while preserving the numerical discrepancy.,3551
21678,10% difference noticed between jit and python model,Remove parts of the transformer network while preserving the numerical discrepancy.,3551
21679,torch.Size is not pickleable in Python 2,torch dtype object is already serializable.,11443
21680,torch.Size is not pickleable in Python 2,torch dtype object is already serializable.,11443
21681,NervanaSystems/nervanagpu Repository not found for 0.4.0,"```bash
 
 git submodule deinit third_party/nervanagpu
 
 git rm --cached third_party/nervanagpu
 
 ```",3188
21682,NervanaSystems/nervanagpu Repository not found for 0.4.0,"```bash
 
 git submodule deinit third_party/nervanagpu
 
 git rm --cached third_party/nervanagpu
 
 ```",3188
21683,Different behavior numpy / pytorch with broadcasting & in-place operators,Solution to this problem is by solving a diophantine equation like numpy does.,7596
21684,Different behavior numpy / pytorch with broadcasting & in-place operators,Solution to this problem is by solving a diophantine equation like numpy does.,7596
21685,cublas runtime error on torch.bmm() with CUDA10 and RTX2080Ti,cuda 10 version of pytorch needs to be installed to run on RTX.,8642
21686,cublas runtime error on torch.bmm() with CUDA10 and RTX2080Ti,cuda 10 version of pytorch needs to be installed to run on RTX.,8642
21687,BatchNorm2d implementation returns different results than expected,Need to be in eval mode.,11445
21688,BatchNorm2d implementation returns different results than expected,Need to be in eval mode.,11445
21689,A bug for torch.clone() when clone a MKLDNN tensor,Fix the bug for torch.clone() when clone a MKLDNN tensor,3141
21690,A bug for torch.clone() when clone a MKLDNN tensor,Fix the bug for torch.clone() when clone a MKLDNN tensor,3141
21691,"torch.empty(n, dtype=torch.int) produces non-deterministic arrays, not empty ones","`torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`",6901
21692,"torch.empty(n, dtype=torch.int) produces non-deterministic arrays, not empty ones","`torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`",6901
21693,[jit] Changes to TorchScript API,Inheriting from `ScriptModule` should not be allowed.,8289
21694,[jit] Changes to TorchScript API,Inheriting from `ScriptModule` should not be allowed.,8289
21695,Tensor identity comparisons not working / unclear,Tensor.data returns an alias like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing.,6325
21696,Tensor identity comparisons not working / unclear,Tensor.data returns an alias like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing.,6325
21697,assignment to a twice-sliced tensor does nothing,Because assignment will call __newindex__ on the same tensor,8907
21698,assignment to a twice-sliced tensor does nothing,Because assignment will call __newindex__ on the same tensor,8907
21699,Windows 10/Python 3.5 Pytorch 1.1.0 CPU increase versus 1.0.0 mnist,"Due to a bug, version 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores.",8951
21700,Windows 10/Python 3.5 Pytorch 1.1.0 CPU increase versus 1.0.0 mnist,"Due to a bug, version 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores.",8951
21701,JIT-compiled function produces incorrect results,"The problem is that there is a dependency from `sqrt` to `+` in function `f`, due to which we need to add `SyncThreads` between those statements.",6869
21702,JIT-compiled function produces incorrect results,"The problem is that there is a dependency from `sqrt` to `+` in function `f`, due to which we need to add `SyncThreads` between those statements.",6869
21703,Model output difference between android device and desktop,"On `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels.",9023
21704,Model output difference between android device and desktop,"On `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels.",9023
21705,[complex] torch.sigmoid: sigmoid does not support automatic differentiation for outputs with complex dtype,SciPy's expit does not support complex input.,6782
21706,[complex] torch.sigmoid: sigmoid does not support automatic differentiation for outputs with complex dtype,SciPy's expit does not support complex input.,6782
21707,NCCL backend doesn't use MASTER_PORT during reconnect,Restart everything and reinitialize a new process group or use torchelastic.,3759
21708,NCCL backend doesn't use MASTER_PORT during reconnect,Restart everything and reinitialize a new process group or use torchelastic.,3759
21709,PyTorch cannot be linked (libtorch_cuda.so) at the last step of compilation with CUDA 11.1 with PyTorch/builder,"`export TORCH_CUDA_ARCH_LIST=""8.6""`",8013
21710,PyTorch cannot be linked (libtorch_cuda.so) at the last step of compilation with CUDA 11.1 with PyTorch/builder,"`export TORCH_CUDA_ARCH_LIST=""8.6""`",8013
21711,[FR] add huber option for smooth_l1_loss,Add a new `huber` arg (default `False`) to the existing `SmoothL1Loss`.,3322
21712,[FR] add huber option for smooth_l1_loss,Add a new `huber` arg (default `False`) to the existing `SmoothL1Loss`.,3322
21713,torch.nn.Module.named_parameters has wrong type annotation,add type annotations to torch.nn.modules.module,3008
21714,torch.nn.Module.named_parameters has wrong type annotation,add type annotations to torch.nn.modules.module,3008
21715,Torchscript does not work with type: ignore comments for mypy,"The bug is due to an assumption in TorchScript when parsing source code: In function declaration, any line that contains `# type:` is considered to be a type line. Then TorchScript lexer expects the type line to contain only type comment, nothing else.",7235
21716,Torchscript does not work with type: ignore comments for mypy,"The bug is due to an assumption in TorchScript when parsing source code: In function declaration, any line that contains `# type:` is considered to be a type line. Then TorchScript lexer expects the type line to contain only type comment, nothing else.",7235
21717,Enhanced generators with grad-mode decorators,Support for enhanced generators (generator-based coroutines) by grad-mode decorators.,6794
21718,Enhanced generators with grad-mode decorators,Support for enhanced generators (generator-based coroutines) by grad-mode decorators.,6794
21719,__torch_function__ PR may cause performance regression on GPU training,Skip the `if not torch.jit.is_scripting():` guards on functional and nn.functional by directly registering `has_torch_function` and `object_has_torch_function` to the JIT as statically False.,203
21720,__torch_function__ PR may cause performance regression on GPU training,Skip the `if not torch.jit.is_scripting():` guards on functional and nn.functional by directly registering `has_torch_function` and `object_has_torch_function` to the JIT as statically False.,203
21721,[online docs] anchors to source code are missing,"In 1.6.0 the `id=""Module.eval""` exists.",6868
21722,[online docs] anchors to source code are missing,"In 1.6.0 the `id=""Module.eval""` exists.",6868
21723,[docs] torch.nn.functional.one_hot docs missing,"Find the documentation here:
 
 https://pytorch.org/docs/stable/nn.functional.html",4037
21724,[docs] torch.nn.functional.one_hot docs missing,"Find the documentation here:
 
 https://pytorch.org/docs/stable/nn.functional.html",4037
21725,Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty),The `use_count` of function `shared_ptr` are incorrect on C++ side as we create multiple `shared_ptr` from raw pointers when replicate modules.,264
21726,Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty),The `use_count` of function `shared_ptr` are incorrect on C++ side as we create multiple `shared_ptr` from raw pointers when replicate modules.,264
21727,"Apply for translation of the Chinese version, I hope to get authorization!",Iterated three Chinese versions.,8239
21728,"Apply for translation of the Chinese version, I hope to get authorization!",Iterated three Chinese versions.,8239
21729,C++ link error,Build the lib from source.,3704
21730,C++ link error,Build the lib from source.,3704
21731,"cannot initialize type ""_CudaDeviceProperties"" error","`_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`.",4557
21732,"cannot initialize type ""_CudaDeviceProperties"" error","`_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`.",4557
21733,High scope of error in how view() may be used,`view()` is the same as `reshape`.,8751
21734,High scope of error in how view() may be used,`view()` is the same as `reshape`.,8751
21735,Error on backward pass with sparse matmul with transposed tensor,Don't attempt to multiply by a sparse matrix.,3215
21736,Error on backward pass with sparse matmul with transposed tensor,Don't attempt to multiply by a sparse matrix.,3215
21737,Why is `torch.mean()` so different from `numpy.average()`?,`pytorch_math` package that bridges the difference gap.,3046
21738,Why is `torch.mean()` so different from `numpy.average()`?,`pytorch_math` package that bridges the difference gap.,3046
21739,Make it possible to use mypy to typecheck code that uses PyTorch,"Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` .",250
21740,Make it possible to use mypy to typecheck code that uses PyTorch,"Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` .",250
21741,Weird behavior of torch.int(),Number of precision for printing tensors by default is 4.,10281
21742,Weird behavior of torch.int(),Number of precision for printing tensors by default is 4.,10281
21743,[JIT] Sometimes rewrite argument name,Preserve method parameter names.,1117
21744,[JIT] Sometimes rewrite argument name,Preserve method parameter names.,1117
21745,torch::serialization.load() doesn't support pathlib.Path object for the input argument,Load success from the any kind of '*.pth.tar' or some other extensions.,5804
21746,torch::serialization.load() doesn't support pathlib.Path object for the input argument,Load success from the any kind of '*.pth.tar' or some other extensions.,5804
21747,test_proper_exit is flaky,"""module: build"" means build system problems; it doesn't refer to ci flakiness.",5079
21748,test_proper_exit is flaky,"""module: build"" means build system problems; it doesn't refer to ci flakiness.",5079
21749,Potential instability in ConvTranspose2d with cudnn,Use `cudnn.benchmark=True` .,8870
21750,Potential instability in ConvTranspose2d with cudnn,Use `cudnn.benchmark=True` .,8870
21751,Allow C++ inference of JITted modules to run different modules on different streams,Add support of `with torch.cuda.stream(...):` in TorchScript.,6871
21752,Allow C++ inference of JITted modules to run different modules on different streams,Add support of `with torch.cuda.stream(...):` in TorchScript.,6871
21753,torch.to_dense() adds random values,Random indices tensor can generate multiple entries with value `1` but same index.,8682
21754,torch.to_dense() adds random values,Random indices tensor can generate multiple entries with value `1` but same index.,8682
21755,Installing Windows 10,Have the 64-bit installation of Python.,2804
21756,Installing Windows 10,Have the 64-bit installation of Python.,2804
21757,About a conditional expression of cutoffs in torch.nn.AdaptiveLogSoftmaxWithLoss.,Fix AdaptiveLogSoftmaxWithLoss's constructor,3253
21758,About a conditional expression of cutoffs in torch.nn.AdaptiveLogSoftmaxWithLoss.,Fix AdaptiveLogSoftmaxWithLoss's constructor,3253
21759,[JIT] Additional list methods in script,"Clear, Pop, Reverse, Copy, Extend, Insert, Remove, Index, Count are added. Sort is yet to be added.",2045
21760,[JIT] Additional list methods in script,"Clear, Pop, Reverse, Copy, Extend, Insert, Remove, Index, Count are added. Sort is yet to be added.",2045
21761,JIT Script module support for pad_packed_sequence and pack_padded_sequence,Support added for pack_padded_sequence and pad_packed_sequence.,9757
21762,JIT Script module support for pad_packed_sequence and pack_padded_sequence,Support added for pack_padded_sequence and pad_packed_sequence.,9757
21763,[Feature] Sparse tensor persistence/save.,Implement pickle support for sparse tensors and torch.layout instances.,9572
21764,[Feature] Sparse tensor persistence/save.,Implement pickle support for sparse tensors and torch.layout instances.,9572
21765,[jit] Autodiff crash when some chunk outputs aren't used in backward,Change the chunk autodiff formula to handle this case where some of the gradients are undefined.,10837
21766,[jit] Autodiff crash when some chunk outputs aren't used in backward,Change the chunk autodiff formula to handle this case where some of the gradients are undefined.,10837
21767,[Feature] 'None' for arbitrarily shaped buffer/parameter when loading models.,"Write a `load_model(model, state_dict)` that works for your model.",11365
21768,[Feature] 'None' for arbitrarily shaped buffer/parameter when loading models.,"Write a `load_model(model, state_dict)` that works for your model.",11365
21769,[python setup.py install] g++ error: stub.o file format not recognized,Run `python setup.py clean` and then temporarily rename Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation or remove `-B` option from the installation stage.,1609
21770,[python setup.py install] g++ error: stub.o file format not recognized,Run `python setup.py clean` and then temporarily rename Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation or remove `-B` option from the installation stage.,1609
21771,"Incorrect size for __rpow__(scalar, float)",Fix issue with scalars and __rpow__ .,3352
21772,"Incorrect size for __rpow__(scalar, float)",Fix issue with scalars and __rpow__ .,3352
21773,[Feature Request] LuaRocks style package manager for community made packages,Use conda.,8276
21774,[Feature Request] LuaRocks style package manager for community made packages,Use conda.,8276
21775,"Inconsistency: can instantiate Tensor from List, but cannot instantiate Variable",`Variable`s can only be built from an existing `Tensor`.,8714
21776,"Inconsistency: can instantiate Tensor from List, but cannot instantiate Variable",`Variable`s can only be built from an existing `Tensor`.,8714
21777,Reductions returning scalars cause implicit sync-point,"Introduce a scalar type into PyTorch, autograd compatible. With this, the sync-points issue will be solved.",4890
21778,Reductions returning scalars cause implicit sync-point,"Introduce a scalar type into PyTorch, autograd compatible. With this, the sync-points issue will be solved.",4890
21779,[Feature request] Allow exceptions in load_state_dict,"```python
 
 new_params = model.state_dict()
 
 new_params.update(updated_params)
 
 model.load_state_dict(new_params)
 
 ```",2964
21780,[Feature request] Allow exceptions in load_state_dict,"```python
 
 new_params = model.state_dict()
 
 new_params.update(updated_params)
 
 model.load_state_dict(new_params)
 
 ```",2964
21781,Broadcasting doesn't match docs for conda package,Broadcasting will drop in the next release.,9608
21782,Broadcasting doesn't match docs for conda package,Broadcasting will drop in the next release.,9608
21783,torch.save is saving too much,"You can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage.",7639
21784,torch.save is saving too much,"You can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage.",7639
21785,gpu version of pytorch not working on docker image,Issue not found with latest version of docker and pytorch.,3035
21786,gpu version of pytorch not working on docker image,Issue not found with latest version of docker and pytorch.,3035
21787,Documentation for how pytorch tensor.view() rearranges dimensions?,"`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.",8750
21788,Documentation for how pytorch tensor.view() rearranges dimensions?,"`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.",8750
21789,type error when calling 'adagrad' optimization method,Interchange the order of model and optimizer definition.,3307
21790,type error when calling 'adagrad' optimization method,Interchange the order of model and optimizer definition.,3307
21791,cublas runtime error when both bmm's arguments have been expanded,Fix baddbmm for expanded tensors.,10835
21792,cublas runtime error when both bmm's arguments have been expanded,Fix baddbmm for expanded tensors.,10835
21793,[Feature Request] Add NoisyLinear layer,Noisy linear layers are fairly generic layers and could see use in problems outside of RL.,7451
21794,[Feature Request] Add NoisyLinear layer,Noisy linear layers are fairly generic layers and could see use in problems outside of RL.,7451
21795,"MacOSX, CUDA, OS call failed or operation not supported on this OS",Upgrade to CUDA 10.12.,4220
21796,"MacOSX, CUDA, OS call failed or operation not supported on this OS",Upgrade to CUDA 10.12.,4220
21797,[Feature request] unique operation,Automatic update of fbcode/onnx.,2754
21798,[Feature request] unique operation,Automatic update of fbcode/onnx.,2754
21799,undefined symbol: THLongStorage_inferSizeN,It is ok to uninstall `libtorch` from conda.,11294
21800,undefined symbol: THLongStorage_inferSizeN,It is ok to uninstall `libtorch` from conda.,11294
21801,shared cuda tensor consumes GPU memory in every process,"Change the size of the tensor to orders of (1000,1000,200).",3180
21802,shared cuda tensor consumes GPU memory in every process,"Change the size of the tensor to orders of (1000,1000,200).",3180
21803,An error occurred when installing pytorch from source,Set environment variable `TMPDIR` to some other directory.,8735
21804,An error occurred when installing pytorch from source,Set environment variable `TMPDIR` to some other directory.,8735
21805,"Completely deterministic network, but result is different from the past and different on different machines.",Some of our kernels are not deterministic.,7645
21806,"Completely deterministic network, but result is different from the past and different on different machines.",Some of our kernels are not deterministic.,7645
21807,Padding size should be less than the corresponding input dimension,A warning to let the user decide if they still want to use this padding mode for images.,249
21808,Padding size should be less than the corresponding input dimension,A warning to let the user decide if they still want to use this padding mode for images.,249
21809,training with Multi-GPU hangs,Close ACS of the PLX PCI-e switch.,6872
21810,training with Multi-GPU hangs,Close ACS of the PLX PCI-e switch.,6872
21811,logdet evals to -inf in an overly pessimistic way,Improve numerical precision of (s)logdet.,9825
21812,logdet evals to -inf in an overly pessimistic way,Improve numerical precision of (s)logdet.,9825
21813,ImportError: cannot import name '_update_worker_pids',Install torch 1.0.1 and torchvision 0.2.2.,10418
21814,ImportError: cannot import name '_update_worker_pids',Install torch 1.0.1 and torchvision 0.2.2.,10418
21815,Wrong BatchNorm2d momentum value in ONNX export,Fix momentum setting in BatchNorm forward pass.,3025
21816,Wrong BatchNorm2d momentum value in ONNX export,Fix momentum setting in BatchNorm forward pass.,3025
21817,Error trying to convert to JitScript,I just ran your gist on master and did not get an error.,4670
21818,Error trying to convert to JitScript,I just ran your gist on master and did not get an error.,4670
21819,[docs] torch.(a)range dtype doc is wrong,Fixed range.,9547
21820,[docs] torch.(a)range dtype doc is wrong,Fixed range.,9547
21821,torch.svd seems to have accuracy problem,"No need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy.",8625
21822,torch.svd seems to have accuracy problem,"No need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy.",8625
21823,grid_sample cuDNN error,https://github.com/pytorch/pytorch/issues/18561#issuecomment-477906432,524
21824,grid_sample cuDNN error,https://github.com/pytorch/pytorch/issues/18561#issuecomment-477906432,524
21825,Checkpointing evaluates irrelevant tasks,Documentation and warning about it would be a better choice.,7769
21826,Checkpointing evaluates irrelevant tasks,Documentation and warning about it would be a better choice.,7769
21827,[FR] warn/error when MAGMA is built with a different CUDA,Raise an error when using magma built against wrong version of cuda.,5131
21828,[FR] warn/error when MAGMA is built with a different CUDA,Raise an error when using magma built against wrong version of cuda.,5131
21829,"c++ error:""std"":ambiguous symbol it is in visual studio 2017.",change the `conformance mode` property to **`No`**.,3211
21830,"c++ error:""std"":ambiguous symbol it is in visual studio 2017.",change the `conformance mode` property to **`No`**.,3211
21831,Fail in repeated evaluation of 2nd derivative of a custom autograd function,Invert ownership between PyFunction and THPFunction.,2460
21832,Fail in repeated evaluation of 2nd derivative of a custom autograd function,Invert ownership between PyFunction and THPFunction.,2460
21833,torch.matmul not working in cuda 9.0,Cuda 9 doesn't support NVidia RTX cards.,9137
21834,torch.matmul not working in cuda 9.0,Cuda 9 doesn't support NVidia RTX cards.,9137
21835,torch.matrix_exp() doc is missing a signature,Invert ownership between PyFunction and THPFunction.,2817
21836,torch.matrix_exp() doc is missing a signature,Invert ownership between PyFunction and THPFunction.,2817
21837,torch.linalg.eigh fails on gradcheck with complex Hermitian matrix,"The sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors.",3278
21838,torch.linalg.eigh fails on gradcheck with complex Hermitian matrix,"The sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors.",3278
21839,Request for a test code snippet of Distributed Data Parallel.,Here is the API document - https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html,10901
21840,Request for a test code snippet of Distributed Data Parallel.,Here is the API document - https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html,10901
21841,Preserve PyObject even when it is dead from Python side,PyObject reference Tensor has to be made non-owning.,8513
21842,Preserve PyObject even when it is dead from Python side,PyObject reference Tensor has to be made non-owning.,8513
21843,PYTORCH_TESTING_DEVICE_ONLY_FOR environment variable don't play well with development environment.,"If the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run.",1267
21844,PYTORCH_TESTING_DEVICE_ONLY_FOR environment variable don't play well with development environment.,"If the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run.",1267
21845,Error in optim/adamw.py,`pytorch-1.8.1` doesn't have this fix.,3042
21846,Error in optim/adamw.py,`pytorch-1.8.1` doesn't have this fix.,3042
21847,_ConvNd weight initialization does not match docs,Docstring is correct in this case.,7037
21848,_ConvNd weight initialization does not match docs,Docstring is correct in this case.,7037
21849,[ROCm] Tests fail on my system.,Problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening.,8698
21850,[ROCm] Tests fail on my system.,Problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening.,8698
21851,JIT: torch.jit.Future annotations cannot be spread over multiple lines,Add support for refinement for torch.jit.Future,7668
21852,JIT: torch.jit.Future annotations cannot be spread over multiple lines,Add support for refinement for torch.jit.Future,7668
21853,"""Add annotations"" workflow fails when base branch is pinned to old commit",Using a general strategy of expecting malformed input will prevent these sorts of failures from happening when future changes to these workflows are made.,253
21854,"""Add annotations"" workflow fails when base branch is pinned to old commit",Using a general strategy of expecting malformed input will prevent these sorts of failures from happening when future changes to these workflows are made.,253
21855,"""Build"" step in CircleCI outputs a ton of pthreadpool warnings",It is only happening with certain flags.,3995
21856,"""Build"" step in CircleCI outputs a ton of pthreadpool warnings",It is only happening with certain flags.,3995
21857,Unable to build PyTorch without Numpy installed (again),`BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`.,6895
21858,Unable to build PyTorch without Numpy installed (again),`BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`.,6895
21859,[bug] run test local,Allow tests to run locally without setting environment variables.,4590
21860,[bug] run test local,Allow tests to run locally without setting environment variables.,4590
21861,[doc] is_grad_enabled is not documented,`torch.is_grad_enabled` isn't searchable.,2227
21862,[doc] is_grad_enabled is not documented,`torch.is_grad_enabled` isn't searchable.,2227
21863,Print Bug of Tensor Grad After Backward,"Multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards.",8057
21864,Print Bug of Tensor Grad After Backward,"Multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards.",8057
21865,Add Kaiming initialization functions to C++ API,It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same.,2803
21866,Add Kaiming initialization functions to C++ API,It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same.,2803
21867,torch.nn.Conv2d will try to allocate gpu memory far more than it needs.,Cudnn automatically select the fastest possible algorithm. Set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel.,8313
21868,torch.nn.Conv2d will try to allocate gpu memory far more than it needs.,Cudnn automatically select the fastest possible algorithm. Set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel.,8313
21869,Mean and max slow,`mean` goes through recently added reduction kernels in TensorIterator and is fast. `max` uses older kernels in THC.,8730
21870,Mean and max slow,`mean` goes through recently added reduction kernels in TensorIterator and is fast. `max` uses older kernels in THC.,8730
21871,Build libtorch with -D_GLIBCXX_USE_CXX11_ABI=1,Change to a lower OpenCV version.,3850
21872,Build libtorch with -D_GLIBCXX_USE_CXX11_ABI=1,Change to a lower OpenCV version.,3850
21873,load_lua() does not know how to deserialize Lua class nn.PixelShuffle.,"Use ""https://github.com/bshillingford/python-torchfile"" .",9584
21874,load_lua() does not know how to deserialize Lua class nn.PixelShuffle.,"Use ""https://github.com/bshillingford/python-torchfile"" .",9584
21875,Inconsistent behavior for log_prob when values are outside of support,Set `validate_args` to `True` for the distributions individually or by setting a global flag.,2828
21876,Inconsistent behavior for log_prob when values are outside of support,Set `validate_args` to `True` for the distributions individually or by setting a global flag.,2828
21877,Deepcopy doesn't copy to same gpu,Add a `.to(device=desired_device)` after deepcopy.,3177
21878,Deepcopy doesn't copy to same gpu,Add a `.to(device=desired_device)` after deepcopy.,3177
21879,test_distributed - failed gloo test_all_reduce_multigpu and test_barrier_group_cuda on 8-GPU machines,"Likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it.",6970
21880,test_distributed - failed gloo test_all_reduce_multigpu and test_barrier_group_cuda on 8-GPU machines,"Likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it.",6970
21881,libtorch c++API: terminate called after throwing an instance of 'c10::Error',Use pytorch-dev11.28 to trace the model.,196
21882,libtorch c++API: terminate called after throwing an instance of 'c10::Error',Use pytorch-dev11.28 to trace the model.,196
21883,[libtorch] build failed with opencv-4.0.0 using cmake (CPU and GPU),Export model.pt and build libtorch from the same version of Pytorch.,4903
21884,[libtorch] build failed with opencv-4.0.0 using cmake (CPU and GPU),Export model.pt and build libtorch from the same version of Pytorch.,4903
21885,Large documentation pages take a long time to load,Fixed in the doc restructure in gh-37419.,7835
21886,Large documentation pages take a long time to load,Fixed in the doc restructure in gh-37419.,7835
21887,Comply with XDG (X Design Group) Base Directory Specification,Comply with XDG_CACHE_HOME and keep TORCH_HOME .,1585
21888,Comply with XDG (X Design Group) Base Directory Specification,Comply with XDG_CACHE_HOME and keep TORCH_HOME .,1585
21889,tensor() and Tensor() return different values for a list of bools,Use `torch.tensor` and not `torch.Tensor` .,6786
21890,tensor() and Tensor() return different values for a list of bools,Use `torch.tensor` and not `torch.Tensor` .,6786
21891,Which torch version is this?,Export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly.,8027
21892,Which torch version is this?,Export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly.,8027
21893,Windows nightly build failed (CUDA + DEBUG + LIBTORCH),Calling benchmark.Timer with default num_threads=1 disables parallelism.,4322
21894,Windows nightly build failed (CUDA + DEBUG + LIBTORCH),Calling benchmark.Timer with default num_threads=1 disables parallelism.,4322
21895,Libtorch : Comparing identical shapes(sizes) always returns false when in debug mode,A Visual Studio 2019 issue.,3044
21896,Libtorch : Comparing identical shapes(sizes) always returns false when in debug mode,A Visual Studio 2019 issue.,3044
21897,Difference behavior between FrozenBatchNorm2d and BatchNorm2d,They have different eps.,10883
21898,Difference behavior between FrozenBatchNorm2d and BatchNorm2d,They have different eps.,10883
21899,JIT changes method kwarg argument names,Fix argument name capturing in tracing.,3185
21900,JIT changes method kwarg argument names,Fix argument name capturing in tracing.,3185
21901,Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py,Allow torch.hub.load() to load models from any local directory with a hubconf.py.,7829
21902,Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py,Allow torch.hub.load() to load models from any local directory with a hubconf.py.,7829
21903,[jit] determine whether ModuleList is empty or its length,"```
 
 model_jit = torch.jit.script(Test())
 
 print(model_jit.forward.code)
 
 
 
 def forward(self,
 
  x: Tensor) -> Tensor:
 
  if torch.gt((self.layers).__len__(), 0):
 
  x1 = (getattr(self.layers, ""0"")).forward(x, )
 
  x0 = x1
 
  else:
 
  x0 = x
 
  return x0
 
 ```",2910
21904,[jit] determine whether ModuleList is empty or its length,"```
 
 model_jit = torch.jit.script(Test())
 
 print(model_jit.forward.code)
 
 
 
 def forward(self,
 
  x: Tensor) -> Tensor:
 
  if torch.gt((self.layers).__len__(), 0):
 
  x1 = (getattr(self.layers, ""0"")).forward(x, )
 
  x0 = x1
 
  else:
 
  x0 = x
 
  return x0
 
 ```",2910
21905,Make torch.nn.Hardswish ONNX exportable,"```
 
 class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()
 
  @staticmethod
 
  def forward(x):
 
  # return x * F.hardsigmoid(x) # for torchscript and CoreML
 
  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX
 
 ```",8581
21906,Make torch.nn.Hardswish ONNX exportable,"```
 
 class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()
 
  @staticmethod
 
  def forward(x):
 
  # return x * F.hardsigmoid(x) # for torchscript and CoreML
 
  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX
 
 ```",8581
21907,torch\include\ATen/core/ivalue_inl.h... cl.exe' failed with exit status 2,VS2019 is not supported with cuda 10.,691
21908,torch\include\ATen/core/ivalue_inl.h... cl.exe' failed with exit status 2,VS2019 is not supported with cuda 10.,691
21909,torch.empty_like supports memory_format=torch.preserve_format,Change the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`,5805
21910,torch.empty_like supports memory_format=torch.preserve_format,Change the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`,5805
21911,embedding_bag running on CPU produces wrong result when weight tensor is non-contiguous.,"Let ModuleTest raise when they fail on non-contiguous inputs. Fix legacy modules.
 Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.
 Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling .contiguous() on in both forward and backward,
 a. prefix all current embedding_bag* functions with _, indicating that they require input to be contiguous (there is a check in each function).
 b. create embedding_bag, which makes input arguments .contiguous(), and calls _embedding_bag
 Make many ATen embedding* functions to work on non-contiguous inputs so we don't need to call input = input.contiguous() in Python nn.functional.embedding.
 Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with sparse=True. Added tests.
 Update TensorUtils.cpp to use AT_* macros.",4212
21912,embedding_bag running on CPU produces wrong result when weight tensor is non-contiguous.,"Let ModuleTest raise when they fail on non-contiguous inputs. Fix legacy modules.
 Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.
 Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling .contiguous() on in both forward and backward,
 a. prefix all current embedding_bag* functions with _, indicating that they require input to be contiguous (there is a check in each function).
 b. create embedding_bag, which makes input arguments .contiguous(), and calls _embedding_bag
 Make many ATen embedding* functions to work on non-contiguous inputs so we don't need to call input = input.contiguous() in Python nn.functional.embedding.
 Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with sparse=True. Added tests.
 Update TensorUtils.cpp to use AT_* macros.",4212
21913,torch.jit bug torch.cat() not working properly,"There's been a lot of changes in fusers/executors since 1.6, so it probably is no longer an issue.",221
21914,torch.jit bug torch.cat() not working properly,"There's been a lot of changes in fusers/executors since 1.6, so it probably is no longer an issue.",221
21915,Make non-literal indexing for ModuleList / ModuleDict work with ModuleInterface type in TorchScript,Iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient.,7671
21916,Make non-literal indexing for ModuleList / ModuleDict work with ModuleInterface type in TorchScript,Iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient.,7671
21917,nightly.py error,Nightly robustness fixes for linking across devices,8759
21918,nightly.py error,Nightly robustness fixes for linking across devices,8759
21919,[ERROR] PyTorch dependency on Windows Python Package not working,Use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`.,8323
21920,[ERROR] PyTorch dependency on Windows Python Package not working,Use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`.,8323
21921,"RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name)) when loading model weights",Upgrade to 1.6.,2867
21922,"RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name)) when loading model weights",Upgrade to 1.6.,2867
21923,Fix exception chaining all over the codebase,"``` try: 
  print(d[key]) # raises KeyError
 except KeyError as e:
  raise RuntimeError(""no member called {}"".format(key)) from e
 ```",6799
21924,Fix exception chaining all over the codebase,"``` try: 
  print(d[key]) # raises KeyError
 except KeyError as e:
  raise RuntimeError(""no member called {}"".format(key)) from e
 ```",6799
21925,"PyTorch's div, which performs true division, cannot be exported to ONNX with consistent semantics",Throw a runtime error if a user tries to use div to perform integer division in 1.6,6154
21926,"PyTorch's div, which performs true division, cannot be exported to ONNX with consistent semantics",Throw a runtime error if a user tries to use div to perform integer division in 1.6,6154
21927,Segmentation fault in DataLoader worker in PyTorch 1.8.0 if set_num_threads is called beforehand,Register pthread_atfork that would leak pthread pool.,3164
21928,Segmentation fault in DataLoader worker in PyTorch 1.8.0 if set_num_threads is called beforehand,Register pthread_atfork that would leak pthread pool.,3164
21929,RRef.to_here() does not synchronize CUDA Streams properly,"The ctx in the following code is an instance of CudaLazyStreamContext when CUDA is enabled. Its streams_
 field holds the devices and current streams where this request will be run on. The proposed solution is to
 extract the devices vector from the ctx and pass that to cb_->operator()(requestMessage); invocation, and
 then propogate it to the created RRef accordingly.",8458
21930,RRef.to_here() does not synchronize CUDA Streams properly,"The ctx in the following code is an instance of CudaLazyStreamContext when CUDA is enabled. Its streams_
 field holds the devices and current streams where this request will be run on. The proposed solution is to
 extract the devices vector from the ctx and pass that to cb_->operator()(requestMessage); invocation, and
 then propogate it to the created RRef accordingly.",8458
21931,Single-matrix cholesky much slower than batch mode with batch_size=1?,Have an `ldl` operator in pytorch.,6849
21932,Single-matrix cholesky much slower than batch mode with batch_size=1?,Have an `ldl` operator in pytorch.,6849
21933,Irrelevant named tensor warnings,"Fixed the problems with max_pool{1, 2, 3}d.",8309
21934,Irrelevant named tensor warnings,"Fixed the problems with max_pool{1, 2, 3}d.",8309
21935,"Currently, LibTorch cannot be downloaded",Update libtorch links for CUDA 10.2 .,1956
21936,"Currently, LibTorch cannot be downloaded",Update libtorch links for CUDA 10.2 .,1956
21937,The latest code cannot build due to missing file CPUFuntion.h,"Pulled the latest code, and compiled successfully.",8494
21938,The latest code cannot build due to missing file CPUFuntion.h,"Pulled the latest code, and compiled successfully.",8494
21939,Improve torch.linalg documentation,Some Unicode symbols (for example â€œâ‚™â€ (U+2099)) do not render properly on Mac with the system default font.,7937
21940,skip_if_not_multigpu decorator skipping/passing tests,The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it.,90
21941,skip_if_not_multigpu decorator skipping/passing tests,The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it.,90
21942,torch.ceil wrong formula,`ceil(x) != floor(x) + 1` when x is already an integer.,9035
21943,torch.ceil wrong formula,`ceil(x) != floor(x) + 1` when x is already an integer.,9035
21944,Doc of SVD is erroneous and inconsistent,Improve the docs of all `torch.linalg`.,7808
21945,Doc of SVD is erroneous and inconsistent,Improve the docs of all `torch.linalg`.,7808
21946,Maybe one more '=' sign?,Update previous-versions.md.,6821
21947,Maybe one more '=' sign?,Update previous-versions.md.,6821
21948,CUDA error: device-side assert triggered(torch1.8.1+cuda11.1),Try the `-fno-gnu-unique` option.,3265
21949,CUDA error: device-side assert triggered(torch1.8.1+cuda11.1),Try the `-fno-gnu-unique` option.,3265
21950,AccessDeniedAccess when download libtorch 1.8.1 for Linux for CUDA 10.2 from download.pytorch.org,Unable to download libtorch (1.8.1) from pytorch.org.,2224
21951,AccessDeniedAccess when download libtorch 1.8.1 for Linux for CUDA 10.2 from download.pytorch.org,Unable to download libtorch (1.8.1) from pytorch.org.,2224
21952,The results of pipeline parallelism cannot be reproduced when using different numbers of partitions,Integrate Sentence Embedding training and fine-tuning in DVC pipeline.,3350
21953,The results of pipeline parallelism cannot be reproduced when using different numbers of partitions,Integrate Sentence Embedding training and fine-tuning in DVC pipeline.,3350
21954,torch.jit.trace is not working and causing program stop working,Check for wrong pytorch version.,10836
21955,torch.jit.trace is not working and causing program stop working,Check for wrong pytorch version.,10836
21956,"DDP checkpointing tests, test failures",Enable static graph training in DDP.,10926
21957,"DDP checkpointing tests, test failures",Enable static graph training in DDP.,10926
21958,F.embedding has unexpected behavior with non-2d weights,F.embedding() is not intended to support non-2D weight.,6834
21959,F.embedding has unexpected behavior with non-2d weights,F.embedding() is not intended to support non-2D weight.,6834
21960,[torch.jit.script] Python type cannot be used as a value:,`nn.Module` initialization isn't supported in TorchScript.,7628
21961,[torch.jit.script] Python type cannot be used as a value:,`nn.Module` initialization isn't supported in TorchScript.,7628
21962,[libtorch] tensor.to(torch::Device(torch::kCPU)) is VERY SLOW,Measurement for the forward propagation is not synchronizing the CUDA device.,2907
21963,[libtorch] tensor.to(torch::Device(torch::kCPU)) is VERY SLOW,Measurement for the forward propagation is not synchronizing the CUDA device.,2907
21964,Improve torch.distributed.new_group() documentation in the context of SyncBatchNorm,"This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group.",199
21965,Improve torch.distributed.new_group() documentation in the context of SyncBatchNorm,"This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group.",199
21966,Enable linear algebra functions on ROCm platform,"Code changes so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h.",245
21967,Enable linear algebra functions on ROCm platform,"Code changes so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h.",245
21968,"the result of torch.addcmul(input, tensor1, tensor2, value) != the result of inputâ€‹+value*tensor1â€‹*tensor2",Difference is well below the order of 1e-5 and is expected for fp32 arithmetics.,7030
21969,"the result of torch.addcmul(input, tensor1, tensor2, value) != the result of inputâ€‹+value*tensor1â€‹*tensor2",Difference is well below the order of 1e-5 and is expected for fp32 arithmetics.,7030
21970,torch.multinomial selects elements with zero weight,"Problem where ""0"" was generated at the same position as a non-zero probability, effectively masking it.",8417
21971,torch.multinomial selects elements with zero weight,"Problem where ""0"" was generated at the same position as a non-zero probability, effectively masking it.",8417
21972,Support autograd in `torch.svd` with complex inputs,Add autograd tests for complex matrix norm nuclear and +/-2 .,9057
21973,Support autograd in `torch.svd` with complex inputs,Add autograd tests for complex matrix norm nuclear and +/-2 .,9057
21974,Problem with building from source in win7,Do an explicit type casting there.,2365
21975,Problem with building from source in win7,Do an explicit type casting there.,2365
21976,aarch64 CMake checks are incorrect on Apple Silicon,"No updates will be submitted to QNNPACK as it is an archive repository. None of the PyTorch operators depend on third_party/QNNPACK, only some legacy caffe2 ops.",9033
21977,aarch64 CMake checks are incorrect on Apple Silicon,"No updates will be submitted to QNNPACK as it is an archive repository. None of the PyTorch operators depend on third_party/QNNPACK, only some legacy caffe2 ops.",9033
21978,Test self.assertEqual with msg does not print numerical comparison results,It's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests.,42
21979,Test self.assertEqual with msg does not print numerical comparison results,It's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests.,42
21980,Compile error in Windows SDK,Reorganize and refine the Windows.h import in C++ files.,10515
21981,Compile error in Windows SDK,Reorganize and refine the Windows.h import in C++ files.,10515
21982,Transcendental functions broken on Jetson Xavier NX,"If PyTorch for AARCH64 is compiled by clang, problem goes away.",6783
21983,Transcendental functions broken on Jetson Xavier NX,"If PyTorch for AARCH64 is compiled by clang, problem goes away.",6783
21984,pytorch 1.4 can not load model saved by 1.7,`torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4.,5026
21985,pytorch 1.4 can not load model saved by 1.7,`torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4.,5026
21986,Could we wrap some pytorch operations into a large symbolic Op and export it to onnx ?,Can export a PyTorch op as a custom op to ONNX through the PyTorch-ONNX exporter.,2890
21987,Could we wrap some pytorch operations into a large symbolic Op and export it to onnx ?,Can export a PyTorch op as a custom op to ONNX through the PyTorch-ONNX exporter.,2890
21988,"On macOS Big Sur, parallel DataLoader causes global variables to be reinitialised","1. move `set_var` outside the `if __name__ == '__main__':`
 OR
 2. use `multiprocessing_context='fork'` in creating `DataLoader`",5806
21989,"On macOS Big Sur, parallel DataLoader causes global variables to be reinitialised","1. move `set_var` outside the `if __name__ == '__main__':`
 OR
 2. use `multiprocessing_context='fork'` in creating `DataLoader`",5806
21990,`test_variant_consistency_jit` for BFloat16 are skipped in a confusing way,Skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level.,884
21991,`test_variant_consistency_jit` for BFloat16 are skipped in a confusing way,Skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level.,884
21992,Profiling distributed NCCL collectives deadlocks when profiler run with use_cuda=True,It happens as a result of profiler creating events on each device. We can probably mitigate this by setting `CUDA_VISIBLE_DEVICES` properly.,9756
21993,Profiling distributed NCCL collectives deadlocks when profiler run with use_cuda=True,It happens as a result of profiler creating events on each device. We can probably mitigate this by setting `CUDA_VISIBLE_DEVICES` properly.,9756
21994,How to do polymorphism on torch::nn::ModuleHolder?,Use std::shared_ptr<M> .,6873
21995,How to do polymorphism on torch::nn::ModuleHolder?,Use std::shared_ptr<M> .,6873
21996,test_cholesky_solve_batched_many_batches_cuda_complex128 has cuda illegal memory access,Workaround for MAGMA accessing illegal memory in batched cholesky.,3024
21997,test_cholesky_solve_batched_many_batches_cuda_complex128 has cuda illegal memory access,Workaround for MAGMA accessing illegal memory in batched cholesky.,3024
21998,`torch.digamma`: Inconsistent with SciPy,Fix inconsistency of digamma with SciPy.,7570
21999,`torch.digamma`: Inconsistent with SciPy,Fix inconsistency of digamma with SciPy.,7570
22000,"If test suite triggers CUDA assert, should stop running tests",Abort only if one of context-invalidating (sticky) errors happened.,11354
22001,"If test suite triggers CUDA assert, should stop running tests",Abort only if one of context-invalidating (sticky) errors happened.,11354
22002,RuntimeError : PyTorch was compiled without NumPy support,Install another version of numpy.,8630
22003,RuntimeError : PyTorch was compiled without NumPy support,Install another version of numpy.,8630
22004,Cannot find libcaffe2_gpu.so,Add the location of the file to the `LD_LIBRARY_PATH` .,8334
22005,Cannot find libcaffe2_gpu.so,Add the location of the file to the `LD_LIBRARY_PATH` .,8334
22006,[Report] nightly build with Jupyter stuck,"Clone and install the latest version of pytorch in a new environment, install jupyter notebook after updating pip.",3136
22007,[Report] nightly build with Jupyter stuck,"Clone and install the latest version of pytorch in a new environment, install jupyter notebook after updating pip.",3136
22008,[Caffe2] Unable to clone Caffe2 repo,`$ git clone --recursive https://github.com/pytorch/pytorch.git`,7146
22009,[Caffe2] Unable to clone Caffe2 repo,`$ git clone --recursive https://github.com/pytorch/pytorch.git`,7146
22010,[ONNX] Cannot export upsample op,ONNX Interpolate Add Scales Params.,3184
22011,[ONNX] Cannot export upsample op,ONNX Interpolate Add Scales Params.,3184
22012,[Caffe2] Unable to compile Caffe 2 with CUDA 9,works with 5.5.0.,4040
22013,[Caffe2] Unable to compile Caffe 2 with CUDA 9,works with 5.5.0.,4040
22014,error when loading model saved under newer version,Loading model is not forward compatible. It is backward compatible.,9806
22015,error when loading model saved under newer version,Loading model is not forward compatible. It is backward compatible.,9806
22016,[caffe2] EigenTranspose problem in math_cpu.cc,Eigen transpose is disabled in the latest commit.,3272
22017,[caffe2] EigenTranspose problem in math_cpu.cc,Eigen transpose is disabled in the latest commit.,3272
22018,backward not working properly in svd (pytorch-nightly),Fix SVD backward on non-square matrices when some=False .,3997
22019,backward not working properly in svd (pytorch-nightly),Fix SVD backward on non-square matrices when some=False .,3997
22020,Arguments are located on different GPU with torch.nn.EmbeddingBag,Fixed on both 0.4 release and master.,6837
22021,Arguments are located on different GPU with torch.nn.EmbeddingBag,Fixed on both 0.4 release and master.,6837
22022,[PyTorch] Printing large tensors is slow,Fix half tensor printing plus speedup large tensor printing.,3336
22023,[PyTorch] Printing large tensors is slow,Fix half tensor printing plus speedup large tensor printing.,3336
22024,[PyTorch] Don't use scientific notation for printing integer tensors,"Integer tensors won't be printed in scientific notation.
 Removed scaling factor for all tensors.
 Only leave spaces for negative signs if one of the printed elements is going to have an negative sign.",9225
22025,[PyTorch] Don't use scientific notation for printing integer tensors,"Integer tensors won't be printed in scientific notation.
 Removed scaling factor for all tensors.
 Only leave spaces for negative signs if one of the printed elements is going to have an negative sign.",9225
22026,[pytorch] randperm lacks CUDA implementation,"Change two lines in the `utils/data/sampler.py` file.
 
 
 
 ```python
 
 class RandomSampler(Sampler):
 
  r""""""Samples elements randomly, without replacement.
 
 
 
  Arguments:
 
  data_source (Dataset): dataset to sample from
 
  """"""
 
 
 
  def __init__(self, data_source):
 
  self.data_source = data_source
 
 
 
  def __iter__(self):
 
  cpu = torch.device('cpu')
 
  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())
 
 
 
  def __len__(self):
 
  return len(self.data_source)
 
 ```",4598
22027,[pytorch] randperm lacks CUDA implementation,"Change two lines in the `utils/data/sampler.py` file.
 
 
 
 ```python
 
 class RandomSampler(Sampler):
 
  r""""""Samples elements randomly, without replacement.
 
 
 
  Arguments:
 
  data_source (Dataset): dataset to sample from
 
  """"""
 
 
 
  def __init__(self, data_source):
 
  self.data_source = data_source
 
 
 
  def __iter__(self):
 
  cpu = torch.device('cpu')
 
  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())
 
 
 
  def __len__(self):
 
  return len(self.data_source)
 
 ```",4598
22028,"Compilation errors on master, Ubuntu 17.10, CUDA 9, GCC 6.4 / GCC 5.4.1","Define the following in `c++config.h`:
 
 `#define _GLIBCXX_USE_C99 1`
 
 `#define _GLIBCXX_USE_C99_MATH 1`",2788
22029,"Compilation errors on master, Ubuntu 17.10, CUDA 9, GCC 6.4 / GCC 5.4.1","Define the following in `c++config.h`:
 
 `#define _GLIBCXX_USE_C99 1`
 
 `#define _GLIBCXX_USE_C99_MATH 1`",2788
22030,Unnecessary memcopies emitted by autograd engine,"Redefine nn/functional.py linear as :
 
 ```.py
 
 def linear(input, weight, bias=None):
 
  """""" 
 
  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. 
 
  
 
  Shape: 
 
  - Input: :math:`(N, *, in\_features)` where `*` means any number of 
 
  additional dimensions 
 
  - Weight: :math:`(out\_features, in\_features)` 
 
  - Bias: :math:`(out\_features)` 
 
  - Output: :math:`(N, *, out\_features)` 
 
  """"""
 
 
 
  input = input.contiguous()
 
  sizes = input.size()[:-1]
 
  input = input.view(-1, input.size(-1))
 
  if input.dim() == 2 and bias is not None:
 
  # fused op is marginally faster 
 
  output = torch.addmm(bias, input, weight.t())
 
  return output.view(*sizes, -1)
 
 
 
  output = input.matmul(weight.t())
 
  if bias is not None:
 
  output += bias
 
  return output
 
 ```",5111
22031,Unnecessary memcopies emitted by autograd engine,"Redefine nn/functional.py linear as :
 
 ```.py
 
 def linear(input, weight, bias=None):
 
  """""" 
 
  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. 
 
  
 
  Shape: 
 
  - Input: :math:`(N, *, in\_features)` where `*` means any number of 
 
  additional dimensions 
 
  - Weight: :math:`(out\_features, in\_features)` 
 
  - Bias: :math:`(out\_features)` 
 
  - Output: :math:`(N, *, out\_features)` 
 
  """"""
 
 
 
  input = input.contiguous()
 
  sizes = input.size()[:-1]
 
  input = input.view(-1, input.size(-1))
 
  if input.dim() == 2 and bias is not None:
 
  # fused op is marginally faster 
 
  output = torch.addmm(bias, input, weight.t())
 
  return output.view(*sizes, -1)
 
 
 
  output = input.matmul(weight.t())
 
  if bias is not None:
 
  output += bias
 
  return output
 
 ```",5111
22032,[feature request] support batch diag,`torch.diag(batch=True)` and similar to other operators.,9528
22033,[feature request] support batch diag,`torch.diag(batch=True)` and similar to other operators.,9528
22034,[feature request] torch.where to support Tensors and python scalars,Tensors will be gone within a week or two. Won't fix.,6757
22035,[feature request] torch.where to support Tensors and python scalars,Tensors will be gone within a week or two. Won't fix.,6757
22036,discuss.pytorch.org is down,Ran into unexpected maintenance.,9739
22037,discuss.pytorch.org is down,Ran into unexpected maintenance.,9739
22038,RuntimeError: cuda runtime error (2) : out of memory when using loss function,"As long as output Variables are in scope, the underlying graphs can't be freed.",1422
22039,RuntimeError: cuda runtime error (2) : out of memory when using loss function,"As long as output Variables are in scope, the underlying graphs can't be freed.",1422
22040,Error message when wrong call to forward of RNN (nn.GRU and also nn.LSTM),The error occurs when the dimension 2 of the input is not correct.,975
22041,Error message when wrong call to forward of RNN (nn.GRU and also nn.LSTM),The error occurs when the dimension 2 of the input is not correct.,975
22042,CUDNN_STATUS_EXECUTION_FAILED with RNN on GPU,"Missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.",8338
22043,CUDNN_STATUS_EXECUTION_FAILED with RNN on GPU,"Missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.",8338
22044,ONNX export issue for model with multiple outputs,Only Variables or lists or tuples of Variables (maybe nested) are supported.,1529
22045,ONNX export issue for model with multiple outputs,Only Variables or lists or tuples of Variables (maybe nested) are supported.,1529
22046,Documentation code for sampling from categorical distribution throws an error on master,Appears to be a bug in `.view()` .,3254
22047,Documentation code for sampling from categorical distribution throws an error on master,Appears to be a bug in `.view()` .,3254
22048,"Compilation errors, when compiling pytorch on Ubuntu 17.10 or newer with GCC 5",Remove the `build` directory and add `gcc7` into `$PATH` .,230
22049,"Compilation errors, when compiling pytorch on Ubuntu 17.10 or newer with GCC 5",Remove the `build` directory and add `gcc7` into `$PATH` .,230
22050,"""Bus error"" on /dev/shm OOM; hard/impossible to fix","Touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them.",8696
22051,"""Bus error"" on /dev/shm OOM; hard/impossible to fix","Touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them.",8696
22052,Pytorch inconsistent behavior for boundary checks,"It would be nice to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all).",4104
22053,Pytorch inconsistent behavior for boundary checks,"It would be nice to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all).",4104
22054,Training and learning rate finder utilities,"Promising trainer abstractions in the pytorch community are:
 
 
 
 - ignite: http://github.com/pytorch/ignite
 
 - lightning: https://github.com/williamFalcon/pytorch-lightning
 
 - skorch: https://github.com/skorch-dev/skorch",10852
22055,Training and learning rate finder utilities,"Promising trainer abstractions in the pytorch community are:
 
 
 
 - ignite: http://github.com/pytorch/ignite
 
 - lightning: https://github.com/williamFalcon/pytorch-lightning
 
 - skorch: https://github.com/skorch-dev/skorch",10852
22056,Crash using PyTorch built with TBB support and multiple DataLoader instance with pin_memory=True,Relax restrictions on set_num_threads.,8532
22057,Crash using PyTorch built with TBB support and multiple DataLoader instance with pin_memory=True,Relax restrictions on set_num_threads.,8532
22058,Remove use of Variable wrapper in test files?,Migrate away from using legacy Variable constructor in test_nn.py.,9478
22059,Remove use of Variable wrapper in test files?,Migrate away from using legacy Variable constructor in test_nn.py.,9478
22060,cdist gradient computation is broken,"This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect.",7572
22061,cdist gradient computation is broken,"This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect.",7572
22062,Significant GPU Memory leak,"Set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there.",6788
22063,Significant GPU Memory leak,"Set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there.",6788
22064,multi_head_attention_forward produces NaN,"Error in padding mask, as padded locations were marked with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs.",3987
22065,multi_head_attention_forward produces NaN,"Error in padding mask, as padded locations were marked with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs.",3987
22066,lstm module with None in input on CUDA segfaults.,Example doesn't segfault without PackedSequence .,4595
22067,lstm module with None in input on CUDA segfaults.,Example doesn't segfault without PackedSequence .,4595
22068,Exclude generated source docs from Google,Have [source] links in the docs.,1898
22069,Exclude generated source docs from Google,Have [source] links in the docs.,1898
22070,[ONNX] export with dynamic_axes should generate Shape node for intermediate tensor .shape calls even without jit.script,Set traceable=true in arange's arg parser.,7665
22071,[ONNX] export with dynamic_axes should generate Shape node for intermediate tensor .shape calls even without jit.script,Set traceable=true in arange's arg parser.,7665
22072,TorchScript fails to compile methods with misindented comments,Allow for source code comments at any level of indentation.,2225
22073,TorchScript fails to compile methods with misindented comments,Allow for source code comments at any level of indentation.,2225
22074,Allreduce for sparse tensors is not working,Put sparse all reduce results to input tensors.,1924
22075,Allreduce for sparse tensors is not working,Put sparse all reduce results to input tensors.,1924
22076,"On pytorch mobile (android), module forward returns org.pytorch.IValue type. But on my detection module, supposes to return a tuple IValue.","IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method.",2765
22077,"On pytorch mobile (android), module forward returns org.pytorch.IValue type. But on my detection module, supposes to return a tuple IValue.","IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method.",2765
22078,[android] initHybrid missing/broken in pytorch 1.4.0 nightly,The error should not happen if you update to the nightlies with version >= 78 (gradle key `--refresh-dependencies`),7069
22079,[android] initHybrid missing/broken in pytorch 1.4.0 nightly,The error should not happen if you update to the nightlies with version >= 78 (gradle key `--refresh-dependencies`),7069
22080,Matrix multiplication returns wrong result when middle dimension is 0,"We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time.",4142
22081,Matrix multiplication returns wrong result when middle dimension is 0,"We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time.",4142
22082,Should we expose CircleCI CUDA10 tests as XImportant and run on PRs?,Switch important CI from CUDA 9 to 10.1.,4096
22083,Should we expose CircleCI CUDA10 tests as XImportant and run on PRs?,Switch important CI from CUDA 9 to 10.1.,4096
22084,Power8/P100 node pytorch compilation from source with cuda 10.1: bus error - out of memory,Set `MAX_JOBS` in the env.,3374
22085,Power8/P100 node pytorch compilation from source with cuda 10.1: bus error - out of memory,Set `MAX_JOBS` in the env.,3374
22086,Missing edge case information in BCELoss documentation,It is using a safe log function that replaces `log(0)` specifically with `log(EPS)` where EPS = 1e-12.,8668
22087,Missing edge case information in BCELoss documentation,It is using a safe log function that replaces `log(0)` specifically with `log(EPS)` where EPS = 1e-12.,8668
22088,torch.gather in pytorch.onnx and onnxruntime,"With the ""verbose = True"", torch.gather(right_input,dim=3, index=right_y_coordinate_a.long()) .",267
22089,torch.gather in pytorch.onnx and onnxruntime,"With the ""verbose = True"", torch.gather(right_input,dim=3, index=right_y_coordinate_a.long()) .",267
22090,BrokenPipeError on Windows when setting the num_workers as 4 in DataLoader,Increase the size of the page file.,5099
22091,BrokenPipeError on Windows when setting the num_workers as 4 in DataLoader,Increase the size of the page file.,5099
22092,torch.as_tensor returns different value for 'is_leaf' for different dtypes,It is a noop for all but the 3rd usage.,9729
22093,torch.as_tensor returns different value for 'is_leaf' for different dtypes,It is a noop for all but the 3rd usage.,9729
22094,torch.no_grad() context manager seems to leak memory,"Since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done.",7020
22095,torch.no_grad() context manager seems to leak memory,"Since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done.",7020
22096,cuDNN convolution memory usage,Set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`).,2585
22097,cuDNN convolution memory usage,Set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`).,2585
22098,PyTorch nightly fails on Windows,Make fully_qualified_type_name_impl() compatible with VS2017 15.9,2226
22099,PyTorch nightly fails on Windows,Make fully_qualified_type_name_impl() compatible with VS2017 15.9,2226
22100,RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory,Include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm.,3093
22101,RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory,Include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm.,3093
22102,fused multiply add returns wrong result for bool tensor,Enabled 'add_cuda' for bool and fixed alpha scalar bug.,6958
22103,fused multiply add returns wrong result for bool tensor,Enabled 'add_cuda' for bool and fixed alpha scalar bug.,6958
22104,RuntimeError when using multiple DistributedDataParallel model,"train your model on a single node without the DDP wrapper. after loss.backward() and before optimizer.step() call add the below lines
 
 ```for name, param in model.named_parameters():
  if param.grad is None:
  print(name)```
 This will print any param which did not get used in loss calculation, their grad will be None.",3888
22105,RuntimeError when using multiple DistributedDataParallel model,"train your model on a single node without the DDP wrapper. after loss.backward() and before optimizer.step() call add the below lines
 
 ```for name, param in model.named_parameters():
  if param.grad is None:
  print(name)```
 This will print any param which did not get used in loss calculation, their grad will be None.",3888
22106,2 tensors for the output...Abou the libtorch | torch.jit.trace,Call `toTuple` to get the Tuple object and then extract the tensor.,5931
22107,2 tensors for the output...Abou the libtorch | torch.jit.trace,Call `toTuple` to get the Tuple object and then extract the tensor.,5931
22108,torch.nn.functional.gumbel_softmax yields NaNs,gumbel_softmax stability issue.,3175
22109,torch.nn.functional.gumbel_softmax yields NaNs,gumbel_softmax stability issue.,3175
22110,Reduction operations magical auto-casting non documented,Specify a `dtype` in the function.,8271
22111,Reduction operations magical auto-casting non documented,Specify a `dtype` in the function.,8271
22112,Pytorch 1.1 with distributed data parallel,`export OMP_NUM_THREADS=1` .,8496
22113,Pytorch 1.1 with distributed data parallel,`export OMP_NUM_THREADS=1` .,8496
22114,Conv3d fails with bigger batch size,"Split a single convolution into a few calls, each with less than 2**31 elements.",8252
22115,Conv3d fails with bigger batch size,"Split a single convolution into a few calls, each with less than 2**31 elements.",8252
22116,RuntimeError: CUDA error: invalid configuration argument,Add striding on channel dimension and limit the launch config on GPU *grid* properties.,5729
22117,RuntimeError: CUDA error: invalid configuration argument,Add striding on channel dimension and limit the launch config on GPU *grid* properties.,5729
22118,nn.parallel.DistributedDataParallel nccl backend multiple-gpu multiple-node deadlock,Export the environment variable `NCCL_LL_THRESHOLD=0`,7415
22119,nn.parallel.DistributedDataParallel nccl backend multiple-gpu multiple-node deadlock,Export the environment variable `NCCL_LL_THRESHOLD=0`,7415
22120,DataLoader runs too many threads,"Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data.",2451
22121,DataLoader runs too many threads,"Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data.",2451
22122,SyncBatchNorm test mode,"If it is just for local test, you can initialize the process group using `world_size=1`.",3818
22123,SyncBatchNorm test mode,"If it is just for local test, you can initialize the process group using `world_size=1`.",3818
22124,RuntimeError: expected backend CPU and dtype Double but got backend CPU and dtype Float - in torch normalize function,Fixed in the master version of torchvision.,7578
22125,RuntimeError: expected backend CPU and dtype Double but got backend CPU and dtype Float - in torch normalize function,Fixed in the master version of torchvision.,7578
22126,Batched Triu And Tril Incorrect for Some Inputs,Fix behavior of `torch.triu` / `torch.tril` on certain unsqueezed tensors that lead to uninitialized values on CPU.,3328
22127,Batched Triu And Tril Incorrect for Some Inputs,Fix behavior of `torch.triu` / `torch.tril` on certain unsqueezed tensors that lead to uninitialized values on CPU.,3328
22128,Drop 'add extra samples to make it evenly divisible' constraint in DistributedSampler,Have a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO).,3292
22129,Drop 'add extra samples to make it evenly divisible' constraint in DistributedSampler,Have a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO).,3292
22130,New VSX support breaks compilation if using g++ v7 (ppc64le),Inline in `vsx_helpers.h` is simply missing the `return vec_out;`,9372
22131,New VSX support breaks compilation if using g++ v7 (ppc64le),Inline in `vsx_helpers.h` is simply missing the `return vec_out;`,9372
22132,CUDNN_STATUS_EXECUTION_FAILED when using AMP,Faulty kernel might be in cudnn7.6.5.,3182
22133,CUDNN_STATUS_EXECUTION_FAILED when using AMP,Faulty kernel might be in cudnn7.6.5.,3182
22134,distributions.Independent does not correctly update distribution's support,"Fix a number of inconsistencies in torch.distributions.constraints as used for parameters and supports of probability distributions.
 
 1. Add a `constraints.independent` and replaces `real_vector` with `independent(real, 1)`. (this pattern has long been used in Pyro)
 2. Add an `.event_dim` attribute to all constraints.
 3. Test that `constraint.check(data)` has the correct shape. (Previously the shapes were incorrect).
 4. Add machinery to set `static .is_discrete` and `.event_dim` for `constraints.dependent`.
 5. Fix constraints for a number of distributions.",928
22135,distributions.Independent does not correctly update distribution's support,"Fix a number of inconsistencies in torch.distributions.constraints as used for parameters and supports of probability distributions.
 
 1. Add a `constraints.independent` and replaces `real_vector` with `independent(real, 1)`. (this pattern has long been used in Pyro)
 2. Add an `.event_dim` attribute to all constraints.
 3. Test that `constraint.check(data)` has the correct shape. (Previously the shapes were incorrect).
 4. Add machinery to set `static .is_discrete` and `.event_dim` for `constraints.dependent`.
 5. Fix constraints for a number of distributions.",928
22136,torch.nn.parallel.scatter_gather.gather cannot handle namedtuple as output,Fix torch.nn.parallel.scatter_gather.gather to handle NamedTuples and handle moving output to CPU.,9041
22137,torch.nn.parallel.scatter_gather.gather cannot handle namedtuple as output,Fix torch.nn.parallel.scatter_gather.gather to handle NamedTuples and handle moving output to CPU.,9041
22138,Make it easier to run mypy locally,"Build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`.",202
22139,Make it easier to run mypy locally,"Build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`.",202
22140,rpc memory leak,"Add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map.",4150
22141,rpc memory leak,"Add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map.",4150
22142,"When I was compiling, these errors appeared, what should I do",Switch to 1.7.,1526
22143,"When I was compiling, these errors appeared, what should I do",Switch to 1.7.,1526
22144,The computation of weighted CrossEntropyLoss is not reasonable,Enable min & max for Float16 & BFloat16.,8543
22145,The computation of weighted CrossEntropyLoss is not reasonable,Enable min & max for Float16 & BFloat16.,8543
22146,`ATen/cpu/vec256/` should be a header only library for all but quantized types,Make Vec256 header only library.,1110
22147,`ATen/cpu/vec256/` should be a header only library for all but quantized types,Make Vec256 header only library.,1110
22148,torch.linalg.qr ignores zero batched dimensions,Fix MAGMA qr for empty batched inputs.,10443
22149,torch.linalg.qr ignores zero batched dimensions,Fix MAGMA qr for empty batched inputs.,10443
22150,Learning rate scheduler in C++ API,Implement ReduceLROnPlateau and StepLR.,6051
22151,Learning rate scheduler in C++ API,Implement ReduceLROnPlateau and StepLR.,6051
22152,"Add a `vectorize` flag to torch.autograd.functional.{jacobian, hessian}",Set `create_graph=True` when we call autograd.grad.,240
22153,"Add a `vectorize` flag to torch.autograd.functional.{jacobian, hessian}",Set `create_graph=True` when we call autograd.grad.,240
22154,Add batched grad checking to Opinfo,"1. add new `check_batched_grad=True` and `check_batched_gradgrad=True`
 attributes to OpInfo. These are `True` by default because we expect most
 operators to support batched gradient computation.
 2. If `check_batched_grad=True`, then `test_fn_grad` invokes gradcheck
 with `check_batched_grad=True`.
 3. If `check_batched_gradgrad=True`, then `test_fn_gradgradgrad` invokes
 `gradgradcheck` with `check_batched_grad=True`.",3848
22155,Add batched grad checking to Opinfo,"1. add new `check_batched_grad=True` and `check_batched_gradgrad=True`
 attributes to OpInfo. These are `True` by default because we expect most
 operators to support batched gradient computation.
 2. If `check_batched_grad=True`, then `test_fn_grad` invokes gradcheck
 with `check_batched_grad=True`.
 3. If `check_batched_gradgrad=True`, then `test_fn_gradgradgrad` invokes
 `gradgradcheck` with `check_batched_grad=True`.",3848
22156,C++ API: `at::empty(size)` modifies `size` ArrayRef in-place when it owns the ArrayRef,"Output from `t.sizes()` is `IntArrayRef` which is a reference type, so shouldn't really have expected this to work. Re-assigning the tensor shouldn't provide any guarantees about whether or not the reference returned from t.sizes() gets modified.",3752
22157,C++ API: `at::empty(size)` modifies `size` ArrayRef in-place when it owns the ArrayRef,"Output from `t.sizes()` is `IntArrayRef` which is a reference type, so shouldn't really have expected this to work. Re-assigning the tensor shouldn't provide any guarantees about whether or not the reference returned from t.sizes() gets modified.",3752
22158,Don't support torch.Size() in the script,"By the way, this is the major reason we built torch. To be able to use the legacy torch, you needed to be of a certain size. The tensor function Object() { [native code] } was overloaded in terms of size and data.
Now that we have a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch, we are mostly heading towards having a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch.
I believe we could deprecate torch at some point.
Size, and when (if) we eliminate torch support.
We can probably just make tensors as a whole.",1397
22159,Don't support torch.Size() in the script,"By the way, this is the major reason we built torch. To be able to use the legacy torch, you needed to be of a certain size. The tensor function Object() { [native code] } was overloaded in terms of size and data.
Now that we have a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch, we are mostly heading towards having a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch.
I believe we could deprecate torch at some point.
Size, and when (if) we eliminate torch support.
We can probably just make tensors as a whole.",1397
22160,opening nn page is very slow,"https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize as an example.
There are two factors at play:
A lot of mathjax is rendered.
On the same html, we included both modules and functionals.
Is it possible to refactor it to make it go faster?",2525
22161,opening nn page is very slow,"https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize as an example.
There are two factors at play:
A lot of mathjax is rendered.
On the same html, we included both modules and functionals.
Is it possible to refactor it to make it go faster?",2525
22162,Add document page for torch.distributed.rpc/autograd,"For 2, I believe that since rpc and dist autograd interact so much and a user would almost certainly use both if they only used one, they should be on the same page.",2481
22163,Add document page for torch.distributed.rpc/autograd,"For 2, I believe that since rpc and dist autograd interact so much and a user would almost certainly use both if they only used one, they should be on the same page.",2481
22164,[jit] support self._modules in TorchScript,"The accessor methods (e.g. modules(), named children(), named modules(), children()) should be provided, but not _modules itself.

Unfortunately, the majority of these are iterators. Perhaps we should override them in ScriptModule and instead return a list.",8704
22165,[jit] support self._modules in TorchScript,"The accessor methods (e.g. modules(), named children(), named modules(), children()) should be provided, but not _modules itself.

Unfortunately, the majority of these are iterators. Perhaps we should override them in ScriptModule and instead return a list.",8704
22166,Proposal to change the offsets for the EmbeddingBag operator in PyTorch,"To implement this new change, we'll need a new flag in the frontend. This new move isn't merely for the sake of convenience. We normally add a larger buffer and zero pad to reduce this op to accelerator. For example, we want to look up the indices [0, 1, 2, 3], but because the indices have a maximum size of 8, the input to the accelerator is [0, 1, 2, 3, 0, 0, 0, 0]. We don't use 0 because it's a valid lookup index.",8470
22167,Proposal to change the offsets for the EmbeddingBag operator in PyTorch,"To implement this new change, we'll need a new flag in the frontend. This new move isn't merely for the sake of convenience. We normally add a larger buffer and zero pad to reduce this op to accelerator. For example, we want to look up the indices [0, 1, 2, 3], but because the indices have a maximum size of 8, the input to the accelerator is [0, 1, 2, 3, 0, 0, 0, 0]. We don't use 0 because it's a valid lookup index.",8470
22168,register_forward_hooks (and register_backward_hooks) support for TorchScript.,"Thank you for requesting a feature!
We intend to implement hooks in the near future, but I don't know when that will be.
I'll keep this updated. ",6852
22169,register_forward_hooks (and register_backward_hooks) support for TorchScript.,"Thank you for requesting a feature!
We intend to implement hooks in the near future, but I don't know when that will be.
I'll keep this updated. ",6852
22170,PyTorch deadlocks when freeing memory in embedded Pybind interpreter,"I was discussing GIL deadlocks when I realised a couple flaws in the original example that resulted in the deadlock (or crashes in some case).
Additionally, even after the example was patched, pybind11 v.2.3.0 (used in the repro) had a [major bug](https://github.com/pybind/pybind11/issues/1364) that caused crashes.
You'll need at least version 2.4.0. (I tested with v2.5.0).



Here is my fixed version of main.cpp:

```c++

#include <pybind11/embed.h>

#include <thread>



namespace py = pybind11;



int main() {

    py::scoped_interpreter guard{};

    auto threading = py::module::import(""threading""); // (1)

    auto thread = std::thread([]() {

      py::gil_scoped_acquire acquire; // (2)

      auto hello = py::module::import(""hello"");

      py::object a =  hello.attr(""A"")();

      a.attr(""run"")();

    });

    py::gil_scoped_release release;  // (3)

    thread.join();

}",3313
22171,PyTorch deadlocks when freeing memory in embedded Pybind interpreter,"I was discussing GIL deadlocks when I realised a couple flaws in the original example that resulted in the deadlock (or crashes in some case).
Additionally, even after the example was patched, pybind11 v.2.3.0 (used in the repro) had a [major bug](https://github.com/pybind/pybind11/issues/1364) that caused crashes.
You'll need at least version 2.4.0. (I tested with v2.5.0).



Here is my fixed version of main.cpp:

```c++

#include <pybind11/embed.h>

#include <thread>



namespace py = pybind11;



int main() {

    py::scoped_interpreter guard{};

    auto threading = py::module::import(""threading""); // (1)

    auto thread = std::thread([]() {

      py::gil_scoped_acquire acquire; // (2)

      auto hello = py::module::import(""hello"");

      py::object a =  hello.attr(""A"")();

      a.attr(""run"")();

    });

    py::gil_scoped_release release;  // (3)

    thread.join();

}",3313
22172,initializedContextIds_ field in DistEngine grows without bound,"The designation of ""high priority"" denotes that it will be reviewed by a triage team (i.e. is the right person working on this issue right now).
That wasn't your aim, I'm sure.


Is there a better term for prioritisation by (module|topic|queue)? ",21
22173,initializedContextIds_ field in DistEngine grows without bound,"The designation of ""high priority"" denotes that it will be reviewed by a triage team (i.e. is the right person working on this issue right now).
That wasn't your aim, I'm sure.


Is there a better term for prioritisation by (module|topic|queue)? ",21
22174,[Feature request] Provide binaries for Python 3.8,"Python 3.8 was recently upgraded on arch linux based distros, and I'm sure many arch or manjaro users are eagerly awaiting 3.8 binaries. ",5011
22175,[Feature request] Provide binaries for Python 3.8,"Python 3.8 was recently upgraded on arch linux based distros, and I'm sure many arch or manjaro users are eagerly awaiting 3.8 binaries. ",5011
22176,magma functionality isn't working on K80 GPU with official binaries,The torch.solve method actually works! ,10878
22177,magma functionality isn't working on K80 GPU with official binaries,The torch.solve method actually works! ,10878
22178,Indexed assignment of quantized Tensors yields unexpected results,copy and clobber_? ,9123
22179,Indexed assignment of quantized Tensors yields unexpected results,copy and clobber_? ,9123
22180,RPC tests are flaky,"To assist turn this into a tracked issue, let's keep posting additional cases of flakiness here. ",8026
22181,RPC tests are flaky,"To assist turn this into a tracked issue, let's keep posting additional cases of flakiness here. ",8026
22182,Windows CI build is flaky: cannot delete workspace... because it is being used by another process,"On paper, the plan is to migrate these CI jobs to CircleCI, then kill the Jenkins job.",7224
22183,Windows CI build is flaky: cannot delete workspace... because it is being used by another process,"On paper, the plan is to migrate these CI jobs to CircleCI, then kill the Jenkins job.",7224
22184,"Gradle build is flaky: ""could not get resource""",I'm working on it right now. ,8430
22185,"Gradle build is flaky: ""could not get resource""",I'm working on it right now. ,8430
22186,PytorchStreamWriter failed opening archive.,Changed the save route and it worked for me. ,11259
22187,PytorchStreamWriter failed opening archive.,Changed the save route and it worked for me. ,11259
22188,Blank value for _GLIBCXX_USE_CXX11_ABI when compiling with Homebrew's GCC (on macOS),"We don't think we support building with gcc on Mac OS X; instead, we propose clang.
(See https://github.com/pytorch/pytorch for instructions) ",3339
22189,Blank value for _GLIBCXX_USE_CXX11_ABI when compiling with Homebrew's GCC (on macOS),"We don't think we support building with gcc on Mac OS X; instead, we propose clang.
(See https://github.com/pytorch/pytorch for instructions) ",3339
22190,CPU memory leak when using torch.no_grad(),https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484,2042
22191,CPU memory leak when using torch.no_grad(),https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484,2042
22192,test_torch.py/test_inverse_cuda causes illegal memory access on some platforms,"The PR has been merged, therefore it should fix the issue you were experiencing on 10.1.105.
If you've double-checked that and don't see any other crashes, feel free to close it.
Thanks! ",6937
22193,test_torch.py/test_inverse_cuda causes illegal memory access on some platforms,"The PR has been merged, therefore it should fix the issue you were experiencing on 10.1.105.
If you've double-checked that and don't see any other crashes, feel free to close it.
Thanks! ",6937
22194,Why doesn't amp improve training speed,"See


https://pytorch.org/tutorials/recipes/recipes/amp recipe.html#speedup-with-amp-is-minor


https://pytorch.org/tutorials/recipes/recipes/tuning guide.html


for general and amplification advice




Amp won't assist, and may hurt, if your model/batch size is tiny and you obtain poor device use.
Also, amp isn't expected if your GPU isn't Tensor Core enabled (in other words, if it's Pascal or before). ",5757
22195,Why doesn't amp improve training speed,"See


https://pytorch.org/tutorials/recipes/recipes/amp recipe.html#speedup-with-amp-is-minor


https://pytorch.org/tutorials/recipes/recipes/tuning guide.html


for general and amplification advice




Amp won't assist, and may hurt, if your model/batch size is tiny and you obtain poor device use.
Also, amp isn't expected if your GPU isn't Tensor Core enabled (in other words, if it's Pascal or before). ",5757
22196,torch.autograd.profiler.load_nvprof() crash,PR #45004 resolves the issue; relevant parts of the patch will be incorporated into 1.7.,4701
22197,torch.autograd.profiler.load_nvprof() crash,PR #45004 resolves the issue; relevant parts of the patch will be incorporated into 1.7.,4701
22198,"Wrong signature for torch.max_pool(1d, 2d, 3d) in get_testing_overrides()","
I assume one was supposed to be made, but I'm not sure if it has yet.
However, if it isn't, we have module: numpy. ",3004
22199,"Wrong signature for torch.max_pool(1d, 2d, 3d) in get_testing_overrides()","
I assume one was supposed to be made, but I'm not sure if it has yet.
However, if it isn't, we have module: numpy. ",3004
22200,JIT Profiling executor is not fusing Dropout while legacy executor does,"To give you a sense of the difficulty, the dropout operator is widely utilised in Transformer/Bert NLP networks, and being able to fuse to dropout is a significant efficiency opportunity in those networks. ",4038
22201,JIT Profiling executor is not fusing Dropout while legacy executor does,"To give you a sense of the difficulty, the dropout operator is widely utilised in Transformer/Bert NLP networks, and being able to fuse to dropout is a significant efficiency opportunity in those networks. ",4038
22202,Tensors with complex dtypes fail to be cloned contiguously,"Hi,

Yes, the design contrast is deliberate.

The goal we have to fix this problem is to build on top of https://github.com/pytorch/issues/49171 so that the user may submit the weights as suitable inputs to the function and therefore interact nicely with the autograd API. ",2888
22203,Tensors with complex dtypes fail to be cloned contiguously,"Hi,

Yes, the design contrast is deliberate.

The goal we have to fix this problem is to build on top of https://github.com/pytorch/issues/49171 so that the user may submit the weights as suitable inputs to the function and therefore interact nicely with the autograd API. ",2888
22204,No documentation for `torch.sgn`,"It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs.
Here's an example:

https://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517 ",3924
22205,No documentation for `torch.sgn`,"It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs.
Here's an example:

https://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517 ",3924
22206,"libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than ","Is this the reason why NGC containers have consistently outperformed official conda builds for a number of PyTorch versions?
With this issue, NGC == dynamic link, conda/pip = static.
If that is the case, it has a huge influence.

I performed some benchmarks to see what was going on because I kept running into it with fresh releases...
https://gist.github.com/rwightm ",5951
22207,"libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than ","Is this the reason why NGC containers have consistently outperformed official conda builds for a number of PyTorch versions?
With this issue, NGC == dynamic link, conda/pip = static.
If that is the case, it has a huge influence.

I performed some benchmarks to see what was going on because I kept running into it with fresh releases...
https://gist.github.com/rwightm ",5951
22208, Broken formatting for the function signature of `torch.randperm`,We'd accept a PR to address this (unless there's a reason why this isn't documented). ,8302
22209, Broken formatting for the function signature of `torch.randperm`,We'd accept a PR to address this (unless there's a reason why this isn't documented). ,8302
22210,Broken formatting on `torch.repeat_interleave`,We'd be happy to take a PR to address this. ,8303
22211,Broken formatting on `torch.repeat_interleave`,We'd be happy to take a PR to address this. ,8303
22212,Add torch.linalg.vector_norm and torch.linalg.matrix_norm functions,"The reduction method is used to implement all vector norms and frobenius norms.
Splitting matrix norm into separate functions makes sensible, however I'd still keep +/-inf and +/-one matrix norm in the same function because they share a lot of implementation.
That leaves nuclear norm and +/-2-norms, which can coexist because they both employ svd as the first computation step. What is the purpose of Frobenius norm? ```

    if (self.is_complex()){

      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));

    } else {

      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));

    }

``` 

? This is super inefficient compared to vector norm, and produces the same result.    ",1097
22213,Add torch.linalg.vector_norm and torch.linalg.matrix_norm functions,"The reduction method is used to implement all vector norms and frobenius norms.
Splitting matrix norm into separate functions makes sensible, however I'd still keep +/-inf and +/-one matrix norm in the same function because they share a lot of implementation.
That leaves nuclear norm and +/-2-norms, which can coexist because they both employ svd as the first computation step. What is the purpose of Frobenius norm? ```

    if (self.is_complex()){

      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));

    } else {

      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));

    }

``` 

? This is super inefficient compared to vector norm, and produces the same result.    ",1097
22214,"Let loss_func(empty_inputs, reduction=""mean"") return 0","I agree that for empty Tensors, mean should return nan, and this is compatible with numpy: ",3048
22215,"Let loss_func(empty_inputs, reduction=""mean"") return 0","I agree that for empty Tensors, mean should return nan, and this is compatible with numpy: ",3048
22216,Can you add higher order derivative support for torch's embedding function? ,"There are known concerns with creating PyTorch with CUDA11.2 that we are currently experiencing in terms of functionality and performance.
We're working closely with the CUDA team to isolate and resolve these issues.
Users should avoid using CUDA 11.2 with PyTorch until we have resolutions. ",7400
22217,Can you add higher order derivative support for torch's embedding function? ,"There are known concerns with creating PyTorch with CUDA11.2 that we are currently experiencing in terms of functionality and performance.
We're working closely with the CUDA team to isolate and resolve these issues.
Users should avoid using CUDA 11.2 with PyTorch until we have resolutions. ",7400
22218,Discuss options for implementing einsum_path,"I've never seen einsum path used very often, so I wouldn't include it as a separate function.
The keyword ""optimise"" is all that is needed. ",3357
22219,Discuss options for implementing einsum_path,"I've never seen einsum path used very often, so I wouldn't include it as a separate function.
The keyword ""optimise"" is all that is needed. ",3357
22220,Docs build failures are difficult to identify,"The simplest solution is to change the error message at the conclusion of the build step, either in the Makefile or in the circleCI script that calls make html, to say ""if this step fails, look back in the log for WARNING: (in capital letters)"".
Something like this would be a more advanced solution.
",7040
22221,Docs build failures are difficult to identify,"The simplest solution is to change the error message at the conclusion of the build step, either in the Makefile or in the circleCI script that calls make html, to say ""if this step fails, look back in the log for WARNING: (in capital letters)"".
Something like this would be a more advanced solution.
",7040
22222,Categorical logits argument is treated as log probabilities,"Thank you for clarifying.
The discrepancy between binary and multinomial logits had never occurred to me before, and it had tripped me up.


Because logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point. ",6845
22223,"`Expected self.scalar_type() == ScalarType::Float to be true, but got false.` when doing quantization aware training?
","logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point",6845
22224,Categorical logits argument is treated as log probabilities,"Thank you for clarifying.
The discrepancy between binary and multinomial logits had never occurred to me before, and it had tripped me up.


Because logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point. ",6845
22225,"`Expected self.scalar_type() == ScalarType::Float to be true, but got false.` when doing quantization aware training?
","logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point",6845
22226,Casting from long to int may cause wrong result,"There will still be a problem with the code. You'll need to utilise an intermediary Tensor variable like
auto int_t = t.to(at::kInt);
auto p = int t.data ptrint>();",6990
22227,Casting from long to int may cause wrong result,"There will still be a problem with the code. You'll need to utilise an intermediary Tensor variable like
auto int_t = t.to(at::kInt);
auto p = int t.data ptrint>();",6990
22228,torch.cat different Windows vs Linux behavior,"torch. version > '1.4.0'
torch.cat(( torch.tensor([]), torch.tensor([1,2,3], dtype=torch.long)), torch.tensor([1,2,3], dtype=torch.long)))
(Last call) Traceback (most recent call):
RuntimeError: file """", line 1 For sequence element 1 in sequence, I expected a scalar type Float object, but instead got a scalar type Long object.
This issue may have been resolved in v1.6.0: https://github.com/pytorch/pytorch/releases/tag/v1.6.0
 torch.cat has been updated to include a missing type prom.",10981
22229,ProcessGroupNCCL::all_reduce is not profiled correctly,"The record function before/end invocation only produces a host side event in the new PyTorch profiler (kineto based profiler). CUPTI is used to track GPU side events in an async manner. In the end, the profiler will correlate host/device events.
To stay true to the design philosophy, it appears that broad idea #2 will suffice.
Only include the time it took to launch allreduce (i.e. enqueue it) in your report.",3781
22230,ðŸ› memory leaks with coalesce method,"In the issue reproducer we can replace torch.sparse.sum(S) with S.coalesce() and get the same memory leak. The reason is that calling coalesce() on an already coalesced tensor returns self. With autograd, the result gets it's grad_fn set to a node that contains a reference to the input tensor, creating a reference cycle. Cloning the tensor fixes this, so coalesce always returns a new tensor.

As an aside, torch.sparse.sum(S) doesn't need to coalesce. The result should be the same either way.",8305
22231,Impossible to export ONNX as RAW," a copy of the site with these changes here: https://garymm.github.io/onnx.html

Probably best to review that",6900
22232,Pytorch autograd sometimes fails with CUDNN_STATUS_INTERNAL_ERROR,"Issue is reproduced using cudnn8.0.5 on an RTX2080Ti and this issue is apparently fixed in the cudnn8.1 release.
To test it, you could install the `1.8` release candidate via:
```
conda install pytorch cudatoolkit=11.2 -c pytorch-test -c conda-forge

which ships with cudnn8.1:
```python
>>> import torch
>>> torch.__version__
'1.8.0'
>>",6831
22233,[nnc][perf] 50x performance regression from `_jit_override_can_fuse_on_cpu(True)`,"For the current behavior, I'd suggest `_jit_override_can_fuse_on_cpu(True)` throws an error if you don't have LLVM.  Once fusing becomes the default maybe we need to switch to the `NO_LLVM` flag.",2579
22234,tutorials :: beginner :: audio_classifier_tutorial,"That document was a remnant of a tutorial that was removed, so please accept my apologies. I made a home redirect.",5993
22235,Errors in scripting when using `_pair_from_first`,"Hypothesis is attempting to re-run a failing test, but once there is a jit compiler error connected to the compilation of the Conv1d module, it is left in an inconsistent state, and further compilations report the 'Can't rename method' error. If you need to use hypothesis in a situation where script compilation can fail,  calling this at the start of the test to clean up the jit compilation state.
```
from torch.testing._internal.jit_utils import clear_class_registry
clear_class_registry()",7043
22236,"Build failure with ""TORCH_CUDA_API"" is undefined and more","PATH=/third party/libtorch/include 1.XXX=======================================

pytorch1.YYY/build

= make $ Currently running compilation

-DCMAKE INSTALL PREFIX=**/usr/local/src/pytorch/src/pytorch-git/torch**

The install directory is /usr/local/src/pytorch/src/pytorch-git/torch.

**/usr/local/src/pytorch/src/p",2889
22237,torch.empty_like example seems wrong,"The example presented for torch.empty_like seems wrong:

>>> torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
I guess the desided example would be something like:

>>> a = torch.empty((2,3), dtype=torch.int64)
>>> torch.empty_like(a, dtype=torch.float64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
",7051
22238,Result of  torch.Tensor.__mod__  is different from torch.fmod() function.,"FWIW, numpy has the same semantics as we do:
```
import numpy as np
x = np.array([5., 5. -5. , -5.])
y = np.array([3., -3., 3.,  -3.])

[ins] In [9]: x % y
Out[9]: array([ 2., -1.,  1., -2.])

[ins] In [10]: np.fmod
Out[10]: <ufunc 'fmod'>

[ins] In [11]: np.fmod(x, y)
Out[11]: array([ 2.,  2., -2., -2.])

[ins] In [12]: x % y
Out[12]: array([ 2., -1.,  1., -2.])

[ins] In [13]: x %= y; print(x)
[ 2. -1.  1. -2.]
```",2370
22239,training hanged with torch.distributed.launch," It's working with NCCL_SOCKET_IFNAME=lo from this thread.

both of the below were working now -

!NCCL_SOCKET_IFNAME=lo python -m torch.distributed.launch --nproc_per_node=1 ./Seq2Seq.py --output_dir ./out_dir/results --overwrite_output_dir --do_train \
--do_eval --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --learning_rate 3e-5 --weight_decay 0.01 \
--num_train_epochs 1 --load_best_model_at_end --local_rank 0
and

!NCCL_SOCKET_IFNAME=lo deepspeed ./Seq2Seq.py --output_dir ./out_dir/results --overwrite_output_dir --do_train \
--do_eval --per_device_train_batch_size 12 --per_device_eval_batch_size 12 --learning_rate 3e-5 --weight_decay 0.01 \
--num_train_epochs 1 --load_best_model_at_end --local_rank 0 --deepspeed ds_config.json",23
22240,nn.Conv1d non-deterministic behavior ,you created two layers with differently sampled parameters. how could the result be the same?,11454
22241,Windows CXX11-ABI libtorch debug builds for CUDA 11.1 fail with unresolved external symbol,TORCH_CUDA_SPLIT is only enabled on CUDA 11.1.,694
22242,Windows libtorch nightly builds for CUDA 11.1 failing binary smoke tests with unresolved symbols,https://github.com/pytorch/builder/pull/661 should fix this.,9535
22243,Torchscript tutorial example is broken,"Already fixed, waiting for review: https://github.com/pytorch/tutorials/pull/866",1100
22244,Negative impact of Python versions on using Pytorch with Cuda11,"> Executing these two commands
conda create -n myenv python=3.8.5 and conda install pytorch cudatoolkit=11.0 -c pytorch results in installing pytorch=1.5.1., (not pytorch=1.7.1.) as shown below.
```
(myenv) datascienceadmin@limbo:~$ conda install pytorch cudatoolkit=11.0 -c pytorch
[...]
The following NEW packages will be INSTALLED:
[...]
  pytorch            pytorch/linux-64::pytorch-1.7.1-py3.8_cuda11.0.221_cudnn8.0.5_0
```However, since `1.5.1` is found in your REPL, I guess you might have multiple PyTorch versions installed, where the `1.5.1` version is picked. This would also explain the reported ""hang"", since `1.5.1` doesn't ship with CUDA11.0 and the CUDA JIT would kick in, which might be slow.

I would recommend to uninstall all conda, pip, and source builds in the current environment, make sure that `import torch` fails, and reinstall `1.7.1`.
",218
22245,AMP + CUDA + tensor.sum op leads to dtype mismatch,"I don't believe the output dtype is a type mismatch, and the predicted result based on the [float32 cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) isn't it?",3062
22246,Negative impact of Python versions on using Pytorch with Cuda11,"I don't believe the output dtype is a type mismatch, and the predicted result based on the [float32 cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) isn't it?",3062
22247,error making: python-torchvision-cuda,"Please, post this issue in the torchvision repo https://github.com/pytorch/vision instead",4868
22248,error making: python-torchvision-cuda,Post this issue in the torchvision repo https://github.com/pytorch/vision instead,4868
22249,Cannot import torch on latest nightly,"~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~
Actually I think they're two separate issues",11478
22250,[JIT] Profiling executor is not fusing Dropout while legacy executor does,"(https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I found that this issue happens if you're missing some `libpython` headers on your system. Can you reinstall `python` with `apt-get install python3.7-dev`, if you're using Ubuntu?
I verified with Python 3.9.4 that installing `python-dev` instead of `python` fixes the issue.
The reason I was able to build with python 3.8.5 earlier is that `python3.8-dev` was preinstalled in Ubuntu 20.04.",719
22251,Cmake Error on Custom Pytorch Build from Source,"After doing some research (https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I discovered that this problem occurs when some libpython headers are missing from your system. If you're using Ubuntu, can you reinstall Python with apt-get install python3.7-dev?

I confirmed that installing python-dev instead of python addresses the issue with Python 3.9.4.",719
22252,Tensors with complex dtypes fail to be cloned contiguously,Being fixed in #45487.,1435
22253,[Feature Request] Hessian support for model weights (nn.Parameter),"Yes this design difference is done on purpose.
The plan we have to solve this issue is to build on top of https://github.com/pytorch/pytorch/issues/49171 so that the user can give the weights as proper inputs to the function and thus work well with the function autograd API.",8488
22254,No documentation for `torch.sgn`,"It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs. Here's an example:

https://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517",3923
22255,"libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than dynamically linked","So, does this issue explain why NGC containers have been consistently faster than the official conda builds for a number of PyTorch versions now? NGC == dynamic link, conda/pip = static w/ this issue? This has pretty significant impact if that is the case.

I ran some benchmarks trying to figure out what was happening as I've kept bumping into it with new releases... https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f",5952
22256,Support broadcasting for clip / clamp,This is a duplicate of #2793 which is implemented in #52695.,7595
22257,Cannot import torch on latest nightly,"~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~

Actually I think they're two separate issues",11477
22258,Kineto dependency CUPTI detection should use standard cmake mechanisms,find_package(CUDA) sets CUDA_cupti_LIBRARY.,9343
22259,`getattr` does not support default parameter when jitted,"the right syntax tree to emit is a prim::If node with two blocks, one for True one for False

something whose structure is similiar to this Python function and its corresponding generated code:

`
def g(x):
a = T()
return 1 if a.my == ""HI"" else 2

Graph:

graph(%x : Tensor):
%5 : str = prim::Constantvalue=""HI"" # ts1.py:13:21
%8 : int = prim::Constantvalue=1 # ts1.py:13:8
%9 : int = prim::Constantvalue=2 # ts1.py:13:31
%a.1 : torch.T = prim::CreateObject()
%2 : NoneType = prim::CallMethodname=""init"" # ts1.py:12:5
%my : str = prim::GetAttrname=""my""
%6 : bool = aten::eq(%my, %5) # ts1.py:13:13
%10 : int = prim::If(%6) # ts1.py:13:8
block0():
-> (%8)
block1():
-> (%9)
return (%10)",8800
22260,`getattr` does not support default parameter when jitted,"the right syntax tree to emit is a prim::If node with two blocks, one for True one for False
something whose structure is similiar to this Python function and its corresponding generated code:

`
def g(x):
a = T()
return 1 if a.my == ""HI"" else 2

Graph:

graph(%x : Tensor):
%5 : str = prim::Constantvalue=""HI"" # ts1.py:13:21
%8 : int = prim::Constantvalue=1 # ts1.py:13:8
%9 : int = prim::Constantvalue=2 # ts1.py:13:31
%a.1 : torch.T = prim::CreateObject()
%2 : NoneType = prim::CallMethodname=""init"" # ts1.py:12:5
%my : str = prim::GetAttrname=""my""
%6 : bool = aten::eq(%my, %5) # ts1.py:13:13
%10 : int = prim::If(%6) # ts1.py:13:8
block0():
-> (%8)
block1():
-> (%9)
return (%10)
`",8801
22261,Compiling Libtorch in Vs 2019?,PyTorch never really supports ICC. The only compiler supported on Windows is MSVC and the code is C++14 currently.,3223
22262,[feature request] incomplete cholesky factorization, There is an ongoing PR https://github.com/pytorch/pytorch/pull/56724 that adds a variant that allows suppressing the error check.,2841
22263,Backward hook is not registered for traced modules ,I commented on the internal post; JIT doesn't have support for backward hooks. See this issue for tracking: https://github.com/pytorch/pytorch/issues/34329 ,3040
22264,[grad] index_fill : Incorrect gradient when index has duplicates,"Forward should not error out, all the indexing operations accept duplicate indices. 
Perf hit in backward it ok and unavoidable.",2593
22265,about the torch.randperm in torch1.8.1+cuda11.1   å…³äºŽtorch1.8.1å’Œcuda11.1çŽ¯å¢ƒä¸‹randpermå‡½æ•°çš„å¥‡æ€ªè¾“å‡º,This was fixed on master in #55292,7836
22266,torch.cholesky with upper=True is wrong for batched CUDA inputs,"Will this issue be resolved when we deprecate torch.cholesky in favour of torch.linalg.cholesky and develop its deprecation guide

Torch.linalg.cholesky always uses the lower triangular part of the input, hence the answer is yes.

Also, the cholesky ex PR (https://github.com/pytorch/pull/56724) involves some rewrites of how MAGMA is called, so this issue would be fixed there, I believe.",257
22267,CI check `binary_linux_libtorch_3_7m_cpu_devtoolset7_shared-with-deps_build` is failing intermittently,"Last year, when MAX JOBS were set too high, I believe we had some system pipe performance issues. But I'm not clear why this doesn't happen with other binary or near-zero jobs.
* look into different binary continuous integration pipelines
* See whether switching to nproc/2 lessens flakiness
",3275
22268,nn.Linear weight initalization - uniform or kaiming_uniform?," We should probably document the sqrt(3) factor we're using. I recall there being some justification for it.

When you draw a uniform distribution, the stdv is basically multiplied by sqrt(3): www.nist.gov/cuu/Uncertainty/typeb.html and https://physics.stackexchange.com/questions/110242/why-is-uncertainty-divided-by-sqrt3

If you look at my ""equivalent logic"" [snippet](https:/), you'll discover that",255
22269,Build failure with USE_CUDA=1,Not sure if this is what you are looking for but you can turn on https://github.com/pytorch/pytorch/blob/master/caffe2/CMakeLists.txt#L17-L19,4349
22270,[test] index_fill : gradgradcheck CUDA takes around 40 seconds,"The same happens with `index_copy` iirc.
It comes from the fact that the current tests are very thorough. They have cases for all cases of contiguous / non-contiguous inputs with all the possible flags.
Making the input sizes smaller would alleviate but not completely solve this problem ",7263
22271,Expose RNN cells in C++ frontend,"Is it not exposed already? lstm_cell is in the docs...
https://pytorch.org/cppdocs/api/function_namespaceat_1a189463e15e5542e5d21f544fd70ee967.html?highlight=lstm_cell",3883
22272,Port multinomial to Aten,Warning: there may be a bunch of transitive dependencies you have to port first before you can do this.,8230
22273,[C++/pytorch] data loading and working with complex data structures with the C++ frontend,"1. If you can do all modifications on the matrix/tensor in-place, you could call `from_blob` without cloning, mutate the tensor in-place, and since the data would be shared between the tensor and the `cv::Mat` the mutations would be reflected in the original `cv::Mat`, which you could then use for further processing/visualization. E.g.

    ```cpp
     void process(cv::Mat mat) {
         at::Tensor tensor = torch::from_blob(...); // No clone, just a ""view"" of the mat as a tensor
         tensor.mul_(2); // The multiplication by two is visible in both the tensor and the original `mat`
         visualize(mat); // Since we mutated the underlying data, we can just use the original `mat`
     }
    ```

    However, if you are calling a `ScriptModule` like you do in your example, that mutation will likely not be in-place. As in, the result of `module->forward(...)` will likely be a new tensor pointing at new memory, not associated with any `cv::Mat` (yet). So this does not apply.

2. If the end result of your processing is a brand new tensor, you likely want to convert it back into a `cv::Mat`. For this you can use [this constructor](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e) from OpenCV, like the `cv::Mat warp(3, 3, CV_32FC1, w.data<float>());` I used above. In fact, if you read through the ""Detailed Description"" section, you'll notice that their data model is the same as for our tensors: creating a `cv::Mat` from some data blob only creates a view, and you can use the [`clone()`](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#adff2ea98da45eae0833e73582dd4a660) method to create a deep-copy of the data.

All of that said, I think the path for you is:

1. You start out with a `cv::Mat mat`,
2. If all you're doing is passing the Mat to `module->forward` immediately, you can create a tensor from it without copying, i.e. `tensor = from_blob(mat.ptr<float>(), sizes)` without cloning. For this make sure the original `mat` is still alive while you call `module->forward`.
3. Get a new tensor when invoking the module: `auto new_tensor = module->forward(tensor);`
4. Then create a `cv::Mat` from the resulting tensor: `cv::Mat output(sizes, CV_32FC1, new_tensor.data<float>())`. At this point the `output` mat will be pointing at data owned by the `new_tensor`. If you simply want to visualize it straight away, no need to copy the data into the `Mat`.
5. If you want to return the `Mat` from a function or otherwise store it, call `clone()` on the `Mat` to take ownership of the data by copying it.

Notice that there is a case where no additional copies are incurred at all, which is when you're only converting to a tensor to call `module->forward`, and then only converting back to a `Mat` to visualize it. Just be aware of how the lifetimes are bound to each other and when you need to call `clone()`. Hope this helps.",751
22274,ModuleNotFoundError: No module named 'torchvision',"It is in a separate repository https://github.com/pytorch/vision, should be quite easy to install from source since you already compiled pytorch.",3935
22275,Expose RNN cells in C++ frontend,this should definitely be deleted. Would you mind sending a PR for it? If not we can route it to one of the core devs,10913
22276,torch.cholesky with upper=True is wrong for batched CUDA inputs,picking up a header/library from the old install.,8510
22277,[grad] index_fill : Incorrect gradient when index has duplicates,We dont ship 0.4.0 with CUDA 9.2. You can pick a link for 0.4.0 here: https://pytorch.org/get-started/previous-versions/,8267
22278,Cannot import torch on latest nightly,`::to` is being added in https://github.com/pytorch/pytorch/pull/12710 which will enable it for C++ interfaces.,546
22279,Printing: mismatch result with numpy in pytorch 0.4, this is a printing issue: setting `torch.set_printoptions(precision=8)` prints the right values.,3151
22280,undefined reference to `vmdLog2',This looks like an MKL error. ,7715
22281,Add `return_counts` to `torch.unique`,Expose RNN cells in C++ frontend,2362
22282,Cross entropy Loss : Ignore list of indexes ,"it's not popular enough that we want to complicate the function even more in it's implementation.
You can do some pre-processing on your `targets` to get to the same place, or as you mentioned, use the weight=0 trick. In fact, `weight` is provided for this precise reason.",11392
22283,caffe2  GetMutableTensor,"Linking against glog, and explicitly using BlobGetMutableTensor(Blob* blob, DeviceType device_type) did the job for me. But I think that it should also work as presented above. Changes to the CMakeLists.txt and the main.cpp are:

cmake:

```
cmake_minimum_required(VERSION 3.11 FATAL_ERROR)
set(CMAKE_CXX_STANDARD 11)

find_library(CAFFE2_LIB caffe2)
find_package(Protobuf 3 REQUIRED)
find_library(GLOG_LIB glog) # MODIFIED

add_executable(main main.cpp)
target_link_libraries(main ${CAFFE2_LIB} ${PROTOBUF_LIBRARIES} ${GLOG_LIB}) # MODIFIED
```

main:

```
#include <caffe2/core/init.h>
#include <caffe2/core/tensor.h>
#include <caffe2/core/workspace.h>

using namespace std;
using namespace caffe2;

int main(int argc, char** argv) {

	caffe2::GlobalInit(&argc, &argv);

	// Tensor.
	vector<int64_t> dims({10});
	vector<float> values(10, 1);

	Tensor tensor(dims, CPU);
	tensor.ShareExternalPointer(values.data());

	// Workspace.
	Workspace workspace;

	//Tensor* data = workspace.CreateBlob(""data"")->GetMutable<TensorCPU>(); // Doesn't work for me.
	Tensor* data = BlobGetMutableTensor(workspace.CreateBlob(""data""), CPU); // MODIFIED
	data->ResizeLike(tensor); // works

	
	google::protobuf::ShutdownProtobufLibrary();
	
	return 0;
}
```

I don't know if it is intended to use the caffe2 cpp api like this for future releases, or whether it is simply a bug on my system.",4116
22284,Using nn.Parameter as args to torch.distributions.Normal,Fixed on master,2468
22285,nn.html page has too much content,"I second this. At least the `functional`  part could go to an entirely new webpage
1. most items there have a link to relevant ""objective"" `nn` item anyway
2. they double the keywords in objective `nn`, which means even ctrl+f doesn't really do its job

Another pain point is that (at least in firefox) the math formulae are rendered after the page itself, which interacts with links. When I click a link to an item halfway through the `nn` page (often from the corresponding `functional` item), I will be directed to the right item, and after a second it will slide off my screen due to all the formulae above expanding and taking its place. Not that I have a good idea how to deal with that...",3191
22286,Dataset loader ToCuda() transform collate_fn fails,use `torch.multiprocessing.set_start_method('spawn')` to get CUDA + Multiprocessing working,567
22287,OMP: Error about doubly-linked OpenMP when loading CIFAR10 dataset,"A workaround seems to be
```
import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
```
but it would be better to find the root cause.",885
22288,5x slowdown on Windows vs Ubuntu,"Used PyTorch 1.5 with CUDA-10.2 in both Windows-10 and Ubuntu-18.04. Both OS are on the same machine. Speed using the diagnosis code provided [at the beginning of the thread](https://github.com/pytorch/pytorch/issues/13807#issue-379409817), shows that PyTorch is ~4 times slower in Windows.
I was running [another project](https://github.com/NVlabs/Deep_Object_Pose) which uses PyTorch. I adapted it to use python 3.6 and PyTorch 1.5. Same speed problem between Windows and Ubuntu.
",2992
22289,Unable to do transfer tensors to GPU using `.cuda()` when using multiprocessing.Process with `fork`,"closing this as it cannot be fixed::


 CUDA is totally incompatible with forking.
This simply *cannot* be fixed in CUDA, PyTorch or anyone else. The problems inherent in `fork()`'ing *any* _multithreaded_ program are fundamentally unsolvable, and simply beyond the power of anyone to fix, at least not until a revolution in OS design happens.

There's several blogs out there discussing why it's dangerous to fork in a multithreaded program, such as https://thorstenball.com/blog/2014/10/13/why-threads-cant-fork/

The gist of it is that when a thread `fork()`'s, *in the child process, all other threads _die instantly._* It doesn't matter what they were doing, they're _gone_.

- If they had locked a mutex, that mutex will never be unlocked again.
- If they were modifying a data structure, that data structure might be invalid.
- If they `malloc()`'ed some memory, that memory might never be deallocated, and `malloc()` may use locks and data structures anyways.
- If a thread was doing useful work, that work will never be complete, because the threads no longer exist.
- You can't join these non-existent threads.
- `pthread_atfork()` is a function meant to solve the problems above, but it's simply incapable of doing what it was meant to do safely, and that's why the POSIX.1 standard explicitly says that this function may be formally deprecated in the next version of the standard. It was a mistake.

So almost the only safe thing to do if you `fork()`'ed from a multi-threaded process is to call `exec()`. That's what `spawn` does.

Because the CUDA runtime uses threads to implement its runtime and asynchronous streams, once the CUDA runtime is initialized, it's not advisable to `fork()`
```",10583
22290,In-place matrix multiplication gives erroneous results.,Setting `out` to overlap with input is highly advised against. It is really unstable.,5872
22291,Cannot load a saved torch.jit.trace using C++'s torch::jit::load,The model is serialized exactly the same way as in the tutorial. ,604
22292,Importing an opencv GpuMat,Fixing in https://github.com/pytorch/pytorch/pull/13982,2472
22293,Better error messaging for requiring ModuleList to be constant,"Need to append the resnet blocks into an array and then initialize the module_list
For example

m = []
m.append(blah)
self.module_list = nn.ModuleList(m)",8643
22294,Build broken with NO_CUDNN = 1 flag,A fix should be landing soon in https://github.com/pytorch/pytorch/pull/8340,2829
22295,out= parameters don't check for aliasing (so inplace broadcast is incorrectly accepted),"Both of the following fail now:
```

In [3]: x.add_(torch.ones(2,2))
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-3-5a1004c71ec3> in <module>
----> 1 x.add_(torch.ones(2,2))

RuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]

In [4]: x = torch.zeros(2)

In [5]: torch.add(x, torch.ones(2,2), out=x)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-c79cf56ad398> in <module>
----> 1 torch.add(x, torch.ones(2,2), out=x)

RuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]

```

",1458
22296,Cosine Embedding Loss,"According to the documentation, y is supposed to be 1 or -1 to indicate whether the two vectors are supposed to be similar or not. Why are  Pass a tensor containing only 1's and -1's.",923
22297,import error,"If you are using conda, add this to your ~/.bashrc
export LD_LIBRARY_PATH=""/home/username/miniconda3/lib:$LD_LIBRARY_PATH""",4262
22298,torch.utils.ffi--â€œcuda.hâ€: No such file or directory,"May specify `include_dirs` of the `Extension` object, making it point to `%CUDA_PATH%/include`.",488
22299,Error when building pytorch under develop mode, what `develop` should do (`build` should be one of its steps).,8411
22300,Linux CentOS pytorch build from source ignores CC env variable,"Same error on CentOS 6 and was able to get pytorch to compile by adding `env` at the beginning like so:  
`env CXX=$(which c++) python setup.py install`  
where `c++` was a compiler in my path that supported c++11.  

It would be nice if the official released binaries were linked against glibc 2.12 if pytorch doesn't depend on anything >2.12. ",5943
22301,[Bug] Segmentation fault when importing sentencepiece (with v0.4.0),"PyTorch binaries are compiled with gcc 4.9.2. However, gcc before and after that version are not incompatible. Hence the segfault you see when pulling them into the same address space. Using a sentencepiece compiled with later gcc will solve the issue.",547
22302,[feature request] add deviceTensor interface to ATen,"This link works better :
https://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments

Is there a rough vision of how this ATen DeviceTensors should look like yet?",5950
22303,PyTorch can't build LongTensor from Numpy Long tensor,"About your point that we need to specify the type of the tensor, we actually have type inference now, so that if you pass
```python
a = torch.tensor([1, 2])  # long tensor
b = torch.tensor([1.])  # float tensor
c = torch.tensor([True])  # byte tensor
```
which is very handy.

But it seems that we are missing support for numpy integer types:
```python
a = torch.tensor(np.float64(1))  # works
b = torch.tensor(np.int64(1))  # fails
```",914
22304,[caffe2] segmentation fault for running 'from caffe2.python import core',"I am installing caffe2 on a debian OS without sudo right, I cannot use `apt install` to install nccl. My solution is to install nccl on my own laptop and copied libnccl.2.x.x to this PC. Now it works well. ",673
22305,torch.where produces confusing error messages on cpu/cuda mismatches,I remember that this was fixed on master. ,3187
22306,[ONNX] Export fails when the same instance of a module appears multiple times in a module list,"using `copy.deepcopy()` over the modules seem to solve the problem:

```python
from copy import deepcopy

self.enc0 = nn.Sequential(deepcopy(self.resnet.conv1), deepcopy(self.resnet.bn1), deepcopy(self.resnet.relu), deepcopy(self.resnet.maxpool))
self.enc1 = deepcopy(self.resnet.layer1)
...
```",11324
22307,[jit] Remove symbolic autodiff's reliance on shape information,"Existing formulas with multiple inputs, e.g., as in convolution double backwards (in `aten/src/ATen/native/Convolution.cpp`) already handle undefined inputs for `ggI`, `ggO`, etc. correctly.

> assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.

You shouldn't assume this if you want double backwards to work :

> To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.

I would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that *any* computation reachable from `AutogradZero` is eligible for zeroing.",7817
22308,"Just updated source tree, build fails."," This started with the recent build system changes. Maybe @orionr can look into this. I also observe in my builds that these warnings are emitted, but the builds proceed fine.",3052
22309,Is a there a function like np.vectorize() that can realize an efficient value mapping?,"Cross post from the forum: https://discuss.pytorch.org/t/how-to-realize-the-function-of-looking-up-table-with-pytorch-in-a-most-efficient-way/70747

We only answer questions on the forum and we use git issues only for bug/features. So we will answer there.",2041
22310,torch.Tensor does not have zeros_like_,"Thank you for your report, we will accept a PR fixing this. ",6802
22311,Error in forward() docs for RNNCell,"We'll either fix the docs or add that functionality. It seems useful for the hidden state to default to 0 in my opinion so I'm leaning towards a solution involving adding the functionality, especially if it used to exist.",6825
22312,Is there a way to specify the -j flag for number of threads used to build the library?,"Hi, you can specify number of parallel build jobs by setting `MAX_JOBS` env variable
 
 this should work: `env MAX_JOBS=4 python setup.py build`",2913
22313,Tensor length mismatch in index_put_ with torch.uint8 indices,"Thank you for your report. This is indeed a bug, but the bug is not what it appears to be. Before bool tensors were supported, uint8 tensors could be used for indexing, but they were interpreted only as boolean mask, not as actual indices values. If your example is rewritten to use uint8 tensor as a boolean mask, it works:
 
 ```
 
 In [24]: import torch 
 
  ...: 
 
  ...: N = 21 
 
  ...: 
 
  ...: dt = torch.int8 
 
  ...: # device = ""cuda:0"" 
 
  ...: device = ""cpu"" 
 
  ...: 
 
  ...: a = torch.ones(N, dtype=dt, device=device) 
 
  ...: indices = torch.ones(N, dtype=torch.uint8, device=device) 
 
  ...: values = torch.ones(N, dtype=dt, device=device) 
 
  ...: 
 
  ...: print(a.shape) 
 
  ...: print(indices.shape) 
 
  ...: print(values.shape) 
 
  ...: 
 
  ...: a.index_put_((indices, ), values, accumulate=True) 
 
  ...: 
 
  ...: print(a) 
 
 ```
 
 but we indeed should fix misinterpretation of uint8 tensor as actual indices values - that should be simply forbidden and error out. Setting high priority because of a crash.",6803
22314,Flaky test test_quantized_rnn on Windows,Thanks for reporting! Temporarily disable the test: https://github.com/pytorch/pytorch/pull/33750,6836
22315,Mypy cannot find torch.optim.AdamW,close this issue as it has been resolved in [#34299](https://github.com/pytorch/pytorch/pull/34299),744
22316,Adam optimizer is using a deprecated add_() signature,"This is fixed in #33428, waiting to be merged.",7651
22317,loss.backward() keeps running for hours,"pytorch version 0.4.1 --->1.2.0, I also encountered the same situation",10525
22318,RFC: JIT C++ source code reorganization,"LGTM.
 
 
 
 Where does Operator-related stuff go? e.g. `register_prim_ops`",4056
22319,Can't save to ostream using documentation example,Would you like to take a look at this?,538
22320,Scalar cuda Tensors are automatically moved to different device,"We should support this in autograd:
 
 
 
 1) For backwards compatibility. There's likely code that depends on this (in the ""forward"" pass).
 
 2) Even before we supported mixed-type operations, we supported mixing types with a scalar. The same logic applies to devices.
 
 
 
 It should be fixable at the place where the error is raised in the autograd engine, without too much work. If the `output_device != metadata.device()` (and the shape is a scalar), just copy to the metadata.device().",8291
22321,Segfault upon calling torch.Tensor on a gpu tensor,similar issue encounter here,10688
22322,[jit] inplace `_construct` RecursiveScriptModule,I think it would if the annotations are preserved when copying these functions from `nn.Module` -> `ScriptModule`. Let me try and see.,3232
22323,torch.blkdiag [A way to create a block-diagonal matrix],"u said it, then we better implement a sparse matrix version and a normal version. I have to say, a sparse `blkdiag` is faster than `blkdiag` of dense matrix, let alone a multiplication on dense matrix...",11276
22324,Device Object doesn't work as JIT annotation in PY3,It's supposed to be `torch.device`,4009
22325,"[libtorch, jit] Unexpected right-hand found in assignment in class body. This is not yet supported.",for Final constant handling,9032
22326,RuntimeError: CUDA error: no kernel image is available for execution on the device,"```bash
 
 git clone https://github.com/pytorch/pytorch.git
 
 cd pytorch
 
 git checkout v1.3.1
 
 git submodule sync
 
 git submodule update --init --recursive
 
 export TORCH_CUDA_ARCH_LIST=""3.5""
 
 python setup.py install
 
 ```",8721
22327,Tutorials should be run in CI,"I think it is too much to run pytorch/example CI on every pytorch/pytorch PR push or merge. What would you think of this strategy:
 
 - Run the CI on pytorch/examples on a daily cron trigger
 
 - If it fails open a new issue in that repo, we could use something like this github action https://github.com/rishabhgupta/git-action-issue.",3231
22328,Memory Leak & Performance Decrease on AMD CPU with PyTorch >1.3,"When cleaning my repos, I did not remember the use case of the repo and deleted it. However, I found a local copy on my machine and uploaded it again at:
 
 https://github.com/milutter/pytorch_debugging",5990
22329,MKLDNN convolution leaks memory when input sizes vary,"Same problem said in https://github.com/pytorch/pytorch/issues/27971 which IDEEP have primitive cache for the performance, you can set **LRU_CACHE_CAPACITY** to a smaller number(the default number is 1024, you can set it to 1) to reduce the memory use.",512
22330,Calling tensor.backward() from within a C++ extension hangs,as a forum responser.,2396
22331,torch.mean keepdim is not working correctly,I think it's `[0]` that removes the first dimension,3233
22332,deterministic sampling,Just take the argmax of your distribution,4033
22333,np.mean breaks when called on a list of cuda tensors,"the error is generated by NumPy, not PyTorch. Think of it as if you're sticking a strange Python object into a NumPy function: the NumPy function goes to look for an attribute but discovers it's not there. Fixing this likely requires changes in NumPy.",581
22334,RuntimeError: block->inputs().size() >= num_initializers ASSERT FAILED,"check if you apply https://github.com/pytorch/pytorch/pull/18139, whether your problem will go away?",683
22335,Multinomial (GPU ONLY) without replacement generates repeated items,"I ran script on my branch of #14957 (which has a ~1 week old version of master, it's a debug build) and it didn't find anything in 10000 seeds (took ~8 hours on my GTX1080Ti).ran it for a while (thanks!), but it seems nothing turned up.
 
 
 
 I'd be happy to look at it, but I'd need help with a reliable repro on master - I don't have many more GPU hours to spare.",5941
22336,Support for Nvidia Tesla T4,"After installing Pytorch with the following instructions (as per https://pytorch.org/):
 
 ```
 
 pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html
 
 ```
 
 
 
 I'm getting this error message:
 
 ```
 
 GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.
 
 The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
 
 ```
 
 Is sm_75 code not included for cuda 10.1? Training still runs, but it seems not to take advantage of the instruction set.",981
22337,enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory,"Thank you for the response. Our Python code is similar to this one:
 
 
 
 https://github.com/LaurentMazare/deep-models/blob/5aa353f88065150b0a024578f4af09d5c4f2d651/cifar-10/train.py
 
 
 
 We used the same way to generate ""model.pt"".
 
 
 
 Btw, when we cmake our testing app, we pointed to the libtorch downloaded (2/4/2019) with the following configurations:
 
 ![screen shot 2019-02-05 at 2 40 48 pm](https://user-images.githubusercontent.com/24870971/52309199-3aae3d80-2954-11e9-85dd-4ad09e76708c.png)",6793
22338,Defined layers by @property to allow better modularity,"I see your point, and I totally agree! Thank you",3192
22339,Implementing Normalizing Flows with TransformedDistribution,Would it make sense to define the TransformModule in PyTorch itself?,8431
22340,Pip upgrade to torch 1.0.1 breaks under Python 2.7 (No module named typing),"`pip install typing` should fix your problem.
 
 
 
 Did we add a runtime dependency on mypy? cc @ezyang",8739
22341,batch mode torch.cholesky does not raise RuntimeError on GPU if not pos. def.,I will take a look at this. Thank you for reporting.,3314
22342,Namedtuples and compatibility for python < 3.2,I am currently reading these lines of tuple checks and report the unexpected behavior right now.,2989
22343,pytorch cpp api the derivative for 'target' is not implemented,"it's still a bug, that we should fix. I believe we shouldn't have to `detach` explicitly, because we dont need to do that in python",9748
22344,cuDNN error: CUDNN_STATUS_INTERNAL_ERROR on cudnn.benchmark = True,We believe we have identified the root cause of this issue and are working on a fix in cuDNN.,8240
22345,[Feature Request] HParams Class of TensorFlow,"That would be a first step to support hparams in tensorboard. This is a really important missing feature.
 
 
 
 <img width=""1225"" alt=""Screen Shot 2019-05-20 at 16 59 07"" src=""https://user-images.githubusercontent.com/8831940/58031415-a9d52980-7b20-11e9-9fa2-7c7f2d1a15cc.png"">
 
 
 
 EDIT: Should I create a feature request for that ?",6892
22346,[jit] Support static lists/dicts as trace inputs,"try annotating your function. 
 
 ```
 
 @torch.jit.script
 
 def dict_test(x: Dict[str, Tensor]):
 
  return x['a']
 
 
 
 dict_input = dict(a=torch.rand(2, 10), b=torch.rand(2, 10))
 
 y = dict_test(dict_input)
 
 ```",720
22347,Unable to cast .item() number to int,This worked! :),7842
22348,Not easy to use JIT from libtorch due to lack of explicit nvrtc dependency,a try/catch `dlopen` is what would solve it on the cient end.,571
22349,Forward attribute access on DataParallel to underlying `module` attribute.,"A shorter one:
 
 
 
 ```python
 
 class DataParallelPassthrough(torch.nn.DataParallel):
 
  def __getattr__(self, name):
 
  try:
 
  return super().__getattr__(name)
 
  except AttributeError:
 
  return getattr(self.module, name)
 
 ```",838
22350,Number of CPU threads for the python process,"In this case, you are seeing multithread usage from OpenMP parallelized CPU kernels, which run on multiple threads. You can control the number of threads with https://pytorch.org/docs/stable/torch.html#torch.set_num_threads although this was broken on some versions of PyTorch. You can also try setting `OMP_NUM_THREADS=1` and `MKL_NUM_THREADS=1` if that doesn't work for you.
 ",3802
22351,cpp api - IntList loses initial values,"No, that's just UB and has nothing to do with the type you use. Basically, `ArrayRef`s don't own the memory (unlike e.g. `std::vector`s). So in your case, you construct an `ArrayRef` that points to the (stack allocated!) initializer list you pass to `a`, which is destructed immediately after the constructor finishes, so you end up using freed memory in `Print()`. Please use `std::vector` if you need to manage lifetime of your memory.",4324
22352,Port `sign` operator from the TH code to Aten,"I started working on this and CPU backend is available there (draft #22861)
 
 
 
 I'll be looking at CUDA later today.",6815
22353,Inconsistent gradients through torch.symeig," I think we will have to modify the implementation of the gradients to make them symmetric. I can send in a PR with this fix tomorrow, if thatâ€™s fine.",586
22354,"RuntimeError 'DivBackward0' nan values in its 0th output, but works when tensors loaded from disk?",Update: Finomnis on Stack Overflow found the issue - `with autograd.detect_anomaly():` was the cause of the problem. Removing that statement allows the model to train unimpeded. So now the question is - why was `autograd.detect_anomaly():` raising an error?,8067
22355,Script freezes with no output when using DistributedDataParallel,"You have timing code in there. It is possible that one process exits before the other one, causing the hang.
 make sure to use timing information from a single process by broadcasting it, instead of having each process take its own measurements. With the snippet you posted it is possible for one process to break out of the loop before others do.",8628
22356,Zero gradients beyond a certain buffer size on CUDA,"particular probably the specialization for ""accum=True"":

 https://github.com/pytorch/pytorch/blob/2ee0f0bc3a8b81e554e9cf7eded598ac7ecf33be/aten/src/ATen/native/cuda/Indexing.cu#L30-L40
 
 The `y` and `z` components of a grid of thread blocks have a maximum value of 65535.
 
 (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications)
",8505
22357,Generated type hints failed with latest mypy version (0.720),Confirmed fixed on master by #22841 (thanks @vishwakftw!),1823
22358,Argmax is not deterministic,"NumPy returns the first occurrence. It might be worth matching their behavior.
 
 
 
 https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html
 
 
 
 ""In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.""",4548
22359,High CPU usage by torch.Tensor,"
 Try : torch.set_num_threads(1)
 
 ```python
 
 import torch
 
 torch.set_num_threads(1)
 
 x = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)
 
 while True:
 
  x.cuda()
 
 ```",478
22360,Build Fails for Raspberry Pi 4 B: libcaffe2.so: undefined reference to `__atomic_xxx_8',"I spent a whole day and resolved this issue by the following steps:
 
 1. sudo apt-get install libatomics-ops-dev
 
 2. Change CMAKE_CXX_FLAGS variable in CMakeLists.txt file (in the main directory). i.e. add line ` set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -latomic"")`
 
 
 
 I also uploaded the torch 1.0.0 .whl file if anyone needs it: https://drive.google.com/file/d/1D3A5YSWiY-EnRWzWbzSqvj4YdY90wuXq/view?usp=sharing",3208
22361,torch.flatten() returns a 0-dim (not 1-dim) tensor for 0-dim tensors,"personal opinion: I think it should return a 1-dimensional tensor. It's essentially turning the range of dimensions (start_dim to end_dim) into a 1-dimensional tensor, and then stacking the dimensions before start_dim and after end_dim onto it.",10406
22362,TorchScript/ONNX unsupported constant kind s,"This was fixed in https://github.com/pytorch/pytorch/pull/27566, which should go out as part of 1.4",7834
22363,Inconsistent Behavior in torch.distributions log_prob for float input in uniform distribution.,"This should be an easy fix. Those lines should be made such that they work with Python floats as well, so replacing `ge` etc with `>=` would be a first step, or wrapping `value` in a tensor if it's a python float.
 
 
 
 https://github.com/pytorch/pytorch/blob/7a99f3987bde3f3508707990680a5f1f06d4c901/torch/distributions/uniform.py#L73-L75",7807
22364,Substraction returns wrong result for bool tensors,"Note that NumPy doesn't allow subtraction for bool:
 
 
 
 ```
 
 TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.
 
 ```
 
 
 
 I'm not sure we should allow it either.",4457
22365,Conv2d ONNX export via TorchScript,Closing this as this issue is fixed in master. Try it out on the latest nightly release and let us know if it works. Feel free to reopen if it's not working and we can follow up. Thanks!,1606
22366,Typing Error in nn.Module,This will get fixed when we move the Generic include inline into module.py... assuming I can fix the JIT bugs in this PR: https://github.com/pytorch/pytorch/pull/38211,7839
22367,DistributedSampler can't shuffle the dataset,you need to call `set_epoch` as here: https://github.com/pytorch/examples/blob/master/imagenet/main.py#L239,619
22368,"Improve gradient stability of logsumexp, softmax, log_softmax, logsigmoid at -inf (replace nan by zero)","At this time we have a workaround like below, it works well on all these three cases,
 
 1. containing only `-float(""inf"")`
 
 2. containing no `-float(""inf"")`
 
 3. containing some `-float(""inf"")`
 
 
 
 ```python
 
 import torch
 
 from torch import Tensor
 
 from torch import jit
 
 
 
 
 
 @jit.script
 
 def logsumexp(x: Tensor, dim: int) -> Tensor:
 
  m, _ = x.max(dim=dim)
 
  mask = m == -float('inf')
 
 
 
  s = (x - m.masked_fill_(mask, 0).unsqueeze(dim=dim)).exp().sum(dim=dim)
 
  return s.masked_fill_(mask, 1).log() + m.masked_fill_(mask, -float('inf'))
 
 
 
 
 
 def check(x: Tensor, fn):
 
  x.grad = None
 
  y = fn(x, dim=-1)
 
  print(f'y => {y.view(-1)}')
 
  y.backward(torch.ones_like(y))
 
  print(f'x.grad => {a.grad.view(-1)}')
 
 
 
 
 
 if __name__ == '__main__':
 
  a = torch.full((2, 2), -float('inf'), requires_grad=True)
 
 
 
  check(a, logsumexp)
 
  check(a, torch.logsumexp)
 
 
 
  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([0., 0., 0., 0.])
 
  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([nan, nan, nan, nan])
 
 
 
  a = torch.randn((2, 2), requires_grad=True)
 
 
 
  check(a, logsumexp)
 
  check(a, torch.logsumexp)
 
 
 
  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])
 
  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])
 
 
 
  a = torch.randn((2, 2))
 
  a[0, 0] = -float('inf')
 
  a.requires_grad_(True)
 
 
 
  check(a, logsumexp)
 
  check(a, torch.logsumexp)
 
 
 
  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])
 
  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)
 
  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])
 
 ```",1355
22369,test_debug_info is flaky,"The error is:
 
 
 
 ```
 
 Jan 03 21:53:38 ......Process process 0:
 
 Jan 03 21:53:38 Traceback (most recent call last):
 
 Jan 03 21:53:38 File ""/opt/python/3.5/lib/python3.5/multiprocessing/process.py"", line 252, in _bootstrap
 
 Jan 03 21:53:38 self.run()
 
 Jan 03 21:53:38 File ""/opt/python/3.5/lib/python3.5/multiprocessing/process.py"", line 93, in run
 
 Jan 03 21:53:38 self._target(*self._args, **self._kwargs)
 
 Jan 03 21:53:38 File ""/var/lib/jenkins/workspace/test/common_distributed.py"", line 198, in _run
 
 Jan 03 21:53:38 getattr(self, test_name)()
 
 Jan 03 21:53:38 File ""/var/lib/jenkins/workspace/test/dist_utils.py"", line 96, in new_test_method
 
 Jan 03 21:53:38 return_value = old_test_method(self, *arg, **kwargs)
 
 Jan 03 21:53:38 File ""/var/lib/jenkins/workspace/test/rpc_test.py"", line 1325, in test_debug_info
 
 Jan 03 21:53:38 self.assertEqual(expected.keys(), info.keys())
 
 Jan 03 21:53:38 File ""/var/lib/jenkins/workspace/test/common_utils.py"", line 867, in assertEqual
 
 Jan 03 21:53:38 allow_inf=allow_inf)
 
 Jan 03 21:53:38 File ""/var/lib/jenkins/workspace/test/common_utils.py"", line 848, in assertEqual
 
 Jan 03 21:53:38 super(TestCase, self).assertEqual(x, y, message)
 
 Jan 03 21:53:38 File ""/opt/python/3.5/lib/python3.5/unittest/case.py"", line 829, in assertEqual
 
 Jan 03 21:53:38 assertion_func(first, second, msg=msg)
 
 Jan 03 21:53:38 File ""/opt/python/3.5/lib/python3.5/unittest/case.py"", line 1202, in assertMultiLineEqual
 
 Jan 03 21:53:38 self.fail(self._formatMessage(msg, standardMsg))
 
 Jan 03 21:53:38 File ""/opt/python/3.5/lib/python3.5/unittest/case.py"", line 670, in fail
 
 Jan 03 21:53:38 raise self.failureException(msg)
 
 Jan 03 21:53:38 AssertionError: 'num_pending_users' != 'thread_pool_size'
 
 Jan 03 21:53:38 - num_pending_users
 
 Jan 03 21:53:38 + thread_pool_size
 
 ```
 
 
 
 Seems to be related to adding `num_pending_users` in https://github.com/pytorch/pytorch/pull/31539? Although not sure why the test does not fail every time. Will investigate.",7046
22370,The histogram is empty,"Ah my bad, probably because I trained without calling `net.train()`, then try writes to histogram.
 
 ````python
 
 import torch
 
 from torch.utils.tensorboard import SummaryWriter
 
 
 
 writer = SummaryWriter(log_dir='runs/temp')
 
 net = torch.hub.load('RF5/danbooru-pretrained', 'resnet50')
 
 criterion = torch.nn.CrossEntropyLoss()
 
 optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
 
 
 
 labels = torch.randint(10, size=(2,))
 
 for i in range(5):
 
  # net.train() <--- uncomment this line then no error
 
  print(i)
 
  optimizer.zero_grad()
 
  output = net(torch.randn(2, 3, 1, 1))
 
  loss = criterion(output, labels)
 
  loss.backward()
 
  optimizer.step()
 
 
 
  net.eval() # <--- or comment this line then no error as well
 
  for name, w in net.named_parameters():
 
  writer.add_histogram(name, w, i)
 
 ````",992
22371,About learning rate scheduler,"Please update to the latest version of PyTorch.
 
 
 
 As discussed in #26423, `get_lr` was not meant to be the appropriate way of getting the learning rate. A warning is raised in #31125. 
 
 
 
 The correct way is `scheduler.get_last_lr` or `optimizer.param_groups[0][""lr""]`.
 
 
 
 ```
 
 optimizer = optim.SGD(net.parameters(), lr=0.1)
 
 scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
 
 
 
 for i in range(15):
 
  lr = scheduler.get_last_lr()[0]
 
  lr1 = optimizer.param_groups[0][""lr""]
 
  print(i, lr, lr1)
 
  scheduler.step()
 
 ```
 
 
 
 I'll close this PR, but please feel free to re-open if the problem persists.",4861
22372,A pattern fuse or pattern replace tool for quantization.,"@parvizp I just re-enables it few days ago: https://github.com/pytorch/pytorch/pull/29220
 
 Ideally we should actually fuse ConvBn in the aten function level, but that requires more refactoring and cleanup, that's why we are doing it in the module level right now.
 
 It is ready for common vision models as long as you can script them.",665
22373,Third party PyTorch models may execute arbitrary code during deserialization,One option that might be worth considering is using https://docs.python.org/3/library/pickle.html#restricting-globals to try to make the unpickling more safe. (With perhaps an additional flag to remove those protections).,4658
22374,PyTorch serialization formats," little to say about the exact details of how we are version testing, or what the variants of the versions should be named (aligning them with PyTorch releases sounds reasonable, whether or not the team is going to commit to accurately reporting versions on the file format going forward, and if so, what mechanisms we can put in place to make sure that we update it when we make changes to the format (since it seems the lightweight mechanism of code review isn't working). A simple stopgap is to have it report the version of PyTorch which exported the model...",568
22375,INT tensor with gradients,"It seems that the quantized tensor is mainly **for inference** and the quantization-aware training utilizes fake quant and the data flow with gradient is still in float format. The quantized tensor does not support autograd and requres_grad (The following code snippet still raises ""**only Tensors of floating point dtype can require gradients**""). 
 
 ```
 
 int_tensor = torch.randint(0, 10, size=((4,4,3,3)), dtype=torch.uint8)
 
 scale, zero_point = 1e-1, 0
 
 q = torch._make_per_tensor_quantized_tensor(int_tensor, scale, zero_point)
 
 q.requires_grad = True
 
 ```
 
 
 
 However, we want to directly use INT8 tensor with gradients rather than fake quant.
 
 Then we can accelerate the forward process and save memory consumption **for training**.
 
 I wonder that if it is possible to support autograd and requires_grad for INT tensor.
 
 
 
 Thanks!
 
 
 
 @soumith",3974
22376,"Segmentation fault in ""torch::autograd::Engine::evaluate_function"" in 1.3.1","Reading the code, I think this is just a simple data race. Patch coming.",5103
22377,Output of batch doesn't contain the output of a subet of a batch,"In the future, please ask questions on our forums, discuss.pytorch.org. We like to keep our issue tracker filled with issues and feature requests.",3776
22378,[feature request] Set limit on GPU memory use,"Wanted to support this feature request as well. 
 
 
 
 For context, this could be used with the work done by Alibaba for kubernetes (See [Kubernetes #52757](https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-464645334)). While there is probably a significant speed/latency penalty, it may still be beneficial for certain tasks.",8137
22379,[jit] Python built-in function support,are pull requests welcome for this issue? I am a first time contributor so dont know,8878
22380,distributed.all_gather function stuck when using NCCL backend,"
 1. Before init the process group, call `torch.cuda.set_device(args.rank)` to assign different GPUs to different processes. 
 
 2. Change `tmp = [t.randn(5).cuda()] * 2` to `tmp = [t.randn(5).cuda() for _ in range(2)]`, otherwise you are using the same tensor to receive two outputs. 
 
 
 
 Does this solve your problem?",628
22381,<torch/torch.h> should be <torch/script.h>,"You want `torch/extension.h` for cpp extensions; `torch/torch.h` is valid for regular C++ use of Torch. `torch/script.h` is if you specifically want TorchScript functionality. I'm not sure about your include path question.
 
 
 
 You do have a legitimate bug report, which is that we are recommending `torch/torch.h` when it is inappropriate. I refiled your issue in tutorials https://github.com/pytorch/tutorials/issues/473 I verified the two occurrences of this include in PyTorch repo are proper:
 
 
 
 ```
 
 docs/cpp/source/installing.rst
 
 47: #include <torch/torch.h>
 
 56:of the PyTorch C++ API, including `torch/torch.h` is the most sure-proof way of
 
 
 
 docs/cpp/source/frontend.rst
 
 42: #include <torch/torch.h>
 
 ```",507
22382,Magic value not found when loading traced model,"```
 
 root@b54eacad8078:/data4/zjf/code# python collect_env.py
 
 Collecting environment information...
 
 PyTorch version: 1.0.0a0+4ec6bd7
 
 Is debug build: No
 
 CUDA used to build PyTorch: 9.1.85
 
 
 
 OS: Ubuntu 16.04.6 LTS
 
 GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
 
 CMake version: version 3.5.1
 
 
 
 Python version: 3.6
 
 Is CUDA available: Yes
 
 CUDA runtime version: 9.1.85
 
 GPU models and configuration:
 
 GPU 0: Tesla P40
 
 GPU 1: GeForce GTX 1080 Ti
 
 GPU 2: GeForce GTX 1080 Ti
 
 GPU 3: GeForce GTX 1080 Ti
 
 GPU 4: Tesla P40
 
 
 
 Nvidia driver version: 390.25
 
 cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2
 
 
 
 Versions of relevant libraries:
 
 [pip3] msgpack-numpy==0.4.3.1
 
 [pip3] numpy==1.15.1
 
 [pip3] torch==1.0.0a0+4ec6bd7
 
 [pip3] torchfile==0.1.0
 
 [pip3] torchtext==0.3.1
 
 [pip3] torchvision==0.2.1
 
 [pip3] torchvision-nightly==0.2.1
 
 [conda] Could not collect
 
 ```",8716
22383,"""dim"" argument for .unfold method doesn't work",Related to https://github.com/pytorch/pytorch/issues/8698,5132
22384,[CUDA] torch.sum over smaller inner dimension is 4x slower than it should be,please feel free to unassign yourself or deprioritize as necessary.,560
22385,Wrong derivative backpropagating through Cholesky factorization?,"Upon further investigation, I think that the derivative is correct, but misleading.
 
 
 
 As it is implemented, `mat.grad` produced by `torch.cholesky` is lower triangular. This is because `mat.grad[i, j]` is assigned the gradient for `mat[i, j] = mat[j, i]`.
 
 
 
 Take the PD matrix:
 
 ```python
 
 mat = torch.tensor([
 
  [3, 1, 0.5, 0],
 
  [1, 3, -1, 1],
 
  [0.5, -1, 4, 2],
 
  [0, 1, 2, 4]
 
 ])
 
 ```
 
 
 
 If you construct `mat` in the following way:
 
 ```python
 
 off_diag = torch.tensor([
 
  [0.0000, 0.0000, 0.0000, 0.0000],
 
  [ 1.0000, 0.0000, 0.0000, 0.0000],
 
  [ 0.5000, -1.0000, 0.0000, 0.0000],
 
  [ 0.0000, 1.0000, 2.0000, 0.0000]
 
 ], requires_grad=True)
 
 diag = torch.tensor([3, 3, 4, 4], requires_grad=True)
 
 mat = off_diag + off_diag.t() + diag.diag()
 
 ```
 
 
 
 then the gradients of `off_diag` and `diag` be the same for Cholesky solves as well as direct inversion solves.",8071
22386,Inconsistent recovery from CUDA OOMs,"This is quite serious and should be dealt with before the next PyTorch release. It appears to be a leak in the autograd engine -- note that if you remove the `loss.backward()` line in @stephenroller's test case all the tests pass (except for ddp_multi which has a different error that also should be fixed).
 
 
 
 Basically, if you're lucky and the OOM happens during forward it's recoverable. If it happens during backward() it leaks memory and is effectively unrecoverable. Unclear if this is a regression or also a bug in older versions of PyTorch.
 
 
 
 I would suspect that some tasks live on in some of the Engine state. (Are the ready_queues empty after an error?)",7678
22387,matmul uses too much memory in some batched cases,"The bug is still there for A.ndim is 3 and B.ndim is 3
 
 please run the code below with version 1.8.1
 
 ```
 
 x = torch.randn(1, 4096, 4096)
 
 y = torch.randn(192, 4096, 1)
 
 z1 = torch.bmm(x.expand(192, 4096, 4096), y) # it works
 
 z2 = torch.matmul(x, y) # out of memory",6982
22388,cannot import name '_update_worker_pids' from 'torch._C',"You should remove your previous install first. Try 
 
 ```sh
 
 conda remove pytorch torchvision -y
 
 pip uninstall torch -y
 
 pip uninstall torch -y # yes twice
 
 conda install pytorch torchvision -c pytorch
 
 ```",8655
22389,Using F.affine_grid caused cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (PyTorch version: 1.0.1.post2),"i use pytorch 1.0.1.post2, the same problem.",9565
22390,addmv performance 2.8x worse for non-square matrix,I am able to reproduce the results on my MBP. Will take a look and try to understand what's going on.,2985
22391,multinomial performance regressed 2x from 0.4.1 to 1.0.1,closed via #17121,9087
22392,Autograd custom functions do not work with keyword arguments,"You can do:
 
 
 
 ```python
 
 # You can move the default value declaration here and remove it from the Function.
 
 def my_sin(input, factor=1):
 
  return MySin.apply(input, factor)
 
 
 
 # Or if you prefer to keep the default value in the Function.
 
 def my_sin(input, factor=None):
 
  if factor is None:
 
  return MySin.apply(input)
 
  else:
 
  return MySin.apply(input, factor)
 
 ```",8576
22393,"Full-range random_() generation broken for cuda.IntTensor, cuda.LongTensor and LongTensor.","```
 
 In [1]: import torch
 
 
 
 In [2]: ""{0:b}"".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])
 
 Out[2]: '111111111111111111111111111111111111111111111111111111111111111'
 
 
 
 In [3]: ""{0:b}"".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])
 
 Out[3]: '111111111111111111111111111111111111111111111111111111111111111'
 
 ```",8715
22394,Building test_cpp_extensions fails for macOS + CUDA,"I just tested this on Linux, and yes, `ccache` does seem to cause it.",3155
22395,cpp_extension._is_binary_build() is not reliable,"1. I've stopped relying on `_is_binary_build` for CXX ABI flags, so hopefully it shouldn't matter anymore for cpp extensions purposes.
 
 
 
 2. 1.1.0 tag has `1.1.0` as the version, no git hash.",97
22396,"namespaces torch.cuda, torch.device missing from pyi","I am using `torch==1.0.1.post2` and `Pycharm professional 2019.1`. I tested and sorted out the situation mentioned by everyone in front.
 
 
 
 #### can't find reference
 
 
 
 * `torch.cat`
 
 * `torch.clamp`
 
 * `torch.exp`
 
 * `torch.FloatTensor` **deprecated**
 
 * `torch.from_numpy` **deprecated**
 
 * `torch.log`
 
 * `torch.LongTensor` **deprecated**
 
 * `torch.max`
 
 * `torch.min`
 
 * `torch.ones`
 
 * `torch.randn`
 
 * `torch.reshape`
 
 * `torch.sum`
 
 * `torch.zeros`
 
 
 
 #### not callable
 
 
 
 * `torch.tensor`
 
 * `torch.as_tensor`",2997
22397,Batchnormalization fails with CUDA on very large batches,"https://github.com/pytorch/pytorch/blob/f8d4a14f6dc7c709c86b8bb25ccaaa486470e4a7/aten/src/ATen/native/cuda/Normalization.cuh#L443
 
 
 
 This limiter on the grid dimension might exceeds the maximum 65535 on y/z dimension.
 
 Should be a one liner fix. Let me get that.",9538
22398,Tensor class should implement __contains__,You may also need `__reversed__` in newer versions of Python,8632
22399,cuDNN version mismatch (again)," I installed a lower version of cudnn and it worked in my case
 
 try conda install cudnn=7.1.2",716
22400,Generate CircleCI config.yml from a script,Sounds reasonable. It's really funny how you always end up writing a yaml generator for this kind of thing lol,5999
22401,Previous dll loading tricks don't work anymore with newer Anaconda Python on Windows," We'd like to actually not require this change on your side, if we can avoid it. It's already gone in, and that's fine, but we want to un-break the existing packages if we can. We've already done a lot of work to re-sync PATH changes in our latest efforts on that python patch (https://github.com/AnacondaRecipes/python-feedstock/blob/master/recipe/0020-Add-CondaEcosystemModifyDllSearchPath.patch). For some reason, cuda detection is still not working correctly. Are there any specific libraries or paths from which loads happen that we should watch out for? We generally use procmon to debug this stuff, and knowing the name of a library that should be loaded can dramatically reduce the amount of stuff that we need to look through.",2895
22402,[JIT] terminate called after throwing an instance of 'torch::jit::script::ErrorReport',"I am using C++ version of torch from here ```https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip```
 
 
 
 Pytorch version is from master branch.",2996
22403,Allow DataParallel to wrap CPU modules,"> Creating a model on CPU and then wrapping the model with `DataParallel` should automatically replicate the model on destination GPUs. Are there any reason to enforce that `DataParallel`'s input model must be on GPU?
 
 
 
 This is not true. The model is broadcast at the beginning of each forward, not when constructing the `DataParallel` wrapper. The disadvantage of having the model on CPU, of course, is that the gradients are reduced to **CPU** at each iteration, which is slow and undesirable. IMO, automatically convert to one GPU upon construction is also not desirable because:
 
 1. Users may save a pointer to the wrapped module and reasonably expect it to still be on the original device.
 
 2. It can initialize a CUDA context, which isn't obvious to users that it should.",214
22404,Saved PyTorch network does not load in libtorch C++,"you must've missed it in the tutorials, `torch.save` saves in python pickle format, it cannot be loaded into `libtorch`. `model.save` of a torch.jit.ScriptModule object can save something in a format that libtorch can loadl",11458
22405,"drop_last (bool, optional) - more options needed","I have following order in my head: idea, notion, concept, definition. Idea can be dumb, notion can be whimsy, concept sounds more serious, definition is precise. We don't even have to agree to continue main discussion.
 
 
 
 1. I finally get why you said 'if I want to discard the notion of epoch'. Maybe I could forget about epoch in order to not lose any data.
 
 2. If iterator knows in advance amount of all samples, it can plan accordingly. If you say iterator does not know amount of all samples in advance, that is only for the first epoch. In all following epochs it can plan accordingly.",3134
22406,Numpy-Like RandomState object,"torch.Generator() is now documented here: https://pytorch.org/docs/master/torch.html#generators and should behave like numpy `RandomState` now. Following is an example usage. @petered Please feel free to document here if there is something we have missed, otherwise we can close this issue.
 
 
 
 ```
 
 >>> import torch
 
 >>> a = torch.Generator(device='cuda')
 
 >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)
 
 >>> b
 
 tensor([0.3931, 0.5116, 0.8851, 0.4404, 0.8998, 0.1218, 0.5304, 0.3333, 0.2389,
 
  0.5041], device='cuda:0')
 
 >>> a = torch.Generator()
 
 >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)
 
 Traceback (most recent call last):
 
  File ""<stdin>"", line 1, in <module>
 
 RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
 
 
 
 >>> a = torch.Generator(device='cuda')
 
 >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)
 
 tensor([-0.3292], device='cuda:0')
 
 
 
 >>> a = torch.Generator()
 
 >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)
 
 Traceback (most recent call last):
 
  File ""<stdin>"", line 1, in <module>
 
 RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
 
 
 
 ```",10952
22407,libtorch elevated memory usage.,"equivalent to `with torch.no_grad()` in C++ is `torch::NoGradGuard no_grad`. For example:
 
 Python:
 
 ```python
 
 with torch.no_grad():
 
  module.weight += 1
 
 ```
 
 C++:
 
 ```cpp
 
 {
 
  torch::NoGradGuard no_grad;
 
  module->weight += 1;
 
 } // Note that anything out of this scope will still record gradients
 
 ``` 
 
 Please try it out and let me know if it resolves the memory usage issue.",565
22408,[JIT] Scripting arguments for call are not valid,"Hi, is there an update on this issue? I also ran into this error. Here's my code snippet
 
 ```
 
 import torch
 
 from torch import nn
 
 class CrossScale(torch.jit.ScriptModule):
 
  __constants__ = ['xc']
 
 
 
  def __init__(self):
 
  super(CrossScale, self).__init__()
 
  self.xc = nn.ModuleList((nn.Conv2d(2, 2, 2, bias=True), nn.Upsample(scale_factor=2, mode='bilinear')))
 
 
 
  @torch.jit.script_method
 
  def forward(self, x):
 
  cols = []
 
  i = 0
 
  for xc in self.xc:
 
  out = xc(x[i])
 
  i += 1
 
  cols.append(out)
 
  return cols
 
 
 
 
 
 if __name__ == ""__main__"":
 
  cs = CrossScale()
 
 ```
 
 Changing `scale_factor=2` to `scale_factor=float(2)` fixes the issue.",2906
22409,[Running on windows 10] cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87,"OK, I did some extra tests, and it seems that it is some weird behavior **only when running on an interactive shell**. Here's what I have done (step-by-step)
 
 
 
 1. Prepare a simple file with the example:
 
 ```
 
 > type torch_test.ipy
 
 import torch
 
 print(""torch.cuda.is_available() ="", torch.cuda.is_available())
 
 print(""torch.cuda.device_count() ="", torch.cuda.device_count())
 
 print(""torch.cuda.device('cuda') ="", torch.cuda.device('cuda'))
 
 print(""torch.cuda.current_device() ="", torch.cuda.current_device())
 
 ```
 
 
 
 I can run this file with either `Python `or `iPython`, and it all works fine:
 
 ```
 
 > python torch_test.ipy
 
 torch.cuda.is_available() = True
 
 torch.cuda.device_count() = 1
 
 torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000021B331A0160>
 
 torch.cuda.current_device() = 0
 
 
 
 > ipython torch_test.ipy
 
 torch.cuda.is_available() = True
 
 torch.cuda.device_count() = 1
 
 torch.cuda.device('cuda') = <torch.cuda.device object at 0x000002B39C1FD390>
 
 torch.cuda.current_device() = 0
 
 ```
 
 
 
 Now, if I try to use _**exactly the same**_ commands in an _**interactive**_ shell, I get the error:
 
 
 
 With python:
 
 ```
 
 >python
 
 Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
 
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 
 >>> import torch
 
 >>> print(""torch.cuda.is_available() ="", torch.cuda.is_available())
 
 torch.cuda.is_available() = True
 
 >>> print(""torch.cuda.device_count() ="", torch.cuda.device_count())
 
 torch.cuda.device_count() = 1
 
 >>> print(""torch.cuda.device('cuda') ="", torch.cuda.device('cuda'))
 
 torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000028CBD034198>
 
 >>> print(""torch.cuda.current_device() ="", torch.cuda.current_device())
 
 THCudaCheck FAIL file=..\aten\src\THC\THCGeneral.cpp line=87 error=30 : unknown error
 
 Traceback (most recent call last):
 
  File ""<stdin>"", line 1, in <module>
 
  File ""C:\Anaconda3\lib\site-packages\torch\cuda\__init__.py"", line 341, in current_device
 
  _lazy_init()
 
  File ""C:\Anaconda3\lib\site-packages\torch\cuda\__init__.py"", line 162, in _lazy_init
 
  torch._C._cuda_init()
 
 RuntimeError: cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87
 
 >>> ^Z
 
 ```
 
 
 
 or with ipython:
 
 ```
 
 >ipython
 
 Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
 
 Type 'copyright', 'credits' or 'license' for more information
 
 IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.
 
 
 
 In [1]: import torch
 
 
 
 In [2]: print(""torch.cuda.is_available() ="", torch.cuda.is_available())
 
 torch.cuda.is_available() = True
 
 
 
 In [3]: print(""torch.cuda.device_count() ="", torch.cuda.device_count())
 
 torch.cuda.device_count() = 1
 
 
 
 In [4]: print(""torch.cuda.device('cuda') ="", torch.cuda.device('cuda'))
 
 torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000018A068007F0>
 
 
 
 In [5]: print(""torch.cuda.current_device() ="", torch.cuda.current_device())
 
 THCudaCheck FAIL file=..\aten\src\THC\THCGeneral.cpp line=87 error=30 : unknown error
 
 ---------------------------------------------------------------------------
 
 RuntimeError Traceback (most recent call last)
 
 <ipython-input-5-f8c552eb6277> in <module>
 
 ----> 1 print(""torch.cuda.current_device() ="", torch.cuda.current_device())
 
 
 
 C:\Anaconda3\lib\site-packages\torch\cuda\__init__.py in current_device()
 
  339 def current_device():
 
  340 r""""""Returns the index of a currently selected device.""""""
 
 --> 341 _lazy_init()
 
  342 return torch._C._cuda_getDevice()
 
  343
 
 
 
 C:\Anaconda3\lib\site-packages\torch\cuda\__init__.py in _lazy_init()
 
  160 ""Cannot re-initialize CUDA in forked subprocess. "" + msg)
 
  161 _check_driver()
 
 --> 162 torch._C._cuda_init()
 
  163 _cudart = _load_cudart()
 
  164 _cudart.cudaGetErrorName.restype = ctypes.c_char_p
 
 
 
 RuntimeError: cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87
 
 
 
 In [6]:
 
 ```
 
 
 
 Any hints?",4553
22410,Namedtuples prune the tensor representations too much,@zasdfgbnm Could we overwrite that?,750
22411,clamp makes grad None,"The reason why in your first snippet the grad is `None` is because you are modifying the leaf variable `a` and you overwrite it with the result of `a.clamp`. This means that `a` now is an intermediate variable, and the gradient gets freed once it's not needed anymore.
 
 You should instead do:
 
 ```python
 
 a = torch.rand(1, requires_grad=True)
 
 b = torch.clamp(a, 0, 1)
 
 z = 2 * b
 
 z.backward()
 
 print(a.requires_grad) # True
 
 print(a.grad) # tensor([ 2.])
 
 ```",7250
22412,torch.clamp kills gradients at the border,"if all values were random the probability would be small, but that's not the case generally. The [Equilibrium propagation](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full#note1) algorithm by Bengio et al. initializes states to zero which are clamped to 0 and 1 and updated based on gradients of an energy function. It seems to rely on the gradient being 1 when states are 0 to update properly. I will try the suggestion by @zou3519. Thanks.",515
22413,multinomial with inf fails in cuda,"I actually think that the CUDA behavior is reasonable, we should raise an error in the CPU case as well. You can't have a probability distribution where one of the elements has an infinite probability, so there's nothing we can do.

 Sorry for the long debugging, unfortunately getting meaningful errors from CUDA kernels is a bit hard. In the future you might want to use `CUDA_LAUNCH_BLOCKING=1` which should report the error in the same operation that caused it.",2980
22414,PyTorch 0.4 hangs with nn.DataParallel but PyTorch 0.3.1 does not,"who knows all NCCL2 caveats
 
who knows everything about NVIDIA-things",9046
22415,Batchnorm's running_var is wrong(I test it use momentum = 1),"Anyway, `numpy.std` and `numpy.var` divide sum of square by `n` (see [numpy document](https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html)), 
 
 otoh, pytorch divide sum of square by `n-1` ([source](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L54-L58)).
 
 So, there should be some differences.",1151
22416,[pytorch 0.4.0] Cuda runtime error (30) after upgrading from 0.3.1, I came across the same problem after upgrading from 0.3.1 to 0.4. Do you solve this problem now ?,598
22417,PyTorch RNN Tutorials don't work on Windows,https://github.com/pytorch/tutorials/pull/232 will fix this.,9545
22418,torch.stft output size is not right.,"librosa by default has `center=True` which pads the signal so that returned frequency at index `i` is for signal **centered** at index `i * hop`. So return size = `ceil(224960 / 275) = 819`.
 
 
 
 PyTorch has returned frequency at index `i` beginning at index `i * hop`. So return size = `ceil((224960 - 1102) / 275) = 815`.
 
 
 
 I believe if you set `center=False` for librosa or pad PyTorch tensor properly you will get same outputs.",9789
22419,[bug report] (Pytorch 0.4.0) Dropout layer error when input is longtensor with 0,"I had a similar error, but it was because I was accidentally feeding a `2D` tensor to the `Dropout2D()` function.
 
 
 
 In my code, the shape of each `batch` is this: `sequence_len x batch_size x embedding_size`. 
 
 
 
 You seem to be making the same mistake I did by feeding a `tensor` of `sequence_len x batch_size` where the values are indexes to rows in your embedding matrix. Once you ""lookup"" the indexes and get a shape of `sequenc_len x batch_size x embedding_size`, it should work.
 
 
 
 If you want to do the dropout *before* looking up embeddings, then you just need `Dropout[1D]()`.",748
22420,module 'torch' has no attribute 'no_grad',The tutorial is based on PyTorch 0.4 which introduces torch.no_grad. You can consider updating your PyTorch installation.,7346
22421,can't reproduce results even set all random seeds,"Here are some points to check:
 
 * https://pytorch.org/docs/stable/notes/randomness.html : Pytorch is not reproducible between CPU and GPU, between different platforms, and different commits.
 
 * Call this before running any of your functions: (you can put it in the beginning of your main code, right after importing your modules. If you do not use a module among these in your main code, it is ok, just import it and seed it so when you import it a second time, Python will find that it has already been imported so there will be nothing to do. Example: `torch`):
 
 ```python
 
  torch.manual_seed(seed)
 
  torch.cuda.manual_seed(seed)
 
  torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
 
  np.random.seed(seed) # Numpy module.
 
  random.seed(seed) # Python random module.
 
  torch.manual_seed(seed)
 
  torch.backends.cudnn.benchmark = False
 
  torch.backends.cudnn.deterministic = True
 
 ```
 
 * Following the above, you need to seed EVERY external module (outside Pytorch) that may introduce randomness in your entire code.
 
 * You need to set the init function of the worker(s) to be fed to the `DataLoader`:
 
 ```python
 
 def _init_fn(worker_id):
 
  np.random.seed(int(seed)
 
 ```
 
 Make sure that your dataloader loads samples in the same order every call. If you do cropping, or other preprocessing steps, make sure that they are deterministic. As I remember, such modules can provide a deterministic result (in Pytorch 1.0.0 or even before this version).
 
 * I learned this recently, despite it was written in Pytoch doc the whole time!!!!: Unfortunately, some Pytorch modules follow non-deterministic behavior; and most of the time you can not get rid of it by following the above steps. Example: the BACKWARD of upsampling and interpolation functionals/classes is non-deterministic (see [here](https://discuss.pytorch.org/t/non-deterministic-behavior-of-pytorch-upsample-interpolate/42842?u=sbelharbi)). This means, if you use such modules in the training graph, you will never obtain a deterministic results no matter what you do. `torch.nn.ConvTranspose2d` is not deterministic unless you set `torch.backends.cudnn.deterministic = True` (they said `you can try to make the operation deterministic ... by setting torch.backends.cudnn.deterministic = True`. So I am not sure if by doing so, you certainly obtain a deterministic result). So, make sure you are not using any of the non-deterministic Pytorch modules.
 
 
 
 Print your intermediate results to find out from where the non-deterministic behavior comes from: loaded samples, crops, losses, ...
 
 
 
 In my experience, following the above steps lead to exactly same results.
 
 If it is not your case, you may be using an external library that is a source of randomness, or you are using a non-deterministic Pytorch module (or something else that I am not aware of it). If reproducibility is important for you (and I assume it is), you can check step by step your code to trace the source of randomness (run your code gradually, and inspect the results at every step). Make sure that the part of your code that you think it is deterministic is indeed deterministic.
 
 Let me know how it goes, and please, let me know what was the source of randomness in your case. I am curious. Thanks!",2774
22422,[BUG Report] torch.from_numpy(),"@JimLee1996 Yes, I know. When I say that I can print I meant that I can print it quickly. I just manually patched #6863 in 0.4 and verified that it works. So this is fixed.",491
22423,import error: from torch._C import * RuntimeError: stoi,"I had the same error. I fixed it using: system('env -i /usr/bin/python3 -c ""import torch""').
 
 Some environment variable set by matlab is messing with pytorch, but I don't know which.",3122
22424,Pytorch build finds local mkl instead of conda version,think a simpler way might be to simply `export CMAKE_PREFIX_PATH=...` before running the script.,642
22425,Multi-GPU autograd error with Pytorch 0.4, This is very helpful. We'll look into it.,577
22426,Crash when creating a tensor from a tensor on a different device,Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes.,533
22427,"IndexError: Dimension out of range (expected to be in range of [-3, 2], but got -4)","It's this line https://github.com/pytorch/pytorch/blob/82d58ed484eb04f894475de0551055f7e070f481/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu#L361 introduced by #34519.can accept 3d tensors, in this case batch dimension is implicitly assumed to be 1. For convenience, a slightly shorter repro
 
 ```
 
 import torch
 
 
 
 device = ""cuda""
 
 
 
 prev_layer = torch.randn(1, 84, 84, device=device)
 
 prev_layer = torch.nn.functional.max_pool2d(prev_layer, kernel_size=(3, 3))
 
 torch.cuda.synchronize()
 
 ```",4011
22428,Add moveaxis function,"single dim version
 
 
 
 ```py
 
 def _moveaxis(tensor: torch.Tensor, source: int, destination: int) -> torch.Tensor:
 
  dim = tensor.dim()
 
  perm = list(range(dim))
 
  if destination < 0:
 
  destination += dim
 
  perm.pop(source)
 
  perm.insert(destination, source)
 
  return tensor.permute(*perm)
 
 ```",10691
22429,C++ Adam optimizer,"I don't think the changed _adam_ implementation is to blame here.
 
 I was tripping over the new betas option specification (from two separate scalars to a 2-element tuple), so ended up with a misspecified set of options.
 
 I'm seeing similar convergence now on some simple GANs & conditional GANs.
 
 
 
 I'll try to submit a PR to fix the check messages later this evening.
 
 Thanks",3061
22430,[RFC] Async User Function for RPC,"> Currently, every RPC request occupies an RPC thread on the server side until done.
 
 
 
 fwiw, for async TorchScript that uses jit.fork/wait underneath, this was actually addressed last week, but plumbing through interpreter runAsync() call into the request_callback_impl handler.
 
 
 
 Definitely true about pure Python though.",215
22431,[question] Is amp CUDA-specific?,"Right now Amp is Cuda-specific. The FP32 and FP16 oplists are based on the hardware capabilities of Volta and Turing GPUs and the properties of Pytorch's own Cuda kernels.
 
 
 
 For different precisions/backends, I expect you could use the same Python-side API, but you'd probably need different sets of backend wrappers: The ""Smaller/Faster Type"" and FP32 oplists would need to be different, because the hardware capabilities and kernel implementations would be different.
 
 
 
 bfloat16 has (almost) the same dynamic range as FP32, so it shouldn't require gradient scaling, but definitely needs the optimizer to act on FP32 weights to capture small updates. An autocasting-style API where model leaves remain in default precision, and tensors are casted as they enter certain functions, is one way to ensure FP32 weight updates.",5706
22432,len(DataLoader) doesn't take into account DataLoader.batch_size when using IterableDataset,"Thanks for reporting! Yes, this is a bug and should be fixed.",6839
22433,Calling onnx export hangs using multiprocessing," call `torch.set_num_threads(1)` before multiprocessing is started
 
 
 
 ```bash
 
 python torch_export_bug.py
 
 Threads before: 4
 
 Threads after: 1
 
 [+] Start
 
 [+] Got model
 
 [+] Starting process
 
 [+] Waiting process
 
  Getting model inside proc
 
  Got model inside proc
 
 [+] End
 
 ```
 
 
 
 Another option is `export OMP_NUM_THREADS=1` on your Linux terminal",637
22434,Pruning doesn't affect speed nor memory usage,"As far as I know, for memory concerns, you can use `.to_sparse()`.
 
 Example:
 
 ```python
 
 import torch
 
 import torch.nn.utils.prune as prune
 
 
 
 t = torch.randn(1000, 1000)
 
 torch.save(t, 'original.pth')
 
 
 
 p = prune.L1Unstructured(amount=0.99)
 
 pruned_t = p.prune(t)
 
 torch.save(pruned_t, 'pruned.pth')
 
 
 
 sparse_t = pruned_t.to_sparse()
 
 torch.save(sparse_t, 'sparse.pth')
 
 ```
 
 Then `ls -lht *.pth` should return:
 
 ```
 
 196K sparse.pth
 
 3.9M pruned.pth
 
 3.9M original.pth
 
 ```
 
 
 
 This only works if you prune your tensor by a large enough amount that representing it in coordinate space is actually memory efficient.
 
 
 
 You can look at `torch.sparse` to see what ops are then supported on these sparse tensors. Note that this is marked as experimental and the API might change in the future: https://pytorch.org/docs/stable/sparse.html
 
 
 
 I'm not sure about the computational speed-up concerns... @bwasti, maybe?",1300
22435,DistributedDataParallel don't work at nightly build(1.6.0.dev20200408+cu101),cc @osalpekar,9040
22436,nn.Dropout layer TypeError: â€˜<â€™ not supported between instances of â€˜Dropoutâ€™ and â€˜intâ€™,"This makes sense since you are doing this:
 
 ```python
 
 self.embedding_dropout = nn.Dropout(p = self.embedding_dropout)
 
 ```
 
 
 
 If you assign a different name to the `p` argument, you should be fine.",7719
22437,Extra whitespace issue when printing complex tensors,"I will work on this issue, but in the meantime, if there are any other suggestions on any additional changes we'd like to see in the expected print behavior, I would love your input!",3316
22438,"NCCL Connection Failed Using PyTorch Distributed with Error ProcessGroupNCCL.cpp:290, unhandled system error","> > Thanks for your help! I found that this problem is caused by the network connection problem. I will close this issue.
 
 > 
 
 > I have exactly the same problem with that random port number. Can you tell me how you solved it? Thanks!
 
 
 
 I found that the two servers that I used were not under the same VPC. So I used other servers and it worked. Maybe you can consider using other servers.",198
22439,DistributedDataParallel on multiple GPU nodes slower than one GPU node.,"The network bandwidth is indeed the bottleneck when synchronizing large models. NCCL folks provide some numbers in the other thread. Basically, the bandwidth for all reduce operation on intra-gpu communication would be much higher than that of inter-node communication. (NCCl folks mentioned 120 GB/s vs 10 GB/s, https://github.com/NVIDIA/nccl/issues/318).
 
 
 
 Thanks for the help. I am closing this issue here.",7195
22440,Batched torch.solve on GPU fails with: invalid device function,"Yes, thanks for the help.",8514
22441,Build system cannot find MKL libraries,"a simple workaround: `export CMAKE_PREFIX_PATH=""/opt/intel/compilers_and_libraries_2019.4.233/mac/mkl:$PATH""`",525
22442,cdist allocates a huge amount of memory in the bachward pass (pytorch 1.2.0),"I also have this issue.
 
 
 
 ```
 
 PyTorch version: 1.2.0
 
 Is debug build: No
 
 CUDA used to build PyTorch: 10.0.130
 
 
 
 OS: Manjaro Linux
 
 GCC version: (GCC) 9.1.0
 
 CMake version: version 3.15.1
 
 
 
 Python version: 3.7
 
 Is CUDA available: Yes
 
 CUDA runtime version: 10.1.168
 
 GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
 
 Nvidia driver version: 430.26
 
 cuDNN version: /usr/lib/libcudnn.so.7.6.1
 
 
 
 Versions of relevant libraries:
 
 [pip3] numpy==1.16.4
 
 [pip3] torch==1.2.0
 
 [pip3] torch-cluster==1.4.2
 
 [pip3] torch-geometric==1.3.0
 
 [pip3] torch-scatter==1.3.1
 
 [pip3] torch-sparse==0.4.0
 
 [pip3] torch-spline-conv==1.1.0
 
 [pip3] torchvision==0.4.0
 
 [conda] Could not collect
 
 ```
 
 
 
 For me it was a bit tricky to reach this bug report, since the error it gives is the following:
 
 
 
 ```
 
 ---------------------------------------------------------------------------
 
 RuntimeError Traceback (most recent call last)
 
 <ipython-input-4-281f7d587d04> in <module>
 
  71 torch.cuda.synchronize()
 
  72 res = loss_func_vec(y_pred, batch)
 
 ---> 73 res.backward()
 
  74 opt.step()
 
  75 opt.zero_grad()
 
 
 
 ~/venv/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
 
  116 products. Defaults to ``False``.
 
  117 """"""
 
 --> 118 torch.autograd.backward(self, gradient, retain_graph, create_graph)
 
  119 
 
  120 def register_hook(self, hook):
 
 
 
 ~/venv/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
 
  91 Variable._execution_engine.run_backward(
 
  92 tensors, grad_tensors, retain_graph, create_graph,
 
 ---> 93 allow_unreachable=True) # allow_unreachable flag
 
  94 
 
  95 
 
 
 
 RuntimeError: CUDA error: invalid configuration argument
 
 ```",2984
22443,"Build fails on FreeBSD with the unclear cause, see the attached log","Now I've succeeded building. -)
 
 
 
 Thanks!",4531
22444,Issue on installation from source,"
 
 > Also, the reason why I am trying to install pytorch from source is to modify affine_grid functon to make it work for 3D data. Is there any way to modify this function without instally pytorch from source?
 
 > 
 
 > Any help will be much appreciated.
 
 
 
 This is not documented, but `affine_grid` actually _already works for 3D data_, as with the following example:
 
 ```
 
 affine_matrix = torch.tensor([[[1,0,0,0],[0,1,0,0],[0,0,1,0]]], dtype=torch.float)
 
 grid = torch.nn.functional.affine_grid(affine_matrix, (1,1,3,3,3))
 
 ```
 
 You can then transform your data using `torch.nn.functional.grid_sample(image_3d, grid)`.
 
 
 
 Of course, this doesn't help you with the build issue, but at least it does get around having to build it from source. I hope this helps.",485
22445,Incorrect LibTorch download links on pytorch.org,"@mdlockyer This is the full list of download links:
 
 Linux CPU:
 
 https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cpu/libtorch-shared-without-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cpu/libtorch-static-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cpu/libtorch-static-without-deps-1.2.0.zip
 
 
 
 Linux CUDA 9.2:
 
 https://download.pytorch.org/libtorch/cu92/libtorch-shared-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu92/libtorch-shared-without-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu92/libtorch-static-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu92/libtorch-static-without-deps-1.2.0.zip
 
 
 
 Linux CUDA 10.0:
 
 https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu100/libtorch-shared-without-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu100/libtorch-static-with-deps-1.2.0.zip
 
 https://download.pytorch.org/libtorch/cu100/libtorch-static-without-deps-1.2.0.zip
 
 
 
 macOS CPU:
 
 https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.2.0.zip",639
22446,In-place modification detection does not work if a Tensor does not own its storage,this and other autograd tasks (especially high pri ones) would be great for Alban to tackle once he's back into the world next week :),10884
22447,Example in torchscript documentation does not work,"This is fixed in the [master docs](https://pytorch.org/docs/master/jit.html#torch.jit.trace), but we still need to push that to the stable docs so it's what people actually see. We're also in the process of adding our doc examples to our CI so this won't happen in the future.",7652
22448,Default initial hidden states for recurrent layers,Can we add this to the RNN documentation page as well? Took me a while to find this information here..,1522
22449,Reduce package size,"You can remove 3.7 - I don't think you use K80's large shared memory anywhere, and code generated for 3.5 should just run on 3.7. I'm also not sure how valuable and/or common 5.0 is. Finally, try adding -Xfatbin -compress-all to compiler args.",8589
22450,add transforms to DataLoader,"batching along 2nd dimension is common, since the standard input format for all recurrent layers in `nn` is `seq_length x batch_size x dim`.",559
22451,Implement remaining Variable functions,"The two references for Cholesky gradients that I know of are 
 
 * https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf
 
 * https://arxiv.org/abs/1602.07527
 
 
 
 It's implemented in tensorflow and autograd here:
 
 * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L58
 
 * https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L110

 I haven't thought too much about triangular solve gradients yet but I know that they're implemented in tensorflow here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L207.",699
22452,Making Variable.grad a Variable breaks dropout backprop,"Ok, everything is correct. I forgot I didn't do one last thing, that will change user-facing parts of the autograd API - `Function.backward` still accepts Tensors (and that's why dropout code is correct), but it will have to accept `Variable`s. I left it out for now, because it's a huge change that I didn't want to make hastily before the release, because it requires rewriting most of the code in autograd an a few other places, and will likely introduce new bugs... Only the code of users that define custom functions will need some changes in the future, but I think it's not going to be very common, and I'm willing to accept this, just to be sure that I'm not breaking it for everyone.",4601
22453,Dropout mode,"I guess `y = F.dropout(x,p,mode) * (1-p)` should do the trick?",3109
22454,wheels for python3.5 fail to install: .dist-info directory not found,this is fixed now,10894
22455,OpenCL Support,"Porting code to OpenCL by hand is not very maintainable. I think a more automated approach could be good.
 
 
 
 Here is a table of how I see things:
 
 
 
 | What | Who | Input | Backend | Comments |
 
 |----|----|---|---|----|
 
 |[coriander](https://github.com/hughperkins/coriander) | Me :-) | NVIDIAÂ® CUDAâ„¢ | OpenCL 1.2 | Works on Mac :-) Opensource |
 
 | [HIP](https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP) | AMD | HIP | AMD | |
 
 | [ComputeCpp](https://www.codeplay.com/products/computesuite/computecpp)â„¢ | CodeplayÂ® | SYCL | SPIR 1.2 | Official Tensorflow approach to OpenCL. |
 
 | [triSYCL](https://github.com/Xilinx/triSYCL) | Keryell | SYCL | ""SPIR 2.0"" | Opensource |
 
 | [OpenCLâ„¢](https://www.khronos.org/opencl/) by hand | | OpenCL | OpenCL | High maintenance, unportable, means forking the code ... |
 
 | [NVIDIAÂ® CUDAâ„¢](https://www.nvidia.com/object/cuda_home_new.html) | NVIDIA | NVIDIAÂ® CUDAâ„¢ | CUDA/PTX/SASS | Reference implementation for most/all projects |
 
 
 
 Note: quick introduction to 'SPIR', well I will just quote https://www.khronos.org/spir:
 
 
 
 ""SPIR (Standard Portable Intermediate Representation) was initially developed for use by OpenCL and SPIR versions 1.2 and 2.0 were based on LLVM. SPIR has now evolved into a true cross-API standard that is fully defined by Khronos with native support for shader and kernel features â€“ called SPIR-V. [...]
 
 
 
 ""For developers, using SPIR-V means that kernel source code no longer has to be directly exposed, kernel load times can be accelerated and developers can choose the use of a common language front-end, improving kernel reliability and portability across multiple hardware implementations.""",4875
22456,Implement Broadcasting with exact semantics as NumPy,"Actually, it's more efficient to use `b.unqueeze(0).expand(X.size(0), b.size(0))` (`repeat` makes a new tensor, while `expand` does strides). I'll add `b.broadcast` that does it as a single op",933
22457,wheels for python2.7 install failed: cuda 8.0,"I think this is related to https://github.com/pytorch/pytorch/issues/482
 
 Could you try seeing if `import torch` works for you?",3261
22458,Add windows support please, I guess we could close this issue now.,702
22459,DOCS: Links to Source Code Broken,This seems to be fixed now.,7801
22460,Feature Request: Spatial Transformer Network,i'm working on this today.,9567
22461,shoud we support play pytorch without Anaconda?,"we have wheels ready, but cant upload them until PyPI switches to their new infrastructure. See: https://github.com/pypa/packaging-problems/issues/120#issuecomment-366515498
 
 
 
 They said the move is in the coming weeks.",11360
22462,DOCS: Search feature hangs,now fixed :),10272
22463,How can I use CTC loss function ?,"I'll be opening it soon, but @apaszke is right. ATM I've just wrapped a numpy API for quick use, but I'll be working on an actual C extension to use the C api directly",3323
22464,undefined symbol: sormqr_,"I'm running on an up-to-date Ubuntu 16.04.01 LTS, kernel 4.4.0-59 @ Lenovo Y50.
 
 Cuda 8.0.44; Nvidia drivers 367.57 @ GeForce GTX 860M
 
 gcc 4.8.5
 
 
 
 You were right, the math and pthread libraries were missing and they were not linked during installation. On my machine the two libraries are in this folder `/usr/lib/x86_64-linux-gnu/`. This folder is among those listed by `gcc --print-search-dirs`, but it's not in `$LIBRARY_PATH`.
 
 
 
 Executing `export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/` before installing pytorch solved the problem on my machine.",3344
22465,AttributeError: type object 'torch._C.FloatTensorBase' has no attribute 'numpy',"You need to open python from a different folder than `~/pytorch`, as can be seen in https://github.com/pytorch/pytorch/issues/7 .",8646
22466,look into why the NVIDIA CUDA Dockerfile doesn't build pytorch,fixed now,657
22467,Feature Request: Length Masking for RNNs,"But sometimes we have more than one input. We can't sort both seq1 and seq2 at the same time. Like machine comprehension model, our inputs is: question-passage.",1477
22468,Half precision convolution not working,"In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):
 
 https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361
 
 https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51
 
 Math type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because
 
 1) computations with half math are not supported anywhere except Pascal and TX1
 
 2) Even on Pascal, 6.1 cards have very low performance fp16 math
 
 3) fp16 math comes with a set of accuracy problems
 ",3712
22469,cuda runtime error (8) : invalid device function during ops between cuda tensors,"if you reinstall pytorch from conda, this should now be fixed.
 
 I am also going to commit refreshed pip wheels in ~2 hours",9583
22470,"Error installing from source on Mac(10.12),CUDA=8.0, CUDNN=5.1.5","Try this if you have added the Anaconda directory to your bash shell PATH environment variable:
 
 
 
 ```shell
 
 CC=clang CXX=clang++ python setup.py install
 
 ```",8020
22471,Initial call to .cuda() very slow with Titan X,"you installed with Conda.
 
 Your Titan-X is Titan-X Pascal.
 
 
 
 I am aware of the issue, i am building Conda binaries which have pre-compiled code fore 6.0 and 6.1 architectures :)",11456
22472,Inferring NumPy array type when using `from_numpy`,"It will always create a tensor of a corresponding type. It doesn't do a memory copy, and only views onto `ndarray`'s memory, so it can't use a different data type, or you'd see complete garbage there.
 
 
 
 `y.dtype` is `dtype('float64')` i.e. `double`. E.g. this:
 
 
 
 ```python
 
 y = np.array([[1., 2., 3.],
 
  [4., 5., 6.]], dtype=np.int32)
 
 
 
 torch.from_numpy(y)
 
 ```
 
 
 
 gives
 
 ```
 
  1 2 3
 
  4 5 6
 
 [torch.IntTensor of size 2x3]
 
 ```",3990
22473,"API suggestion: "".gpu()"" instead of "".cuda()""","As a Torch/PyTorch beginner, I found the naming a little confusing at first and requiring education what exactly is going on when I call this. I thought it was configuration-only at first, so it's not as self-explanatory as it could be. Fortunately, this can be addressed entirely with an extra helper method â€” hence backward-compatible and Torch-friendly too.
 
 
 
 I'd personally suggest naming to reflect the operation actually done under the hood. I tend to name functions as verbs, so something like `transfer(gpu=0)` or some combination of `upload(device='gpu0')` / `download(...)`.",1281
22474,NO_DISTRIBUTED environment variable documentation needs to be updated,"I think we standardize on USE_... now, so the first variant would be it. ~~Maybe I'll tack this onto #17295.~~
 
 On second thought, maybe it's best to fix all of the variables in the description that aren't up to date in one go.",3277
22475,torch.exp on CPU is incorrect for tensors with more than 2^31 or more elements,This is due to the fact that we link against MKL LP64 and not MKL ILP64. As such the MKL VML calls only accept 32bit types. I'll write something that will call into VML repeatedly if this is the case.,7633
22476,Maximum value of torch.rand,"It will work if you use low precision.
 
 
 
 PyTorch supports very high precision like 1e-15 or more.
 
 
 
 Thatâ€™s why I confused.
 
 
 
 For very precisely, I recommend you to use my method. Itâ€™s simple and fast in terms of probablistic points :)
 
 
 
 If you donâ€™t mind about sampling 0.0000001, then use your method.
 
 
 
 In my experience, sometimes very small value is important.
 
 
 
 But it is your choice. :)",735
22477,RuntimeError: cublas runtime error : the GPU program failed to execute at at pytorch/work/aten/src/THC/THCBlas.cu:258,"For RTX 2080, you need to use CUDA10, not CUDA 9.2",2483
22478,"Multiprocessing's spawn says ""Only Tensors of floating point dtype can require gradients"" while it isn't","Bug is here:
 
 
 
 https://github.com/pytorch/pytorch/blob/3145f46a2287619e79add11a8d543729b2590a14/torch/multiprocessing/reductions.py#L83-L84
 
 
 
 nn.Parameter() needs `requires_grad=False` in constructor in this case",1470
22479,"ProcessGroupNCCL.cpp:260, unhandled cuda error","I've met a similar issue on my machine with pytorch 1.1, CUDA 10.0 and NCCL 2.4.2. The only difference is that my error message is ""RuntimeError: ...... ProcessGroupNCCL.cpp:272, unhandled system error"". I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1. 
 
 
 
 Hope it will help!",3356
22480,cuda runtime error (3): we're not detecting bad forks,"@colesbury @mrshenli If description of this issue is going to be changed to fixing the ""bad fork"" error detection, we can close https://github.com/pytorch/pytorch/issues/17359 as duplicate.",557
22481,What is a version/git-hash of nightly build?,"Fwiwi t's not much work at all. When we built the linux wheel and libtorch packages we unzip them, mess around with them, and then rezip them back up. If you just want a txt file called build_hash or something like that you could just add that file before the wheel gets zipped up here https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L300 . A line like
 
 ```
 
 echo ""$(pushd $pytorch_rootdir && git rev-list --max-count 1 HEAD)"" > $PREFIX/build_hash
 
 ```
 
 would probably do the trick. There is more work to get the same file in the python packages or on non-linux.",2664
22482,Support ... indexing in JIT,I was thinking to give this one a try if you don't mind.,710
22483,Missing (dynamic) libraries in current libtorch (1.0.1) - macOS,"As of 23rd June, the archive at https://download.pytorch.org/libtorch/nightly/cpu/libtorch-macos-latest.zip still doesn't contain libmklml.dylib
 
 
 
 ```
 
âžœ build git:(AddModel) âœ— ll ../lib/libtorch/lib/
 
 total 283904
 
 -rw-r--r-- 1 wilmot_p staff 7.5K Jun 23 07:27 libCaffe2_perfkernels_avx.a
 
 -rw-r--r-- 1 wilmot_p staff 183K Jun 23 07:27 libCaffe2_perfkernels_avx2.a
 
 -rw-r--r-- 1 wilmot_p staff 384B Jun 23 07:27 libCaffe2_perfkernels_avx512.a
 
 -rw-r--r-- 1 wilmot_p staff 398K Jun 23 07:27 libTHD.a
 
 -rw-r--r-- 1 wilmot_p staff 480K Jun 23 07:27 libbenchmark.a
 
 -rw-r--r-- 1 wilmot_p staff 1.4K Jun 23 07:27 libbenchmark_main.a
 
 -rwxr-xr-x 1 wilmot_p staff 293K Jun 23 07:27 libc10.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 799K Jun 23 07:27 libcaffe2_detectron_ops.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 130K Jun 23 07:27 libcaffe2_module_test_dynamic.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 145K Jun 23 07:27 libcaffe2_observers.dylib
 
 -rw-r--r-- 1 wilmot_p staff 914K Jun 23 07:27 libcaffe2_protos.a
 
 -rw-r--r-- 1 wilmot_p staff 5.0K Jun 23 07:27 libclog.a
 
 -rw-r--r-- 1 wilmot_p staff 43K Jun 23 07:27 libcpuinfo.a
 
 -rw-r--r-- 1 wilmot_p staff 40K Jun 23 07:27 libcpuinfo_internals.a
 
 -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libfoxi.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libfoxi_dummy.dylib
 
 -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libfoxi_loader.a
 
 -rw-r--r-- 1 wilmot_p staff 161K Jun 23 07:27 libgmock.a
 
 -rw-r--r-- 1 wilmot_p staff 3.9K Jun 23 07:27 libgmock_main.a
 
 -rw-r--r-- 1 wilmot_p staff 492K Jun 23 07:27 libgtest.a
 
 -rw-r--r-- 1 wilmot_p staff 1.2K Jun 23 07:27 libgtest_main.a
 
 -rw-r--r-- 1 wilmot_p staff 20M Jun 23 07:27 libmkldnn.a
 
 -rw-r--r-- 1 wilmot_p staff 206K Jun 23 07:27 libnnpack.a
 
 -rw-r--r-- 1 wilmot_p staff 16K Jun 23 07:27 libnnpack_reference_layers.a
 
 -rw-r--r-- 1 wilmot_p staff 4.2M Jun 23 07:27 libonnx.a
 
 -rw-r--r-- 1 wilmot_p staff 363K Jun 23 07:27 libonnx_proto.a
 
 -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libonnxifi.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libonnxifi_dummy.dylib
 
 -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libonnxifi_loader.a
 
 -rw-r--r-- 1 wilmot_p staff 529K Jun 23 07:27 libprotobuf-lite.a
 
 -rw-r--r-- 1 wilmot_p staff 3.9M Jun 23 07:27 libprotobuf.a
 
 -rw-r--r-- 1 wilmot_p staff 3.5M Jun 23 07:27 libprotoc.a
 
 -rw-r--r-- 1 wilmot_p staff 9.5K Jun 23 07:27 libpthreadpool.a
 
 -rw-r--r-- 1 wilmot_p staff 122K Jun 23 07:27 libqnnpack.a
 
 -rwxr-xr-x 1 wilmot_p staff 54K Jun 23 07:27 libshm.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 83M Jun 23 07:27 libtorch.dylib
 
 -rwxr-xr-x 1 wilmot_p staff 10M Jun 23 07:27 libtorch_python.dylib
 
 ```",1311
22484,CuDNN detected while compile; returns false on compiledWithCuDNN() check,"this log is not complete, as it's the log of a second invocation.
 
 
 
 First run `git clean -xfd` to clean all build files, and then run
 
 
 
 ```
 
 CUDNN_INCLUDE_DIR=~/cuda/include CUDNN_LIBRARY=~/cuda/lib64 python setup.py install
 
 ```
 
 
 
 Then paste that full log.
 
 
 
 thanks.",10904
22485,The GPU memory of tensor will not release in libtorch,@soumith Thanks a lot! `cudaDeviceReset()` can release the resource associated with the current process.,704
22486,"RuntimeError: cur_offset == offset ASSERT FAILED at ..\aten\src\ATen\native\cudnn\RNN.cpp:471, please report a bug to PyTorch. cur_offset = 36864; offset = 18432","Hi, you should check if the data type is suitable; in my case, the embedding weight needs float64 type, then I fix it.
 
 Cheers,",2914
22487,Why do I have to install CUDA and CUDNN first before installing pytorch GPU version ?,"No, `conda install` will include the necessary cuda and cudnn binaries, you don't have to install them separately. However you do have to specify the cuda version you want to use, e.g. `conda install pytorch cudatoolkit=9.0 -c pytorch`.
 
 See PyTorch's [Get started](https://pytorch.org/get-started/locally/) guide for more info and detailed installation instructions :smile:",4316
22488,"inaccurate description of torch.mul(tensor, tensor)","yea, I agree, the wording there is a bit off.
 
 Would you mind sending a Pull Request improving the doc?",11437
22489,torch.utils.ffi is deprecated. How do I use cpp extensions instead?,"damn, weeks of work to the trash...
 
 
 
 do you plan on deprecating such low-level stuff on a regular basis or is it really worth it trying to make a cpp extension now ?",9150
22490,"bce(sigmoid(a), b) and bce_with_logits(a, b) return different values","Yes, that's the problem of numerical stability. Sigmoid of 17 and 15 is very close to 1, and loses precision. Convert a and b to double and see results become the same.",8517
22491,[jit] Python functions can't take list argument, Is this the same issue? https://github.com/pytorch/pytorch/issues/15809,754
22492,torch.einsum equation works in NumPy but not in Pytorch,"Ah, sorry, I meant to post the workaround but forgot. Just replace the letter in size-1-dimension with anything not appearing in the equation, e.g.:
 
 ```python
 
 torch.einsum(""xijk, bilm -> bjklm"", torch.randn(1, 3, 24, 20), torch.randn(5, 3, 24, 20))
 
 ```
 
 This works because you can split out the constant (after broadcasting) from the product.
 ",993
22493,Tests for Vec256 classes," I think it would be a very useful activity for unit tests to come into existence as part of the POWER work, if you can put that into your work scope.",680
22494,jit_type.h Type Class's cast function,Thank you!,6806
22495,Consolidate flag/environment variable handling in setup.py,one step towards this that I'm working on is removing the middle layer (build_pytorch_libs.sh),10298
22496,cuda10 version of 1.0 doesn't work on linux,"this is a bug in our packaging i think, I'll try to get it fixed for 1.0.1 release. cc: @pjh5 for visibility.
 
 
 
 In the meanwhile, you can use the `anaconda` version via `conda install` and it works because we also pull the anaconda `cudatoolkit` dependency.",695
22497,torch.Tensor.bernoulli_ gives different results in mac and linux,"Determinism is not guaranteed across version/hardware/platforms.
 
 On Fri, Jan 4, 2019 at 06:46 Alasdair Tran <notifications@github.com> wrote:
 
 > ðŸ› Bug
 >
 > Running the following commands
 >
 > import torch
 >
 > torch.manual_seed(0)
 >
 > a = torch.zeros(5).long()
 >
 > a.bernoulli_()
 >
 > give different samples in linux and mac. On my Ubuntu 18.04, I get tensor([1,
 > 0, 0, 1, 1]). On my MacOS Mojave, I get tensor([0, 0, 1, 0, 0]). On the
 > same machine, it always gives the same sample, but it's not reproducible
 > across machines.
 >
 > I sampled from other distributions such as normal_(), uniform_() and
 > random_(), and they all work fine (i.e. give the same sample across
 > machines). The non-inplace version of Bernoulli bernoulli() works fine
 > too.
 >
 > This non-reproducible problem seems to only occur with the in-place
 > bernoulli_() function.
 > Environment
 >
 > - PyTorch Version (e.g., 1.0): 1.0.0
 > - How you installed Pytorch: conda
 > - Python version: 3.7
 >
 > â€”
 > You are receiving this because you are subscribed to this thread.
 > Reply to this email directly, view it on GitHub
 > <https://github.com/pytorch/pytorch/issues/15714>, or mute the thread
 > <https://github.com/notifications/unsubscribe-auth/AFaWZfY55m7eJEzapcURzGbHrMJM2bOLks5u_ogugaJpZM4Zo4_i>
 > .
 >",2121
22498,PyTorch doesn't report useful error when forking after CUDA initialization (originally: Unknown cuda error with multiprocessing),"We can't fix this directly, because CUDA API must not be initialized before you fork (this is a well known limitation), but I was pretty sure we had a good error message for this case, which seems to have regressed.
 
 
 
 You can use the spawn method instead. But you also have to make sure you wrap the parent script in an `if __name__ == '__main__'` block or you'll get an oblique error message about `freeze_support`:
 
 
 
 ```
 
 import torch.multiprocessing as multiprocessing
 
 import torch
 
 
 
 def mp_worker(gpu):
 
  print(torch.cuda.get_device_properties(gpu))
 
 
 
 if __name__ == '__main__':
 
  gpus = list(range(torch.cuda.device_count()))
 
 
 
  ctx = multiprocessing.get_context('spawn')
 
 
 
  processes = [ctx.Process(target=mp_worker, args=(gpui,)) for gpui in gpus]
 
  for process in processes:
 
  process.start()
 
  for process in processes:
 
  process.join() 
 
 ```",8253
22499,torch.bincount() seems problematic,"Got it, I tried on a mac and I can reproduce this. We will look into it, thank you for the report",2733
22500,torch.argsort descends wrongly,"Okay I think the problem is caused by `NaN`. Here's a simpler repro:
 
 ```
 
 In [33]: a = torch.tensor([3, float('NaN'), 2])
 
 
 
 In [34]: torch.argsort(a, dim=0)
 
 Out[34]: tensor([0, 1, 2]) # wrong!
 
 
 
 In [36]: a = torch.tensor([1, float('NaN'), 2])
 
 
 
 In [37]: torch.argsort(a, dim=0, descending=True)
 
 Out[37]: tensor([0, 1, 2]) # wrong!
 
 ```
 
 Similarly if we remove the leading `Nan` in the array you provided, the problem is gone. 
 
 ```
 
 In [38]: a = numpy.load(""log"")
 
 
 
 In [39]: a = torch.tensor(a)
 
 
 
 In [40]: b = a[1:]
 
 
 
 In [41]: b
 
 Out[41]: tensor([ 0.0791, -0.0522, 0.0226, ..., -0.0267, 0.1027, 0.0064])
 
 
 
 In [49]: print(torch.argsort(b, dim=0, descending=True).narrow(0,0,8))
 
 tensor([ 49, 117, 46, 12, 143, 16, 19, 327])
 
 
 
 In [50]: print(torch.argsort(b, dim=0).narrow(0,b.shape[0]-8,8))
 
 tensor([327, 19, 16, 143, 12, 46, 117, 49])
 
 ```
 
 Actually the same problem exists for `torch.sort` as well, that `nan` breaks the monotonic increasing in the middle, but it was hidden in the printed sequence. 
 
 ```
 
 In [51]: torch.sort(a)
 
 Out[51]:
 
 (tensor([-0.2427, -0.2274, -0.2272, ..., 0.5829, 0.7206, 1.0000]),
 
  tensor([26606, 25901, 32186, ..., 47, 118, 50]))
 
 In [57]: torch.sort(a)[0][16360:16365]
 
 Out[57]: tensor([ 0.2460, 0.2769, nan, -0.2238, -0.2159])
 
 ```",4610
22501,`torch.linalg.eigh` is much slower than `torch.linalg.svd` on GPU,"> It seems that torch.linalg.svd is also optimized.
 
 
 
 That's right. For the problem size of 1000 x 4x4 matrices, an efficient batched algorithm is used for SVD. However, for symmetric eigenvalue decomposition, we currently have to use a for-loop based algorithm for all single matrix because there was a known numerical issue on the batched algorithm. 
 
 
 
 We plan to update the code and use the more efficient batched algorithm for symmetric eigenvalue decomposition soon. Stay tuned! ðŸ˜„",236
22502,"What does ""build from source"" mean?",I think [this](https://github.com/pytorch/pytorch#from-source) might be helpful.,3224
22503,nn.FeatureAlphaDropout is missing from the docs, I have filed a pull request. #60590,610
22504,"init_rpc fails after setting CUDA_VISIBLE_DEVICES env var to """"",Also tracked in https://github.com/pytorch/tensorpipe/issues/393.,1108
22505,Exporting normalization modules introduces numerical errors,There may be some formal or informal expectations between different implentations of a given ONNX operator (I'm not sure). For that you could ask in the ONNX GitHub or Slack. But between ONNX and PyTorch I don't think there's any standard other than basic correctness.,7419
22506,[CI] Report wall-time and execution time stats from build/test stages,"If possible, you should get fine grained time for individual gcc invocations, because the timings will vary wildly depending on if you get a hit on sccache or not, and the easiest way to reduce noise here is to record these separately.",3472
22507,"CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.","I ran into the same problem with cuda 11.1 with debian 10 on GPC 
 
 First uninstalling the cuda driver and then installing cuda 11.3 helped me solve it
 
 
 
 ```bash
 
 # uninstall cuda 11.1
 
 sudo nvidia-installer --uninstall
 
 
 
 # install cuda 11.3 for debian 10
 
 # you might need to match your distribution https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Debian&target_version=10&target_type=deb_local
 
 
 
 wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb
 
 sudo dpkg -i cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb
 
 sudo apt-key add /var/cuda-repo-debian10-11-3-local/7fa2af80.pub
 
 sudo add-apt-repository contrib
 
 sudo apt-get update
 
 sudo apt-get -y install cuda
 
 ```",3178
22508,FileNotFoundError: [Errno 2] No such file or directory when putting tensor in multiprocessing queue,"Seems that you are not following the best practices of sending Tensor in Queue. In short, you need to setup an event to ask sub process to wait for main process to read the queue.
 
 
 
 Please refer to the following link for more information:
 
 https://discuss.pytorch.org/t/tensors-as-items-in-multiprocessing-queue/411
 
 
 
 And the following is the sample code I modified, which works well:
 
 
 
  from torch import multiprocessing as mp
 
  import torch
 
  
 
  def f(q, evt):
 
  x = torch.rand(5, 5)
 
  q.put(x) # <---- Change this to `q.put(x.numpy())` to fix
 
  evt.wait()
 
  # q.close()
 
  # q.join_thread()
 
  
 
  
 
  if __name__ == '__main__':
 
  mp.set_start_method('spawn')
 
  nworkers = 5
 
  
 
  evt = mp.Event()
 
  
 
  q = mp.Queue()
 
  workers = [mp.Process(target=f, args=(q,evt,)) for _ in range(nworkers)]
 
  [w.start() for w in workers]
 
  
 
  for _ in range(nworkers):
 
  x = q.get()
 
  print(x)
 
  
 
  evt.set()
 
  
 
  [w.join() for w in workers]",5808
22509,DISABLED test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithFork),@rohan-varma,684
22510,Windows binary wheels are broken on master,"Failures started with the inclusion of `pyproject.toml`
 
 
 
 ![image](https://user-images.githubusercontent.com/1700823/123308102-1e38de80-d4d8-11eb-85cb-1385ab010698.png)",2400
22511,torch.nn.utils.clip_grad_norm_: bad GPU utilization due to GPU-data-dependent control-flow,"It should be possible to get rid of `clip_coef` synchronization either like you suggest, or creating some custom op that would skip pointless multiplication.",3979
22512,`TensorPipeJitRpcTestWithSpawn.test_call_fork_in_jit_with_profiling` is failing on the master branch,"Thanks for letting us know! The engineers who maintain these tests are looking into the failure, so I'm going to close the issue for now",6824
22513,torch.distributed runs under INFO log level in 1.9.0,"Apologies for the many mishaps with `torch.distributed.run/launch` and thanks for your patience with the migration. 
 
 
 
 The warning message stating the future deprecation of `--use_env` flag is definitely misleading and has been corrected in this PR: https://github.com/pytorch/pytorch/pull/60808 (we will make sure to add this into the next patch release).
 
 
 
 As for the log level. This requires a bit of code search in our internal repo. Long story is that `torch.distributed.launch` has historically not been used in our internal launches of distributed torch, but `torch.distributed.run` (formally known as `torchelastic.distributed.launch`) actually is used in production at Facebook. Since `torch.distributed.launch` is a proxy to `torch.distributed.run` we've essentially put forth the same launcher we use for FB production use-cases for our OSS customers. Since our prod stack runs off the HEAD of master, we believe that long term we can offer our OSS users a ""production"" stable version of pytorch on each release.
 
 
 
 Issue with the log level is that our internal log analyzers expect INFO level logging by default. While I do some research to see if we can make the default log level WARNING and override it to INFO for our internal production jobs, in the interim you could set the `LOGLEVEL=WARNING` env var when invoking the launcher (see: https://github.com/pytorch/pytorch/blob/6ff0002b12a07b66e65600de0028cf0ad902e503/torch/distributed/elastic/utils/logging.py#L35)
 
 
 
 ```
 
 LOGLEVEL=WARNING python -m torch.distributed.launch ...
 
 ```",1152
22514,torch.distributed.run future warning in 1.9.0,Thank you for the report. This warning message should have been removed. I've gone ahead and submitted a PR to address this. https://github.com/pytorch/pytorch/pull/60807,6792
22515,`test_reference_numerics_hard_polygamma_polygamma_n_2_cpu_bfloat16` is failing on `pytorch_linux_xenial_py3_clang5_asan_test2`,"Should we close this issue then?
 
 We just need to make sure we don;t break master when relanding 60444 ?",5893
22516,[autograd-custom-function] output variables' number check in backward path,"
 
 We understood the risk of varargs in custom Function, and will follow your suggestion.
",531
22517,more `torch.distributed.launch` issues in 1.9.0,"Thanks for reporting this issue! I can indeed repro this on my local dev desktop. But I have a reason to believe the issue is not in the launcher but rather somewhere in the NCCL process group. Here's the reason why: I tried running the script as a single worker without the launcher as such:
 
 
 
 ```
 
 CUDA_VISIBLE_DEVICES=0 RANK=0 WORLD_SIZE=1 MASTER_ADDR=localhost MASTER_PORT=8081 python test.py --local_rank 0
 
 ```
 
 
 
 and get 
 
 ```
 
 [W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
 
 ```
 
 I'll investigate further and report back. In the meanwhile please do not hesitate to point out glaring misses on my hypothesis or my repro steps!",6832
22518,Exception when DataLoader's num_workers parameter is specified,Well it appeared again and I believe it's PyCharm problem as I could not reproduce it elsewhere. Putting `del dataiter` at the end of the code seems to solve it.,8314
22519,[caffe2]Compiled error with CUDA9.0,"It's work for me
 
  
 
 Eigen with CUDA9: fatal errorï¼šmath_functions.hppï¼šNo such file or directory.
 
 
 
 Was fixed in eigen at https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default",4015
22520,Pytorch in java,there is no plan to add Java support (not Android only) to pytorch in a near future correct? I am considering switching from TF to pytorch but the lack of Java support is currently a limitation in my case.,2770
22521,[feature request] smarter module matching for model.load_state_dict(),"I think that's a bit too automagical, will regularly guess wrong, and you won't even realize because debugging ML models is almost impossible. We haven't implemented any kind of auto-matching for state dicts on purpose. If you have multiple models, you should know how they are structured, and you should implement the matching yourself.",3240
22522,compile from source error,"You have `WITH_DISTRIBUTED_MW` enabled, but this is not a flag we support at the moment. Disable it and it should build just fine.",8624
22523,[pytorch] Incorrect error message in NLL_Loss,Closed by #6617. ,1594
22524,glibc error while importing torch,I am also having this issue. I know that the version of GLIBC in my conda environment is up to date (version 2.55) but pytorch seems to be using the global GLIBC on my machine (version 2.12). how can I configure pytorch so that it uses the correct GLIBC installation?,2987
22525,How to export pytorch to ONNX alternative forward path,"You could try making a module that wraps the module, but its forward calls incremental_forward instead.",8617
22526,Seg Fault when using more than one layer in torch.nn.GRU, I have reproduced the segfault on 0.3.1 and on master.,520
22527,[Caffe2] Windows CI not building with CUDA enabled,Fixed https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda9.0-cudnn7-windows-build/557/console now builds with CUDA,2462
22528,libcaffe2.a for Android is too large,"It doesn't make much sense to look at the size of `libcaffe2.a`, because it is just a collection of object files. They contain duplicates of the same symbols, and much debug information. The size you should be looking it is increase in the size of your shared library after you link `libcaffe2.a`: the linker will remove duplicate symbols, and can strip debug symbols.",3913
22529,Same program much slower in 0.4.0 than in 0.3.0,it seems roughly at the same speed (not 100% sure cuz of randomness in the program). Thanks very much!,585
22530,"`torch.Tensor.new_empty((3, 2))` gives weird error","Right now, it's
 
 ```
 
 TypeError: descriptor 'new' requires a 'torch._C._TensorBase' object but received a 'tuple'
 
 ```
 
 This is what I think it should be:
 
 TypeError: descriptor 'new' requires a tensor but received a 'tuple'.
 
 
 
 Or even better:
 
 TypeError: descriptor 'new': Incorrect argument #1: expected a tensor but received a 'tuple'.",5708
22531,dataloader issue python3.6,"I met the same problem with pytorch docker image provided by NVIDIA, by setting `--ipc=host` in the container starting script the problem is solved.",3168
22532,in-place operation error thrown after mutation instead of before,"It can, but it would work differently than all other methods and would have to be moved to C. Also, we would need to go over all of our functions and make sure it is called before the tensors are actually mutated. But yes, as long as we do it this way it should actually be possible to raise errors before mutation",3903
22533,"`parameters()` should be a list, not an iterator","I don't think I agree. A lot of things like this are generators/iterators in Python3 (think `map`, `filter`, `zip`, `range`, ...) and you can shoot yourself in the same way. If you want to iterate over the thing multiple times just do `list(model.parameters())` and if you just want the first one do `next(model.parameters())` (which is also going to be much faster, because it won't iterate over all of your model at all).",3057
22534,Warning when compiling on ppc64le,"I found the line responsible line
 
 ```
 
 static inline __host__ __device__ char max() { return CHAR_MAX; }
 
 ```
 
 https://github.com/torch/cutorch/blob/5e9d86cb982a6048d3077aeb0e0cee19847b4c08/lib/THC/THCNumerics.cuh#L38
 
 
 
 It is (relatively) harmless I would say.",3102
22535,DataLoader and HalfTensors,I'm not sure why this issue is closed. I fully understand that it makes sense in the context of data loading. Creating a shared storage is not math.,3341
22536,Add compiled CUDA version in torch.version,Makes sense. Let's do it,4195
22537,Can't compile with gcc 6.4 and cuda 9,"this is likely fixes with
 
 python setup.py clean",10896
22538,broadcasting inconsistency?,"i meant the following scenario:
 
 
 
 loss = torch.mean((x - y) ** 2)
 
 
 
 loss is a scalar regardless of whether (x-y).size() is (10,10) or (10,1). this makes debugging pretty much impossible :(",9561
22539,DataParallel Gather works only with iterable outputs,Has anyone ever taken a look at this? Thanks!,2752
22540,Trying to do advanced indexing using a Variable causes a hang," try this on `master`? I just built a clean copy of PyTorch and the code snippet succeeds for me. You may not have a version of PyTorch that includes https://github.com/pytorch/pytorch/pull/2590, which addresses this issue.",2831
22541,SVD MAGMA gesdd : the updating process of SBDSDC did not converge,"Ran into the same issue. This seems to be a feature of underlying algorithm (GESDD) and can be fixed by improving condition number to be better than 8000. GESVD seems to not have this problem, so if you care about tiny singular values, a workaround is to use scipy with lapack_driver `gesvd` 
 
 https://github.com/pytorch/pytorch/issues/25978",5085
22542,CUDA initialization hangs on CUDA 9,"Magma is not used to boost the performance. It's only that without it some linear algebra functions (linear system solvers, Cholesky decomposition, and similar) will not work. You won't find any difference unless you use those.",4194
22543,[windows] driver shut down,"I noticed I have installed two different packages(0.1.12 and 0.2.1) . I deleted and uninstalled all (Anaconda + pycharm+pytorch) and then reinstalled them(Python 3.5.2+pytorch 0.1.12). And I think this problem has something to do with multiprocess. So I set the worker 0(I find the training becomes a little slower). For now, everything works well. 
 
 Thanks!",666
22544,Reliably repeating pytorch system crash/reboot when using imagenet examples,"Had the same issue with _GTX1070_ but reboots were not random. 
 
 I had a code that was able to make my PC reboot every time i run it after at most 1 epoch. 
 
 At first i thought it can be PSU since mine has only 500W. However after closer investigation and even setting max power consumption to lower values with `nvidia-smi` i realized the issue is somewhere else. 
 
 It was not an overheating problem as well so i started to think that it might be because of _I7-7820x_ Turbo mode. After disabling Turbo mode in BIOS settings of my _Asus X299-A_ and changing Ubuntu's configuration as stated [here](https://askubuntu.com/questions/619875/disabling-intel-turbo-boost-in-ubuntu/620114) the issue seems to be gone. 
 
 
 
 What did **NOT** work:
 
 - Changing `pin_memory` for dataloaders.
 
 - Playing with batch size.
 
 - Increasing system shared memory limits.
 
 - Setting `nvidia-smi -pl 150` out of 195 possible for my system.
 
 
 
 Not sure if this is related to native BIOS issues. I am running 1203 version while the latest is 3 releases ahead -- 1503 and they put 
 
 
 
 > improved stability
 
 
 
  into the description of each of those 3. [Asus X299-A BIOS versions](https://www.asus.com/us/Motherboards/PRIME-X299-A/HelpDesk_BIOS/) One of those releases had also 
 
 
 
 > Updated Intel CPU microcode.
 ",2749
22545,Function request: np.isin,"Sure! Also, instead of implementing `in1d`, I think we should implement the more general `isin`, as recommended by [the numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.in1d.html)
 
 Here is a slightly more runtime efficient (but memory-hungry and suboptimal) implementation (requires PyTorch compiled from master):
 
 ```python
 
 def isin(ar1, ar2):
 
  return (ar1[..., None] == ar2).any(-1)
 
 ```",6155
22546,Feature Request: Scheduler should be able to load and save state dictionaries,"Definitely a +1 on this one from me.
 
 
 
 However, a simple workaround might be to simply keep track of the epoch index during training (as many probably do anyway). If training is halted only to be resumed later, the lr_scheduler can be loaded into its previous state simply by reinstatiating it and then setting the last_epoch property. 
 
 
 
 It appears that `self.last_epoch` is the only ""state"" of most learning rate schedulers, and setting the property should correctly restore the learning rate scheduler to its previous state.
 
 
 
 ```
 
 last_epoch_trained_upon = 42
 
 lr_scheduler = optim.ExponentialLRS(optimizer, gamma=0.99)
 
 lr_scheduler.last_epoch = last_epoch_trained_upon
 
 ```",2096
22547,JIT scripted indexing with ellipsis and None is not working correctly,@One-sixth Thank you for reporting this and providing a concise example. That really helps a lot tracking down these bugs!,505
22548,libtorch does not initialize OpenMP/MKL by default,"I build the C++ API example as the tutorials on website, using cmake to build.
 
 CMakeLists:
 
 ```
 
 cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
 
 project(pytorch-cpp-example)
 
 find_package(Torch REQUIRED)
 
 add_executable(pytorch-cpp-example pytorch-cpp-example.cpp)
 
 target_link_libraries(pytorch-cpp-example ""${TORCH_LIBRARIES}"")
 
 set_property(TARGET pytorch-cpp-example PROPERTY CXX_STANDARD 11)
 
 ```
 
 I get libtorch from url on website : 
 
 ```
 
 wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip
 
 ```
 
 I install pytorch using conda:
 
 ```
 
 conda install pytorch-cpu torchvision-cpu -c pytorch
 
 ```
 
 
 
 Both are CPU-only.
 
 
 
 How to rerun the benchmark with OMP_NUM_THREADS=1? I'm new to pytorch so I don't know much about running parameters.",3013
22549,test_namedtuple_return is flakey,"@zou3519 Yes, this test was added by me, I will take a look.",756
22550,In-place modification with double slicing (using boolean mask),"This behavior is expected and is the same as in numpy.
 
 
 
 There are a few subtleties in here:
 
 - indexing with a boolean tensor always returns a new tensor, and not a view to the existing tensor
 
 - this means that operations on the result of boolean indexing do not modify the indexed tensor
 
 ```python
 
 a = torch.randn(5)
 
 b = a[a > 0]
 
 b.fill_(0) # doesn't modify `a`
 
 # BUT
 
 a[a > 0] = 0 # changes `a`
 
 ```
 
 Now, why does `a[a > 0] = 0` modify `a`? Because this dispatches to `__setitem__` internally, which calls the appropriate masking functions.
 
 
 
 Now, when you do `a[a > 0.5][a < -0.5] = 0`, what this is in fact doing is that it is first creating a new (temporary) tensor `a[a > 0.5]`, and then on this temporary tensor it applies `t[a < -0.5] = 0` by calling into `__setitem__`. Which means that the result of the operation gets discarded with the temporary tensor.
 
 
 
 Now you can say: but why does `x[0][1] = 5` works then? Because `x[0]` returns a view to the underlying tensor, which shares storage.
 
 
 
 ```python
 
 a = torch.rand(5, 5)
 
 b = a[0]
 
 b.fill_(0)
 
 print(a) # modified!
 
 ```
 
 
 
 If you want to compose together boolean indexing, you should first compose the boolean mask (using `&` and `|`), and then apply the indexing. Also, using `torch.where` can be helpful here.
 
 
 
 ---
 
 
 
 Given that this is not a bug and the behavior is expected and follow numpy semantics, I'm closing this issue, but please feel free to reopen it if you feel otherwise.",7467
22551,cuDNN arch mismatch - software or hardware error?,"Wait, I'll fix it in the related code. Looks like we can do better in choosing the right cudnn library during runtime.",8135
22552,[jit] add support for tuple unpacking in for loops,is this case going to be supported with your latest `Iterable` changes?,737
22553,Cumulative Maximum,"I think you can, last message from previous worker was in November, it is unlikely that he is still working",664
22554,"Problem running custom operator test, pytorch 1.1.0","As libtorch for cuda 10.0 has just a nightly build, I also tried installing torch-nightly (1.1.0.dev20190507) (via pip), and again save and load, but the same error exist. It seems that save has changed the way it creates .pt file, but load is not aligned with it, as load can load models which are saved by Pytorch 1.0 and not the ones saved by 1.1.0.
 
 
 
 gdb gives the same stack trace:
 
 ```bash
 
 Reading symbols from ./test_custom_ops...(no debugging symbols found)...done.
 
 (gdb) r
 
 Starting program: /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops ../test_op_nighty.pt
 
 [Thread debugging using libthread_db enabled]
 
 Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
 
 terminate called after throwing an instance of 'c10::Error'
 
  what(): isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)
 
 frame #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fffb3450441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)
 
 frame #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fffb344fd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)
 
 frame #2: <unknown function> + 0x9728f2 (0x7ffff6e8b8f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)
 
 frame #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ffff6e88f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)
 
 frame #4: <unknown function> + 0xa6f91b (0x7ffff6f8891b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)
 
 frame #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ffff6f89f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)
 
 frame #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ffff6f8a088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)
 
 frame #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)
 
 frame #8: main + 0x85 (0x445787 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)
 
 frame #9: __libc_start_main + 0xf0 (0x7fffb2ae5830 in /lib/x86_64-linux-gnu/libc.so.6)
 
 frame #10: _start + 0x29 (0x442969 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)
 
 
 
 
 
 Program received signal SIGABRT, Aborted.
 
 0x00007fffb2afa428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
 
 54 ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
 
 ```
 
 
 
 p.s: I updated my post (cmake should have -DCMAKE_PRIFIX_PATH=/path/to/libtorch/)",1303
22555,C++ Can only index with tensors that are scalars (zero-dim),"This feature is available in libtorch v1.5. Following the docs in https://pytorch.org/cppdocs/notes/tensor_indexing.html, we can write
 
 ```python
 
 data=torch.tensor([[[1],[1]],[[1],[1]]]);
 
 index=torch.tensor([[[1],[1]],[[1],[1]]]);
 
 print(data[index])
 
 ```
 
 in C++ as
 
 ```cpp
 
 auto data = torch::tensor({{{1}, {1}}, {{1}, {1}}});
 
 auto index = torch::tensor({{{1}, {1}}, {{1}, {1}}});
 
 std::cout << data.index({index}) <<std::endl;
 
 ```",7504
22556,Memory (CPU/sys) leak with custom batch norm layer,"Yeah, that `running_mean` update is exactly your problem. You keep extending the graph.",8466
22557,Get people to stop improperly using AT_ASSERT,"@Krovatkin The `ASSERT` macro, for internal implementation reasons, currently requires you use to a separate `ASSERTM` for messages.
 
 
 
 ```
 
 AT_ASSERTM(vec.size(), ""Vector size should never be equal to zero context ( node = "", *node, "" ) some more useful information "");
 
 ```
 
 
 
 I plan to eliminate this inconsistency. (I'll have to construct two strings on error case because of variadic macro comma pasting shenanigans, but that's a small price to pay).",493
22558,Softmax Docs Example Deprecated,"Closing as the PR is merged. @vadimkantorov please make a separate issue for running examples code. (I'm not sure how it's implemented, but I guess it's manually pasted there :P",1600
22559,pytorch1.1 training speed slower than pytorch1.0,"It is not only `DataLoader` threads, OMP by default can set up to 24 threads per process. Having `--nproc_per_node=8` you potentially ending up with 192 threads, which is way higher than your available threads. I recommend to run everything with `OMP_NUM_THREADS=10` and `OMP_NUM_THREADS=5` accordingly and compare results.",3948
22560,nccl runtime error: unhandled system error,"I think I fixed the issue. It's my fault not to set `NCCL_SOCKET_IFNAME`, `MASTER_ADDR` and `MASTER_PORT` correctly. Debug with `NCCL_DEBUG=INFO` is very useful!",3216
22561,cat()/stack() calls in JIT do not accept lists of tensors, it appears whatever type promotion we have for lists isn't working,569
22562,Backwards hangs,"In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.
 
 
 
 Heres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)
 
 
 
 Hope it helps",3743
22563,Load tensor from file in C++ fails,"It's still hidden behind a flag on the Python `torch.save` side that will eventually be flipped. Right now on nightly you can do this (copied from #22954):
 
 
 
 Python -> C++
 
 ```python
 
 import io
 
 f = io.BytesIO()
 
 torch.save(x, f, _use_new_zipfile_serialization=True)
 
 # send f wherever
 
 ```
 
 
 
 ```cpp
 
 // receive bytes in a `std::vector<char>`
 
 std::vector<char> f = get_the_bytes();
 
 torch::IValue x = torch::pickle_load(f);
 
 ```
 
 
 
 and the reverse (C++ -> Python)
 
 
 
 ```cpp
 
 torch::Tensor x = torch::ones({1, 2, 3});
 
 std::vector<char> f = torch::pickle_save(x);
 
 ```
 
 
 
 ```python
 
 x = torch.load(f)
 
 ```",4008
22564,load_state_dict: KeyError: 'unexpected key in state_dict',"I don't really know what's your plan. As @jekbradbury noticed, you're reinitializing the layer at every forward. You probably want to do sth like that:
 
 ```python
 
 class Maxout(nn.Module):
 
  def __init__(self, num_pieces, input_shape):
 
  super(Maxout, self).__init__()
 
  self.num_pieces = num_pieces
 
  self.linear = nn.Linear(input_shape, input_shape * num_pieces, bias=False)
 
 
 
  def forwad(self, input):
 
  # here you can use self.linear
 
 ```",3053
22565,pytorch does not build on ppc64le,"@gchanan is going to look into it, we got access to minicloud.",591
22566,Segmentation fault (core dumped) on Ubuntu 14.04.5,"can you do:
 
 
 
 ```
 
 gdb python
 
 ```
 
 
 
 and in gdb:
 
 
 
 ```
 
 r main.py
 
 ```
 
 
 
 And when segfault happens, you can type:
 
 
 
 ```
 
 where
 
 ```
 
 
 
 Then it will show a stack-trace. Can you paste that here. It'll be helpful to figure out the issue.
 
 
 
 Thanks.",9021
22567,ImportError: libiomp5.so: cannot open shared object file,"I got the same problem. My environment is CentOS 7, cuda 8.0, python 2.7. I installed it by pip as well.",3107
22568,Placeholder source builds for PyPI,"Thanks Luke.
 
 I've done this for `pip install torch`.
 
 I'm not the owner/maintainer of pytorch, @graingert is, and he said he'll transfer it to me / give commit rights. Once he gives me those rights, will do for `pip install pytorch` too.",6818
22569,"In-place sub with shared memory, wrong result",I think this is a duplicate of #906,3256
22570,issue when using LSTM with Dropout,"Could it be that you are operating with a very constrained memory? From the error messages, it looks like you are erroring out in cudnnSetDropoutDescriptor. The kernel initializing dropout states requires a lot of stack memory (grrrrrrrrr, XORWOW!), and it might return the error you are seeing if it is not able to allocate it.",1919
22571,"load_lua yields ""object has no attribute 'running_var'"" for BatchNormalization","The whole code looks like this:
 
 
 
 Torch:
 
 ```
 
 require 'nn'
 
 require 'dpnn'
 
 require 'cunn'
 
 model = torch.load('n4.small2.v1.t7')
 
 
 
 -- Forward pass a random tensor
 
 x = torch.FloatTensor()
 
 model:forward(x:resize(1,3,96,96))
 
 
 
 torch.save('nn4.small2.v1.resaved.t7',model)
 
 ```
 
 
 
 Then in pytorch:
 
 ```
 
 from torch.utils.serialization import load_lua
 
 model = load_lua('nn4.small2.v1.resaved.t7',unknown_classes=True)
 
 ```",7364
22572,RuntimeError: cuda runtime error (2) : out of memory at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487346124464/work/torch/lib/THC/generic/THCStorage.cu:66,"If you only consider the weights of a single Linear layer from that model. You get 
 
 ```
 
 49200^2 = 2Â 420Â 640Â 000
 
 ```
 
 elements + each element takes 4 bytes, which gives you
 
 ```
 
 2Â 420Â 640Â 000 * 4 / 1024^3 = 9,01GB
 
 ```
 
 for the weights alone. Then, you need another memory chunk of this size to store gradients. You also need to save the intermediate results so you can compute the gradient.",3572
22573,Sparse and CosineLoss only support Float on CUDA,"If nobody is working on fixing it, I'll send a PR later today",3458
22574,backward() in Autograd Index Function is broken when masking with ByteTensor,"Yes, just tested with last master, it seems to have been fixed, sorry!",8511
22575,"file_descriptor sharing strategy may be leaking FDs, resulting in DataLoader causing `RuntimeError: received 0 items of ancdata`","This does seem to be an issue in pytorch as best as we can tell - FDs aren't being released when they should be. As @sampathweb mentioned, it's reproducible using the above code. The workaround for now is:
 
 
 
 ```
 
 import resource
 
 rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)
 
 resource.setrlimit(resource.RLIMIT_NOFILE, (2048, rlimit[1]))
 
 ```
 
 (You may need to set 2048 to a higher number.)",7495
22576,pytorch not respecting torch.set_num_threads,"To answer @mattmacy 's high level question, I've bought an additional machine to setup perf-regression contbuilds, I'll get that setup in the next week or two. we will additionally be setting up OSX contbuilds as well.",7862
22577,Inconsistent argument names in Linear and Conv layers,"It could be said that Linear layers operate on 2D Tensor of N x Channels, and that ConvXd operates on consumes a produces a set of feature maps.",3904
22578,Download speed is too slow in China.,"pip install -i https://pypi.tuna.tsinghua.edu.cn/simple [some-package]
 
 you can use it to speed up.",10417
22579,ImportError: libpython3.6m.so.1.0,I solved the same issue by installing a `libpython3.6-dev` package on Ubuntu system.,3207
22580,[FEATURE REQUEST] Java extensions for PyTorch,"
 We dont have an explicit plan on this for Java.
 
 Our razor-sharp focus as core developers is Python / C / C++.
 
 If anyone in the community will develop other language extensions we are looking forward to how it turns out.",2854
22581,GPU goes away after an error occurs,Another common reason for this error could be if the number of classes in the labels does not match with the number of units in the final softmaxed Linear Layer for a classification problem. I recently encountered this accidentally.,1141
22582,[PyTorch] Unary Operator Vectorization,"consider moving the CUDA implementations as well following the pattern in [BinaryOpsKernel.cu](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/BinaryOpsKernel.cu). My guess is that it's not much more work and will result in simpler dispatch and more opportunities to delete dead code. If it turns out to be a significant amount of extra work, don't bother. 
 
 
 
 (I don't expect many perf gains for CUDA, except maybe for column-major tensors or other ""compact"" but not ""contiguous"" tensors)",609
22583,`torch.irfft` and `np.fft.irfft` disagree,"https://pytorch.org/docs/master/torch.html#torch.irfft
 
 
 
 > Due to the conjugate symmetry, input do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when input is given by rfft() with rfft(signal, onesided=True). In such case, set the onesided argument of this method to True. Moreover, the original signal shape information can sometimes be lost, optionally set signal_sizes to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.",9548
22584,[feature request] torch.trapz or some other method for numerical integration similar to numpy,"Is anyone working on this, I would be happy take this up with some guidance.",3881
22585,[feature request] torch.pi,"We generally run on less of a bar than consensus, but I opened a more general feature request (again( in #19172",8269
22586,flat_hash_map.h error C3203 'templated_iterator' unspecialized class template,"Would you please update your VS 2017 to the latest version? If you don't want to update, then you can try to wrap these code with `#ifndef __CUDACC__ ` and `#endif`. cc @ezyang",8435
22587,"""undefined symbol: PySlice_Unpack"" of pytorch 1.0.0 with Python 3.6.0","Upgrading to Python 3.6.1 was suggested in https://github.com/pytorch/pytorch/issues/14931#issuecomment-445448141.
 
 
 
 Duplicate of #14931.",8070
22588,Torch.tensor.sum() is not accurate enough,"One thing that is worth investigating is why numpy, for float32 arrays, produces more accurate results than pytorch, when forcing the reduction to be float32 as well.",4660
22589,better CUDA optimization using read-only cache and __restrict__,"Thanks :)
 
 
 
 There was also some dead code so I'm going to clean that up too and commit in a couple days.",6811
22590,Building from source failed,"
 The issue is still unsolved",697
22591,Poisson NLL loss in libtorch,yea you're right. just put it in aten/native,11436
22592,`with torch.enable_grad` also works outside a `no_grad` context,"""Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`"" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you!",40
22593,Serving Trained model for pytorch,"We run PyTorch in production at Facebook, at enormous scale, so we've actually invested a lot in production serving capabilities and will only continue to improve things. It really just depends on what your use case is.
 
 
 
 Running inference on a trained model is as simple as `module(input)`, whether that module is traced using the JIT. The TorchScript JIT is one of the most exciting new features focused on this production use case. It allows for compilation of your high-level Python program. Here's a good example of how to use the hybrid frontend of the JIT: https://pytorch.org/tutorials/beginner/hybrid_frontend/learning_hybrid_frontend_through_example_tutorial.html I also like this tutorial showing you how to take advantage of LibTorch for Python-free C++ serving: https://pytorch.org/tutorials/beginner/saving_loading_models.html 
 
 
 
 Of course, you can also take advantage of ONNX export as @Amiralae suggests. This can be useful in combination with mobile runtimes or other runtimes that support ONNX on the server side, like NVIDIA's TensorRT or AWS Elastic Inference.
 
 
 
 If you want the model management features of a model server, you can get some of that from the cloud services that run PyTorch and ONNX. There are also several groups trying to build PyTorch compatible OSS model servers. As @Amiralae mentions, there's Kubeflow, which I would characterize as in development. Similarly, [MLFlow](https://mlflow.org/) is also working on a level PyTorch support within their model server. And the are PyTorch community projects, like [EuclidesDB](https://euclidesdb.readthedocs.io/en/latest/). Lots of innovation going on in open source model serving these days.
 
 
 
 Hope that helps.",8286
22594,torch::set_num_threads does not work,@kishwarshafin looks fine to me. we dont globally set a config -- you set the number of threads in your process' lifetime.,622
22595,The cuFFT plan cache is not CUDA context-aware,Itâ€™s alright. I figured out a way to trigger it.,4017
22596,"InstanceNorm, GroupNorm doesn't crash with # of elements == 1","Yep the issue and proposal make sense, would be a sensible thing to catch :+1:",8472
22597,[CUDA] torch::stack / torch::cat fails on a single element,"Hi, I am from @quansight team. I would like to work on this issue.",2892
22598,nn.functional.binary_cross_entropy_with_logits got error when work with 'weight',"your `weights` variable has requires_grad which is not supported: `binary_cross_entropy_with_logits` doesn't support back-propagating through the weights attribute.
 
 
 
 If you don't need the derivative w.r.t. `weights` then you can use `weights.detach()` instead of `weights`. If you need the derivative, then you'll having to implement `binary_cross_entropy_with_logits` yourself.",605
22599,checkpoint_sequential doesn't respect nn.Sequential's behavior, i agree with this approach,708
22600,DataLoader with sparse matrices adds a dimension to the batch,"How do i establish a custom collate function? is there a way around this?
 
 I am facing a similar issue",2961
22601,import failure (macOS),"My workaround
 
 ` $ conda activate [MYENV]`
 
 `$ pip uninstall torch `
 
 `$ pip install torch `",4291
22602,is it possible to add c++ api to load .pth model directly?,"we dont plan to provide an interface other than the one via TorchScript.
 
 
 
 > I really puzzled why pytorch DO NOT want to provide unified interface for both .pth and .pt, which is not difficult to implement
 
 
 
 Without re-implementing Python, it's simply not possible. So, no you are wrong, it **is** difficult to implement.",11359
22603,Inconsistent dimension argument numbering between function,there's some overlap. The issue you mentioned didn't address torch.diag_embed.,734
22604,mobilenet_v2 not working with the newest version(1.1.0),"great, thanks. this will be parts of the upcoming release next week.
 
 
 
nothing to fix, looks like already fixed in a newer MKL-DNN release.",9492
22605,PyTorch->Onnx conversion incorrect,"My mistake! I am using the correct PyTorch nightly build (1.2.0.dev20190731+cpu
 
 ), and I got this output from PyTorch nightly over dynamic_axes, 
 
 
 
 /home/charles/anaconda2/lib/python2.7/site-packages/torch/onnx/utils.py:718: UserWarning: Provided key output for dynamic axes is not a valid input/output name
 
  warnings.warn(""Provided key {} for dynamic axes is not a valid input/output name"".format(key))
 
 
 
 but it turns out that a super_resolution.onnx file was created anyways, and after exporting the model and after testing it in other NN frameworks I do indeed get the correct output image. Thanks for all the help!",4289
22606,Add nn.functional.interpolate slow down backward process more than 10 times,"Hi, thank you so much, after updating to nightly version, time cost for 2-layer model with interpolate is about 0.013s/iter and 2-layer model without interpolate is 0.009s/iter. Much better now.",2908
22607,Not able to install after building successfully from source,"I followed readme, used conda, etc. Still get this error. The libs it's looking for are in build/lib but for some reason, it can;t find them when it links. Done so many clean builds, all day.
 
 v1.3.0, but I saw it with latest version as well before.",3091
22608,[v1.2.0] Release Tracker,"@Nikronic Yeah, there was some trouble with the builds. Hopefully they'll be up today.",503
22609,Compilation error with CUDA 10.0/ninja on Linux,"enable the environment variable `export TORCH_CUDA_ARCH_LIST=7.0`, and this should be fixed.
 
 
 
 Basically, what's happening here is that you are building in a docker container that has CUDA, but doesn't have access to a GPU. So PyTorch is defaulting to building for all CUDA architectures, and I suspect it's including some old architectures. If you paste the full build log, we can tell what it's attempting to do.",9253
22610,Speed Regression in torch.qr,"QR batching PR was done after 1.1, please allow me to investigate.",5044
22611,torchvision0.3.0 is incompatible with pytorch1.2(nightly),"@ganleiboy If you use PyTorch 1.2, then you need torchvision 0.4.0. You cannot use torchvision 0.3.0 as it relies on the C++ API of PyTorch 1.1.",588
22612,torch.Tensor.repeat() fails for 0 repeats,"Hi, I'd like to try this. Expect a PR soon :+1:",2893
22613,[FR][hub] load_state_dict_from_url should avoid using tempfile or allow specifying tmp dir,"That sounds a pretty elegant fix. Thanks so much for the quick reply! :) 
 
 There is no rush. I live on the edge and run my code on master builds anyways :D",529
22614,Unnecessary extra memory allocation when using circular padding,Resolved by https://github.com/pytorch/pytorch/pull/39273.,5167
22615,[RFC] DDP Communication Hook,"
 Thank you for your reply.
 
 
 
 That's some really good work out there. I experiment with different gradient compression techniques. I am eagerly waiting to try out some compression algorithms using this new development.",2845
22616,[jit] `@staticmethod`s retrieved from `self` don't work on normal class,"Oh, maybe you mean to say that the arguments should be inferred to be `Tensor` instead of `MyCell`?",4594
22617,Use nn.spectral_norm in LSTM,"fwiw, the following script works on master (and likely in 1.5)
 
 ```
 
 import torch
 
 
 
 def lstm_spectral_norm(input_size, hidden_size, n_layers=1):
 
  lstm = torch.nn.LSTM(input_size, hidden_size, n_layers)
 
  name_pre = 'weight'
 
  for i in range(n_layers):
 
  name = name_pre+'_hh_l'+str(i)
 
  torch.nn.utils.spectral_norm(lstm, name)
 
  name = name_pre+'_ih_l'+str(i)
 
  torch.nn.utils.spectral_norm(lstm, name)
 
  return lstm
 
 
 
 ninp = 128
 
 m = lstm_spectral_norm(ninp, ninp)
 
 name = 'weight_hh_l0'
 
 m = m.cuda()
 
 print([(name, p.device) for name, p in m.named_parameters()])
 
 inp = torch.randn(3, 32, 128, device=""cuda"")
 
 ```
 
 There was a bug that was fixed couple months back.",9435
22618,Pytorch could save the model in FP16(Half) mode and reload it to FP32.,"@vadimkantorov This is confusing, while i saevd fp16 model in pth, when exporting it to onnx, it return a bug to me",726
22619,verify_ninja_availability does not return True,"
 
 It seems this doc line is for _is_ninja_available instead of verify_ninja_availability, if you need a true or false return value, you can use _is_ninja_available. I will update the doc and will change _is_ninja_available to is_ninja_available since this should be a public api. ",476
22620,"[JIT] Scripted model second run cost 5 mins, after that 1-2 secs","> Did you run the same script I did? Do you mind sharing the output of that script? I will try with your torch configuration
 
 
 
 I tried the exported models from some days ago. So, I think that this was the problem. 
 
 
 
 I have tested your code on new conda venv and is working again. I am going to try to export them again now and see if they works correctly now.",216
22621,DISABLED test_backward_node_failure (__main__.TensorPipeAgentDistAutogradTestWithSpawn),"For now it is expected. There's a fixme in the code, but I haven't gotten to address it yet:
 
 https://github.com/pytorch/pytorch/blob/ec5d579929b2c56418aacaec0874b92937d095a4/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L308-L316
 
 
 
 It's harmless though.",2567
22622,pytorch README build table cleanup for ppc64le,"PR https://github.com/pytorch/pytorch/pull/39475/files submitted.
 
 
 
 The build has now been renamed, so the link to the old name is briefly incorrect (until this file is merged).",4702
22623,DISABLED test_backward_node_failure_python_udf (__main__.TensorPipeAgentDistAutogradTestWithSpawn),"Yes, I wanted to get a few more commits in but actually let's first get this fixed and worry about those later. I'll send out a PR right away.",8495
22624,"When the name of running file is ""dis.py"", the pytorch environment will be crashed.","You have a file that shadows a builtin library of python https://docs.python.org/3/library/dis.html , which should not happen unless you are messing with Python's import path or PATH, neither of which is recommended. It's not really PyTorch's fault. Any code that uses `inspect` would be broken.",8626
22625,iOS predict got different result in simulator and device,"@yyang9887 This issue is duplicated with https://github.com/pytorch/pytorch/issues/38186. Root cause found, working on the solution.",749
22626,CTCLoss documentation is unclear,"Raise your PR! I'm sure the documentation can be much improved over what I put in there. (Maybe one could consider linking to the [distill.pub article](https://distill.pub/2017/ctc/) on CTC, too - but I haven't looked at the docs lately to see this.)
 
 Some of your points are not yet actionable (e.g. that the targets start at one because 0 is blank in the example is written as a question).
 
 I'd be very happy to review your PR, too, so please don't hesitate to tag me on it.",682
22627,[ONNX] Enable argmax and argmin with no dim and no keepdims,"Sure, I will",603
22628,DistributedDataParallel,"set up GPU ,however I still noticed that DistributedDataParallel wrapper cost more GPU memory:
 
 the model cost around 7300 MB when loaded into one GPU, and 21129 when wrapped in DistributedDataParallel.",703
22629,cmake build stuck,Would you please clean the build cache `python setup.py clean` and try again?,8434
22630,"Mac OS X build is broken: ld: warning: ignoring file lib/libonnxifi_loader.a, file was built for archive which is not the architecture being linked (x86_64): lib/libonnxifi_loader.a",`brew unlink binutils` solved my problem.,8725
22631,Gradient of torch.where not handling nan correctly,the good old mask problem with the autograd looking only at each step..,10853
22632,Model Quantization for PyTorch (Proposal),"> How to export to mobile
 
 
 
 Ha!",224
22633,Add Windows tests for multiple configurations,"Now that we have enabled Windows CI on Circle. Due to its large parallelism, we could proceed much further this time. Below are the things that we could do first.
 
 1. Separate CPU and CUDA builds. It will make the build script more readable. https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat#L79
 
 2. Build and test job for more python versions, at least python latest should be included. By doing this, we could avoid errors like this to happen. https://github.com/pytorch/pytorch/pull/30809
 
 3. VS 2019 latest builds. (~Usually we don't need this, unless MS made some significant changes to their compiler toolchain~ But actually, there are far more bugs than we might think.)
 
 4. VS 2017 latest builds (Same reason with the above one)
 
 
 
 The following are all the available options that are of low priority.
 
 1. Debug LibTorch builds. (Already tested in nightly jobs)
 
 2. Build and test job for more cuda versions. (Code changes related to CUDA are usually platform-agnostic)
 
 3. 32-bit CPU builds. (Users are limited)
 
 4. Static builds. (Not tested yet)
 
 5. Legacy OS test builds. (Not possible in CircleCI)
 
 7. Builds with other compilers, such as GCC, Intel CC, Clang. (Not easy to fix or maintain)
 
 8. Test with Python in Microsoft Store (Windows 10 only) (Not possible in CircleCI)
 
 
 
 cc @mingbowan",4537
22634,JITed program is getting stuck,"Great, thanks a lot for the explanation. I did try with `torch.no_grad()` but apparently, we can't do that inside the JIT yet. I haven't tried `.required_grad=True`. Will give it a shot.",2746
22635,ModuleNotFoundError: No module named 'models',**torch.load() requires model module in the same folder** pytorch/pytorch#3678,81
22636,Unserialize autograd,"Hi,
 
 May I ask if there is any progress?",2887
22637,Pass Multiple Tensors in CPP Module Forward,"[solved] Turned out it works to push multiple input tensors to the IValue like so:
 
 
 
 ```cpp
 
 std::vector<torch::jit::IValue> i;
 
 std::vector<float> x = {2.0, 1.0, 0.5, 3.0, 4.0};
 
 std::vector<float> y = {3.0, 2.0};
 
 i.push_back(torch::tensor(x));
 
 i.push_back(torch::tensor(y));
 
 
 
 torch::Tensor output = module->forward(i).toTensor();
 
 ```",8700
22638,Port TemporalReplicationPadding to ATen,I filed a ticket to give you rights.,3082
22639,mkl-dnn build breaks with mkl 2019.3,"MKL-DNN version breaks on ClearLinux 28320 as well for mkl 2019.3
 
 
 
 I can confirm that Pytorch builds successfully with mkl=2019.1 and mkl-include=2019.1 for Python 3.7.1",4170
22640,[jit] be more permissive with bool casting.,"Yeah, that would be good to see the behavior depicted in the docs, I am sure I am not the first one to stumble on this issue",8468
22641,[FR] make torch.nn.utils.convert_sync_batchnorm a classmethod of SyncBatchNorm,"Just want to comment. If we do this, we need to do this before next release.",4041
22642,pip install torch==1.0.1.post2,"Sorry, pytorch for Windows is currently not available through PYPI. You have to enter the commands in https://pytorch.org.",5992
22643,Can't find `Python.h` when include `torch/extension.h`,"Soumith solved the OP's problem already, but if anyone else arrives here from Google just looking for `Python.h`, the solution is to get the Python include path from
 
 ```python
 
 import sysconfig
 
 print(sysconfig.get_paths()['include'])
 
 ```
 
 and add it to your includes.",5995
22644,update from source,a PR here: https://github.com/pytorch/pytorch/pull/18409,706
22645,Build from source in virtualenv causes libiomp5.so load failure,(2) sounds simple and easy. I don't want mkl-dnn downloading stuff in the build.,62
22646,Unable to locate SpatialAdaptiveAveragePooling.c,"SpatialAdaptiveAveragePooling was removed in [this commit](https://github.com/pytorch/pytorch/commit/64b336420918d31ee3da623bbcb7afadc73edb6d).
 
 
 
 Please close the issue if you think it is resolved.",6007
22647,Regarding porting of functions in ATen directory,some backend code is simply not used by any function in python. it's old code from a while ago.,10716
22648,AttributeError: Can't get attribute '_rebuild_parameter' on,"ty 
 
 i found solution 
 
 i created my model with version 1.0
 
 when i upload it in machine ,pytorch version is 0.4",11274
22649,Are there any difference of autograd between these two ways to get the gradient of FFT?,"pytorch native fft works on cuda, and should be preferred as we have extensive tests on it. the tutorial is just an example of how to write extensions.",10523
22650,Cannot create operator of type 'ReduceL2' on the device 'CUDA' when use caffe2 to run export model,"Sorry for this mistake. Keep me informed if you find any bug please.
 
 `// simple implementation of ReduceL2
 
 template <typename T>
 
 __device__ void warpReduce(volatile T* sdata, int tid) {
 
  sdata[tid] += sdata[tid + 32];
 
  sdata[tid] += sdata[tid + 16];
 
  sdata[tid] += sdata[tid + 8];
 
  sdata[tid] += sdata[tid + 4];
 
  sdata[tid] += sdata[tid + 2];
 
  sdata[tid] += sdata[tid + 1];
 
 }
 
 template <typename T>
 
 __global__ void
 
 kRowwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {
 
  extern __shared__ T sh[];
 
  int tid = threadIdx.x;
 
  // int rid = blockIdx.x; // each block process one row of the matrix
 
  for (int rid = blockIdx.x; rid < nrows; rid += gridDim.x) {
 
  const T* x = data + rid * ncols;
 
  //T* y = output + rid * ncols;
 
  sh[tid] = 0.0f;
 
  __syncthreads();
 
  float tmp = 0.0f;
 
  for (int j = tid; j < ncols; j += blockDim.x) {
 
  tmp += x[j] * x[j];
 
  }
 
  sh[tid] = tmp;
 
  __syncthreads();
 
  for (int s = blockDim.x / 2; s > 32; s >>= 1) {
 
  if (tid < s) {
 
  sh[tid] += sh[tid + s];
 
  }
 
  __syncthreads();
 
  }
 
  if (tid < 32) {
 
  warpReduce(sh, tid);
 
  }
 
  if (tid == 0) {
 
  output[rid] = sqrtf(sh[tid]);
 
  // sh[tid] = sqrtf(sh[tid]);
 
  }
 
  //__syncthreads();
 
  }
 
 }
 
 template <typename T>
 
 __global__ void
 
 kColwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {
 
  extern __shared__ T sh[];
 
  int tid = threadIdx.x;
 
  // int rid = blockIdx.x; // each block process one row of the matrix
 
  for (int cid = blockIdx.x; cid < ncols; cid += gridDim.x) {
 
  //const T* x = data + rid * ncols;
 
  //T* y = output + cid * ncols;
 
  sh[tid] = 0.0f;
 
  __syncthreads();
 
  T tmp = 0.0f;
 
  for (int j = tid; j < nrows; j += blockDim.x) {
 
  tmp += data[j * ncols + cid] * data[j * ncols + cid];
 
  }
 
  sh[tid] = tmp;
 
  __syncthreads();
 
  for (int s = blockDim.x / 2; s > 32; s >>= 1) {
 
  if (tid < s) {
 
  sh[tid] += sh[tid + s];
 
  }
 
  __syncthreads();
 
  }
 
  if (tid < 32) {
 
  warpReduce(sh, tid);
 
  }
 
  if (tid == 0) {
 
  output[cid] = sqrtf(sh[tid]);
 
  // sh[tid] = sqrtf(sh[tid]);
 
  }
 
  //__syncthreads();
 
  }
 
 }
 
 
 
 
 
 #define CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(T) \
 
  template <> \
 
  C10_EXPORT void ReduceL2<T, CUDAContext>( \
 
  const int num_dims, \
 
  const int* dims, \
 
  const int num_axes, \
 
  const int* axes, \
 
  const T alpha, \
 
  const T* X, \
 
  T* Y, \
 
  CUDAContext* context) { \
 
  CAFFE_ENFORCE_LE(num_axes, num_dims); \
 
  std::vector<int> Y_dims_vector(dims, dims + num_dims); \
 
  for (int i = 0; i < num_axes; ++i) { \
 
  Y_dims_vector[axes[i]] = 1; \
 
  } \
 
  const int* X_dims = dims; \
 
  const int* Y_dims = Y_dims_vector.data(); \
 
  const int X_size = \
 
  std::accumulate(X_dims, X_dims + num_dims, 1, std::multiplies<int>()); \
 
  const int Y_size = \
 
  std::accumulate(Y_dims, Y_dims + num_dims, 1, std::multiplies<int>()); \
 
  if (X_size == 0) { \
 
  Set<T, CUDAContext>(Y_size, 0, Y, context); \
 
  return; \
 
  } \
 
  if (alpha == T(0)) { \
 
  Set<T, CUDAContext>(Y_size, 0, Y, context); \
 
  return; \
 
  } \
 
  if (std::equal(X_dims, X_dims + num_dims, Y_dims)) { \
 
  Abs<T, CUDAContext>(X_size, X, Y, context); \
 
  Scale<T, T, CUDAContext>(Y_size, alpha, Y, Y, context); \
 
  return; \
 
  } \
 
  int rows; \
 
  int cols; \
 
  if (utils::IsRowwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \
 
  kRowwiseL2Reduce<T> \
 
  <<<CAFFE_GET_BLOCKS(rows), \
 
  CAFFE_CUDA_NUM_THREADS, \
 
  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \
 
  context->cuda_stream()>>>(X, Y, rows, cols); \
 
  return; \
 
  } \
 
  if (utils::IsColwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \
 
  kColwiseL2Reduce<T> \
 
  <<<CAFFE_GET_BLOCKS(cols), \
 
  CAFFE_CUDA_NUM_THREADS, \
 
  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \
 
  context->cuda_stream()>>>(X, Y, rows, cols); \
 
  return; \
 
  } \
 
  int pre; \
 
  int mid; \
 
  int nxt; \
 
  if (utils::IsBothEndsReduce(num_dims, X_dims, Y_dims, &pre, &mid, &nxt)) { \
 
  LOG(FATAL) << ""Currently ReduceL2 only supports row/col wise reduce""; \
 
  return; \
 
  } \
 
  LOG(FATAL) << ""Currently ReduceL2 only supports row/col wise reduce""; \
 
  }
 
 CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(float)`",5985
22651,"Compile fail on HEAD, `autodiff.cpp:156:20: error: â€˜inlineCallToâ€™ is not a member of â€˜torch::jit::scriptâ€™` and others",Please reopen if there's any further problems. Thanks!,4853
22652,Bad error message when creating a class instance in script functions,@apaszke This issue can be closed. This [PR](https://github.com/pytorch/pytorch/pull/16416) resolves it.,542
22653,Core dump on threadripper/Ryzen,"OK, found the issue. I had an old version of MKL-DNN installed as a dynamic library:
 
 
 
 `#6 0x00007fffe3f873bf in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp () from /usr/local/lib/libmkldnn.so.0`
 
 
 
 ...oversaw this as pytorch source includes mkl-dnn repo so I (wrongly) assumed it would be linked statically, but it looks it's linked dynamically and if you have another version in your system it picks the dynamic library.
 
 
 
 Solution: I reinstalled last official release of mkl-dnn from Intel repo and works now.",4556
22654,libtorch API params error with outdated documentation,"In
 
 
 
 `auto img_tensor = torch::CUDA(torch::kFloat32).tensorFromBlob(img_float.data, {1, 224, 224, 3});`
 
 
 
 You are creating a CUDA tensor and then assigning it a blob that is on CPU. This is not allowed. You have to create a CPU tensor first and then *copy* the data to CUDA memory. You should write `torch::CPU(torch::kFloat32).tensorFromBlob(...).to(torch::kCUDA)`.
 
 
 
 Also note that `torch::CPU` and `torch::CUDA` are deprecated in PyTorch 1.0. You should write this as `torch::from_blob(img_float.data, {1, 224, 224, 3}).to(torch::kCUDA)`.",3687
22655,Pytorch Build: Caffe2 libcaffe2_gpu.so undefined reference to '__cudaPopCallConfiguration',"I noticed that I had installed a potentially incorrect version of 'magma-cuda'.
 
 
 
 I'd installed magma-cuda92 rather than magma-cuda90.
 
 
 
 Might be worth checking.
 
 
 
This appears to have resolved this for me!",3171
22656,Loading a PyTorch Model in C++ tutorial fails with No rule to make target '/usr/local/cuda/lib64/libculibos.a',"Bandaid solution:
 
 
 
 In the file `/path/to/libtorch/share/cmake/Caffe2/Caffe2Targets.cmake`
 
 
 
 Replace the references to `/usr/local/cuda/...` with what ever path your CUDA is installed to",1400
22657,nan propagates through backward pass even when not accessed,"Why is selection handled through multiplication by 0, though? And doesnâ€™t the multiplication incur an overhead too? Shouldnâ€™t it be handled by a conditional like torch.where?",8410
22658,pdist with large inputs is giving illegal memory exception on CUDA,"E.g. here https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu#L264, add cudaGetLastError call after kernel launch. 
 
 If launch parameters are invalid (too many blocks, too many threads etc) it will error out immediately.
 
 2. Yes, I was talking about number of vectors. Rather than adding a loop, it may be easier to set gridDim.x to the number of output elements, because max value of gridDim.x is 2^31.",2231
22659,Can't open discuss.pytorch.org,I can not get discuss.pytorch.org via chrome on my mac pro but it is reachable for example by opera,3023
22660,[feature request] torch.cuda.is_supported(),torch.cuda._check_capability(),11051
22661,`torch.hub` cannot download if branch name starts with `v`,cc @ailzhang,9030
22662,c++ Frontend: example mnist hits nan,The NaN/instability issue is now fixed. The model converges successfully.,6929
22663,[jit] the torch script and c++ api using,"concatenation works. Otherwise, you could do the following to handle multiple outputs:
 
 ```
 
 ...
 
 auto outputs = module->forward(inputs).toTuple();
 
 torch::Tensor out1 = outputs->elements()[0].toTensor();
 
 torch::Tensor out2 = outputs->elements()[1].toTensor();
 
 ...
 
 ```",714
22664,Feature Request: Nested state_dict (OrderedDict) for nested modules,"We discussed this in the early days, when implementing `state_dict`. The conclusion was that we dont want to complicate the `state_dict` API by adding nesting (or other schemes that were under discussion).
 
 The nesting is already given in the keys of the dict, such as `layer1.layer2.layer3.conv1.weight`, and it's trivial to filter the keys according to the nesting you want.
 
 We wanted to keep it simple and stupid, and let users refilter / reorder as they want.",8260
22665,Migrate `gather` from the TH to Aten (CUDA),"You can do it with TensorIterator, using an approach similar to https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/IndexKernel.cu. I'm concerned that it'll lead to regressions compared to THC, but you can experiment with maybe simplifying indexing computations. Directly copying approach that you used for cpu, with separating the indexed dimension into a loop is not going to work here, both because the remaining exposed parallelism won't be enough on the gpu, and because memory accesses from a single thread are going to be very inefficient. So my suggestion is, experiment with TensorIterator-like approaches for some time (trying to reproduce general THC access pattern with TI), but if that does not work, just port THC code. 
 
 As for determinism, let's ignore that for now and just use atomic operations.",8574
22666,Migrate `index_select` from the TH to Aten (CUDA),"Actually, I'm going to start working on this now, since I'm now on idle in all my other tasks and it looks like kshitij12345 has not committed himself to this issue yet.",931
22667,PyTorch breaks Matplotlib,"The issue could be reproduced on my system too.
 
 Ubuntu 16.04.2 LTS, Python version 2.7.12, PyTorch version 0.1.12_2, Matplotlib version 2.0.2
 
 ```
 
 Python 2.7.12 (default, Nov 19 2016, 06:48:10) 
 
 [GCC 5.4.0 20160609] on linux2
 
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 
 >>> import matplotlib
 
 >>> matplotlib.use('Agg')
 
 >>> import matplotlib.pyplot as plt
 
 >>> import torch
 
 >>> 
 
 >>> fig = plt.figure()
 
 >>> plt.plot([1,2])
 
 [<matplotlib.lines.Line2D object at 0x7f160844bd50>]
 
 >>> fig.savefig('test.pdf')
 
 *** Error in `python': free(): invalid pointer: 0x00007f16361d6ac0 ***
 
 ======= Backtrace: =========
 
 /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f165ca1b7e5]
 
 /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f165ca2437a]
 
 /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f165ca2853c]
 
 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7f163a193fa1]
 
 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7f163a1eae79]
 
 ```",7154
22668,[Feature Request] PyTorch RNN State Dropout,what about now ?,11364
22669,libAten undefined symbol,"Yeah, I found that removing the build and torch/lib/build folders and rebuilding fixed it. I do not know why the normal install procedure doesn't pick up the change.",8460
22670,support slicing on nn.Sequential,"I attempted getting a `slice` to work. Is this fine?
 
 ```
 
 def __getitem__(self, inp):
 
  if isinstance(inp, slice):
 
  ret_seq = Sequential()
 
  seq_list = list(self._modules.items())
 
  for i in range(*inp.indices(len(self._modules))):
 
  ret_seq.add_module(seq_list[i][0], seq_list[i][1])
 
  return ret_seq
 
 ```
 
 If this is fine, I can issue a PR too. Let me know what you think. I believe this modification needs to be done in the `Sequential` class.",3001
22671,should weight norm only recompute weights at the beginning and after each call to backward?,"Right, so in recurrent nets we're recomputing the `weight` multiple times, which is unnecessary and leads to extra memory usage.
 
 
 
 The backward hook solution isn't robust: `weight` needs to be re-computed if `weight_v` or `weight_g` changes, not after they're gradients are computed. For example:
 
 
 
 ```python
 
 output = module(input) # 1
 
 output.backward() 
 
 output2 = module(input) # 2
 
 optimizer.step()
 
 output3 = module(input) # 3
 
 ```
 
 
 
 With the backward hook, weight would be recomputed at (2) when it should be recomputed at (3).",5711
22672,Problem when using DataLoader with a 1-dimensional FloatTensor,We can solve this once we get scalars,8251
22673,Missing numpy dependency at installation,I've fixed this on master and it'll be correctly installed with the wheel files of the next release,3351
22674,Memory leak problem in LSTM and RNN,"You may refer to this example to get precise usage of `detach` in a long sequence:
 
 https://github.com/pytorch/examples/blob/master/word_language_model/main.py
 
 line132 `hidden = repackage_hidden(hidden)` does the same thing as `detach`.",8636
22675,Issues with Loss1 + 0*Loss2 and graph computation,BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case,1394
22676,"""LogSoftmax can only be differentiated once""",I'll take a look.,3326
22677,"when i use the max_unpool2d ,i got some error","Have a look a the [full documentation of MaxUnpool](http://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool2d). You first need to get the indices from `MaxPool`, and then pass those indices to `MaxUnpool`. For example:
 
 ```python
 
 out1, indices = F.max_pool2d(x, kernel_size=2, return_indices=True)
 
 result = F.max_unpool2d(out1, indices, kernel_size=2, stride=2, output_size=x.size())
 
 ```
 
 Note that you need to pass the `stride` parameter manually to `max_unpool2d` for the moment.
 
 We should change a bit how we handle the default in `max_unpool` so that we don't have to pass the `stride` manually. I'll send a fix for it.",2755
22678,CUDA OOM for tiny Variable broadcast-add,"It makes sense. Broadcasting will make the output be of size (500000, 500000, 1) because it aligns dims to the right (not to the left)
 
 ```
 
 500000 1 1
 
  500000 1
 
 ---------------------
 
 500000 500000 1
 
 ```",3967
22679,where is the torch.nn.NLLLoss ?,"`torch.nn.NLLLoss` is here:
 
 
 
 https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py?hl=64#L64
 
 
 
 Like many modules, this module is just a thin wrapper around the corresponding function, in this case `nll_loss`:
 
 
 
 line 131: `return F.nll_loss(...)`
 
 
 
 Tracing the F import, we find:
 
 
 
 https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L644
 
 
 
 This function, in turn, calls:
 
 
 
 `return _functions.thnn.NLLLoss(...)`
 
 
 
 `_functions` is here:
 
 https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/loss.py
 
 
 
 But this is kind of a dead end, as there is no Python code implementing `NLLLoss`.
 
 
 
 Like many low-level functions, `NLLLoss` is implemented in C. The PyTorch C code is here:
 
 
 
 https://github.com/pytorch/pytorch/blob/master/torch/lib
 
 
 
 `torch.nn` implementations are in THNN, which leads to the actual implementation:
 
 
 
 https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/ClassNLLCriterion.c#L5",8749
22680,PyTorch with numpy syntax?,"> The Tensor API matches numpy's exactly
 
 
 
 This is, IMHO, the biggest stopper in teaching (otherwise close to perfect) pytorch as a first deep learning framework.
 
 Really looking forward to the day pytorch will match numpy API.",247
22681,tensorflow conflicts with nn.DataParallel,"It's too bad that Tensorflow changes the current device, but this is the expected PyTorch behavior. The model must be on `device_ids[0]`. `device_ids` defaults to `0, 1, 2, ...`. If you're model is on device 7, you must manually specify `device_ids`.
 
 
 
 Or, just set the current device after the TensorFlow call:
 
 
 
 ```python
 
 config = tf.ConfigProto()
 
 config.gpu_options.allow_growth = True
 
 with tf.Session(config=config) as sess:
 
  sess.run(emb.initializer)
 
 
 
 torch.cuda.set_device(0) # set the device back to 0
 
 
 
 model = torch.nn.Linear(128, 1).cuda()
 
 model = torch.nn.DataParallel(model).cuda()
 
 
 
 data = Variable(torch.Tensor(8,128)).cuda()
 
 x = model(data)
 
 ```",4012
22682,Error installing pytorch vision,"Try pip install with --no-deps 
 
 Courtsey: https://discuss.pytorch.org/t/failed-instalation-with-pip3-for-torchvision/12667/6",8018
22683,Error building torch from source for python 2.7 on macOS,going back to f1fd4ac7edeb2477404883b21c3c4fe8420c0757 makes the build work,9479
22684,Implementing hashing (2016 paper) to improve computational performance by 95%,questions and discussions are usually better at https://discuss.pytorch.org,10546
22685,linux python3.6 pip install command wrong,we provide one package that has everything and supports GPU and CPU. This is better and consistent packaging in our opinion. This is not a typo/mistake.,705
22686,Bus error (core dumped) model share memory,"Okay. I think I solved it. Looks like the shared memory of the docker container wasn't set high enough. Setting a higher amount by adding `--shm-size 8G` to the `docker run` command seems to be the trick as [mentioned here](https://github.com/pytorch/pytorch/issues/1355#issuecomment-308587289). Let me fully test it, if solved I'll close issue.",4618
22687,Fork start method is susceptible to deadlocks,`mp.set_start_method('spawn')` seems to resolve the issue.,8731
22688,what is exactly batch_size in pytorch?,"sample == example
 
 
 
 If you have 10 samples or examples in a batch, then the batch size is 10. Maximum batch_size is limited by the memory that your system has -- main memory in case of CPU and GPU memory if you are using the GPU. 
 
 
 
 Also, please use the PyTorch discussion forum for questions like these, rather than opening an issue.",10640
22689,Unable to use multiple GPUs,"Please use forum (discuss.pytorch.org) for questions. DataParallel uses multiple GPUs only for forward method, if you want to use multiple GPUs for other methods, you have to manually do it.",4864
22690,Should I expect a performance difference between jit and native c++ models?,"Yes, you should expect a performance difference. The C++ API has the same execution model as PyTorch eager, and re-uses the same infrastructure. It will execute your model code op by op, as you wrote it.
 
 
 
 The JIT has a domain-specific optimizing runtime, so in general you should expect your code to be faster than the C++ frontend. However, we're still early on in delivering perf wins using compilation techniques (it's a big focus for us for 2020), so I'd encourage you to benchmark different approaches for your workload.
 
 
 
 Going to close this issue to help with our triage process, but feel free to comment if you have followup questions!",8535
22691,log cdf and log survival function of normal distribution,"We have `.cdf` for most distributions, you can get what you want (e.g. for normal distribution) via `log_cdf = normal_dist.cdf(x).log()` and `log_sf = torch.log1p(-normal_dist.cdf(x))`.",8272
22692,Unbound local variable in LR scheduler,"Indeed, this should remain opened for 1.",3838
22693,LambdaLR type bug,"I agree that the type should be updated. `lr_lambda` returns a float though. Did you mean the following?
 
 
 
 ```python
 
 LRLambdaType = Callable[[int], float]
 
 
 
 class LambdaLR(_LRScheduler):
 
  def __init__(self, optimizer: Optimizer, lr_lambda: Union[LRLambdaType, List[float]], last_epoch: int=...) -> None: ...
 
 ```
 
 
 
 Please feel free to open a PR for this.",2983
22694,Missing default value in torch/optim/lr_scheduler.pyi,"> Thanks for pointing this out. Please feel free to open a PR for this.
 ",246
22695,[Question]Pipeline batches inferencing," 
 > The idea is to start the calculations of the next batch while the previous batch is still calculating
 
 
 
 If you are looking for GPipe-like parallelism, yes, it is possible and it's quite simple. Checkout this [tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html).
 
 
 
 The paper you cited seems to do it differently though:
 
 
 
 > we overwrite network activations whenever new ones, i.e., from new frames, become available.
 
 Such a more gradual accumulation of information from both passes breaks the precise correspondence between gradients and activations, leading to theoretically more noisy weight updates.
 
 
 
 I haven't read the full paper, but looks like they could use last batch's gradients and this batch's activations to update parameters? Is that correct?",2806
22696,RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Loops.cuh:197,"Same problem if QAT is performed with 'fbgemm' parameters
 
 ```
 
 model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
 
 torch.backends.quantized.engine = 'fbgemm'
 
 ```
 
 But it works with 'qnnpack' parameters
 
 
 
 The error is following:
 
 ```
 
 Traceback (most recent call last):
 
  File ""train_net.py"", line 120, in <module>
 
  args=(args,),
 
  File ""/root/some_detectron2/detectron2/engine/launch.py"", line 52, in launch
 
  main_func(*args)
 
  File ""train_net.py"", line 78, in main
 
  return trainer.train()
 
  File ""/root/some_detectron2/detectron2/engine/defaults.py"", line 380, in train
 
  super().train(self.start_iter, self.max_iter)
 
  File ""/root/some_detectron2/detectron2/engine/train_loop.py"", line 132, in train
 
  self.run_step()
 
  File ""/root/some_detectron2/detectron2/engine/train_loop.py"", line 215, in run_step
 
  loss_dict = self.model(data)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
 
  result = self.forward(*input, **kwargs)
 
  File ""/root/some_detectron2/detectron2/modeling/meta_arch/rcnn.py"", line 121, in forward
 
  features = self.backbone(images.tensor)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
 
  result = self.forward(*input, **kwargs)
 
  File ""/root/DensePose_ADASE/densepose/modeling/quantize.py"", line 177, in new_forward
 
  p5, p4, p3, p2 = self.bottom_up(x) # top->down
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
 
  result = self.forward(*input, **kwargs)
 
  File ""/root/DensePose_ADASE/densepose/modeling/quantize.py"", line 130, in new_forward
 
  return old_forward(self, x)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/timm/models/efficientnet.py"", line 350, in forward
 
  x = self.conv_stem(x)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
 
  result = self.forward(*input, **kwargs)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py"", line 243, in forward
 
  return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py"", line 95, in _forward
 
  conv = self._conv_forward(input, self.weight_fake_quant(scaled_weight))
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
 
  result = self.forward(*input, **kwargs)
 
  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/quantization/fake_quantize.py"", line 86, in forward
 
  self.ch_axis, self.quant_min, self.quant_max)
 
 RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/ATen/native/cuda/Loops.cuh:56, please report a bug to PyTorch.
 
 ```
 
 
 
 CUDA: 10.2
 
 PyTorch: py3.7_cuda10.2.89_cudnn7.6.5_0
 
 OS: Ubuntu 18",5730
22697,"Quantized weights cant be loaded, when tried, generate ""copy_"" not implemented for \'QInt8' exception","@Coderx7 seems like when you load the quantized state dict you are trying to load it into the float model. You will first have to call `prepare_qat(m,..)` followed by `m = convert(m, ..)` and then call `m.load_state_dict('quantized_state_dict')`
 
 
 
 Closing this issue since it isn't a bug, if there is a further issue please post in the discussion forum.",486
22698,Can't build documentation on master,"It looks like [that feature](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#skipping-members) already exists. I found a [simple example](https://stackoverflow.com/a/21449475) of how to include this, although it would need to check if distributed is available.",3956
22699,Failure on non-contiguous gradients for F.pad/permute combination,I'm optimistically closing this because it seems fixed on master. Please feel free to reopen if this is not the case.,3343
22700,"torch.prod with internal upcasting (fp16 input, dtype=torch.float32 output) produces garbage","Putting high priority because of silent wrong answer. 
 
 The root cause is likely incorrect `prod_kernel_impl`, note there's no out_t, compared to working `sum_kernel_impl`
 
 ```
 
 template <typename scalar_t, typename acc_t=scalar_t, typename out_t=scalar_t>
 
 void sum_kernel_impl(TensorIterator& iter) {
 
  gpu_reduce_kernel<scalar_t, out_t>(iter, func_wrapper<out_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {
 
  return a + b;
 
  }));
 
 }
 
 template <typename scalar_t, typename acc_t=scalar_t>
 
 void prod_kernel_impl(TensorIterator& iter) {
 
  gpu_reduce_kernel<scalar_t, scalar_t>(iter, func_wrapper<scalar_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {
 
  return a * b;
 
  }), 1);
 
 }
 
 ```",4937
22701,"Bizarre ""no kernel image"" error for pytorch built from source","det is implemented in MAGMA, so this might be related to how you compiled against magma. Could you please run the [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)",9163
22702,Pytorch 1.4.0 ARM build failing,The problem was the docker container running low on memory and gcc was crashing. Increasing memory in docker desktop on mac to 8GB solved it.,7234
22703,Windows nightly CPU build failed,"Seems to be resolved by #32116, #32114 and #32112 according to https://ezyang.github.io/pytorch-ci-hud/build/pytorch-master.",5809
22704,Incorrect typing errors,"This is all fixed (3 is a user code snippet that needs changing as in the comment above), so closing.
 
 
 
 gh-38062 replaces `is_tensor` usages in this repo in anticipation of adding mypy typechecking on that code, to avoid potential issues.",7620
22705,Autograd fails if used before multiprocessing Pool,"I am not too familiar with C++ so better someone else takes a look, sorry.
 
 Definitely worth adding some kind of error message as you suggested.",2994
22706,Improve docs search engine indexing,"The ""single page per module"" strategy makes it VERY HARD to search for Pytorch documentation since Google will just redirect you to the top of the page, then you have to hit `Ctrl + f` and type the term again (try finding `nn.Linear` which appears 23 times).
 
 
 
 Why not break it into a page per function / class like most other libraries (numpy, pandas, sklearn, etc)? It would reduce the load time problem discussed in #20984 by virtue of just having to process less content and it would make its content easier to find.",6903
22707,Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2',"could you change this line 
 
 https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/activation_distribution_observer.cc#L500
 
 to 
 
 ```
 
 ist.str(first_line);
 
 ```
 
 And see if it works for you?",484
22708,Libtorch C++ load model error on windows10,"A prebuild pdb version would save many peoples time for quick debugging. 
 
 
 
running on Window (which might be similar to @hset911), it cashes at top level (the sample example on your site) on this line:
 
 
 
  `at::Tensor output = module->forward(inputs).toTensor();`
 
 
 
 Internally crashed at constants.cpp:
 
 
 
 ```cpp
 
 c10::optional<IValue> toIValue(const Value* v) {
 
  if (v->node()->kind() != prim::Constant) {
 
  return c10::nullopt;
 
  }
 
  // use implemenation of prim::Constant to compute the output IValue
 
  auto op = getOperation(v->node());
 
  Stack stack;
 
  op(stack);
 
  return stack.back(); <------------ here (due to the stack is empty)
 
 }
 
 ```
 
 
 
 Callstack:
 
 > torch.dll!torch::jit::toIValue(const torch::jit::Value * v) Line 164 C++
 
   torch.dll!torch::jit::Node::get(c10::Symbol name) Line 640 C++
 
   torch.dll!torch::jit::Node::is_constant(c10::Symbol name) Line 368 C++
 
   torch.dll!torch::jit::Node::matches(const char * signature_literal, c10::ArrayRef<c10::Symbol> const_inputs) Line 653 C++
 
   torch.dll!torch::jit::PeepholeOptimizeImpl(torch::jit::Block * block, bool addmm_fusion_enabled) Line 65 C++
 
   torch.dll!torch::jit::PeepholeOptimize(torch::jit::Block * block, bool addmm_fusion_enabled) Line 175 C++
 
   torch.dll!torch::jit::PeepholeOptimize(const std::shared_ptr<torch::jit::Graph> & graph, bool addmm_fusion_enabled) Line 182 C++
 
   torch.dll!torch::jit::GraphExecutorImpl::runOptimization(std::shared_ptr<torch::jit::Graph> & graph, const torch::jit::ArgumentSpec & spec) Line 503 C++
 
   torch.dll!torch::jit::GraphExecutorImpl::compileSpec(const torch::jit::ArgumentSpec & spec) Line 473 C++
 
   torch.dll!torch::jit::GraphExecutorImpl::getOrCompile(const std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 440 C++
 
   torch.dll!torch::jit::GraphExecutorImpl::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 374 C++
 
   torch.dll!torch::jit::GraphExecutor::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & inputs) Line 610 C++
 
   example-app.exe!torch::jit::script::Method::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 72 C++
 
   example-app.exe!torch::jit::script::Method::operator()(std::vector<c10::IValue,std::allocator<c10::IValue> > stack) Line 77 C++
 
   example-app.exe!torch::jit::script::Module::forward(std::vector<c10::IValue,std::allocator<c10::IValue> > inputs) Line 384 C++
 
   example-app.exe!main(int argc, const char * * argv) Line 45 C++
 
 
 
 It's real cause is due to the line above it op(stack). 
 
 The lambda operator returned which supposed to do a push onto the stack but failed (in constant.cpp):
 
 
 
  return [i](Stack& stack) {
 
  push(stack, i);
 
  return 0;
 
  };
 
 
 
 The push function doesn't work for the above case where only pushing one element on the stack. Pushing more elements like push(stack, 1, 2) worked fine. But not push(stack, 1). Probably due to a combination of this particular implementation and VS compiler (VS 15.6.6).
 
 
 
 In jit\stack.h:
 
 
 
 ```cpp
 
 template <typename... Types>
 
 static inline void push(Stack& stack, Types&&... args) {
 
  constexpr size_t N = sizeof...(args);
 
  int result[N] = {(stack.emplace_back(std::forward<Types>(args)), 0)...};
 
  (void)result;
 
 }
 
 ```
 
 
 
 My colleague Evan who's an expert in c11 suggested trying to change the implementation to the following:
 
 
 
 ```cpp
 
 template <typename... Types>
 
 static inline void push(Stack& stack, Types&&... args) {
 
  std::initializer_list<int>{(stack.emplace_back(std::forward<Types>(args)), 0)...};
 
 }
 
 ```
 
 
 
 Which looks cleaner and worked great on Window! 
 
 So posted here, in case, it's a potential fix for others.",667
22709,Inconsistent behaviour of torch.fmod on CPU and GPU,"Seems like using % for ints is reasonable and matches numpy. For example:
 
 ```
 
 # 2**54+1 can't be represented as a double without loss of precision:
 
 >>> 2**54+1
 
 18014398509481985
 
 >>> int(float(2**54+1))
 
 18014398509481984
 
 
 
 # but np calculates the ""fmod"" correctly without converting to double.
 
 >>> a=np.array([2**54+1, 2**54+1])
 
 >>> np.fmod(a,2)
 
 array([1, 1])
 
 >>> np.fmod(a.astype(np.double),2)
 
 array([0., 0.])
 
 ```",5803
22710,"Cannot use LBFGS in C++ (clang, torch 1.0.0)","I fixed this problem using a `NoGradGuard` in LBFGS before the parameter update, I'll submit a pull request soon",3089
22711,torch.utils.data.dataloader doesn't support multiprocessing with multiple workers,Itâ€™s impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0. This isnâ€™t a bug at all.,4018
22712,[jit] `for in` for lists,"It would also be nice to support `enumerate` and `zip`, they make working with python lists a better experience",3994
22713,Product operation along some axis,Thank you for the suggestion. We do have `torch.prod`. Is there anything that you find lacking with it?,6795
22714,[Caffe2] Enable Windows build with MKL-DNN,I opened an issue at ideep: https://github.com/intel/ideep/issues/35. Hope that they will support ideep on Windows.,3172
22715,Recurrent dropout,Similar to https://github.com/pytorch/pytorch/issues/9572,5908
22716,Issues with tutorial â€œInstalling C++ Distributions of PyTorchâ€,"If you use MSVC, then the alternative command for make is `msbuild` (or `ninja` if you use Ninja-build). The complete command is `msbuild INSTALL.vcxproj /p:Configuration=Release`, which is used in the script https://github.com/pytorch/pytorch/blob/master/tools/build_pytorch_libs.bat#L118.",3583
22717,No error when selecting a stream that belongs to a different device,"As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream.",1282
22718,More readable error message for index error of nn.Embedding in CUDA,It's complicated; if we do specific error checks then the code becomes much slower because we'd have to launch a kernel just to do the error checking.,4000
22719,torch.nn.functional.glu is wrong ?,"I came back to this GLU many years after I implemented it. The figure in the original paper is confusing, and there is no ""split"" for the inputs. Essentially you do two independent convolutions for the same input, get two outputs of the same shape, do sigmoid activation for one of the outputs, then multiply these two together. See [my explanations on GLU](https://leimao.github.io/blog/Gated-Linear-Units/).",3016
22720,Cublas run time error with RTX 2080Ti with Cuda 9.0,"RTX 2080Ti needs CUDA10 version of PyTorch to be installed, not CUDA9.
 
 That's likely the reason for the error.",5084
22721,F.conv2d throws C++ side error when shape of weights and input mismatch,"this is fixed on master, and will be part of the next release.
 
 
 
 The cause of this issue is a libmkldnn.so on your machine that is of a different version than what PyTorch expects.",10895
22722,[RFC] Integration with MKL-DNN,"We (Intel Pytorch team) have a couple of discussions with @bddppq on whether MKL-DNN op should be visible to user and how MKL-DNN op should be implemented in Pytorch C10 backend. We summarize the discussion below to capture the motivation and directions on the high level design. Your feedback is welcome. 
 
 
 
 ====
 
 The Pytorch C10 design would like to make MKL-DNN operation visible to user. So if user wants to use MKL-DNN operations, user needs to explicitly convert cpu tensor to MKL-DNN tensor, and vice versa. The motivation of explicit user conversion is to support performance debugging. If the conversion is done implicitly in the C10 ops/runtime, the model designer may produce a model with hidden conversion cost which it is hard to debug the conversion related performance issues during the deploy time. When the pytorch program is statically compiled to a computation graph, C10 would like to able to differentiate MKL-DNN op from cpu op in the computation graph, and the tensor conversion is represented as a node in the graph. This facilitates the graph optimization and also ease the performance debugging. This involves changes in both Pytorch front-end and back-end. Pytorch front-end needs to expose MKL-DNN to user through certain mechanism, and Pytorch back-end needs to treat MKL-DNN as a backend device so the MKL-DNN OP is represented as Aten OPâ€™s implementation on MKL-DNN device. 
 
 Â 
 
 Intel Pytroch team would like to have the MKL-DNN optimization enabled without user changing code. With the computation graph, an optimization pass may be done to promote CPU op to MKL-DNN op, and add or delete the format conversion op for best performance. The optimization pass ensures that the graph path can enjoy the performance benefit from the latest MKL-DNN version without user changing the program. 
 
 Â 
 
 FB Pytorch team understands that MKL-DNN op decides the input/output tensor format at runtime. The user specified to_mkl-dnn() may not trigger the actual format conversion, which may actually happen at the entry of the following MKL-DNN op. The user specified to_cpu() usually triggers a conversion since the MKL-DNN tensor is typically not the plain format. The format conversion may happen between two consecutive MKL-DNN ops. 
 
 Â 
 
 Intel Pytorch team would like to not confuse Pytroch user taking MKL-DNN as a HW device. As MKL-DNN is essentially an optimization library on the base device (CPU), calling MKL-DNN a device on Pytorch front-end may cause confusion. For example, the MKL-DNN library may extended to cover another Intel device in the future, in that case, user may be confused by a MKL-DNN device on top of the new Intel device. So, if the MKL-DNN OP has to be exposed in Pytorch front-end, it may be exposed as OPs working with special format tensors, not a device. It is like how sparse tensor is presented to user using torch.to_sparse(), so it could be tensor.to_mkl-dnn(), not tensor.to(device='mkl-dnn-x86'). 
 
 Â 
 
 In early experiment stage, if Pytorch user (model designer) complains such usage model, Pytorch team may consider a Python compatible layer, which enlarges MKL-DNN operation scope by simply implementing the non-MKL-DNN Ops by a conversion to cpu tensor and cpu op implementation. This simplifies MKL-DNN op usage since user only needs to specify the conversion to MKL-DNN format at the beginning of the program, but it also defeat the purpose of having user aware of the conversion and an accurate computation graph.",8231
22723,grad_fn missing?,"I think this was flagged as a bug, and a fix is in master. See https://github.com/pytorch/pytorch/issues/15353
 
 
 
 It'll be part of the 1.0.1 release this week.",3269
22724,"Bug: fail to throw error when computing loss between tensors with shapes [n, 1] and [n]",#ERROR!,53
22725,torch.histc doesn't work if input tensor on gpu,"Let's investigate supporting this once #6688 is merged.
 
 
 
histogram on GPU is really difficult. If you want this get prioritized, please just say so. There is no need to blame devs on working on other changes that, while may be BC breaking, are also and may be more important.",4097
22726,conda 3.6 installation currently broken due to 3.6.1 ABI breakage,"Definitely a conda 3.6 thing, fresh install into a 3.5 environment works perfectly :+1:",2097
22727,"Perform autograd directly on Tensor, not Variable","I'd agree for keeping them separate. One of the things I like about PyTorch in general is the separation of concerns between Tensors, Variables/autograd, and Modules; Tensor is just a generic ndarray that knows nothing about deep learning, or computational graphs, or gradients; it's just a numpy replacement that runs on GPU.",3320
22728,Inconsistent behavior between ByteTensor and Variable(ByteTensor): bug or feature?,"This happens because Variables always return a 1-element Tensor for functions that return scalars.
 
 And when we try to construct a 1-element ByteTensor with the value 256, it overflows to 0.
 
 
 
 This is expected until we introduce a scalar type into autograd 
 
 ( https://github.com/pytorch/pytorch/issues/1433 )",7571
22729,Allow to set 0 weight decay for biases and params in batch norm,"For this issue, I basically do something like this and send the output to the optimizer:
 
 ```python
 
 def group_weight(module):
 
  group_decay = []
 
  group_no_decay = []
 
  for m in module.modules():
 
  if isinstance(m, nn.Linear):
 
  group_decay.append(m.weight)
 
  if m.bias is not None:
 
  group_no_decay.append(m.bias)
 
  elif isinstance(m, _ConvNd):
 
  group_decay.append(m.weight)
 
  if m.bias is not None:
 
  group_no_decay.append(m.bias)
 
  elif isinstance(m, _BatchNorm):
 
  if m.bias is not None:
 
  group_no_decay.append(m.weight)
 
  if m.bias is not None:
 
  group_no_decay.append(m.bias)
 
 
 
  assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)
 
  groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]
 
  return groups
 
 ```",2582
22730,How to implement 'Signed Sqrt' under the Tensor level,"How about the following codes?
 
 ```
 
 # signed sqrt
 
  x = torch.mul(torch.sign(x),torch.sqrt(torch.abs(x)+1e-12)) 
 
 ```
 
 BTW, I am implementing bilinear CNN. Do you know anything about it? Can you help me about the problem? [click here](https://discuss.pytorch.org/t/unkonwn-probelm-of-bilinear-cnn-not-error-but-problem-in-my-implemention-of-algorithm/11704) @KaiyuYue",2957
22731,torch.norm named arguments do not work as expected,edit: @SsnL is looking into this,9232
22732,introduce torch.Scalar to represent scalars in autograd,"I had a discussion with @colesbury, @apaszke, @ezyang about this. Here are some notes from that discussion followed by my own proposal. The below discussion is limited to â€œScalarsâ€ in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see https://github.com/pytorch/pytorch/issues/2633), but I donâ€™t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.
 
 
 
 **Scalars vs 0-dimensional objects**
 
 
 
 NumPy has both scalars and 0-dimensional arrays. Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:
 
 ```
 
 >>> isinstance(np.double(5), float)
 
 True
 
 >>> isinstance(np.float(5),float)
 
 True
 
 ```
 
 They also â€œquackâ€ like 0-dimensional arrays, i.e. they have `shape` and `ndim` attributes:
 
 ```
 
 >>> np.double(5).shape
 
 ()
 
 >>> np.double(5).ndim
 
 0
 
 ```
 
 These properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you donâ€™t try to mutate a scalar).
 
 
 
 On the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:
 
 ```
 
 >>> a=np.array(0.5)
 
 >>> a[()]=2
 
 >>> a
 
 array(2.0)
 
 >>> isinstance(np.array(0.5), float)
 
 False
 
 >>> isinstance(np.array(0.5), np.ndarray)
 
 True
 
 ```
 
 Thus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)
 
 
 
 **Do we want Scalars or 0-dimensional Variables or both in PyTorch?**
 
 
 
 Iâ€™m going to argue that we only want 0-dimensional Variables.
 
 
 
 First, we want at least 0-dimensional Variables in PyTorch. Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable. Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesnâ€™t (and you have to hack around that in some way to still be able to do training). You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1). Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.
 
 
 
 Now, if we have 0-dimensional Variables, do we need immutable scalars in autograd? Iâ€™d argue no:
 
 1) The ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.
 
 2) Immutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.
 
 3) A related question is what to return when indexing the last dimension of a Variable. The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality). Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.
 
 
 
 Iâ€™ll also note here that TensorFlow only supports 0-dimensional tensors, although itâ€™s quite different because the execution doesnâ€™t define the graph, as in PyTorch.
 
 
 
 Many of the other arguments for scalars are not about immutability, but convenience, e.g. itâ€™s nice to get pretty prints (i.e. â€œ0â€ or â€œtorch.FloatScalar(0)â€ instead of something like:
 
  ```
 
  5
 
 [torch.FloatTensor of size ()]
 
 ```
 
 
 
 Those kinds of convenience problems are solvable without adding scalar types.
 
 
 
 **A Proposal for adding 0-dimensional support to autograd**
 
 
 
 
 
 As mentioned above, the goal here is to get to a sensible Variable API; this doesnâ€™t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.
 
 
 
 1) Autograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller. Note that they canâ€™t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which donâ€™t support 0-dimensions yet. Note also that this will now match what the equivalent tensor functions do (i.e. `max()` returns a python number), and when they support 0-dimensional tensors, the code will just work as written. This wonâ€™t immediately solve all dimensionality problems, because for example `max(dim=0)` on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.
 
 
 
 2) Variable is now 0-dimensional aware (via its underlying ATen implementation). For example:
 
 `.dim()` will return 0
 
 `.size()` will return ()
 
 `.shape` will return ()
 
 indexing with a dimension will throw an error (i.e. `[()]` and `[...]` will work, everything else will throw an error)
 
 All other functions will behave as before. Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.
 
 
 
 3) We will provide constructors for 0-dimensional Variables. Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. `torch.FloatTensor(5)` creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors. I propose torch.*Scalar (e.g. `torch.FloatScalar`); this has a more object-looking name than say, torch.float(...), which will make it more obvious itâ€™s a mutable type. One question is how this should interact with the work in (https://github.com/pytorch/pytorch/issues/2633). A full â€œleap-of-faithâ€ approach would be to have this â€œconstructorâ€ always return a Variable and have â€˜requires_gradâ€™ and â€˜volatileâ€™ parameters. This name will probably be confusing in the short term because you would expect `torch.FloatScalar` to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.
 
 
 
 4) `__str__` on a 0-dimensional tensor will return the underlying number, `__repr__` will return how it is constructed, i.e. `torch.FloatScalar(5)`. This is consistent with numpy 0-dimensional arrrays:
 
 ```
 
 >>> print(np.array(0))
 
 0
 
 >>> np.array(0)
 
 array(0)
 
 ```
 
 
 
 5) We will provide conversion functions (`__int__`, `__float__`, more?) for 0-dimensional Variables for ease of use with existing python libraries. Calling these functions on non-0-dimensional Variables will throw an error.
 
 
 
 Thoughts?",3117
22733,no member named 'shared_ptr' in namespace 'std',"actually, that file already has memory included.
 
 
 
 Are you following instructions from https://github.com/pytorch/pytorch#from-source
 
 Especially: 
 
 ```
 
 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install
 
 ```
 
 
 
 This part is important: `MACOSX_DEPLOYMENT_TARGET=10.9 `",8799
22734,Conv2d checks raise an error with confusing message,"This error seems to appear when you call `.cuda` on the module and the input tensor, but forget to use the value returned by the call on the tensor (rather than the original tensor, which stays on the cpu). 
 
 
 
 It seems that the error message is getting something wrong: should it say ""expected CUDA tensor (got CPU tensor)""?
 
 
 
 I find it somewhat confusing that calling `.cuda` on a module moves it to gpu wheareas calling `.cuda` on a tensor returns a new tensor but the original one stays on the cpu.",548
22735,Handle bugs in cudnnGet* better,Looks like cudnnGet* is returning the wrong algorithm. A workaround is to use ```torch.backends.cudnn.benchmark=True```,4143
22736,Improper error msg when calling Variable.cuda() in forked subprocess," Generally you will have to run in Python 3,and include 
 
 ``` Python 
 
 from multiprocessing import set_start_method
 
 try:
 
  set_start_method('spawn')
 
 except RuntimeError:
 
  pass
 
 ```
 
 To avoid fork()",620
22737,import error,"Same here, fixed by upgrading Python from 3.6.0 to 3.6.1.",5728
22738,Conv1d only accepts FloatTensor,"If you look at the printed tensors, you'll see that the input is a DoubleTensor, but the weights and biases are FloatTensors. Adding `conv_1.double()` line before using the module fixes it.",3567
22739,Inconsistency with .numpy(),I am using pytorch v0.3.1 which [doesn't has this feature](https://pytorch.org/docs/0.3.1/torch.html?highlight=zeros#torch.zeros).,2998
22740,torch.save does not work with _LRSchedulers,"The same thing happens if you serialize a model and its optimizer in two separate checkpoints, so I'm not very concerned about that.",7264
22741,Grid sampler fails with CUDNN_STATUS_EXECUTION_FAILED for large images (not due to memory exhaustion),"I was running this script until I hit an OOM error, not an illegal memory access. This issue is pretty old and likely to be fixed. I'm going to close it. @emitch, if you're still seeing this issue would you update and reopen it?",3312
22742,Possible memory leak in spectral_norm,"Please, find the script below. The discriminator network is the same that I'm using in my project. 
 
 
 
 On the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.
 
 
 
 Removing the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.
 
 
 
 Let me know if I can do anything else.
 
 Morgan
 
 
 
 ```python
 
 from argparse import ArgumentParser
 
 
 
 import torch
 
 import torch.backends.cudnn as cudnn
 
 import torch.cuda as cuda
 
 
 
 from torch.nn import Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d
 
 from torch.nn.functional import binary_cross_entropy_with_logits, avg_pool2d
 
 from torch.nn.utils import spectral_norm
 
 from torch.optim import Adam
 
 
 
 __author__ = 'Morgan Funtowicz'
 
 
 
 
 
 class Flatten(Module):
 
  def forward(self, x):
 
  x = x.view(x.size(0), -1)
 
  return x
 
 
 
 
 
 class _ResidualDownSamplingBlock(Module):
 
 
 
  def __init__(self, n_in, n_out, ksize, stride=1, padding=1):
 
  super().__init__()
 
  self._f = Sequential(
 
  ReLU(),
 
  spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),
 
  ReLU(True),
 
  spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),
 
  AvgPool2d(2, 2)
 
  )
 
  self._sc = spectral_norm(Conv2d(n_in, n_out, 1, padding=0))
 
 
 
  def forward(self, x):
 
  return avg_pool2d(self._sc(x), 2, 2) + self._f(x)
 
 
 
 
 
 if __name__ == '__main__':
 
  # Ensure Tensor are allocated as FloatTensor
 
  cudnn.benchmark = True
 
  torch.set_default_tensor_type('torch.FloatTensor')
 
  torch.set_default_dtype(torch.float32)
 
 
 
  # Parse provided arguments
 
  args_parser = ArgumentParser()
 
  args_parser.add_argument('-d', type=int, default=-1, dest='device', help='Device to use for training (-1 = CPU)')
 
  args_parser.add_argument('-b', type=int, default=-16, dest='batch', help='Size of the minibatch')
 
 
 
  args = args_parser.parse_args()
 
  args.gpu = args.device >= 0 and cuda.is_available()
 
  device = torch.device(""cuda:%d"" % args.device if args.gpu else ""cpu"")
 
 
 
  # Define the model & Optimizer
 
  model = Sequential(
 
  _ResidualDownSamplingBlock(3, 64, ksize=3),
 
  _ResidualDownSamplingBlock(64, 64, ksize=3),
 
  _ResidualDownSamplingBlock(64, 128, ksize=3),
 
  _ResidualDownSamplingBlock(128, 128, ksize=3),
 
  _ResidualDownSamplingBlock(128, 128, ksize=3),
 
  _ResidualDownSamplingBlock(128, 64, ksize=3),
 
  BatchNorm2d(64), ReLU(True), Flatten(),
 
  spectral_norm(Linear(256, 1))
 
  ).to(device)
 
 
 
  opt = Adam(model.parameters())
 
 
 
  # Train
 
  for epoch in range(20000):
 
  print('Starting epoch %d' % epoch)
 
 
 
  x, y = torch.randn((args.batch, 3, 128, 128), device=device), torch.rand((args.batch, 1), device=device)
 
  y_hat = model(x)
 
 
 
  opt.zero_grad()
 
  loss = binary_cross_entropy_with_logits(y_hat, y)
 
  loss.backward()
 
  opt.step()
 
 ```",4867
22743,FloatTensor constructor bug,That's a question to @ezyang. This issue might actually be an argument to change this convention (with the huge downside being that a lot of our current code will need to be updated).,6896
22744,torch.tensor not copying to correct device if data is a Tensor with correct dtype,"Note, this also occurs when creating the array from a NumPy array (that took me a while to figure out what was causing the problem). I would definitely expect a non-PyTorch datatype to take the device setting into account:
 
 
 
 ```
 
 import torch
 
 import numpy as np
 
 
 
 x = torch.tensor(np.array(1), device='cuda:0')
 
 print(x.device) # Prints `cpu`
 
 x = torch.tensor(1, device='cuda:0')
 
 print(x.device) # Prints `cuda:0`
 
 ```",4518
22745,"[BUG] use dropout in multi-LSTM(GRU) when GPU index is not '0', will cause ""cublas runtime error"" [pytorch0.4.0]","As a workaround, you can add
 
 ```
 
 if cuda:
 
  torch.cuda.set_device(device)
 
 ```
 
 to the beginning of your script.",1291
22746,[feature request] Bucketization,"I updated my searchsorted implementation to work with pytorch v1.0
 
 [https://github.com/aliutkus/torchsearchsorted](https://github.com/aliutkus/torchsearchsorted)",3295
22747,cpp API Adam optimizer test is flaky,"I was about to file the same issue.
 
 
 
 Some random draws will cause these optim tests to fail, and because they use std::rand() whether they pass or fail is dependent not only on the seed given but also how many prior calls to std::rand() are made. Further, std::rand() does not appear to be the only source of pseudorandomness in these tests, as setting different random seeds at the start of them will still result in intermittent failures/success. (Perhaps the network initializations are pseudorandom, too?)
 
 
 
 While controlling the pseudorandomness is one option, the fact that these tests are so fragile calls them into question. Maybe there are better tests for these optimizers?",3305
22748,[BUG/Feature request] Missing documentation of torch.jit ?,it's intentional. we'll add docs when it's ready to use.,9742
22749,"Broken `Type Hints` in PyTorch 0.4.0, related to IDEs(eq. PyCharm)",Going to bump the priority on this because a lot of users have been requesting this... we'll try our best to look into it more.,2729
22750,Adagrad not working with GPU,"Thanks for pointing that out, it makes sense. But I noticed that other optimizers (e.g. Adam) somehow keep track of the parameters even if they are moved to the gpu after initializing the optimizer. So I thought all optimizers would work correctly in that case. I'll stick with what suggested in the documentation for now.",6826
22751,Possible bug with .ge gradient flow?,"â€œgreater than or equalâ€ has zero gradient almost everywhere, and nondifferentiable at other points. Itâ€™s not a bug.",11481
22752,ATen: How to get cusparseHandle_t handle?,"Once you have an ATen context object, you can just call `lazyInitCUDA()` and get a `THCState*`. Then, you can pass that into the THCS functions.
 
 
 
 Also, please remember that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](https://discuss.pytorch.org).",4646
22753,Error msg with conv2D in pytorch 0.4,"Hmm that's not great. We might want to pass the expected dimensionality of the convolution to the generic implementation, so that we can improve the error messages.",2945
22754,no gradients for torch.tensor(np.array(...)) on cuda,"You're hiding the reference to the original tensor you created:
 
 ```
 
 a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, requires_grad=True).cuda()
 
 ```
 
 This creates a tensor that gradients accumulates into, and then you call `.cuda()` on it, which creates a second tensor. 
 
 
 
 Try 
 
 ```
 
 a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, device=""cuda"", requires_grad=True)
 
 ```
 
 instead.",8665
22755,[feature request] Possibility of returning only require_grad parameters in Module.parameters(),"Yeah that makes sense. There's really nothing that prevents people from setting `requires_grad` to `False` after they construct the optimizer, and it won't even complain. We should just remove the check.",8457
22756,Rebuilding Pytorch after modifying C++ code in aten library,"No, you need to run `build_deps`. When I'm compiling PyTorch from source I tend to run `python setup.py build_deps develop` (make sure you haven't installed it previously; if you have, `pip uninstall torch` twice will fix it.)
 
 
 
 See also https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md",4329
22757,"[Caffe 2] common_gpu.cc:55] Found an unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start.",The problem was solved by restarting the computer :).,7232
22758,Difference between dilation=1 and dilation=2 convolution outputs,it's because they are computed with different convolution algorithms. normal convolution possibly uses winograd or fft to compute convolution. dilated convolution uses spatial convolution algorithm,9740
22759,Bad link to a tutorial about Distributed Overview,"this is expected. The page you accessed is the master instead of the stable doc release. The distributed overview page is at https://github.com/pytorch/tutorials/blob/release/1.6/beginner_source/dist_overview.rst. We will flush contents in `release/1.6` branch into the master branch when we announce v1.6 release. After that, the link would work.",2810
22760,Building offline documentation does not work,I like that katex is faster to render and doesn't reflow the page. I think that justifies the slightly more complicated build process for the docs.,3159
22761,libtorch: torch::cuda::is_available() returns zero,"I have tracked down the issue. In my CMakelist.txt after building the pytorch libs I need to link it with -Wl,--no-as-needed if not they only link to the CPU libs. 
 
 
 
 You could add this line to your CMakelist to link properly:
 
 
 
 set(CMAKE_LINK_WHAT_YOU_USE TRUE)
 
 
 
 Closing.",3144
22762,[jit] [transformer] [8-bit quantize] Error when trying to quantize pytorch build-in transformer model and then export it with libtorch,"I realized that this bug has been fixed in the latest master code (I am using pytorch 1.5.1) in 
 
 https://github.com/pytorch/pytorch/blob/890b52e09ff1a758a2ac814f130a1dc1f1a46e9d/torch/nn/modules/activation.py#L869
 
 by changing the Linear class to _LinearWithBias class, which won't be passed to dynamic quantization function to pack the out_proj.
 
 I add this fix in my code and it works well.
 
 Please simply ignore this above error description then
 
 Thanks",3183
22763,support `fftshift` and `ifftshift` in pytorch,I'd like to point out that torch has `torch.roll` function that provides similar funcitonality to `roll_n` with likely better performance.,6812
22764,AttributeError: module 'torch.jit' has no attribute '_script_if_tracing',"Try downgrading ` torchversion ` to ` 0.4.0 `
 
 pip uninstall torchvision
 
 pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html",8016
22765,Import error from torch.distributed,`torch.distributed` does not support Windows yet. I am closing this issue and let's move discussions to #42095. Thanks.,659
22766,"torch.distributed.launch: despite errors, training continues on some GPUs without printing any logs",As long as one can set a timeout that's fine. I'll try that. Thanks a lot!,648
22767,"The cuda version of torch.det is much slower than cpu version, why?","It's correct that the first cuda run is slower because it includes Magma library initialization. The script in the second comment is not performing correct synchronizations either. Please use Timer utility to properly benchmark code, see [README](https://github.com/pytorch/pytorch/tree/master/torch/utils/_benchmark) and simple_timeit in the examples. You'd need a recent pytorch build for that. With proper timing, cuda det is much faster than cpu det.",4001
22768,[Build Failure] Wrong copy path for shared DNNL,"LGTM, will test",4055
22769,"Error, Compiled pytorch from source on power9 machine(cannot import name '_add_docstr' from 'torch._C' (unknown location)..)","Are you starting your python interpreter from pytorch root directory? Depending on how you installed pytorch that might lead to errors, try changing directory.",1270
22770,[feature request] support for division in quantization,we don't have a kernel yet for quantized division (it can be considered for future additions). A workaround you could use in the meanwhile is surround division with `DeQuantStub` and `QuantStub` to let the computation happen in floating point.,9512
22771,Whether PixelShuffle could add support for 3D dataï¼Ÿ,Was this fixed?,611
22772,[bug] Binomial distribution has small chance of returning -1,"I doubt log(u) being rounded to 0 is the culprit. log of 1-2**(-24), (the largest representable fp32 number less than 1) is -5.9605e-8, so unless uniform returns 1. (which it should not) the log term should be non-zero. So my suspicion is, standard_uniform is not 1-exclusive.",3068
22773,Weird result multiplying a cpu tensor by a cuda:1 tensor,"Thanks for the bug report. I can repro the problem on 1.5.1. It appears to be fixed on master but I am not sure which PR fixed it.
 
 ```
 
 [ins] In [2]: import torch
 
  ...: a = torch.tensor([3., 4.]).to(torch.device('cuda:1'))
 
  ...: b: torch.Tensor = torch.tensor(2.)
 
  ...: b * a
 
 Out[2]: tensor([6., 8.], device='cuda:1')
 
 ```",6844
22774,torch.save produces inconsistent results between multiple runs,"PR[29232] (https://github.com/pytorch/pytorch/pull/29232) doesn't Introduce this problem.
 
 Torch 0.4.1 has the same logic.
 
 https://github.com/pytorch/pytorch/blob/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f/torch/serialization.py#L249-L251",675
22775,torch.fft tracking issue,"I've run some simple benchmarks comparing `torch.fft` transforms against `mkl_fft`, `numpy`, `scipy` and `cupy`. I haven't sampled a huge number of shapes because the benchmarks take a fairly long time to run, but do cover transforming over different dimensions which is interesting because the resulting performance depends a lot more on how batching is handled.
 
 
 
 <details>
 
 <summary>Simple benchmark code</summary>
 
 
 
 ```python
 
 import torch
 
 import numpy as np
 
 import scipy.fft
 
 import cupy
 
 import mkl_fft._numpy_fft
 
 import mkl
 
 import itertools
 
 
 
 shape = (40, 40, 100)
 
 #shape = (42, 14, 18)
 
 c = torch.rand(*shape, dtype=torch.cdouble)
 
 cn = c.numpy()
 
 cc = c.cuda()
 
 r = torch.rand(*shape, dtype=torch.double)
 
 rn = r.numpy()
 
 rc = r.cuda()
 
 
 
 operators = ['fft', 'ifft', 'rfft', 'irfft', 'fftn', 'ifftn', 'rfftn', 'irfftn', 'hfft', 'ihfft']
 
 results = []
 
 def add_result(name, operator, dim, times):
 
  results.append((name, operator, dim, times.best, times.average, times.stdev))
 
 
 
 for op, dim in itertools.product(operators, (0, 1, 2)):
 
  torch_fn = getattr(torch.fft, op)
 
  numpy_fn = getattr(np.fft, op)
 
  scipy_fn = getattr(scipy.fft, op)
 
  mkl_fn = getattr(mkl_fft._numpy_fft, op, None)
 
  cupy_fn = getattr(cupy.fft, op)
 
  name = f'{op} dim={dim}'
 
  if op.startswith('rfft') or op.startswith('ihfft'):
 
  x, xn, xc = r, rn, rc
 
  else:
 
  x, xn, xc = c, cn, cc
 
 
 
  xcp = cupy.array(xn)
 
  if op.endswith('n'):
 
  x = x.movedim(dim, -1)
 
  xn = np.moveaxis(xn, dim, -1)
 
  xc = xc.movedim(dim, -1)
 
  xcp = cupy.moveaxis(xcp, dim, -1)
 
  dim_kwargs = axis_kwargs = dict()
 
  else:
 
  axis_kwargs = {'axis': dim}
 
  dim_kwargs = {'dim': dim}
 
 
 
  print(name, 'multi-threaded')
 
  torch.set_num_threads(8)
 
  t = %timeit -o torch_fn(x, **dim_kwargs)
 
  add_result('torch mulithreaded', op, dim, t)
 
  if mkl_fn is not None:
 
  mkl.set_num_threads(8)
 
  t = %timeit -o mkl_fn(xn, **axis_kwargs)
 
  add_result('mkl_fft multi threaded', op, dim, t)
 
  t = %timeit -o scipy_fn(xn, workers=8, **axis_kwargs)
 
  add_result('scipy multi threaded', op, dim, t)
 
 
 
  print(name, 'single threaded')
 
  torch.set_num_threads(1)
 
  t = %timeit -o torch_fn(x, **dim_kwargs)
 
  add_result('torch single threaded', op, dim, t)
 
  if mkl_fn is not None:
 
  mkl.set_num_threads(1)
 
  t = %timeit -o mkl_fn(xn, **axis_kwargs)
 
  add_result('mkl_fft single threaded', op, dim, t)
 
  t = %timeit -o numpy_fn(xn, **axis_kwargs)
 
  add_result('numpy', op, dim, t)
 
  t = %timeit -o scipy_fn(xn, **axis_kwargs)
 
  add_result('scipy single threaded', op, dim, t)
 
 
 
  print(name, 'cufft')
 
  t = %timeit -o torch_fn(xc, **dim_kwargs); torch.cuda.synchronize()
 
  add_result('torch cuda', op, dim, t)
 
  t = %timeit -o cupy_fn(xcp, **axis_kwargs); cupy.cuda.runtime.deviceSynchronize()
 
  add_result('cupy', op, dim, t)
 
 
 
 ```
 
 
 
 </details>
 
 
 
 
 
 
 
 The full results are available in some spread sheets but I'll sumarise here.
 
 [fft-comparison-benchmarks.zip](https://github.com/pytorch/pytorch/files/5721529/fft-comparison-benchmarks.zip)
 
 
 
 
 
 For cuda, `torch.fft.*` either performed similarly or as much as 2x faster than `cupy.fft.*`. That's pretty good considering we're both just calling cuFFT under the hood.
 
 
 
 For single threaded CPU performance I compared against `mkl_fft`, `numpy` and `scipy`.
 
 - `numpy` was slower in all cases, at best around 1.5x slower than pytorch and at worst 3.9x slower. 
 
 - `scipy` hovered around 1-1.5x slower in most cases. For the larger shaped tensor, `scipy` is around 15-20% faster at `ihfft` on all but the last dimension. This is perhaps not surprising though, as `scipy.fft` has custom kernels for `hfft` and `ihfft` whereas intel mkl doesn't.
 
 - Perhaps most interesting is `mkl_fft`. `mkl_fft` is often a few microseconds (around 10% for the smaller tensor) faster for `fft` and `ifft` with `dim=0` or `dim=-1`. I would attribute this to `mkl_fft` caching the transform descriptor between calls whereas `torch.fft` recreates them each time. This significantly simplifies the code though, so may not be worth the extra microseconds.
 
  On the other hand, many `dim=1` transforms were 2-4x slower on `mkl_fft` than `torch.fft` so the way I'm handling batching is clearly good.
 
 
 
 For multithreaded CPU performance, I only compare against `mkl_fft` and `scipy` since `numpy` doesn't support multithreading.
 
 - `scipy` was up to 30% faster for the smaller single dimensional transform on `dim=1`. Probably because It doesn't require contiguous batch dimensions, so avoids an extra copy. However, this difference becomes negligible for multi-dimensional transforms or transforms over larger tensors. In most other cases, `torch.fft` was 2-3x faster.
 
 - `mkl_fft` is again unsurprisingly similar in the easy cases. It does still get a slight advantage from descriptor caching though. I tested with 8 threads and at that level plan creation took up to 40% of the transform time for the smaller tensor, but still was only a few microseconds of overhead. 
 
  However, on the harder cases `torch.fft` can actually be much faster than `mkl_fft`. I see 4-6x faster for the larger tensor on `rfftn` and `irfftn` e.g. in one case `mkl_fft` takes 1.9 ms and `torch.fft` takes only 0.3 ms to do the same transform.",3361
22776,pytorch istft runs slower than torchaudio istft especially at higher n_fft,"I confirm that I could reproduce the issue.
 
 
 
 | | Total | Par Call |
 
 |-----------------|:--------:|:------:|
 
 | PyTorch 1.6.0 | 9.0404 | 0.904 |
 
 | torchaudio 0.5.0 | 0.1052 | 0.010 |
 
 
 
 <Details><Summary>script</Summary>
 
 
 
 ```python
 
 import time
 
 
 
 import torch
 
 import torchaudio
 
 
 
 n_fft = 2**15
 
 x = torch.rand(size=[1, n_fft//2 + 1, 10, 2])
 
 
 
 start = time.monotonic()
 
 n = 10
 
 for _ in range(n):
 
  # y = torchaudio.functional.istft(x,n_fft)
 
  y = torch.functional.istft(x,n_fft)
 
 elapsed = time.monotonic() - start
 
 
 
 print(elapsed, elapsed / n)
 
 ```
 
 
 
 </Details>
 
 
 
 <Details><Summary>Env 1: PyTorch 1.6.0</Summary>
 
 
 
 ```
 
 $ python -m torch.utils.collect_env
 
 Collecting environment information...
 
 PyTorch version: 1.5.0
 
 Is debug build: No
 
 CUDA used to build PyTorch: 10.2
 
 
 
 OS: Ubuntu 18.04.3 LTS
 
 GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
 
 CMake version: version 3.10.2
 
 
 
 Python version: 3.6
 
 Is CUDA available: No
 
 CUDA runtime version: Could not collect
 
 GPU models and configuration:
 
 GPU 0: Quadro GP100
 
 GPU 1: Quadro GP100
 
 
 
 Nvidia driver version: 418.116.00
 
 cuDNN version: Could not collect
 
 
 
 Versions of relevant libraries:
 
 [pip3] numpy==1.18.5
 
 [pip3] torch==1.5.0
 
 [pip3] torchaudio==0.5.0a0+3305d5c
 
 [conda] blas 1.0 mkl
 
 [conda] mkl 2020.1 217
 
 [conda] mkl-service 2.3.0 py36he904b0f_0
 
 [conda] mkl_fft 1.1.0 py36h23d657b_0
 
 [conda] mkl_random 1.1.1 py36h0573a6f_0
 
 [conda] pytorch 1.5.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch
 
 [conda] torchaudio 0.5.0 py36 pytorch
 
 ```
 
 
 
 </Details>
 
 
 
 <Details><Summary>Env 2: torchaudio 0.5.0</Summary>
 
 
 
 ```
 
 $ python -m torch.utils.collect_env
 
 Collecting environment information...
 
 PyTorch version: 1.6.0
 
 Is debug build: No
 
 CUDA used to build PyTorch: 10.2
 
 
 
 OS: Ubuntu 18.04.3 LTS
 
 GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
 
 CMake version: version 3.10.2
 
 
 
 Python version: 3.6
 
 Is CUDA available: No
 
 CUDA runtime version: Could not collect
 
 GPU models and configuration:
 
 GPU 0: Quadro GP100
 
 GPU 1: Quadro GP100
 
 
 
 Nvidia driver version: 418.116.00
 
 cuDNN version: Could not collect
 
 
 
 Versions of relevant libraries:
 
 [pip3] numpy==1.18.5
 
 [pip3] torch==1.6.0
 
 [pip3] torchaudio==0.6.0a0+f17ae39
 
 [conda] blas 1.0 mkl
 
 [conda] cudatoolkit 10.2.89 hfd86e86_1
 
 [conda] mkl 2020.1 217
 
 [conda] mkl-service 2.3.0 py36he904b0f_0
 
 [conda] mkl_fft 1.1.0 py36h23d657b_0
 
 [conda] mkl_random 1.1.1 py36h0573a6f_0
 
 [conda] numpy 1.18.5 py36ha1c710e_0
 
 [conda] numpy-base 1.18.5 py36hde5b4d6_0
 
 [conda] pytorch 1.6.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch
 
 [conda] torchaudio 0.6.0 py36 pytorch
 
 ```
 
 
 
 </Details>",3041
22777,Add torch.nn.Unflatten module into C++ Frontend,@glaringlee no worries then. Thanks for letting me know!,597
22778,Type mismatch error with torch.nn.functional.grid_sample() under AMP,Is this fix? I still get same error in pytorch 1.7.1,3892
22779,"Compilation error, Performing Test SUPPORT_GLIBCXX_USE_C99 Failed.","Pytorch-1.5 needs a C++14 compatible compiler. You are trying to compile PyTorch-1.6.0 with gcc-4.8.5, which is not fully C++14 compatible. 
 
 Is there a reason why you can not try to compile it with at least gcc-5.4?",5030
22780,"when i use amp on LSTM network,errro","Closing as duplicate of https://github.com/pytorch/pytorch/issues/36428.
 
 
 
 If you're using `nn.LSTM`, this is a known issue, amp doesn't work with cudnn RNNs in 1.6. they're more complicated than other ops because autocast needs to flatten weights in a particular format, in addition to casting them to FP16. I didn't have time to fix them before the release.
 
 
 
 This past week I wrote a fix that works on my machine. The PR should be up in the next few days.
 
 
 
 The cell-based RNN API is expected to work with Amp already. If you observed the error while using `nn.LSTMCell`, please reopen the issue and submit a minimal repro.",1599
22781,torch.inverse() performing very poorly on GPU vs CPU,"try to grab the nsight profiles and check the call trace for these methods, please?",742
22782,https://github.com/pytorch/pytorch/issues/42271#issue-668293527,https://github.com/pytorch/pytorch/issues/42271#issuecomment-666055175,9541
22783,"cudaErrorIllegalAddress printing result of torch.nn.Linear(1, 1).cuda()(torch.Tensor([[0.5]]))","btw, the problem is that your input tensor isn't on cuda, but the error is definitely a bug.",8989
22784,elementMap_.count(v) ASSERT FAILED at csrc/jit/passes/alias_analysis.cpp:536,"I simply ran: torch.jit.trace(my_model.pth).
 
 
 
 Previous nightlies had worked fine.
 
 
 
 Network from this repository with the prebuilt model https://github.com/zisianw/FaceBoxes.PyTorch
 
 
 
 
 
 Model: https://drive.google.com/file/d/1aRdcf692S2-oxNdoc6oOjIVvoyvSHl52/view?usp=sharing
 
 
 
 To test you can run within test.py which has model loading code.",3196
22785,[cuda] randn on a non-default stream doesn't work,"I think a better solution is to move all random generator APIs to use philox, that's easily both stream safe (each thread has to compute only its counter, and is not using global state that would need to be synchronized across streams) and thread safe https://github.com/pytorch/pytorch/blob/cf094d4edcc7928f9a4a368b3e2eeb22579b29b0/aten/src/ATen/native/cuda/Distributions.cu#L31-L35 (global counters are stored on the cpu and atomically incremented). Bernoulli is already using philox. 
 
 May be we should postpone major random refactors and just have a targeted PR eliminating mtgp in favor of philox, either from curand or with a standalone implementation that's currently in the fuser.",3226
22786,[FR] torch.cuda.synchronize takes in device objects,"We will accept a PR that makes `torch.cuda.synchronize(device=None)`. Make sure it checks that the input device is actually CUDA.
 
 
 
 not sure what the point of the stream version of this function is, since `stream.synchronize()` is pretty easy.",8300
22787,[jit] jit.trace segfault on variable slicing using `torch.narrow`,"that's the line segfault occurs: `torch.onnx.export(model, torch_in, onnx_path, verbose=True)`",574
22788,C++ Frontend data_parallel Does Not Update Weights,It looks like the [replicated module copies](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/parallel/data_parallel.h#L41) are not [accumulating grads](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/cloneable.h#L36) back to the original module (confirmed with @goldsborough ). I will implement `Broadcast` and `ReduceAddCoalesced` in C++ (as we did in Python). I feel eventually we might want to call C++ `data_parallel` from Python and drop Python's implementation.,3959
22789,using occupancy calculator instead of fixed constant for threads per block,"it's a constructor for a Tensor. It's generated by C macros, what you probably want to look for is `THCTensor_(new)` or `THCudaTensor_(new)`",517
22790,TestJit.test_input_dict_unify is flaky,Should be fixed in master,5892
22791,Multiprocessping-distributed ERROR,"After almost one year later, now, I know why.
 
 If we are using multi-GPU to train models, it would like to start multi-thread for different GPUs. And each thread has to rerun the script. If we change the code during this time, the other threads may load the modified code, which caused this problem.",977
22792,How to do prediction/inference for a batch of images at a time with libtorch?,"The input tensor is usually in the shape of (images, channels, width, height). 
 
 
 
 Use torch::cat or torch::stack to add tensors along dimension 0 and then run forward.",7128
22793,"Norm operator broken when used with `p=1` and `dim=(1, 2)`","I think this is already fixed on master. Closing, please feel free to reopen if you see further issues.
 
 ```
 
 In [1]: import torch
 
  ...:
 
  ...: x = torch.rand(4, 3, 3)
 
  ...: torch.norm(x, dim=(1,2)) # Works
 
  ...: torch.norm(x, p=1) # Works
 
  ...: torch.norm(x, p=1, dim=(1,2)) # Breaks
 
  ...:
 
 Out[1]: tensor([4.8355, 4.1928, 4.2854, 4.0480])
 
 ```",3257
22794,TestJit.test_cpp broken on master,Seems to be this patch https://github.com/pytorch/pytorch/pull/19445 cc @eellison,5810
22795,interpolate bicubic should follow opencv result,"@jonmorton FYI the PR above should fix the issue. 
 
 I didn't put exact same repro script as testcase as I feel it's not good to install cv2 just for this baseline so I added a ""expected result"" check there. 
 
 For reference, after the patch the script (node with `align_corners=False`) matches opencv result. 
 
 ```
 
 ('max abs diff:', tensor(7.4208e-06))
 
 ('max rel diff:', tensor(80458.5469))
 
 ('MSE:', tensor(9.1130e-13))
 
 ```",617
22796,`torch.nn.functional.conv2d` (CPU) is very slow on a specific trained weight,maybe you're hitting a problem with denormalized weights? you may try setting `torch.set_flush_denormal(True)` and see if that helps,9893
22797,Unexpected numeric limit for CUDA integral types,See comment here: https://github.com/pytorch/pytorch/issues/17750#issuecomment-486302496,5758
22798,LSTM batch_first flag does not produce ouputs with the batch as the first dimension.,"This isn't a bug: (batch, seq, feature) corresponds to the output tensor of lstm, not the hidden states.
 
 ```
 
 output, (hx, cx) = lstm(input)
 
 ```
 
 We could improve the documentation around this, but I'm not sure that the hidden state should also have its batch transposed to be first. Do you have a use case for that, @al093?",7700
22799,1.2.0.dev20190723 Conv1D Illegal instruction (core dumped),"fixed in #23292 
",9346
22800,Add collective communication APIs for Python objects.,Another example of where `all_gather` is used with arbitrary Python objects is in [`maskrcnn_benchmark`](https://github.com/facebookresearch/maskrcnn-benchmark/blob/24c8c90efdb7cc51381af5ce0205b23567c3cd21/maskrcnn_benchmark/utils/comm.py#L48-L88),1142
22801,JIT inputs does not support collections.OrderedDict,"@lara-hdr Thanks, it worked.",627
22802,Equations in the document do not display properly: Their LaTeX code is shown instead,"Yeah, given how nicely prerendered katex is shaping up, I think I'm going to just revert the imgmath change and then wait until we can deploy using prerendered katex.",8464
22803,Potentially missing else,@pietern Done: #23429,671
22804,bitwise_not documentation is not rendering correctly,cc @brianjo - Please add this to the list for 1.2 release.,9031
22805,[Feature Request] inferred module dimensions,"This has come up several times in the past, and I spent the last 20 mins searching for the proposal I wrote on a previous issue, but I wasn't able to (the fun of having thousands of issues).
 
 Hence, I'm re-iterating my previous thoughts both from memory and after re-thinking.
 
 
 
 I think this is a feature worth designing, but the following design principles have to be upheld:
 
 - no meta-programming or added magic to support it
 
 - the module hard-errors on all cases where it is not fully initialized, with a clear message stating to the user what is expected of them
 
 - the Auto-Infer mechanism has to be explicitly requested by the user, not via constants such as `-1` or `None`, but something like `torch.nn.Linear(torch.nn.INFER, 32)`.
 
 
 
 To flesh out the implementation details,
 
 - something like `nn.Linear`, when it gets as constructor input a `torch.nn.INFER`, will explicitly set it's weight to `nn._UninitializedParameter()`.
 
 - the downstream tasks that handle parameters have to now explicitly handle and throw exceptions appropriately on getting `nn._UninitializedParameter`, with a clear error message
 
  - for example `.parameters()` should throw, `.apply()` should throw, `jit` entrypoints should throw, `.to()` doesn't need to throw.
 
 - when a user does `model(input)`, `nn.Linear` will flesh out it's uninitalized parameters
 
 - the performance hit of this deferred initialization logic should not affect the non-deferred initialization at all",7583
22806,error in multi node,"@mrshenli After updating pytorch from 1.1 to 1.2(nightly), the default nccl is updated from 2.4.2 to 2.4.8, then the nccl runtime error is fixed under multi-nodes communication. By the way, when will pytorch 1.2 be released?",647
22807,[PyTorch][Feature Request]Lookahead Optimizer,"We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.
 
 
 
 Our reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.
 
 We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).
 
 In terms of rejected methods, we've rejected newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).
 
 
 
 If you have a differing opinion, let us know why, and we can re-think.
 
 
 
 tl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it",8265
22808,Bug of LongTensor/IntTensor ?,"fyi, @nairbv",9436
22809,Fatal error gloo/transport/tcp/device.h: no such file or directory build from source,"USE_DISTRIBUTED=0 is definitely a flag you'd want to set. Generally, look at the Android mobile build and what flags are set in that case. @ljk53 might be able to help here.",8045
22810,cpp_extension should use ninja for setup.py builds too,This is fixed ,7650
22811,torch.mean() calculations are not consistent across CPU/GPU,"Sum itself seems to behave a bit different for gpu & cpu for your array of values.
 
 
 
 This is what I see:
 
 
 
 ```
 
 a=torch.linspace(10000,1.7,10000)
 
 b=a.cuda()
 
 a.sum()
 
 # tensor(50008496.)
 
 b.sum()
 
 # tensor(50008504., device='cuda:0')
 
 ```
 
 I am not sure if this really is to be classified as a bug or as an acceptable artifact of parallel floating point summation on the GPU. The order of fp summation operations generally matters since a summand with larger exponent will ignore bits from a smaller summand's mantissa. Different algorithms exist to counter this, e.g. [Kahan_summation_algorithm](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). Also one should keep in mind that internally different register width might be used, even on CPU e.g. 80 bits for the x87 while SSE uses stictly 64/32 bit float registers etc.
 
 
 
 A GPU summation precision related discussion (with some papers) is in the NVIDIA dev forums [here](https://devtalk.nvidia.com/default/topic/1044661/how-to-improve-float-array-summation-precision-and-stability-/).",6085
22812,RuntimeError: set_storage is not allowed on Tensor created from .data or .detach(),"Hi Imad,
 
 Here is a quick fix:
 
 1. Replace weight.data.set_ with weight.set_.
 
 2. Add with th.no_grad(): block.
 
 
 
 Revised version: 
 
 
 
 ```python
 
 with th.no_grad():
 
  model.weight.set_(((alice_model.weight.data + bob_model.weight.data) / 2).get())
 
 ```
 
 
 
 The line after the colon (:) needs to be indented. 
 
 
 
 Details: https://discuss.pytorch.org/t/api-change-for-tensor-data-set-in-torch-nightly/33310/6
 
 Hope this helps.
 
 Agata",2852
22813,Incorrect gradients for torch.where when one of the target tensors contains inf/nan,"Why is this line using `z` (instead of w)?
 
 
 
 ```
 
 dwdx = torch.autograd.grad(z.sum(), x, retain_graph=True)[0] 
 
 ```",8412
22814,RuntimeError: cuda runtime error (3) : initialization error AFTER daemonization," Hopefully, I'll be able to remark on why it was commented out in #23030. Did it cause any damage?

Read #15782, #15734, and #17357 for the complete narrative.
However, I believe the underlying constraint is that initialising CUDA context when device count is called is not permitted. This is because we have a hack called ""lazy cuda library stubs"" that allows you to run applications that aren't actually cuda libraries.",205
22815,error: no viable conversion from 'script::Module' to 'std::shared_ptr<torch::jit::script::Module>',"Are you using a nightly/master build of libtorch? This was changed recently and we haven't updated the tutorial on the stable docs yet (which apply to v1.1), you can see an updated version [here](https://github.com/pytorch/tutorials/pull/560).
 
 
 
 TLDR: `script::Module` was changed to a value type, so you can just do `script::Module myModule = torch::jit::load(argv[1]);`
 
 
 
 Closing since we already have a PR out to fix the tutorial, but feel free to re-open if you still have problems",1276
22816,Update RPC examples/docs/tutorials to use RRef helper,"> Then I would try to pass PyTorch computation to remote machine with as little modifications as possible. The best is to specify the address of additional remote worker from command line.
 
 
 
 In this case you might want to start from `DistributedDataParallel`?
 
 
 
 > And ideally I'd like to be able to add worker dynamically without losing the progress I made so far.
 
 
 
 This is not possible in `torch.distributed.rpc` yet. For `DistributedDataParallel`, there is a [`torchelastic`](https://pytorch.org/elastic) project built for this purpose. cc @kiukchung",252
22817,Pytorch's jit weight file cannot run on libtorch 1.5release,"The problem has been solved. net->forward() return three tensors.
 
 auto ret = model->forward(jitdata).toTuple().elements();",7226
22818,JIT-traced module incurs bug: nSubTensors_ > 1 ASSERT FAILED,"[solved] The problem was that somewhere in the code `torch.cat` was applied to a single tensor only instead of a list of tensors, i.e., `torch.cat([tensor])`. While this worked fine for normal runs, the JIT tracer threw the above mentioned error without any hint on where it occured. @ailzhang",8699
22819,torch.relu performance appears slower on a single Tensor vs. a Python outer for-loop,"I suspect it's because the second one has to allocate an output of `256 *80 * 80 * 20` (125 MiB) while the first one only has outputs of size ~6.25 MiB (one or two live at a time). 
 
 
 
 The glibc allocator will re-use 6.25 MiB allocations but map and unmap fresh pages for the 125 MiB allocation, which is expensive and repeatedly incurs the kernel's zeroing cost. This cost is a lot bigger than the overhead of 20 additional calls to relu.
 
 
 
 jemalloc might be better; it tends to hold on to large allocations for longer than glibc.
 
 
 
 I don't think this is something we can fix in PyTorch other than possibly changing the memory allocator (which might be a good idea).",3209
22820,DISABLED test_profiler_with_async_rpc_builtin (__main__.RpcTestWithSpawn),"btw, if we simply moved that future's condvar.notify_all() to the end, it still won't get a guarantee that all the callbacks have been executed in the fut.wait() code snippet above.
 
 
 
 Typically in a future-style api, the main contract is simply that the future's value has been set, rather than all the subscribers being notified and having processed the event.",8987
22821,"Windows Libtorch Ver1.5 did't operate at the GPU mode , it operated at the CPU mode.",https://github.com/pytorch/pytorch/issues/37124,9540
22822,"Runtime error while calculating gradients , Version 1.5.0 specific","Oh, I see, that's why its a problem faced while having 'retail graph = True' . Thanks a lot.",4591
22823,SIGILL from libtorch_cpu.so at import with a CPU without AVX," these lines look suspicious to me:
 
 
 
 https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L19
 
 
 
 https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L328
 
 
 
 and similar in other files (e.g. complex).
 
 
 
 We should stop accepting patches that declare data in Vec256 files. There isn't a good reason for these ""1"" constants and they cause trouble -- use `Vec256<float>(1.0f)` instead.",582
22824,[dataloader] Multiple warnings printed when torch.as_tensor applied to readonly NumPy tensor,"Often these non-writable NumPy tensors appear as wrapper of some contig memory from an external source (like read file). An optional no-copy behavior is often desirable even in these cases: e.g. audio after decompression can be huge, and copy 1) asks to have a second free big chunk of memory, 2) introduces copying overhead",4587
22825,Add a CI job using conda compilers,"> The question I have is if it should be an extra CI job entry, or if we should modify an existing one (and if so, which one)?
 
 
 
 Extra CI seems right",251
22826,einsum shape zero,"## zeros in the output indices
 
 If I run `torch.einsum('anything->ijkl', ...)` I expect the output to have dimension 4 and the shape corresponding to the indices `i`, `j`, `k` and `l`.
 
 
 
 Example
 
 ```
 
 torch.einsum('ijk,k->ij', torch.randn(3, 0, 6), torch.randn(6)) == torch.empty(3, 0)
 
 ```
 
 
 
 ## zeros in the contracted indices
 
 If one of the contracted indices is shape zero it means that you sum over nothing, therefore the result is well defined and is a zero tensor.
 
 ```
 
 torch.einsum('ij,j->i', torch.randn(4, 0), torch.randn(0)) == torch.zeros(4)
 
 ```",47
22827,Lack of AVX2 not detected correctly in build,Probably related (or similar to) #37577,4899
22828,[JIT] Error accessing NamedTuple field by name in module's forward,Your second example works fine actually; I think you referenced an attribute called `name` that doesn't exist as per your definition of `Params`.,8684
22829,Remove torch.max/min warning,"We do verify that behaviour in #42004.
 
 
 
 Attaching screen-shot as the github doesn't render the file post 11k lines.
 
 ![image](https://user-images.githubusercontent.com/19503980/92435070-15805180-f1bf-11ea-924e-e7199a187262.png)",8262
22830,Generalized Helper for Parsing Environment Variables from Process Group,I can let you know if any of the FB internal tests are failing and how can we address those. Feel free to put up a PR and we can coordinate from there!,2823
22831,Using DataLoader with num_workers>0 causes re-run of script,"This is expected. Windows uses `spawn` by default (because of Python), which literally reruns the script, which is why you should wrap actual executing code in `if __name__ == '__main__':`. https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection",7648
22832,"""exp_cuda"" not implemented for 'ComplexDouble'","It is fixed. Works on latest master.
 
 ```python
 
 >>> import torch
 
 >>> torch.set_default_tensor_type(torch.cuda.DoubleTensor)
 
 >>> tt = torch.Tensor([1])
 
 >>> torch.exp(1j*tt)
 
 tensor([0.5403+0.8415j])
 
 ```",3934
22833,Current behavior of `as_tuple` argument is inconsistent in `nonzero`,This is not yet fixed for `tensor.nonzero()` due to https://github.com/pytorch/pytorch/pull/45413#issuecomment-703947414. Could you reopen or open a new issue?,7667
22834,"Performance issue with FFTs, numpy vs pytorch","Timing with gh-43011 still shows a performance drop with pytorch:
 
 ```
 
 zero elapsed time: 3.07e-05 seconds
 
 rot elapsed time: 5.59e-05 seconds
 
 loop elapsed time: 0.001341 seconds
 
 NUMPY TIME elapsed time: 0.0015816 seconds
 
 zero elapsed time: 8.81e-05 seconds
 
 rot elapsed time: 0.0002193 seconds
 
 loop elapsed time: 0.0037979 seconds
 
 PYTORCH TIME elapsed time: 0.00421 seconds
 
 ```
 
 
 
 However, I don't think this is a very good benchmark. It does many small non-vectorized 1D transforms which is not what tensor libraries are best at. It's also conflating `diag`, `fft`, `abs` and copy-assign into one single benchmark.
 
 
 
 If I use `timeit` to isolate each line:
 
 * diag is ~3 us for numpy and ~5-6 us for pytorch
 
 * fft is ~10 us for numpy and 20 us for pytorch
 
 * abs is ~9 us for numpy and 12 us for pytorch
 
 * assignment is ~1us in numpy and 11 us for pytorch.
 
 
 
 Every operator has at least 3 us overhead with pytorch. That's pretty bad, but for large tensors I'm sure it balances out a bit better. FFT is about 2x slower for this small 1D tensor, but for a `256 X 512` FFT pytorch is 2x faster than NumPy, even single threaded.
 
 
 
 The biggest standout is copying from one slice to another which is an order of magnitude slower in pytorch.",7856
22835,Bug in pytorch\aten\src\ATen/native/Resize.h,"This was fixed in PyTorch 1.6. The error message is now ""RuntimeError: Index tensor must have the same number of dimensions as self tensor""",7833
22836,getting RuntimeError: CUDA error: device-side assert triggered after using cross enthropy (jupyter nbotebook),"Is a device-side assert the expected behaviour here? 
 
 
 
 It would be nice if a more user-friendly error were produced",3880
22837,Can't solve torch.lstsq() with specific values,"Thanks for the simpler reproduction, @chrisby. 
 
 
 
 Our team is currently working through linear algebra updates, but I've changed this issue to high priority. It should be fixed before the next release.",6870
22838,How to print optimized IR?,"Does using `export PYTORCH_JIT_LOG_LEVEL="">profiling_graph_executor_impl""` work",2189
22839,Distributions are not nn.Modules,"I'd add JIT support to the constraints.
 
 I'm still very vaguely hopeful that introducing a calculated param extension like #7313 might help with the constraints and caching. (Interaction of spectral norm / weight norm with hooks and JIT came up recently, too.)
  I'm not going to be working on it.",3319
22840,"RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1570910687650/work/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.","
 This means you have either a data dependent control flow in your model, or you simply define some parameters and never use them. If it's the former, then great. If it's the latter, I would look for the parameters that you're not using and remove them. There is a small perf cost to finding unused parameters that you can remove if this is the case.",2344
22841,NCCL Error 4: invalid argument,"This was fixed in #29014
 
 
 
 Should probably request a cherry pick for 1.4.0",7832
22842,ONNX exporter: issues with F.interpolate," this issue is fixed in master, I tested your script with the latest pytorch-nighlty and it ran succesfully :)
 
 
 
 Yes, the graph is different when scale_factor and output_size are given; we are also working on some improvements and optimizations to the graph in https://github.com/pytorch/pytorch/pull/28324 that should be merged soon.",623
22843,"torch.logsumexp only works on a single dimension, but doc says tuple of dims should work","I think this is fixed as of version 1.5:
 
 
 
 ```python
 
 >>> A = torch.ones(3, 3) 
 
 >>> torch.logsumexp(A, dim=(0, 1)) 
 
 Out[]: tensor(3.1972)
 
 ```",3260
22844,pin_memory stuck in DDP/Reducer constructor,"Right before the point where you suggest adding a `time.sleep(3)` the model parameters are broadcast from rank 0 to all other ranks. This runs NCCL kernels (see `_broadcast_coalesced`).
 
 
 
 I won't be surprised that this is what's interfering with pinned memory allocation.
 
 
 
 Can you run this with `NCCL_DEBUG=INFO` to see if anything stands out?",5704
22845,TestAutogradDeviceTypeCUDA.test_logdet_1x1_cuda is flaky,"I don't think it has, @kurtamohler. Let's close it.",3058
22846,macOS 10.15 on Ryzen get Intel MKL ERROR: CPU 0 is not supported.,"
 export MKL_DEBUG_CPU_TYPE=5
 
 this seems to work well for me,mac os 15.1 on AMD,è€å“¥ç‰›çš®",631
22847,test_int_pow_cuda failed on Windows,It is both reproducible on circleci and jenkins once you use vs2019 as compiler.,3932
22848,cannot find /usr/local/cuda-* and no nvcc inside pytorch docker container," you are using runtime images that don't contain cuda toolkit and nvcc. If you need cuda toolkit, you should use *devel pytorch image that, as @ezyang says, uses `nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04` as a base.",636
22849,Cannot init cuda under cuda10.1 & pytorch 1.3.1,"jsut for future googlers: I found following solution - https://discuss.pytorch.org/t/cuda-runtime-error-999/69658/11
 
 
 
 > You could try to reload the nvidia kernel module via:
 
 > 
 
 > sudo rmmod nvidia_uvm
 
 > sudo modprobe nvidia_uvm
 
 > Ubuntu seems to have some issues with sleep/suspend (or maybe Linux in general?).
 
 >",9753
22850,CUDNN and NCCL paths are not detected while they are set,"I think you want to set the environment variable `CUDNN_LIBRARY` instead of `CUDNN_LIBRARY_PATH`. I believe `CUDNN_LIBRARY_PATH` is set based on `CUDNN_LIBRARY`. (Alternatively set `CUDNN_ROOT`)
 
 
 
 https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules_CUDA_fix/FindCUDNN.cmake#L4-L7
 
 
 
 Also see the NCCL variables:
 
 
 
 https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules/FindNCCL.cmake#L4-L6",3285
22851,torch.utils.data.dataloader doesn't support multiprocessing with multiple workers,It's impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0.,4019
22852,torch.utils.data.dataloader doesn't support multiprocessing with multiple workers,It's impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0.,4019
22853,Trouble building for ROCm,"Note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series. Please check latest docs.",1612
22854,Trouble building for ROCm,"Note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series. Please check latest docs.",1612
22855,torch.load issue on loading file created by torch.save,Put the model on CPU and then save.I t may be an issue with tensor.cuda,3354
22856,"The mkldnn multiplication goes wrong when compiled with -march=native and the tensors are sufficiently large (>=1024 items) on both Haswell and Cascadelake architecture CPUs.

","Pytorch master has been updated with the required configuration. Tested on Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz, according to wikipedia Xeon E5 v3 has Haswell architecture.",3014
22857,What is the purpose of fp16 training? faster training? or better accuracy?,Training with FP16 on supported GPUs drastically cuts off training time while preserving accuracy as shown in studies and leads to smaller memory footprint.,2611
22858,torch.Tensor([True]) returns different result than torch.tensor([True]),"If you write torch.Tensor([True]) or torch.Tensor([1]) ... it will invoke the ctor of the default tensor type (by default FloatTensor) in both cases even though a list of boolean or an integer values is passed. The default tensor type can be set by torch.set_default_tensor_type(). To explicitly select the type you can use the corresponding tensor class, e.g. torch.BoolTensor([True]).",608
22859,Conda install gives 1.0.0 with CUDA 9.0,upgrade your NVIDIA driver to latest,495
22860,Conda install gives 1.0.0 with CUDA 9.0,upgrade your NVIDIA driver to latest,495
22861,[Feature Request]: Batchwise torch.lstsq,"you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.

> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares",3554
22862,[Feature Request]: Batchwise torch.lstsq,"you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.

> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares",3554
22863,[Android] Unknown builtin op: aten::mul,add `torch::autograd::AutoGradMode guard(false);` ,238
22864,[Android] Unknown builtin op: aten::mul,add `torch::autograd::AutoGradMode guard(false);` ,238
22865,quantizationed model cannot inference with cuda?,"That is correct, we don't support cuda currently.",6884
22866,quantizationed model cannot inference with cuda?,"That is correct, we don't support cuda currently.",6884
22867,Can not use .cuda() function to load the model into GPU using Pytorch 1.3,"upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default.",688
22868,Can not use .cuda() function to load the model into GPU using Pytorch 1.3,"upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default.",688
22869,Try torch.nn.quantized.functional.conv2d failed,"```
qF.conv2d(q_inputs, q_filters, bias)
```
the data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8.",2436
22870,Try torch.nn.quantized.functional.conv2d failed,"```
qF.conv2d(q_inputs, q_filters, bias)
```
the data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8.",2436
22871,Support dictionary outputs in TorchScript tracer,"this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6",646
22872,Support dictionary outputs in TorchScript tracer,"this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6",646
22873,[feature request] Batched (n-1)-D to n-D matrix_diag,"Here is a proposed implementation
```python
def matrix_diag(diagonal):
    N = diagonal.shape[-1]
    shape = diagonal.shape[:-1] + (N, N)
    device, dtype = diagonal.device, diagonal.dtype
    result = torch.zeros(shape, dtype=dtype, device=device)
    indices = torch.arange(result.numel(), device=device).reshape(shape)
    indices = indices.diagonal(dim1=-2, dim2=-1)
    result.view(-1)[indices] = diagonal
    return result
```",556
22874,[feature request] Batched (n-1)-D to n-D matrix_diag,"Here is a proposed implementation
```python
def matrix_diag(diagonal):
    N = diagonal.shape[-1]
    shape = diagonal.shape[:-1] + (N, N)
    device, dtype = diagonal.device, diagonal.dtype
    result = torch.zeros(shape, dtype=dtype, device=device)
    indices = torch.arange(result.numel(), device=device).reshape(shape)
    indices = indices.diagonal(dim1=-2, dim2=-1)
    result.view(-1)[indices] = diagonal
    return result
```",556
22875,[caffe2] VS2017 compiler error for ATen,"updating to VS 2017 15.9.0 fixes it. I'm not trying to build with CUDA, but have CUDA 10.0 installed.",3147
22876,[caffe2] VS2017 compiler error for ATen,"updating to VS 2017 15.9.0 fixes it. I'm not trying to build with CUDA, but have CUDA 10.0 installed.",3147
22877,GPU hangs after killing the program using DistributedDataparallel Model,"`ps aux|grep python` 
and then 
`kill -9 PID`
be sure that kill process **in order**",9514
22878,GPU hangs after killing the program using DistributedDataparallel Model,"`ps aux|grep python` 
and then 
`kill -9 PID`
be sure that kill process **in order**",9514
22879,Error building a custom PyTorch CUDA extension with 1.0 on macOS High Sierra,Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html,8258
22880,Error building a custom PyTorch CUDA extension with 1.0 on macOS High Sierra,Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html,8258
22881,get_cudnn_version in collect_env is flaky,That's because LD_LIBRARY_PATH is not the only path used for lookup.,6897
22882,get_cudnn_version in collect_env is flaky,That's because LD_LIBRARY_PATH is not the only path used for lookup.,6897
22883,Slowdown in distributions log_prob methods,Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow`,8482
22884,Slowdown in distributions log_prob methods,Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow`,8482
22885,[Feature Request]Synchronized batch norm, take a look at this: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example).,3397
22886,[Feature Request]Synchronized batch norm, take a look at this: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example).,3397
22887,Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False,"```python
class MyUpsample2(nn.Module):
    def forward(self, x):
        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)
```
makes this deterministic.
",3063
22888,Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False,"```python
class MyUpsample2(nn.Module):
    def forward(self, x):
        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)
```
makes this deterministic.
",3063
22889,PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10,Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud.,8079
22890,PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10,Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud.,8079
22891,Weight decay modifies grad for SGD but not for Adam,"no adam modifies it in place see:

https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90
",10248
22892,Weight decay modifies grad for SGD but not for Adam,"no adam modifies it in place see:

https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90
",10248
22893,Bug in masked_fill_ for non contiguous tensors,> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux,262
22894,Bug in masked_fill_ for non contiguous tensors,> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux,262
22895,[Conv3D][cudnn] CUDNN_STATUS_NOT_INITIALIZED error when using batch size > 1 with conv layer,"It is more like an ""out of range"" error or something and confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. ",753
22896,[Conv3D][cudnn] CUDNN_STATUS_NOT_INITIALIZED error when using batch size > 1 with conv layer,"It is more like an ""out of range"" error or something and confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. ",753
22897,Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck,All you need to make sure is that the input used in gradcheck don't have overlapping storage.,7200
22898,Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck,All you need to make sure is that the input used in gradcheck don't have overlapping storage.,7200
22899,Build failed when linking bin/test_parallel with multiple openmp functions defined,"
> find . -type f -name 'build.make' -exec sed -i 's:^.*\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \;
> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \;

I think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.
",3163
22900,Build failed when linking bin/test_parallel with multiple openmp functions defined,"
> find . -type f -name 'build.make' -exec sed -i 's:^.*\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \;
> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \;

I think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.
",3163
22901,CTCLoss CUDA backward throws setStorage: storage size mismatch error,"The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).
",6983
22902,CTCLoss CUDA backward throws setStorage: storage size mismatch error,"The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).
",6983
22903,In-place operation on differentiable view leaks memory,"This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.
This only happens because `x` here is a leaf Tensor.
Note that after doing such thing, trying to run backward will fail with ""leaf variable has been moved into the graph interior"" so we should fail earlier before the cycle is created.",7608
22904,In-place operation on differentiable view leaks memory,"This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.
This only happens because `x` here is a leaf Tensor.
Note that after doing such thing, trying to run backward will fail with ""leaf variable has been moved into the graph interior"" so we should fail earlier before the cycle is created.",7608
22905,Converting from IValue to double gives INTERNAL ASSERT failure,"The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type",6955
22906,Converting from IValue to double gives INTERNAL ASSERT failure,"The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type",6955
22907,cmake error build failed,"Two options available: 
1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` 
```cmd
set CMAKE_GENERATOR_TOOLSET_VERSION=
```
2. activate the env.
```cmd
set DISTUTILS_USE_SDK=1
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%
```
It is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation.",8031
22908,cmake error build failed,"Two options available: 
1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` 
```cmd
set CMAKE_GENERATOR_TOOLSET_VERSION=
```
2. activate the env.
```cmd
set DISTUTILS_USE_SDK=1
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%
```
It is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation.",8031
22909,Multiprocessing and tensor problem,"To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not.",7865
22910,Multiprocessing and tensor problem,"To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not.",7865
22911,Bug in saving indexed torch tensors makes it much slower than numpy.,"this is not a bug, it's actually by design.

when you index individual elements into a Tensor, then you are holding a view into the original Tensor.

```
a = torch.randn(10, 20)
b = a[0]
b.add_(10) # changes `a`
```

If you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this ""view"" property is actually preserved across serialization / deserialization.",10897
22912,Bug in saving indexed torch tensors makes it much slower than numpy.,"this is not a bug, it's actually by design.

when you index individual elements into a Tensor, then you are holding a view into the original Tensor.

```
a = torch.randn(10, 20)
b = a[0]
b.add_(10) # changes `a`
```

If you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this ""view"" property is actually preserved across serialization / deserialization.",10897
22913,Unexpected output size for Maxpool,"
The implementation should allow for the output size to be 0 if the kernel is larger than input.

For example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.

The formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**

However, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.

If k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.
",0
22914,Unexpected output size for Maxpool,"
The implementation should allow for the output size to be 0 if the kernel is larger than input.

For example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.

The formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**

However, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.

If k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.
",0
22915,base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set,Schedulers are not currently chainable ,11480
22916,base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set,Schedulers are not currently chainable ,11480
22917,Implement covariance_matrix for Independent distributions,"```py
def batch_diag(batched_variance):
    batch_shape = batched_variance.shape[:-1]
    event_size = batched_variance.size(-1)
    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))
    cov[..., ::1 + event_size] = batched_variance
    return cov.reshape(batch_shape + (event_size, event_size))
```",584
22918,Implement covariance_matrix for Independent distributions,"```py
def batch_diag(batched_variance):
    batch_shape = batched_variance.shape[:-1]
    event_size = batched_variance.size(-1)
    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))
    cov[..., ::1 + event_size] = batched_variance
    return cov.reshape(batch_shape + (event_size, event_size))
```",584
22919,About CMAKE_PREFIX_PATH,`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` ,4551
22920,About CMAKE_PREFIX_PATH,`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` ,4551
22921,ReduceLROnPlateau parent class is not _LRScheduler,`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. ,8713
22922,ReduceLROnPlateau parent class is not _LRScheduler,`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. ,8713
22923,"""Trying to resize storage that is not resizable"" when calling pin_memory() on some zero-dimensional tensors","This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?",678
22924,"""Trying to resize storage that is not resizable"" when calling pin_memory() on some zero-dimensional tensors","This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?",678
22925,Need GPU implementation of dirichlet_grad (originally: Reparameterized gradient on GPU for beta / Dirichlet),"I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:

    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')
    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))
    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))
    s = s1 / (s1+s2)
    torch.sum(s).backward()
    a.grad
",3086
22926,Need GPU implementation of dirichlet_grad (originally: Reparameterized gradient on GPU for beta / Dirichlet),"I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:

    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')
    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))
    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))
    s = s1 / (s1+s2)
    torch.sum(s).backward()
    a.grad
",3086
22927,Seems there's memory issue in pytorch 1.0.0," check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location",3051
22928,Seems there's memory issue in pytorch 1.0.0," check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location",3051
22929,torch::save causes serialization error on mnist example,"You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder",8648
22930,torch::save causes serialization error on mnist example,"You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder",8648
22931,Implementation of ISTFT,Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft,513
22932,Implementation of ISTFT,Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft,513
22933,Dropout on integer tensor types dies with SIGFPE,"@mruberry Think this issue should be closed since the signal is no longer raised.

~~Instead we get this errors:~~

~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~

~~in both cases (the one with Dropout and the second one from @colesbury).~~ 

Ran the code with the nightly build.

For the first example this error is raised:
`RuntimeError: result type Float can't be cast to the desired output type Long`

and for the second one (the one from @colesbury) there is no error, instead we get: 
`tensor([inf, nan, nan, nan, inf])`

I think now it's more clear to the user what's the problem.",653
22934,Dropout on integer tensor types dies with SIGFPE,"@mruberry Think this issue should be closed since the signal is no longer raised.

~~Instead we get this errors:~~

~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~

~~in both cases (the one with Dropout and the second one from @colesbury).~~ 

Ran the code with the nightly build.

For the first example this error is raised:
`RuntimeError: result type Float can't be cast to the desired output type Long`

and for the second one (the one from @colesbury) there is no error, instead we get: 
`tensor([inf, nan, nan, nan, inf])`

I think now it's more clear to the user what's the problem.",653
22935,THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument,I have successfully solved this problem by updating the cuda8.0 to cuda10.0.,3142
22936,THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument,I have successfully solved this problem by updating the cuda8.0 to cuda10.0.,3142
22937,Memory leak during backprop() in PyTorch 1.0.0,"tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.

I tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.

My conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336

Once the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.

I know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. 

",10922
22938,Memory leak during backprop() in PyTorch 1.0.0,"tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.

I tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.

My conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336

Once the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.

I know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. 

",10922
22939,"Debugging feature for ""modified by an inplace operation"" errors","> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.

To clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.

Example:
```
x = torch.rand(10, 20, requires_grad=True)
y = torch.rand(10)
z = (x / y[:, np.newaxis])  # anomaly detection will point here
c = y.abs_()  # but the problem is here
z.sum().backward()
```
The last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later.",274
22940,"Debugging feature for ""modified by an inplace operation"" errors","> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.

To clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.

Example:
```
x = torch.rand(10, 20, requires_grad=True)
y = torch.rand(10)
z = (x / y[:, np.newaxis])  # anomaly detection will point here
c = y.abs_()  # but the problem is here
z.sum().backward()
```
The last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later.",274
22941,Keyboard interrupt and saving the last state of a model,"This is all you need to implement this. I don't see why would this have to be part of the library.
```python
try:
    # training code here
except KeyboardInterrupt:
    # save model here
```",7621
22942,Keyboard interrupt and saving the last state of a model,"This is all you need to implement this. I don't see why would this have to be part of the library.
```python
try:
    # training code here
except KeyboardInterrupt:
    # save model here
```",7621
22943,DataLoader freezes randomly when num_workers > 0 (Multiple threads train models on different GPUs in separate threads),"I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. 

In my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.

Since it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory.",3140
22944,DataLoader freezes randomly when num_workers > 0 (Multiple threads train models on different GPUs in separate threads),"I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. 

In my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.

Since it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory.",3140
22945,[JIT] Support Meta programming on If self.training to conditionally NOT compile training only code,"Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. 

The problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. 

Edited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. ",2844
22946,[JIT] Support Meta programming on If self.training to conditionally NOT compile training only code,"Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. 

The problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. 

Edited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. ",2844
22947,[QUESTION] Should _dummy_type use name instead of storage_name?,Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all,1273
22948,[QUESTION] Should _dummy_type use name instead of storage_name?,Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all,1273
22949,Non-monotonic execution times for increasing kernel sizes,"Maybe cudnn is probably selecting different algorithms.
Could you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?",4222
22950,Non-monotonic execution times for increasing kernel sizes,"Maybe cudnn is probably selecting different algorithms.
Could you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?",4222
22951,How to load a model trained on GPU 0 (cuda: 0) to GPU 1 (cuda:1) for inference?,In all likelihood your `device` value is nonsense. Print it out and see what the problem is.,3702
22952,How to load a model trained on GPU 0 (cuda: 0) to GPU 1 (cuda:1) for inference?,In all likelihood your `device` value is nonsense. Print it out and see what the problem is.,3702
22953,Feature request : Profiler,MXnet’s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile,4171
22954,Feature request : Profiler,MXnet’s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile,4171
22955,LibTorch Windows binaries appear to not be built with CuDNN,"Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch.",4611
22956,LibTorch Windows binaries appear to not be built with CuDNN,"Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch.",4611
22957,RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE',"Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence ""solves"" the bug. 

The torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)",4483
22958,RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE',"Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence ""solves"" the bug. 

The torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)",4483
22959,".size() vs .shape, which one should be used?",".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!",87
22960,".size() vs .shape, which one should be used?",".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!",87
22961,[feature request] ATen Documentation and Tutorial,"See https://pytorch.org/cppdocs/
See https://pytorch.org/cppdocs/notes/tensor_creation.html",5779
22962,[feature request] ATen Documentation and Tutorial,"See https://pytorch.org/cppdocs/
See https://pytorch.org/cppdocs/notes/tensor_creation.html",5779
22963,RuntimeError: dimension specified as 0 but tensor has no dimensions,"Did you try `z = z.unsqueeze(0)`? 

The reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).
",2140
22964,RuntimeError: dimension specified as 0 but tensor has no dimensions,"Did you try `z = z.unsqueeze(0)`? 

The reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).
",2140
22965,NCCL Error 1 when using torch.nn.DataParallel,"I can help -  @aleksod 
Looking at their official build guide: https://github.com/NVIDIA/nccl

you need to clone a different repo for the tests:
`git clone https://github.com/NVIDIA/nccl-tests.git`
Then, in order to build the tests, enter that repo and run:
CUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make

for example
CUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make

then, an example test from their official doc is:
`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`

if you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:

`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`




",3022
22966,NCCL Error 1 when using torch.nn.DataParallel,"I can help -  @aleksod 
Looking at their official build guide: https://github.com/NVIDIA/nccl

you need to clone a different repo for the tests:
`git clone https://github.com/NVIDIA/nccl-tests.git`
Then, in order to build the tests, enter that repo and run:
CUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make

for example
CUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make

then, an example test from their official doc is:
`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`

if you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:

`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`




",3022
22967,"dimension out of range (expected to be in range of [-1, 0], but got 1)","ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. 

 Below is the code/shapes without dimension error.

```
len(pred_y):
torch.Size([1, 3])
tensor([[ 0.0000,  0.0000,  0.1527]])
len(x):
torch.Size([1])
tensor([ 2])
loss_training = loss_fn(pred_y, x)


```",10291
22968,"dimension out of range (expected to be in range of [-1, 0], but got 1)","ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. 

 Below is the code/shapes without dimension error.

```
len(pred_y):
torch.Size([1, 3])
tensor([[ 0.0000,  0.0000,  0.1527]])
len(x):
torch.Size([1])
tensor([ 2])
loss_training = loss_fn(pred_y, x)


```",10291
22969,Incorrect error message for advanced indexing cuda tensor,This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :),7574
22970,Incorrect error message for advanced indexing cuda tensor,This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :),7574
22971,"""Reduce Failed to Synchronise"" in F.binary_cross_entropy ","> See also: #2209
> 
> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case

I got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)

and I tried to use `nn.BCELoss()` like:

```
optimizer = optim.SGD(model.parameters(), lr=0.0001)
criterion = nn.BCELoss()
```

_loop epoch train part:_

```
prediction = model(batch_input)
loss = criterion(torch.sigmoid(prediction), label)

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

Then I solved the problem. Thank you for your comment!
(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)
",244
22972,"""Reduce Failed to Synchronise"" in F.binary_cross_entropy ","> See also: #2209
> 
> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case

I got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)

and I tried to use `nn.BCELoss()` like:

```
optimizer = optim.SGD(model.parameters(), lr=0.0001)
criterion = nn.BCELoss()
```

_loop epoch train part:_

```
prediction = model(batch_input)
loss = criterion(torch.sigmoid(prediction), label)

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

Then I solved the problem. Thank you for your comment!
(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)
",244
22973,Device-side Assert in `THCReduceAll.cuh:339`,You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`,8627
22974,Device-side Assert in `THCReduceAll.cuh:339`,You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`,8627
22975,Import Error : no module  named torch,"Add the path by: export PATH=~/anaconda3/bin:$PATH  
before opening the python.",946
22976,Import Error : no module  named torch,"Add the path by: export PATH=~/anaconda3/bin:$PATH  
before opening the python.",946
22977,torch.autograd.Function memory leak,Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`,8544
22978,torch.autograd.Function memory leak,Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`,8544
22979,Missing torch.* docs,#5443 addresses all but `torch.default_generator`. :),51
22980,Missing torch.* docs,#5443 addresses all but `torch.default_generator`. :),51
22981,Add Scale Factor To SGD,"1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration
2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block
3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility",99
22982,Add Scale Factor To SGD,"1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration
2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block
3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility",99
22983,[bug] Expected GLOO_USE_IBVERBS to be defined,"Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. 

Or if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.

If this works, please close this issue.",4152
22984,[bug] Expected GLOO_USE_IBVERBS to be defined,"Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. 

Or if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.

If this works, please close this issue.",4152
22985,[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch,"As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix",1286
22986,[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch,"As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix",1286
22987,[bug?] Problem with load_state_dict after installing latest pytorch by source,The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors.,6956
22988,[bug?] Problem with load_state_dict after installing latest pytorch by source,The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors.,6956
22989,when pytorch should add Windows support.,Starting from the next release. Should be out within weeks,6068
22990,when pytorch should add Windows support.,Starting from the next release. Should be out within weeks,6068
22991,In IPython torch.dot always returns 0,"please do:

```
pip uninstall -y numpy
conda install -y numpy 
```

The pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`",10429
22992,In IPython torch.dot always returns 0,"please do:

```
pip uninstall -y numpy
conda install -y numpy 
```

The pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`",10429
22993,Feature request: named dimensions,See `xarray` for prior work https://github.com/pydata/xarray,5764
22994,Feature request: named dimensions,See `xarray` for prior work https://github.com/pydata/xarray,5764
22995,[feature request] torch.argmax / torch.argmin,@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin,718
22996,[feature request] torch.argmax / torch.argmin,@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin,718
22997,Pytorch 0.4.0 documentation typo for batchnorm's momentum parameter,closed via https://github.com/pytorch/pytorch/pull/5450/,9089
22998,Pytorch 0.4.0 documentation typo for batchnorm's momentum parameter,closed via https://github.com/pytorch/pytorch/pull/5450/,9089
22999,Can not restart the training to obtain the same results,"Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc.",4211
23000,Can not restart the training to obtain the same results,"Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc.",4211
23001,[feature request] Add C++ test framework for pure C++ autograd+jit tests,"The Catch2 devs recommend to just vendor Catch2 in repositories
https://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake

The alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies using the ExternalProject machinery, but I honestly wouldn't go there.",6916
23002,[feature request] Add C++ test framework for pure C++ autograd+jit tests,"The Catch2 devs recommend to just vendor Catch2 in repositories
https://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake

The alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies using the ExternalProject machinery, but I honestly wouldn't go there.",6916
23003,Handle None gradients in nn.utils.clip_grad_norm,"What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied.",8337
23004,Handle None gradients in nn.utils.clip_grad_norm,"What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied.",8337
23005,Variables are behaving strangely for indexing,"One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.

The other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices.",4659
23006,Variables are behaving strangely for indexing,"One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.

The other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices.",4659
23007,Softmax using multiple threads inhibiting parallel execution of forward passes (?),"Using `torch.set_num_threads(1)` solves the problem.

I figure the problem was a deadlock arising as a result of creating a huge number of threads:

I create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. 

It is funny though that this occurs only on Linux and not Mac OS.",8103
23008,Softmax using multiple threads inhibiting parallel execution of forward passes (?),"Using `torch.set_num_threads(1)` solves the problem.

I figure the problem was a deadlock arising as a result of creating a huge number of threads:

I create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. 

It is funny though that this occurs only on Linux and not Mac OS.",8103
23009,0.3.0 availability on conda,"conda install pytorch=0.3.0 torchvision -c pytorch

I test this command is right to install pytorch3.0.0 in MacOS!",9113
23010,0.3.0 availability on conda,"conda install pytorch=0.3.0 torchvision -c pytorch

I test this command is right to install pytorch3.0.0 in MacOS!",9113
23011,Only nn.Parameters defined directly within nn.Module are listed in module.parameters(),"There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules.",7399
23012,Only nn.Parameters defined directly within nn.Module are listed in module.parameters(),"There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules.",7399
23013,Issues about symeig and svd on GPU,Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls.,2177
23014,Issues about symeig and svd on GPU,Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls.,2177
23015,ModuleNotFoundError: No module named 'torch.version',It looks like you are. Change the directory and you should be good.,3966
23016,ModuleNotFoundError: No module named 'torch.version',It looks like you are. Change the directory and you should be good.,3966
23017,the derivative for 'svd' is not implemented,"the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source",10849
23018,the derivative for 'svd' is not implemented,"the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source",10849
23019,backward(create_graph=True) should raise a warning for potential memory leak,"My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because 
1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.
2. it is almost always clearer in code than accessing `.grad` of leaves.
3. it avoids ref cycle.

In fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?",4284
23020,backward(create_graph=True) should raise a warning for potential memory leak,"My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because 
1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.
2. it is almost always clearer in code than accessing `.grad` of leaves.
3. it avoids ref cycle.

In fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?",4284
23021,Bug in inplace operation after expansion,"Yes, and that's the expected behavior. Don't do in-place on expanded tensors.",8500
23022,Bug in inplace operation after expansion,"Yes, and that's the expected behavior. Don't do in-place on expanded tensors.",8500
23023,"[Feature request] Gradient of cholesky_inverse, cholesky_solve","I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.
```py
def cholesky_solve(b, u):
    ""Like :func:`torch.cholesky_solve` but supports gradients.""
    if not b.requires_grad and not u.requires_grad:
        return b.cholesky_solve(u)
    x = b.triangular_solve(u, upper=False).solution
    return x.triangular_solve(u, upper=False, transpose=True).solution

def cholesky_inverse(u):
    ""Like :func:`torch.cholesky_inverse` but supports batching and gradients.""
    if u.dim() == 2 and not u.requires_grad:
        return u.cholesky_inverse()
    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)
```",3296
23024,"[Feature request] Gradient of cholesky_inverse, cholesky_solve","I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.
```py
def cholesky_solve(b, u):
    ""Like :func:`torch.cholesky_solve` but supports gradients.""
    if not b.requires_grad and not u.requires_grad:
        return b.cholesky_solve(u)
    x = b.triangular_solve(u, upper=False).solution
    return x.triangular_solve(u, upper=False, transpose=True).solution

def cholesky_inverse(u):
    ""Like :func:`torch.cholesky_inverse` but supports batching and gradients.""
    if u.dim() == 2 and not u.requires_grad:
        return u.cholesky_inverse()
    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)
```",3296
23025,No module named torchvision,If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together.,3562
23026,No module named torchvision,If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together.,3562
23027,Failed to load model,"@apaszke I solved it by using the recommended way instead, it's my fault : )
```python
torch.save(the_model.state_dict(), PATH)

the_model = TheModelClass(*args, **kwargs)
the_model.load_state_dict(torch.load(PATH))
```",540
23028,Failed to load model,"@apaszke I solved it by using the recommended way instead, it's my fault : )
```python
torch.save(the_model.state_dict(), PATH)

the_model = TheModelClass(*args, **kwargs)
the_model.load_state_dict(torch.load(PATH))
```",540
23029,"Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss","Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?

From your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.

It could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro.",1527
23030,"Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss","Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?

From your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.

It could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro.",1527
23031,Grad is None after using view,"So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).
```
X0 = torch.tensor([0.25, 0.75], requires_grad=True,)
X_view = X0.view(2, 1, 1)
print(f""X_view.shape: {X_view.shape}"")
X_view.sum().backward()
print(f""X_view.grad: {X_view.grad}"")
print(f""X_view.grad is None: {X_view.grad is None}"")
print(f""X0.grad: {X0.grad}"")
```
Output:
```
X_view.shape: torch.Size([2, 1, 1])
X_view.grad: None
X_view.grad is None: True
X0.grad: tensor([1., 1.])
```
",5949
23032,Grad is None after using view,"So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).
```
X0 = torch.tensor([0.25, 0.75], requires_grad=True,)
X_view = X0.view(2, 1, 1)
print(f""X_view.shape: {X_view.shape}"")
X_view.sum().backward()
print(f""X_view.grad: {X_view.grad}"")
print(f""X_view.grad is None: {X_view.grad is None}"")
print(f""X0.grad: {X0.grad}"")
```
Output:
```
X_view.shape: torch.Size([2, 1, 1])
X_view.grad: None
X_view.grad is None: True
X0.grad: tensor([1., 1.])
```
",5949
23033,Could Pytorch C++ API load tensor from gpu memory directly?,"Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:

```C++
    void deleter(void *arg){};
    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)
    {
        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};
        long long step = image.step / sizeof(float);
        std::vector<int64_t> strides = {step, image.channels(), 1};
        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)
    }
```

If you want to send in bytes, you would update step size and add torch::kByte/kChar to options.

Only tricky part for your GL image may be step size otherwise the code should look almost identical.",2294
23034,Could Pytorch C++ API load tensor from gpu memory directly?,"Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:

```C++
    void deleter(void *arg){};
    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)
    {
        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};
        long long step = image.step / sizeof(float);
        std::vector<int64_t> strides = {step, image.channels(), 1};
        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)
    }
```

If you want to send in bytes, you would update step size and add torch::kByte/kChar to options.

Only tricky part for your GL image may be step size otherwise the code should look almost identical.",2294
23035,[Caffe2] Compile error: onnxTensorDescriptorV1 has no member named quantizationParams,Should be fixed by https://github.com/pytorch/pytorch/pull/19793,5891
23036,[Caffe2] Compile error: onnxTensorDescriptorV1 has no member named quantizationParams,Should be fixed by https://github.com/pytorch/pytorch/pull/19793,5891
23037,AverageUnpooling layer for PyTorch (Proposal),Isn't `AverageUnpooling` very similar to `F.interpolate`?,3897
23038,AverageUnpooling layer for PyTorch (Proposal),Isn't `AverageUnpooling` very similar to `F.interpolate`?,3897
23039,Why nn.Sequential can't handle multiple input?,"@soumith Hi, I changed `nn.Sequential` to this
```python
class mySequential(nn.Sequential):
    def forward(self, *input):
        for module in self._modules.values():
            input = module(*input)
        return input
```
And it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.
```python
class n_to_n(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x1, x2):
        y1 = self.conv1(x1)
        y2 = self.conv2(x2)
        return y1, y2


class n_to_one(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x1, x2):
        y1 = self.conv1(x1)
        y2 = self.conv2(x2)
        return y1 + y2


class one_to_n(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x):
        y1 = self.conv1(x)
        y2 = self.conv2(x)
        return y1, y2

seq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()
td = torch.rand(1, 3, 32, 32).cuda()

out = seq(td)
print(out.size())
```
```shell
torch.Size([1, 3, 32, 32])
```
What do you think?",700
23040,Why nn.Sequential can't handle multiple input?,"@soumith Hi, I changed `nn.Sequential` to this
```python
class mySequential(nn.Sequential):
    def forward(self, *input):
        for module in self._modules.values():
            input = module(*input)
        return input
```
And it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.
```python
class n_to_n(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x1, x2):
        y1 = self.conv1(x1)
        y2 = self.conv2(x2)
        return y1, y2


class n_to_one(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x1, x2):
        y1 = self.conv1(x1)
        y2 = self.conv2(x2)
        return y1 + y2


class one_to_n(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)
        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)

    def forward(self, x):
        y1 = self.conv1(x)
        y2 = self.conv2(x)
        return y1, y2

seq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()
td = torch.rand(1, 3, 32, 32).cuda()

out = seq(td)
print(out.size())
```
```shell
torch.Size([1, 3, 32, 32])
```
What do you think?",700
23041,CTCLoss reduction=’none‘ and calculate average in hand is not equal to 'mean',"@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371

From [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)
> ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken",552
23042,CTCLoss reduction=’none‘ and calculate average in hand is not equal to 'mean',"@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371

From [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)
> ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken",552
23043,Using DistributedDataParallel through NCCL throws RuntimeError,"@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).

An example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.

My case was this. Buggy code (crashed):
```
if positive_class.shape[0] > 0:
   loss += torch.somefunction(positive_class)
```
Fix:
```
# concat with tensor that has no effect on loss calculation, but positive_class always has content
positive_class = torch.cat(positive_class, dummy)
loss += torch.somefunction(positive_class)
```

I don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)",522
23044,Using DistributedDataParallel through NCCL throws RuntimeError,"@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).

An example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.

My case was this. Buggy code (crashed):
```
if positive_class.shape[0] > 0:
   loss += torch.somefunction(positive_class)
```
Fix:
```
# concat with tensor that has no effect on loss calculation, but positive_class always has content
positive_class = torch.cat(positive_class, dummy)
loss += torch.somefunction(positive_class)
```

I don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)",522
23045,"At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var","Finally,I find the problem.
At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.
Change the torch.var unbiased option to True or False for seeing the result.
```
import torch
import torch.nn as nn
from torch.nn import Parameter, init

class MyBatchNorm(nn.Module):
    _version = 2
    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',
                     'running_mean', 'running_var', 'num_batches_tracked']

    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,
                 track_running_stats=True):
        super(MyBatchNorm, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats
        if self.affine:
            self.weight = Parameter(torch.Tensor(num_features))
            self.bias = Parameter(torch.Tensor(num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
        if self.track_running_stats:
            self.register_buffer('running_mean', torch.zeros(num_features))
            self.register_buffer('running_var', torch.ones(num_features))
            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
        else:
            self.register_parameter('running_mean', None)
            self.register_parameter('running_var', None)
            self.register_parameter('num_batches_tracked', None)
        self.reset_parameters()

    def reset_running_stats(self):
        if self.track_running_stats:
            self.running_mean.zero_()
            self.running_var.fill_(1)
            self.num_batches_tracked.zero_()

    def reset_parameters(self):
        self.reset_running_stats()
        if self.affine:
            init.uniform_(self.weight)
            init.zeros_(self.bias)
        
    def forward(self, input):        
        input_size = input.size()
        input = input.transpose(1,0)
        input = input.view(input.size(0), -1)

        if self.training:
            mean = input.mean(dim=1)
            var = torch.var(input,dim=1, unbiased=True)
            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean
            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var
        else:
            mean = self.running_mean
            var = self.running_var

        input = input - mean.view(-1,1)
        input = input / (torch.sqrt(var+self.eps).view(-1,1))
       
        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)
        input = input.transpose(1,0)
        input = input.view(*input_size)
        return input

    def extra_repr(self):
        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \
               'track_running_stats={track_running_stats}'.format(**self.__dict__)

    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
                              missing_keys, unexpected_keys, error_msgs):
        version = local_metadata.get('version', None)

        if (version is None or version < 2) and self.track_running_stats:
            # at version 2: added num_batches_tracked buffer
            #               this should have a default value of 0
            num_batches_tracked_key = prefix + 'num_batches_tracked'
            if num_batches_tracked_key not in state_dict:
                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)

        super(MyBatchNorm, self)._load_from_state_dict(
            state_dict, prefix, local_metadata, strict,
            missing_keys, unexpected_keys, error_msgs)

def test_batch_norm():
    momentum = 1.0
    torch.manual_seed(1234)

    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)
    n1 = batch_norm
    torch.save(n1.state_dict(),'n1.pth')
    
    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)
    n2 = torch_batch_norm
    n2.load_state_dict(torch.load('n1.pth'))
    
    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])
    y = torch.FloatTensor([[2], [3], [1]])
    criterion = nn.MSELoss()

    x = x.cuda()
    y = y.cuda()
    batch_norm.cuda()
    torch_batch_norm.cuda()

    print('Switch to eval mode.')
    batch_norm.eval()
    torch_batch_norm.eval()
    out1 = n1(x)
    out2 = n2(x)
    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)

    print('Swtich to train mode.')
    batch_norm.train()
    torch_batch_norm.train()

    out1 = n1(x)
    out2 = n2(x)
    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)

    print('Switch to eval mode.')
    n1.eval()
    n2.eval()
    print('MyBatchNorm:')
    print('running_mean:',batch_norm.running_mean.cpu().numpy())
    print('running_var:',batch_norm.running_var.cpu().numpy())
    print('weight:',batch_norm.weight.data.cpu().numpy())
    print('bias:',batch_norm.bias.data.cpu().numpy())
    print()
    
    print('TorchBatchNorm:')
    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())
    print('running_var:',torch_batch_norm.running_var.cpu().numpy())
    print('weight:',torch_batch_norm.weight.data.cpu().numpy())
    print('bias:',torch_batch_norm.bias.data.cpu().numpy())
    out1 = n1(x)
    out2 = n2(x)
    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)
    print('eval1,train2,eval3:',eval1,train2,eval3)
    assert eval1 and train2 and eval3

if __name__ == '__main__':
    test_batch_norm()
```",2439
23046,"At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var","Finally,I find the problem.
At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.
Change the torch.var unbiased option to True or False for seeing the result.
```
import torch
import torch.nn as nn
from torch.nn import Parameter, init

class MyBatchNorm(nn.Module):
    _version = 2
    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',
                     'running_mean', 'running_var', 'num_batches_tracked']

    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,
                 track_running_stats=True):
        super(MyBatchNorm, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats
        if self.affine:
            self.weight = Parameter(torch.Tensor(num_features))
            self.bias = Parameter(torch.Tensor(num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
        if self.track_running_stats:
            self.register_buffer('running_mean', torch.zeros(num_features))
            self.register_buffer('running_var', torch.ones(num_features))
            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
        else:
            self.register_parameter('running_mean', None)
            self.register_parameter('running_var', None)
            self.register_parameter('num_batches_tracked', None)
        self.reset_parameters()

    def reset_running_stats(self):
        if self.track_running_stats:
            self.running_mean.zero_()
            self.running_var.fill_(1)
            self.num_batches_tracked.zero_()

    def reset_parameters(self):
        self.reset_running_stats()
        if self.affine:
            init.uniform_(self.weight)
            init.zeros_(self.bias)
        
    def forward(self, input):        
        input_size = input.size()
        input = input.transpose(1,0)
        input = input.view(input.size(0), -1)

        if self.training:
            mean = input.mean(dim=1)
            var = torch.var(input,dim=1, unbiased=True)
            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean
            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var
        else:
            mean = self.running_mean
            var = self.running_var

        input = input - mean.view(-1,1)
        input = input / (torch.sqrt(var+self.eps).view(-1,1))
       
        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)
        input = input.transpose(1,0)
        input = input.view(*input_size)
        return input

    def extra_repr(self):
        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \
               'track_running_stats={track_running_stats}'.format(**self.__dict__)

    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
                              missing_keys, unexpected_keys, error_msgs):
        version = local_metadata.get('version', None)

        if (version is None or version < 2) and self.track_running_stats:
            # at version 2: added num_batches_tracked buffer
            #               this should have a default value of 0
            num_batches_tracked_key = prefix + 'num_batches_tracked'
            if num_batches_tracked_key not in state_dict:
                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)

        super(MyBatchNorm, self)._load_from_state_dict(
            state_dict, prefix, local_metadata, strict,
            missing_keys, unexpected_keys, error_msgs)

def test_batch_norm():
    momentum = 1.0
    torch.manual_seed(1234)

    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)
    n1 = batch_norm
    torch.save(n1.state_dict(),'n1.pth')
    
    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)
    n2 = torch_batch_norm
    n2.load_state_dict(torch.load('n1.pth'))
    
    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])
    y = torch.FloatTensor([[2], [3], [1]])
    criterion = nn.MSELoss()

    x = x.cuda()
    y = y.cuda()
    batch_norm.cuda()
    torch_batch_norm.cuda()

    print('Switch to eval mode.')
    batch_norm.eval()
    torch_batch_norm.eval()
    out1 = n1(x)
    out2 = n2(x)
    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)

    print('Swtich to train mode.')
    batch_norm.train()
    torch_batch_norm.train()

    out1 = n1(x)
    out2 = n2(x)
    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)

    print('Switch to eval mode.')
    n1.eval()
    n2.eval()
    print('MyBatchNorm:')
    print('running_mean:',batch_norm.running_mean.cpu().numpy())
    print('running_var:',batch_norm.running_var.cpu().numpy())
    print('weight:',batch_norm.weight.data.cpu().numpy())
    print('bias:',batch_norm.bias.data.cpu().numpy())
    print()
    
    print('TorchBatchNorm:')
    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())
    print('running_var:',torch_batch_norm.running_var.cpu().numpy())
    print('weight:',torch_batch_norm.weight.data.cpu().numpy())
    print('bias:',torch_batch_norm.bias.data.cpu().numpy())
    out1 = n1(x)
    out2 = n2(x)
    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)
    print('eval1,train2,eval3:',eval1,train2,eval3)
    assert eval1 and train2 and eval3

if __name__ == '__main__':
    test_batch_norm()
```",2439
23047,Is CudnnRNN thread-safe?,"can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1.",9022
23048,Is CudnnRNN thread-safe?,"can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1.",9022
23049,"ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each","@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.

Hope it will help!",510
23050,"ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each","@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.

Hope it will help!",510
23051,DataLoader for Large Corpus File,"This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 .",7631
23052,DataLoader for Large Corpus File,"This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 .",7631
23053,[jit] Slice assignment is completely elided in Onnx graph,"This is an expected behavior, and appropriate warning have sent out. So close the issue.",7626
23054,[jit] Slice assignment is completely elided in Onnx graph,"This is an expected behavior, and appropriate warning have sent out. So close the issue.",7626
23055,torch.nn.LogSoftmax.__repr__() does not include dim argument,@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`.,509
23056,torch.nn.LogSoftmax.__repr__() does not include dim argument,@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`.,509
23057,cuda-runtime error(4) on PyTorch 0.4.1,"> Is there some solution can solve it without upgrade?

I was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. ",235
23058,cuda-runtime error(4) on PyTorch 0.4.1,"> Is there some solution can solve it without upgrade?

I was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. ",235
23059,[ONNX] The shape of PReLU weight is wrong,"> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.

You can try inserting ```onnx::unsqueeze``` for the weight
```
weight = g.op(""Unsqueeze"", axes_i=[1, 2])
return g.op(""PRelu"", self, weight)
```
If the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. 

Edit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here
https://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316",200
23060,[ONNX] The shape of PReLU weight is wrong,"> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.

You can try inserting ```onnx::unsqueeze``` for the weight
```
weight = g.op(""Unsqueeze"", axes_i=[1, 2])
return g.op(""PRelu"", self, weight)
```
If the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. 

Edit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here
https://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316",200
23061,Make DDP failure recoverable,"## Trying Solution 2

#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:

1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. 
2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.

I initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.

~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type.",46
23062,Make DDP failure recoverable,"## Trying Solution 2

#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:

1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. 
2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.

I initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.

~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type.",46
23063,"pytorch 1.1.0 fails to load on windows (python3.6, 3.7)","I extracted the build scripts from your log, that is, the code below.
```cmd
set PYTHON=Python36
set ARCH=-x64 
set PYTORCH=1.1.0-cp36-cp36m

git clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\projects\psyneulink-wuxsn
cd C:\projects\psyneulink-wuxsn
git checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d

choco upgrade graphviz.portable -y
pip --version
pip install --user -U pip
pip --version
pip install --user -U certifi ""numpy<1.16""
pip install --user git+https://github.com/benureau/leabra.git@master
if not ""%PYTORCH%"" == """" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl
if ""%PYTORCH%"" == """" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)
pip install --user -e .[dev]
pytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%
curl -X POST -F ""file=@tests_out.xml"" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%
```
And then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.
```cmd
Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python36\lib\site-packages\torch\__init__.py"", line 79, in <module>
    from torch._C import *
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
>>> quit()
```
After that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again.",3076
23064,"pytorch 1.1.0 fails to load on windows (python3.6, 3.7)","I extracted the build scripts from your log, that is, the code below.
```cmd
set PYTHON=Python36
set ARCH=-x64 
set PYTORCH=1.1.0-cp36-cp36m

git clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\projects\psyneulink-wuxsn
cd C:\projects\psyneulink-wuxsn
git checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d

choco upgrade graphviz.portable -y
pip --version
pip install --user -U pip
pip --version
pip install --user -U certifi ""numpy<1.16""
pip install --user git+https://github.com/benureau/leabra.git@master
if not ""%PYTORCH%"" == """" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl
if ""%PYTORCH%"" == """" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)
pip install --user -e .[dev]
pytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%
curl -X POST -F ""file=@tests_out.xml"" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%
```
And then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.
```cmd
Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python36\lib\site-packages\torch\__init__.py"", line 79, in <module>
    from torch._C import *
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
>>> quit()
```
After that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again.",3076
23065,Why module->eval() doesn't work in C++,"nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. ",10284
23066,Why module->eval() doesn't work in C++,"nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. ",10284
23067,Support sublist arguments for torch.einsum,"Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`

One way to do this:
1. implement additional `einsum` in  `ATen/native/Linear.cpp`
```
Tensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {
      // convert lhs_t and rhs_t to eqn
     std::string eqn =  ... ;
     return at::einsum(eqn, tensors);
}
``` 
2. add definition to `native_funcions.yaml`
```
- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor
  use_c10_dispatcher: unboxed_only
```
3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string 

Another:
 
1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. 

I have the following questions :
1. Which approach suits best to PyTorch?
2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.
3. If it should be a cpp version, which types for subscripts arrays are better to use?  ",5909
23068,Support sublist arguments for torch.einsum,"Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`

One way to do this:
1. implement additional `einsum` in  `ATen/native/Linear.cpp`
```
Tensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {
      // convert lhs_t and rhs_t to eqn
     std::string eqn =  ... ;
     return at::einsum(eqn, tensors);
}
``` 
2. add definition to `native_funcions.yaml`
```
- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor
  use_c10_dispatcher: unboxed_only
```
3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string 

Another:
 
1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. 

I have the following questions :
1. Which approach suits best to PyTorch?
2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.
3. If it should be a cpp version, which types for subscripts arrays are better to use?  ",5909
23069,NCCL process groups don't support `.group_ranks()`,"In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.

The `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead.",3757
23070,NCCL process groups don't support `.group_ranks()`,"In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.

The `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead.",3757
23071,JIT RuntimeError: isTensor() ASSERT FAILED,"@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.

A ""quick"" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?",738
23072,JIT RuntimeError: isTensor() ASSERT FAILED,"@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.

A ""quick"" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?",738
23073,MKLDNN convolution leaks memory,"Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. 

A simple workaround in mkldnn src/common/stream.cpp.
https://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp

```
diff --git a/src/common/stream.cpp b/src/common/stream.cpp
index 054fbb9..be11cf0 100644
--- a/src/common/stream.cpp
+++ b/src/common/stream.cpp
@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,

     const size_t start = stream_.size();
     stream_.insert(stream_.end(), prims.begin(), prims.end());
-    return submit_impl(start, stream_.size(), error_prim);
+    auto res = submit_impl(start, stream_.size(), error_prim);
+    stream_.clear();
+    return res;
 }

 bool stream_t::closed() const { return true; }
```

",6833
23074,MKLDNN convolution leaks memory,"Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. 

A simple workaround in mkldnn src/common/stream.cpp.
https://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp

```
diff --git a/src/common/stream.cpp b/src/common/stream.cpp
index 054fbb9..be11cf0 100644
--- a/src/common/stream.cpp
+++ b/src/common/stream.cpp
@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,

     const size_t start = stream_.size();
     stream_.insert(stream_.end(), prims.begin(), prims.end());
-    return submit_impl(start, stream_.size(), error_prim);
+    auto res = submit_impl(start, stream_.size(), error_prim);
+    stream_.clear();
+    return res;
 }

 bool stream_t::closed() const { return true; }
```

",6833
23075,How to disable MKL-DNN 64-bit compilation?,"It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables.",3968
23076,How to disable MKL-DNN 64-bit compilation?,"It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables.",3968
23077,Using Ninja instead of Make results in inability to find correct headers when building from source on Linux,Yes it seems to all work normally with USE_NINJA=OFF so far,8481
23078,Using Ninja instead of Make results in inability to find correct headers when building from source on Linux,Yes it seems to all work normally with USE_NINJA=OFF so far,8481
23079,dist.new_group() failed. BUG or I misunderstood something?,"Hey @lecoan, the doc says:

> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. 

In the code snippet above, you have:

```python
    start = 0
    while rank not in perm[start: start + 2]:
        start += 2
    group = perm.tolist()[start: start + 2]
    pg = dist.new_group(group, timeout=timedelta(seconds=30))
```

So all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. 

https://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486

Can you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.

```python
dis.new_group([0, 1], ...)
dis.new_group([2, 3], ...)
```",2809
23080,dist.new_group() failed. BUG or I misunderstood something?,"Hey @lecoan, the doc says:

> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. 

In the code snippet above, you have:

```python
    start = 0
    while rank not in perm[start: start + 2]:
        start += 2
    group = perm.tolist()[start: start + 2]
    pg = dist.new_group(group, timeout=timedelta(seconds=30))
```

So all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. 

https://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486

Can you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.

```python
dis.new_group([0, 1], ...)
dis.new_group([2, 3], ...)
```",2809
23081,No type hints on nn.Parameter,"In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it.",3735
23082,No type hints on nn.Parameter,"In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it.",3735
23083,[jit] Can't script .type(),Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`,6863
23084,[jit] Can't script .type(),Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`,6863
23085,torch.distributed.gather(): the type of gather_list parameter must be list[Tensor]?,"This works for me:

```python
import torch
import torch.distributed as dist

dist.init_process_group(""gloo"")

tensor = torch.tensor([dist.get_rank()], dtype=torch.int32)
if dist.get_rank() == 0:
    output = [tensor.clone() for _ in range(dist.get_world_size())]
    dist.gather(tensor=tensor, gather_list=output, dst=0)
    print(output)
else:
    dist.gather(tensor=tensor, gather_list=[], dst=0)
```

That said, we can improve this such that the non-dst doesn't have to specify `gather_list`.",7843
23086,torch.distributed.gather(): the type of gather_list parameter must be list[Tensor]?,"This works for me:

```python
import torch
import torch.distributed as dist

dist.init_process_group(""gloo"")

tensor = torch.tensor([dist.get_rank()], dtype=torch.int32)
if dist.get_rank() == 0:
    output = [tensor.clone() for _ in range(dist.get_world_size())]
    dist.gather(tensor=tensor, gather_list=output, dst=0)
    print(output)
else:
    dist.gather(tensor=tensor, gather_list=[], dst=0)
```

That said, we can improve this such that the non-dst doesn't have to specify `gather_list`.",7843
23087,[jit] torch.jit.script range() input type raises isInt() when used with int tensor values,"You can find more info on https://pytorch.org/ with the Quick Start selector (change it to ""Preview (Nightly)"" at the top), but this should work for your env:

```bash
pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html
```
",8578
23088,[jit] torch.jit.script range() input type raises isInt() when used with int tensor values,"You can find more info on https://pytorch.org/ with the Quick Start selector (change it to ""Preview (Nightly)"" at the top), but this should work for your env:

```bash
pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html
```
",8578
23089,RuntimeError: CUDA error: device-side assert triggered - yesterday it worked,"I met the same error, then I set 'device = torch.device(""cpu"")' to run codes on cpu, the error statements were much more clear.
Anyone has a similar error can have a try.",3167
23090,RuntimeError: CUDA error: device-side assert triggered - yesterday it worked,"I met the same error, then I set 'device = torch.device(""cpu"")' to run codes on cpu, the error statements were much more clear.
Anyone has a similar error can have a try.",3167
23091,nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.,"It seems your performances gets a hit by handling denormal values.
Try to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again.",3977
23092,nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.,"It seems your performances gets a hit by handling denormal values.
Try to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again.",3977
23093,interfaces of many schedulers in lr_scheduler.py are missing in lr_scheduler.pyi,"I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help.",3038
23094,interfaces of many schedulers in lr_scheduler.py are missing in lr_scheduler.pyi,"I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help.",3038
23095,Memory leak when using itertools.cycle,"Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:

```python
def cycle(iterable):
    iterator = iter(iterable)
    while True:
        try:
            yield next(iterator)
        except StopIteration:
            iterator = iter(iterable)
```

solves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.

I will close this issue since it's not a pytorch issue.",4597
23096,Memory leak when using itertools.cycle,"Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:

```python
def cycle(iterable):
    iterator = iter(iterable)
    while True:
        try:
            yield next(iterator)
        except StopIteration:
            iterator = iter(iterable)
```

solves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.

I will close this issue since it's not a pytorch issue.",4597
23097,count_nonzero,"We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero.",8274
23098,count_nonzero,"We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero.",8274
23099,New _batch_mahalanobis slower than in previous commit,"Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).

Btw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider
```
import torch
x = torch.eye(2).cuda()
y = torch.eye(2).cuda()
bx = x.reshape(1, 2, 2)
by = y.reshape(1, 2, 2)

%%timeit
torch.cuda.synchronize()
torch.triangular_solve(x, y)

%%timeit
torch.cuda.synchronize()
torch.triangular_solve(bx, by)
```
which will return
```
45.4 µs ± 318 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```
for the first case and
```
243 µs ± 3.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
for the second case.

I guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me.",8508
23100,New _batch_mahalanobis slower than in previous commit,"Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).

Btw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider
```
import torch
x = torch.eye(2).cuda()
y = torch.eye(2).cuda()
bx = x.reshape(1, 2, 2)
by = y.reshape(1, 2, 2)

%%timeit
torch.cuda.synchronize()
torch.triangular_solve(x, y)

%%timeit
torch.cuda.synchronize()
torch.triangular_solve(bx, by)
```
which will return
```
45.4 µs ± 318 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```
for the first case and
```
243 µs ± 3.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
for the second case.

I guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me.",8508
23101,Quantized Linear does not work for bias=False,"The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with ""indirect"" test coverage is to test `convert` and `quantize` function from here:
https://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21

Previously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403",7156
23102,Quantized Linear does not work for bias=False,"The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with ""indirect"" test coverage is to test `convert` and `quantize` function from here:
https://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21

Previously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403",7156
23103,forward_packed operator in LSTM not supported by jit scriptmodule,"I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well

```
RuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)
```

Your example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors.",3259
23104,forward_packed operator in LSTM not supported by jit scriptmodule,"I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well

```
RuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)
```

Your example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors.",3259
23105,Getting gradient of element of tensor wrt the element itself,"```
print(torch.autograd.grad(b[0][0], b[0], allow_unused=True)
```

This doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way.",8718
23106,Getting gradient of element of tensor wrt the element itself,"```
print(torch.autograd.grad(b[0][0], b[0], allow_unused=True)
```

This doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way.",8718
23107,[naming] Promote _LRScheduler to LRScheduler,"Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. ",5930
23108,[naming] Promote _LRScheduler to LRScheduler,"Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. ",5930
23109,[docs] Update and momentum formulas in SGD docs,"IMO, this is a no brainer. I'll put up a PR.

Should it be
`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)",3367
23110,[docs] Update and momentum formulas in SGD docs,"IMO, this is a no brainer. I'll put up a PR.

Should it be
`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)",3367
23111,torch.jit.trace() does not work without check_trace =False,"> hoe to slove it ?
Just don't pass the same tensor 3 times as the same var. See the next my comment.

",265
23112,torch.jit.trace() does not work without check_trace =False,"> hoe to slove it ?
Just don't pass the same tensor 3 times as the same var. See the next my comment.

",265
23113,TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance,"I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu",3205
23114,TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance,"I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu",3205
23115,[JIT] Making JIT work with Pyro's VAE example,"> Do the examples do a lot of operations with scalar tensors?

The arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?",217
23116,[JIT] Making JIT work with Pyro's VAE example,"> Do the examples do a lot of operations with scalar tensors?

The arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?",217
23117,[feature request][pytorch] finfo as in numpy and finfo for default dtype,If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20,3560
23118,[feature request][pytorch] finfo as in numpy and finfo for default dtype,If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20,3560
23119,Performance improvement on sparse CUDA coalesce(),"Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:
```
>>> from random import *
>>> n = 100000
>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])
>>> V = torch.randn(n)
>>> size = torch.Size([1000, 1000, 1000])
>>> S = torch.sparse_coo_tensor(I.t(), V, size)

>>> %timeit S.coalesce()
10 loops, best of 3: 30.7 ms per loop

>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)
>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();
100 loops, best of 3: 9.59 ms per loop
```",1607
23120,Performance improvement on sparse CUDA coalesce(),"Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:
```
>>> from random import *
>>> n = 100000
>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])
>>> V = torch.randn(n)
>>> size = torch.Size([1000, 1000, 1000])
>>> S = torch.sparse_coo_tensor(I.t(), V, size)

>>> %timeit S.coalesce()
10 loops, best of 3: 30.7 ms per loop

>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)
>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();
100 loops, best of 3: 9.59 ms per loop
```",1607
23121,autograd's elu_backward usage seems not correct,"Closed it thinking I made a mistake but actually indeed seems wrong.

This line is on ATen's Type.h:
`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`

`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen",1595
23122,autograd's elu_backward usage seems not correct,"Closed it thinking I made a mistake but actually indeed seems wrong.

This line is on ATen's Type.h:
`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`

`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen",1595
23123,Parameters in dict not registered,I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose.,3011
23124,Parameters in dict not registered,I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose.,3011
23125,Cuda runtime error : the launch timed out and was terminated,Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/,4900
23126,Cuda runtime error : the launch timed out and was terminated,Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/,4900
23127,[Outdated documentation] Previous Versions Installation with Conda,"pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa

PyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123

Closed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa
",10431
23128,[Outdated documentation] Previous Versions Installation with Conda,"pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa

PyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123

Closed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa
",10431
23129,flip a Tensor,"Here's @dmarnerides code but with cuda support

```py
# https://github.com/pytorch/pytorch/issues/229
def flip(x, dim):
    dim = x.dim() + dim if dim < 0 else dim
    inds = tuple(slice(None, None) if i != dim
             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()
             for i in range(x.dim()))
    return x[inds]

# Code to test it with cpu
a = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)
print(a)
print(flip(a, 0)) # Or -4
print(flip(a, 1)) # Or -3
print(flip(a, 2)) # Or -2
print(flip(a, 3)) # Or -1

# Code to test it with cuda
a = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()
print(a)
print(flip(a, 0)) # Or -4
print(flip(a, 1)) # Or -3
print(flip(a, 2)) # Or -2
print(flip(a, 3)) # Or -1
```",2795
23130,flip a Tensor,"Here's @dmarnerides code but with cuda support

```py
# https://github.com/pytorch/pytorch/issues/229
def flip(x, dim):
    dim = x.dim() + dim if dim < 0 else dim
    inds = tuple(slice(None, None) if i != dim
             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()
             for i in range(x.dim()))
    return x[inds]

# Code to test it with cpu
a = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)
print(a)
print(flip(a, 0)) # Or -4
print(flip(a, 1)) # Or -3
print(flip(a, 2)) # Or -2
print(flip(a, 3)) # Or -1

# Code to test it with cuda
a = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()
print(a)
print(flip(a, 0)) # Or -4
print(flip(a, 1)) # Or -3
print(flip(a, 2)) # Or -2
print(flip(a, 3)) # Or -1
```",2795
23131,PyTorch goes distributed,"Shubho here from SVAIL @ Baidu

One long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.

The framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.

Possibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...

Happy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code",5896
23132,PyTorch goes distributed,"Shubho here from SVAIL @ Baidu

One long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.

The framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.

Possibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...

Happy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code",5896
23133,UnboundLocalError when importing torch.cuda,In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used,3753
23134,UnboundLocalError when importing torch.cuda,In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used,3753
23135,"ubuntu 16.04, CUDA8 and pytorch have compile issues, investigate","Hi @soumith 

I had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)

I updated to 8.0.44 (recent version) and it was solved. 

",2846
23136,"ubuntu 16.04, CUDA8 and pytorch have compile issues, investigate","Hi @soumith 

I had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)

I updated to 8.0.44 (recent version) and it was solved. 

",2846
23137,define default GPU device,"Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.

@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations.",8456
23138,define default GPU device,"Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.

@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations.",8456
23139,allow forward / backward hooks to rewrite outputs and gradients,"You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries.",8584
23140,allow forward / backward hooks to rewrite outputs and gradients,"You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries.",8584
23141,auto-wrap tensors as inputs to autograd,"It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.

Another difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)",4007
23142,auto-wrap tensors as inputs to autograd,"It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.

Another difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)",4007
23143,squeeze dimension after mean / sum,"I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. ""Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility"" isn't quite true).

For example, consider test_nn.test_InstanceNorm1d:
```
        output = IN(input_var)
        
        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)
        mean = input_reshaped.mean(1)

        # do some calculation based on mean.data - IN.running_mean
```

Here, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation ""just works"" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption.",3347
23144,squeeze dimension after mean / sum,"I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. ""Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility"" isn't quite true).

For example, consider test_nn.test_InstanceNorm1d:
```
        output = IN(input_var)
        
        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)
        mean = input_reshaped.mean(1)

        # do some calculation based on mean.data - IN.running_mean
```

Here, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation ""just works"" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption.",3347
23145,gradient clip for optimizer,"For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_",2584
23146,gradient clip for optimizer,"For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_",2584
23147,Segmentation fault when dividing by zero with integer tensors,"integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.

Your second issue looks more like a bug.",9685
23148,Segmentation fault when dividing by zero with integer tensors,"integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.

Your second issue looks more like a bug.",9685
23149,Softmax2d and LogSoftmax don't work for 2d and higher,"It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones.",3955
23150,Softmax2d and LogSoftmax don't work for 2d and higher,"It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones.",3955
23151,CUDA initialization fails fatally in multiprocessing,"Seems that multithreading works in this example. I will first try this line and see how it works. 

Thanks all for the help! ",5807
23152,CUDA initialization fails fatally in multiprocessing,"Seems that multithreading works in this example. I will first try this line and see how it works. 

Thanks all for the help! ",5807
23153,cudnn backend needs contiguity checks to report better error messages,"Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.

Another other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.

@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?",4604
23154,cudnn backend needs contiguity checks to report better error messages,"Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.

Another other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.

@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?",4604
23155,"nn.Sequential should have an add_module(module) instead of add_module(name, module)","If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.

If you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`.",3586
23156,"nn.Sequential should have an add_module(module) instead of add_module(name, module)","If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.

If you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`.",3586
23157,"Some function don't implement ""out=result"" convention",`out` has to be a `ByteTensor`.,8736
23158,"Some function don't implement ""out=result"" convention",`out` has to be a `ByteTensor`.,8736
23159,THNN unnecessary .zero() calls on gradients,this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119,10898
23160,THNN unnecessary .zero() calls on gradients,this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119,10898
23161,"binaries compiled against newer numpy, but executed with older numpy error out",upgrade your numpy,11295
23162,"binaries compiled against newer numpy, but executed with older numpy error out",upgrade your numpy,11295
23163,simpler explode and join,"Just tried `torch.stack`, it's great, thanks!

Re #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`).",4039
23164,simpler explode and join,"Just tried `torch.stack`, it's great, thanks!

Re #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`).",4039
23165,How to package pytorch with the file build from source.,Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory.,8080
23166,How to package pytorch with the file build from source.,Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory.,8080
23167,Missing tests for most torch.*(out=...) tensor operators,"I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done.",3230
23168,Missing tests for most torch.*(out=...) tensor operators,"I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done.",3230
23169,BatchSampler & PEP 479,"try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP.",11271
23170,BatchSampler & PEP 479,"try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP.",11271
23171,`CatTransform` should work with `event_dim > 0` transforms,"@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:
1. The concatenation `dim` can be either a batch dim or an event dim.
2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:
    ```py
    logdeghacs = []
    for trans, length in zip(self.transforms, self.lengths):
        ...
        logdetjacs.append(trans.log_abs_det_jacobian(...))
    return torch.cat(logdetjacs, dim=self.dim)
    ```
3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:
    ```py
    ...
    return sum(logdetjacs)
    ```

If this sounds correct, then I think there's a pretty simple fix:

<details>

```diff
diff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py
index 09e00d55e8..eddce21c25 100644
--- a/torch/distributions/transforms.py
+++ b/torch/distributions/transforms.py
@@ -633,6 +633,7 @@ class CatTransform(Transform):
     """"""
     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):
         assert all(isinstance(t, Transform) for t in tseq)
+        assert len({t.event_dim for t in tseq}) == 1
         if cache_size:
             tseq = [t.with_cache(cache_size) for t in tseq]
         super(CatTransform, self).__init__(cache_size=cache_size)
@@ -686,7 +687,15 @@ class CatTransform(Transform):
             yslice = y.narrow(self.dim, start, length)
             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))
             start = start + length  # avoid += for jit compat
-        return torch.cat(logdetjacs, dim=self.dim)
+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())
+        if dim < 0:  # concatenate along a batch_dim
+            return torch.cat(logdetjacs, dim=dim)
+        else:  # concatenate along an event_dim
+            return sum(logdetjacs)
+
+    @property
+    def event_dim(self):
+        return self.transforms[0].event_dim

     @property
     def bijective(self):
```

</details>",728
23172,`CatTransform` should work with `event_dim > 0` transforms,"@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:
1. The concatenation `dim` can be either a batch dim or an event dim.
2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:
    ```py
    logdeghacs = []
    for trans, length in zip(self.transforms, self.lengths):
        ...
        logdetjacs.append(trans.log_abs_det_jacobian(...))
    return torch.cat(logdetjacs, dim=self.dim)
    ```
3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:
    ```py
    ...
    return sum(logdetjacs)
    ```

If this sounds correct, then I think there's a pretty simple fix:

<details>

```diff
diff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py
index 09e00d55e8..eddce21c25 100644
--- a/torch/distributions/transforms.py
+++ b/torch/distributions/transforms.py
@@ -633,6 +633,7 @@ class CatTransform(Transform):
     """"""
     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):
         assert all(isinstance(t, Transform) for t in tseq)
+        assert len({t.event_dim for t in tseq}) == 1
         if cache_size:
             tseq = [t.with_cache(cache_size) for t in tseq]
         super(CatTransform, self).__init__(cache_size=cache_size)
@@ -686,7 +687,15 @@ class CatTransform(Transform):
             yslice = y.narrow(self.dim, start, length)
             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))
             start = start + length  # avoid += for jit compat
-        return torch.cat(logdetjacs, dim=self.dim)
+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())
+        if dim < 0:  # concatenate along a batch_dim
+            return torch.cat(logdetjacs, dim=dim)
+        else:  # concatenate along an event_dim
+            return sum(logdetjacs)
+
+    @property
+    def event_dim(self):
+        return self.transforms[0].event_dim

     @property
     def bijective(self):
```

</details>",728
23173,Build-from-source Failed on Mac OS,"Hi,

This is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?",2877
23174,Build-from-source Failed on Mac OS,"Hi,

This is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?",2877
23175,[Windows CUDA Build] Unknown option '-Xcompiler /w -w',"Seems it's similar but not exactly that issue (and that's already fixed, probably)

Found 2 ways around:
1. Turn off cuda separable compilation.
2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test.",5802
23176,[Windows CUDA Build] Unknown option '-Xcompiler /w -w',"Seems it's similar but not exactly that issue (and that's already fixed, probably)

Found 2 ways around:
1. Turn off cuda separable compilation.
2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test.",5802
23177,Implementation of 2d bicubic grid sampler,"Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input",1147
23178,Implementation of 2d bicubic grid sampler,"Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input",1147
23179,"Something goes wrong with pytorch build from source,","Hi,

We use github issues only for bugs or feature requests.
Please use the forum to ask questions: https://discuss.pytorch.org/

In your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`.",2879
23180,"Something goes wrong with pytorch build from source,","Hi,

We use github issues only for bugs or feature requests.
Please use the forum to ask questions: https://discuss.pytorch.org/

In your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`.",2879
23181,OneCycleLR Scheduler does not have argument for verbose,Fixed by #41580,2456
23182,OneCycleLR Scheduler does not have argument for verbose,Fixed by #41580,2456
23183,rewrite the torch.sparse main doc page,"For me it was super-confusing multitude of different matrix multiply ops:
torch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm

My practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 ",2555
23184,rewrite the torch.sparse main doc page,"For me it was super-confusing multitude of different matrix multiply ops:
torch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm

My practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 ",2555
23185,[feature request] ONNX export for torch.std_mean / torch.var_mean,We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item.,8282
23186,[feature request] ONNX export for torch.std_mean / torch.var_mean,We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item.,8282
23187,[JIT] RuntimeError:  ill formed octal specifier for List[str],Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D,6856
23188,[JIT] RuntimeError:  ill formed octal specifier for List[str],Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D,6856
23189,torch.utils.data.random_split crashes without an error message with non CPU Generator object,"> > @Kae1101
> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.
> > I find the way to make my code run again, hope it will help you
> > add attribute generator to your DataLoader and set it like this:
> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))
> > Hope it gonna help :)))
> 
> @NLQVan
> Thanks for your solution~But I already fixed the problem by commenting out the following command:
> 
> # torch.set_default_tensor_type(torch.cuda.FloatTensor)
> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...
> 
> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.

Thanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. ",197
23190,torch.utils.data.random_split crashes without an error message with non CPU Generator object,"> > @Kae1101
> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.
> > I find the way to make my code run again, hope it will help you
> > add attribute generator to your DataLoader and set it like this:
> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))
> > Hope it gonna help :)))
> 
> @NLQVan
> Thanks for your solution~But I already fixed the problem by commenting out the following command:
> 
> # torch.set_default_tensor_type(torch.cuda.FloatTensor)
> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...
> 
> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.

Thanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. ",197
23191,Memory surges when loading models,"Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to ""load the checkpoint to cpu first and then move onto GPU"" and the note in the torch.load() function! This easily worked for me:
```
# Bad gives OOM on GPU
# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)
# Good. Load to cpu first, then to GPU
params = torch.load(model_save_path, map_location='cpu')
model.load_state_dict(params['state_dict'])
model = model.to(device)
```
Subsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely
```
# Bad. Gives OOM
# optimizer.load_state_dict(torch.load(model_save_path + '.optim')
# Good. Works.
optimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))
optimizer_to(optimizer,device)
```
Here, optimizer_to is the code snippet from @0phoff posted in #8741 
```
def optimizer_to(optim, device):
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)
```
(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)",1521
23192,Memory surges when loading models,"Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to ""load the checkpoint to cpu first and then move onto GPU"" and the note in the torch.load() function! This easily worked for me:
```
# Bad gives OOM on GPU
# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)
# Good. Load to cpu first, then to GPU
params = torch.load(model_save_path, map_location='cpu')
model.load_state_dict(params['state_dict'])
model = model.to(device)
```
Subsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely
```
# Bad. Gives OOM
# optimizer.load_state_dict(torch.load(model_save_path + '.optim')
# Good. Works.
optimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))
optimizer_to(optimizer,device)
```
Here, optimizer_to is the code snippet from @0phoff posted in #8741 
```
def optimizer_to(optim, device):
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)
```
(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)",1521
23193,[feature request] `select_index` for sparse tensors,"Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix.",6882
23194,[feature request] `select_index` for sparse tensors,"Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix.",6882
23195,grad of output with respect to inputs  functions on cpu but not gpu,"Your code had a bug, I fixed it for you:

```
def test_gpu():
    mod = testModule().cuda()
    t = torch.ones([1, 10], requires_grad=True, device=""cuda:0"")
    output = mod(t)
    output[0].backward()
    test = t.grad
```",8670
23196,grad of output with respect to inputs  functions on cpu but not gpu,"Your code had a bug, I fixed it for you:

```
def test_gpu():
    mod = testModule().cuda()
    t = torch.ones([1, 10], requires_grad=True, device=""cuda:0"")
    output = mod(t)
    output[0].backward()
    test = t.grad
```",8670
23197,Caffe2 install failure,"Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. 

1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.
2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from
```c++
#include “driver_types.h”
```
to
```c++
#include <driver_types.h>
```
So that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road.",5818
23198,Caffe2 install failure,"Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. 

1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.
2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from
```c++
#include “driver_types.h”
```
to
```c++
#include <driver_types.h>
```
So that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road.",5818
23199,THD refactoring,Don't think we need this issue from now on.,2197
23200,THD refactoring,Don't think we need this issue from now on.,2197
23201,[Caffe2] Are you still maintaining Caffe2 docker?,"@jgong5 @pjh5
I think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. 
It's inconvenient to build our own Caffe2 while I just want to use Detectron.",612
23202,[Caffe2] Are you still maintaining Caffe2 docker?,"@jgong5 @pjh5
I think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. 
It's inconvenient to build our own Caffe2 while I just want to use Detectron.",612
23203,[Feature Request] nn.Module should also get a `device` attribute,"That’s not possible. Modules can hold parameters of different types on different devices, and so it’s not always possible to unambiguously determine the device.",6902
23204,[Feature Request] nn.Module should also get a `device` attribute,"That’s not possible. Modules can hold parameters of different types on different devices, and so it’s not always possible to unambiguously determine the device.",6902
23205,magma in pytorch,"because we statically link magma, the magma package is not needed at runtime.",8952
23206,magma in pytorch,"because we statically link magma, the magma package is not needed at runtime.",8952
23207,Cannot deepcopy torch.(int/float/...)*,"That's because `numpy.float32` isn't a numpy dtype:
```
type(numpy.float32)
<class 'type'>


>>> type(numpy.array(0).dtype)
<class 'numpy.dtype'>

>>> isinstance(numpy.array(0).dtype, type)
False
```",6898
23208,Cannot deepcopy torch.(int/float/...)*,"That's because `numpy.float32` isn't a numpy dtype:
```
type(numpy.float32)
<class 'type'>


>>> type(numpy.array(0).dtype)
<class 'numpy.dtype'>

>>> isinstance(numpy.array(0).dtype, type)
False
```",6898
23209,Multiprocessing runtime error freeze_support() in Windows 64 bit,Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please.,5102
23210,Multiprocessing runtime error freeze_support() in Windows 64 bit,Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please.,5102
23211,AttributeError: module 'torch.nn' has no attribute 'BCEWithLogitsLoss',"That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!

If you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function.",6887
23212,AttributeError: module 'torch.nn' has no attribute 'BCEWithLogitsLoss',"That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!

If you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function.",6887
23213,[PyTorch] KeyError: 'momentum' while using SGD optimizer,Because I loaded checkpoints from an Adam optimizer.,1418
23214,[PyTorch] KeyError: 'momentum' while using SGD optimizer,Because I loaded checkpoints from an Adam optimizer.,1418
23215,Drop support for magma v1 (compilation with it is broken right now),Remove all the `MAGMA_V2` ifdefs,5144
23216,Drop support for magma v1 (compilation with it is broken right now),Remove all the `MAGMA_V2` ifdefs,5144
23217,Can't get attribute 'Net' on <module '__main__' from 'D:/demo/cnn/test1.py'>,"You should save & load the statedict instead. :)
See https://pytorch.org/docs/master/notes/serialization.html",8656
23218,Can't get attribute 'Net' on <module '__main__' from 'D:/demo/cnn/test1.py'>,"You should save & load the statedict instead. :)
See https://pytorch.org/docs/master/notes/serialization.html",8656
23219,Failure to install caffe2 builded from source,"Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. ",1608
23220,Failure to install caffe2 builded from source,"Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. ",1608
23221,Compiling Pytorch 0.4 from source for Tegra (arm processor) fails,"I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.

@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. ",3332
23222,Compiling Pytorch 0.4 from source for Tegra (arm processor) fails,"I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.

@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. ",3332
23223,"""Parameters of a model after .cuda() will be different objects with those before the call."" is wrong.","> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.
>
> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.

Starting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it.",232
23224,"""Parameters of a model after .cuda() will be different objects with those before the call."" is wrong.","> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.
>
> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.

Starting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it.",232
23225,[PyTorch] [ONNX] Peephole optimizer transpose fusion broken,"Try this on for size https://github.com/pytorch/pytorch/pull/7872
",8021
23226,[PyTorch] [ONNX] Peephole optimizer transpose fusion broken,"Try this on for size https://github.com/pytorch/pytorch/pull/7872
",8021
23227,[PyTorch] torch.stft is slow on cpu compared to numpy,@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`.,506
23228,[PyTorch] torch.stft is slow on cpu compared to numpy,@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`.,506
23229,Cannot import onnx_caffe2.backend,"I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source.",3084
23230,Cannot import onnx_caffe2.backend,"I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source.",3084
23231,Serious perf drop on CPU,"@mingfeima - Thank you for looking into this further and the update! 

I looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.

Generally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or 
 `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.

EDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml

```
/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)
```

It can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt).",643
23232,Serious perf drop on CPU,"@mingfeima - Thank you for looking into this further and the update! 

I looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.

Generally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or 
 `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.

EDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml

```
/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)
```

It can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt).",643
23233,How to implement the internal zero padding for the feature map?,"This works:
```py
>>> import torch.nn.functional as F
>>>
>>> def pad_within(x, stride=2):
...   w = x.new_zeros(stride, stride)
...   w[0, 0] = 1
...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))
...
>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)
>>> x
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]],

         [[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]],

         [[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]]]])
>>> pad_within(x)
tensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],

         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],

         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])

```

you can wrap this in a module to avoid recomputing the weight every time.",7844
23234,How to implement the internal zero padding for the feature map?,"This works:
```py
>>> import torch.nn.functional as F
>>>
>>> def pad_within(x, stride=2):
...   w = x.new_zeros(stride, stride)
...   w[0, 0] = 1
...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))
...
>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)
>>> x
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]],

         [[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]],

         [[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.]]]])
>>> pad_within(x)
tensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],

         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],

         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],
          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])

```

you can wrap this in a module to avoid recomputing the weight every time.",7844
23235,Some confusion about the grad of torch.sign(),You should call `x_binary.retain_grad()`,8652
23236,Some confusion about the grad of torch.sign(),You should call `x_binary.retain_grad()`,8652
23237,[feature request] Official CUDA support for macOS through eGPU,We do support macOS with CUDA GPUs. However atm you have to build from source.,8261
23238,[feature request] Official CUDA support for macOS through eGPU,We do support macOS with CUDA GPUs. However atm you have to build from source.,8261
23239,Pytorch Model Summary,"So something along the lines of:

```
from prettytable import PrettyTable

def count_parameters(model):
    table = PrettyTable([""Modules"", ""Parameters""])
    total_params = 0
    for name, parameter in model.named_parameters():
        if parameter.requires_grad:
            param = parameter.numel()
            table.add_row([name, param])
            total_params+=param
    print(table)
    return f""{total_params:,}""
    
count_parameters(net)
```

which outputs:

```
+---------------------+------------+
|       Modules       | Parameters |
+---------------------+------------+
|  features.0.weight  |    1728    |
|   features.0.bias   |     64     |
|  features.3.weight  |   73728    |
|   features.3.bias   |    128     |
|  features.6.weight  |   294912   |
|   features.6.bias   |    256     |
|  features.8.weight  |   589824   |
|   features.8.bias   |    256     |
|  features.11.weight |  1179648   |
|   features.11.bias  |    512     |
|  features.13.weight |  2359296   |
|   features.13.bias  |    512     |
|  features.16.weight |  2359296   |
|   features.16.bias  |    512     |
|  features.18.weight |  2359296   |
|   features.18.bias  |    512     |
| classifier.0.weight | 102760448  |
|  classifier.0.bias  |    4096    |
| classifier.3.weight |  16777216  |
|  classifier.3.bias  |    4096    |
| classifier.6.weight |  4096000   |
|  classifier.6.bias  |    1000    |
+---------------------+------------+
```

could be done.

Well, this doesn't give shape after each layer.",5946
23240,Pytorch Model Summary,"So something along the lines of:

```
from prettytable import PrettyTable

def count_parameters(model):
    table = PrettyTable([""Modules"", ""Parameters""])
    total_params = 0
    for name, parameter in model.named_parameters():
        if parameter.requires_grad:
            param = parameter.numel()
            table.add_row([name, param])
            total_params+=param
    print(table)
    return f""{total_params:,}""
    
count_parameters(net)
```

which outputs:

```
+---------------------+------------+
|       Modules       | Parameters |
+---------------------+------------+
|  features.0.weight  |    1728    |
|   features.0.bias   |     64     |
|  features.3.weight  |   73728    |
|   features.3.bias   |    128     |
|  features.6.weight  |   294912   |
|   features.6.bias   |    256     |
|  features.8.weight  |   589824   |
|   features.8.bias   |    256     |
|  features.11.weight |  1179648   |
|   features.11.bias  |    512     |
|  features.13.weight |  2359296   |
|   features.13.bias  |    512     |
|  features.16.weight |  2359296   |
|   features.16.bias  |    512     |
|  features.18.weight |  2359296   |
|   features.18.bias  |    512     |
| classifier.0.weight | 102760448  |
|  classifier.0.bias  |    4096    |
| classifier.3.weight |  16777216  |
|  classifier.3.bias  |    4096    |
| classifier.6.weight |  4096000   |
|  classifier.6.bias  |    1000    |
+---------------------+------------+
```

could be done.

Well, this doesn't give shape after each layer.",5946
23241,register_full_backward_hook does not consistently fire,"The temporary fix you can use is to make the input require gradients.

",7320
23242,register_full_backward_hook does not consistently fire,"The temporary fix you can use is to make the input require gradients.

",7320
23243,[docs] Rendering type hint issues at torchvision,"Hello @vadimkantorov 
This was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4
Documentations look are fine now over the webpages for master as well as stable!

@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!

Feel free to close this as it is fixed!",2766
23244,[docs] Rendering type hint issues at torchvision,"Hello @vadimkantorov 
This was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4
Documentations look are fine now over the webpages for master as well as stable!

@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!

Feel free to close this as it is fixed!",2766
23245,Fail to LOADING A TORCHSCRIPT MODEL IN C++,"> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.

Thank you very much! I have succeed to build the example use the libtorch: 
 downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`.",220
23246,Fail to LOADING A TORCHSCRIPT MODEL IN C++,"> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.

Thank you very much! I have succeed to build the example use the libtorch: 
 downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`.",220
23247,Create a context manager to enable InferenceMode in python frontend,You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative.,8558
23248,Create a context manager to enable InferenceMode in python frontend,You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative.,8558
23249,Cleaner mechanism in the source-code to check if multiple tensors are on the same device,"We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. ",8288
23250,Cleaner mechanism in the source-code to check if multiple tensors are on the same device,"We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. ",8288
23251,Incorrect example output in sparse_csr_tensor doc-string,We would definitely accept PRs correcting examples to reflect the current version of PyTorch!,8304
23252,Incorrect example output in sparse_csr_tensor doc-string,We would definitely accept PRs correcting examples to reflect the current version of PyTorch!,8304
23253,sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors,This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010,7706
23254,sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors,This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010,7706
23255,default_pg_timeout in torch/testing/_internal/distributed/distributed_test.py is not sufficient to change system-wide timeouts,"@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. 

You can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83",580
23256,default_pg_timeout in torch/testing/_internal/distributed/distributed_test.py is not sufficient to change system-wide timeouts,"@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. 

You can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83",580
23257,quick-check didn't capture error until PR landed,"Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.

You can also see this by comparing the PR diff...

- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36

... to the commit that landed to `master`:

- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36",4028
23258,quick-check didn't capture error until PR landed,"Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.

You can also see this by comparing the PR diff...

- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36

... to the commit that landed to `master`:

- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36",4028
23259,Element-wise max of two Tensors computes the wrong gradient in case of equality,"Looks like the [`torch.amax()`]() docs are aware of this behavior:
```
amax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.
```",4148
23260,Element-wise max of two Tensors computes the wrong gradient in case of equality,"Looks like the [`torch.amax()`]() docs are aware of this behavior:
```
amax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.
```",4148
23261,[NNC] Vectorization caused wrong results,Fixed by #59423,2458
23262,[NNC] Vectorization caused wrong results,Fixed by #59423,2458
23263,torch.gather behavior changed from 1.5.1 to master,Fixed by #41672.,2457
23264,torch.gather behavior changed from 1.5.1 to master,Fixed by #41672.,2457
23265,Is STFT in torchaudio consistent with librosa?,"As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)

`istft` was originally `torchaudio`, but recently moved to `pytorch`. 

See https://github.com/pytorch/pytorch/issues/3775 for the development around FT.

We check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.

@LordOfLuck

> consistent with librosa implementations?

If you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`).",1299
23266,Is STFT in torchaudio consistent with librosa?,"As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)

`istft` was originally `torchaudio`, but recently moved to `pytorch`. 

See https://github.com/pytorch/pytorch/issues/3775 for the development around FT.

We check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.

@LordOfLuck

> consistent with librosa implementations?

If you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`).",1299
23267,quantization.fuse_modules fails with Conv1d and BatchNorm1d,Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it.,2773
23268,quantization.fuse_modules fails with Conv1d and BatchNorm1d,Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it.,2773
23269,weird bug of torchscript: it thinks my python bool is a tensor but it's not,"I think this is working as intended (other than the confusing error expr range issue).

EMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.

With that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.

@suo, any better suggestions?",3264
23270,weird bug of torchscript: it thinks my python bool is a tensor but it's not,"I think this is working as intended (other than the confusing error expr range issue).

EMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.

With that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.

@suo, any better suggestions?",3264
23271,Add SpectralOps CPU implementation for ARM/PowerPC processors (where MKL is not available),"Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:
```
import torchaudio
import torch

waveform, sample_rate = torchaudio.load('test.wav')
waveform = waveform.to(""cuda:0"")

spectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(""cuda:0"")(waveform)
```",6816
23272,Add SpectralOps CPU implementation for ARM/PowerPC processors (where MKL is not available),"Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:
```
import torchaudio
import torch

waveform, sample_rate = torchaudio.load('test.wav')
waveform = waveform.to(""cuda:0"")

spectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(""cuda:0"")(waveform)
```",6816
23273,cudnn8 version check fails,"These lines has already been changed to
```cmake
  # Get cuDNN version
  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)
    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)
  else()
    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)
  endif()
```
please update to latest master branch",7438
23274,cudnn8 version check fails,"These lines has already been changed to
```cmake
  # Get cuDNN version
  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)
    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)
  else()
    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)
  endif()
```
please update to latest master branch",7438
23275,AssertionError: Torch not compiled with CUDA enabled - DETECTRON CPU/LINUX TRAINING ERROR,"@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.

If you find that that is not the case, feel free to reopen this issue.",711
23276,AssertionError: Torch not compiled with CUDA enabled - DETECTRON CPU/LINUX TRAINING ERROR,"@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.

If you find that that is not the case, feel free to reopen this issue.",711
23277,torch.distributed and RPC cannot both be initialized with the same host:port pair.,"@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. 

I recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. 

There are different options to implement this behavior.

Option 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. 
Option 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. 

",587
23278,torch.distributed and RPC cannot both be initialized with the same host:port pair.,"@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. 

I recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. 

There are different options to implement this behavior.

Option 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. 
Option 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. 

",587
23279,whitelist keyword to quantization.prepare is implemented incorrectly,the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576,10848
23280,whitelist keyword to quantization.prepare is implemented incorrectly,the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576,10848
23281,Feature request: torch.isclose should set default atol and rtol based on the dtype of the tensors it's given,"Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). ",1413
23282,Feature request: torch.isclose should set default atol and rtol based on the dtype of the tensors it's given,"Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). ",1413
23283,torch.nn.functional.grid_sample segfaults on large inputs,"@erdmann Thank you for your perspective, it's fascinating! 
It's true that as resolution goes up, images become bigger. But in practical terms 
1) GPU memory is usually not big enough to hold multiple large tensors 
2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. 
That said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. ",576
23284,torch.nn.functional.grid_sample segfaults on large inputs,"@erdmann Thank you for your perspective, it's fascinating! 
It's true that as resolution goes up, images become bigger. But in practical terms 
1) GPU memory is usually not big enough to hold multiple large tensors 
2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. 
That said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. ",576
23285,CUDA not found when using latest pre-built version (Libtorch 1.5.1  - CUDA 10.1 - Cudnn7.6.4 - VS2017),Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`.,4844
23286,CUDA not found when using latest pre-built version (Libtorch 1.5.1  - CUDA 10.1 - Cudnn7.6.4 - VS2017),Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`.,4844
23287,Parameter `timeout` in torch.distributed.init_process_group cannot work,"@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. 

For your need, do you want to try to use torchelastic for failure handling?",740
23288,Parameter `timeout` in torch.distributed.init_process_group cannot work,"@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. 

For your need, do you want to try to use torchelastic for failure handling?",740
23289,Provide a convenient way for the user to reset the grad,"Having both zero_grad() and reset_grad() might be kind of confusing to the end users. 

Are zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().

If the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()).",2763
23290,Provide a convenient way for the user to reset the grad,"Having both zero_grad() and reset_grad() might be kind of confusing to the end users. 

Are zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().

If the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()).",2763
23291,Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc,Simply replacing the names does not help. Assistance is needed.,5921
23292,Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc,Simply replacing the names does not help. Assistance is needed.,5921
23293,Behavior of torch.mean (and std) compared to numpy mean (std),"A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116
But it needs someone to complete it.",778
23294,Behavior of torch.mean (and std) compared to numpy mean (std),"A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116
But it needs someone to complete it.",778
23295,Easy way of creating your own custom cuda kernels,we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings,11355
23296,Easy way of creating your own custom cuda kernels,we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings,11355
23297,HalfTensor Training Needs non-Stateless Method in F.Linear,If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap.,3429
23298,HalfTensor Training Needs non-Stateless Method in F.Linear,If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap.,3429
23299,"Just one cpu core in use, until I use numpy...","Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.

Intel's documentation:
https://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic

I came across this solution in this issue:
https://github.com/ContinuumIO/mkl-service/issues/2
",5975
23300,"Just one cpu core in use, until I use numpy...","Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.

Intel's documentation:
https://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic

I came across this solution in this issue:
https://github.com/ContinuumIO/mkl-service/issues/2
",5975
23301,Support Caffe2 export/pure C(++) inference mode,"If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.

Our main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted.",3378
23302,Support Caffe2 export/pure C(++) inference mode,"If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.

Our main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted.",3378
23303,GPU torch.multinomial produces an out-of-bounds index,"In case this is helpful to anyone, a possible temporary workaround is to use
```
_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)
```
where `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).",3706
23304,GPU torch.multinomial produces an out-of-bounds index,"In case this is helpful to anyone, a possible temporary workaround is to use
```
_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)
```
where `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).",3706
23305,Flexible state dict loading for model (or optimizer),"If you have partial state_dict, which is missing some keys you can do the following:

```python
state = model.state_dict()
state.update(partial)
model.load_state_dict(state)
```",3565
23306,Flexible state dict loading for model (or optimizer),"If you have partial state_dict, which is missing some keys you can do the following:

```python
state = model.state_dict()
state.update(partial)
model.load_state_dict(state)
```",3565
23307,Support int16 numpy conversions,The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment.,7047
23308,Support int16 numpy conversions,The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment.,7047
23309,The command '/bin/sh -c' returned a non-zero code: 2 during docker image,"As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\` when necessary to avoid problem.
Your sample:
```
RUN
chmod +x ~/miniconda.sh && 
~/miniconda.sh -b -p /opt/conda && \
rm ~/miniconda.sh && 
/opt/conda/bin/conda install conda-build && 
/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& 
/opt/conda/bin/conda clean -ya
```
should be
```
RUN chmod +x ~/miniconda.sh && \
~/miniconda.sh -b -p /opt/conda && \
rm ~/miniconda.sh && \
/opt/conda/bin/conda install conda-build && \
/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \
/opt/conda/bin/conda clean -ya
```",1319
23310,The command '/bin/sh -c' returned a non-zero code: 2 during docker image,"As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\` when necessary to avoid problem.
Your sample:
```
RUN
chmod +x ~/miniconda.sh && 
~/miniconda.sh -b -p /opt/conda && \
rm ~/miniconda.sh && 
/opt/conda/bin/conda install conda-build && 
/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& 
/opt/conda/bin/conda clean -ya
```
should be
```
RUN chmod +x ~/miniconda.sh && \
~/miniconda.sh -b -p /opt/conda && \
rm ~/miniconda.sh && \
/opt/conda/bin/conda install conda-build && \
/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \
/opt/conda/bin/conda clean -ya
```",1319
23311,"old lua torch model to pytorch, nn.JoinTable(1,3) error","Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input.",1423
23312,"old lua torch model to pytorch, nn.JoinTable(1,3) error","Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input.",1423
23313,torch ModuleNotFoundError in ipython,"1. try to uninstall your installed pytorch.
2. conda install -c peterjc123 pytorch-cpu
3. on your conda type python
4. import torch ",100
23314,torch ModuleNotFoundError in ipython,"1. try to uninstall your installed pytorch.
2. conda install -c peterjc123 pytorch-cpu
3. on your conda type python
4. import torch ",100
23315,Device memory not released,"I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.

Run:
`killall python` (or `killall python3`)",3362
23316,Device memory not released,"I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.

Run:
`killall python` (or `killall python3`)",3362
23317,Manually unrolling cuDNN RNN OOM,"First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:
```
torch.backends.cudnn.enabled=False
```",2452
23318,Manually unrolling cuDNN RNN OOM,"First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:
```
torch.backends.cudnn.enabled=False
```",2452
23319,padding for nn.AvgPool3d?,"there isn't a particular reason, we haven't implemented it yet in our C backend.",10881
23320,padding for nn.AvgPool3d?,"there isn't a particular reason, we haven't implemented it yet in our C backend.",10881
23321,Compiling error of gloo when installing pytorch from source,"I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions.",3148
23322,Compiling error of gloo when installing pytorch from source,"I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions.",3148
23323,RuntimeError: DataLoader worker (pid 23616) is killed by signal: Terminated.,"I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. ",3189
23324,RuntimeError: DataLoader worker (pid 23616) is killed by signal: Terminated.,"I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. ",3189
23325,from torch._C import *  (ImportError: DLL load failed: The specified module could not be found.,"I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.
  ",3124
23326,from torch._C import *  (ImportError: DLL load failed: The specified module could not be found.,"I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.
  ",3124
23327,nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3,"Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.

https://arxiv.org/abs/1502.03167",4106
23328,nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3,"Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.

https://arxiv.org/abs/1502.03167",4106
23329,NVIDIA driver too old error,"I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. 
I followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit",3200
23330,NVIDIA driver too old error,"I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. 
I followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit",3200
23331,Feature request: torch.bincount,"@sunshineatnoon, how about:
```python
>>> import torch
>>> help(torch.bincount)
```
```rst
Help on built-in function bincount:

bincount(...)
    bincount(self, weights=None, minlength=0) -> Tensor

    Count the frequency of each value in an array of non-negative ints.

    The number of bins (size 1) is one larger than the largest value in
    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least
    :attr:`minlength`. If ``n`` is the value at position ``i``,
    :math:`out[n] += weights[i]` if :attr:`weights` is specified else
    :math:`out[n] += 1`.

    Arguments:
        input (Tensor): 1-d int tensor
        weights (Tensor): optional, weight for each value in the input tensor.
            Should be of same size as input tensor.
        minlength (int): optional, min number of bins. Should be non-negative.

    Shape:
        output (Tensor): ``Size([max(input) + 1])``

    Example::

        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)
        >>> weights = torch.linspace(0, 1, steps=5)
        >>> input, weights
        (tensor([4, 3, 6, 3, 4]),
         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

        >>> torch.bincount(input)
        tensor([0, 0, 0, 2, 2, 0, 1])

        >>> input.bincount(weights)
        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
```",709
23332,Feature request: torch.bincount,"@sunshineatnoon, how about:
```python
>>> import torch
>>> help(torch.bincount)
```
```rst
Help on built-in function bincount:

bincount(...)
    bincount(self, weights=None, minlength=0) -> Tensor

    Count the frequency of each value in an array of non-negative ints.

    The number of bins (size 1) is one larger than the largest value in
    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least
    :attr:`minlength`. If ``n`` is the value at position ``i``,
    :math:`out[n] += weights[i]` if :attr:`weights` is specified else
    :math:`out[n] += 1`.

    Arguments:
        input (Tensor): 1-d int tensor
        weights (Tensor): optional, weight for each value in the input tensor.
            Should be of same size as input tensor.
        minlength (int): optional, min number of bins. Should be non-negative.

    Shape:
        output (Tensor): ``Size([max(input) + 1])``

    Example::

        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)
        >>> weights = torch.linspace(0, 1, steps=5)
        >>> input, weights
        (tensor([4, 3, 6, 3, 4]),
         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

        >>> torch.bincount(input)
        tensor([0, 0, 0, 2, 2, 0, 1])

        >>> input.bincount(weights)
        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
```",709
23333,[feature request] Weight norm option for RNN cells,"You can already achieve it now. For instance,
```
>>> rnn = nn.RNN(10,10,2)  # build an RNN
>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want
```",8557
23334,[feature request] Weight norm option for RNN cells,"You can already achieve it now. For instance,
```
>>> rnn = nn.RNN(10,10,2)  # build an RNN
>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want
```",8557
23335,[Feature request] PackedSequence with length = 0,"I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses.",3059
23336,[Feature request] PackedSequence with length = 0,"I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses.",3059
23337,Cannot install pytorch cuda 8.0 using conda,"do you already have the cuda90 feature installed, for some reason...

Try first doing:

`conda uninstall cuda90`",9192
23338,Cannot install pytorch cuda 8.0 using conda,"do you already have the cuda90 feature installed, for some reason...

Try first doing:

`conda uninstall cuda90`",9192
23339,Warning on infinite acos gradients?,"A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that.",811
23340,Warning on infinite acos gradients?,"A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that.",811
23341,Autogradpp issue masterthread,"Regarding `tensor.max()` I think a better way would be to auto-generate ""named-tuples"" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)",5122
23342,Autogradpp issue masterthread,"Regarding `tensor.max()` I think a better way would be to auto-generate ""named-tuples"" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)",5122
23343,`torch.normal` accepts Variables but does not propagate gradients,"if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`",9582
23344,`torch.normal` accepts Variables but does not propagate gradients,"if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`",9582
23345,`torch.jit.freeze`'d models cannot be moved to GPU with `.to()`,"@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.

```
    def forward(self, x):
       if self.twos.device.is_cuda():
              ....
```
Models might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. 

Can I ask what the specific use case is ?


",496
23346,`torch.jit.freeze`'d models cannot be moved to GPU with `.to()`,"@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.

```
    def forward(self, x):
       if self.twos.device.is_cuda():
              ....
```
Models might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. 

Can I ask what the specific use case is ?


",496
23347,gradgradcheck fails if the function does not depend on the input,"Still fails.

    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1212, in gradgradcheck
        return gradcheck(
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1088, in gradcheck
        return _gradcheck_helper(**args)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1096, in _gradcheck_helper
        func_out = func(*tupled_inputs)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1209, in new_func
        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 226, in grad
        return Variable._execution_engine.run_backward(
    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

The problem is this line:
https://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209

Probably the fix is as simple as put `allow_unused=True`",6077
23348,gradgradcheck fails if the function does not depend on the input,"Still fails.

    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1212, in gradgradcheck
        return gradcheck(
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1088, in gradcheck
        return _gradcheck_helper(**args)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1096, in _gradcheck_helper
        func_out = func(*tupled_inputs)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py"", line 1209, in new_func
        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)
      File ""/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 226, in grad
        return Variable._execution_engine.run_backward(
    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

The problem is this line:
https://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209

Probably the fix is as simple as put `allow_unused=True`",6077
23349,"With & without -O2 compilation optimization level, AVX512 Complex multiplication & division results aren't equal","@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.

So, the issue was just that the testing for AVX512 was being done against the default implementation? 🤣 
Thanks to you, all tests pass now! 😄 ",681
23350,"With & without -O2 compilation optimization level, AVX512 Complex multiplication & division results aren't equal","@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.

So, the issue was just that the testing for AVX512 was being done against the default implementation? 🤣 
Thanks to you, all tests pass now! 😄 ",681
23351,Linking statically with CUPTI using gold linker disrupts exception handling,"The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch ""correctly"" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)

To me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)

But yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.
Just from my side I'm happy if USE_CUPTI_SO is now working.",6911
23352,Linking statically with CUPTI using gold linker disrupts exception handling,"The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch ""correctly"" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)

To me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)

But yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.
Just from my side I'm happy if USE_CUPTI_SO is now working.",6911
23353,Retiring ONNX Optimizer,Discussed this issue during triage review: let's disable optimizer support as it is no longer supported.,2159
23354,Retiring ONNX Optimizer,Discussed this issue during triage review: let's disable optimizer support as it is no longer supported.,2159
23355,Cannot pass script remote module object to over the RPC,"@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. ",508
23356,Cannot pass script remote module object to over the RPC,"@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. ",508
23357,torch.cat should not do type promotion when one input is empty tensor,"NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:
```
>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype
dtype('float64')
```",4549
23358,torch.cat should not do type promotion when one input is empty tensor,"NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:
```
>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype
dtype('float64')
```",4549
23359,A bug on the document interpreter in `torch.autograd.profiler` help page,Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages,2469
23360,A bug on the document interpreter in `torch.autograd.profiler` help page,Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages,2469
23361,`nan_to_num` produces incorrect output for `BFloat16` on CUDA,"Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. ",4605
23362,`nan_to_num` produces incorrect output for `BFloat16` on CUDA,"Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. ",4605
23363,torch.cat fails with torch.jit.script and torch.cuda.amp.autocast,"At a wild guess: this patch should fix it:

```
diff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp
index 7d85211e9c..7809b7d323 100644
--- a/aten/src/ATen/autocast_mode.cpp
+++ b/aten/src/ATen/autocast_mode.cpp
@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {
   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), ""tensordot"", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)
   KERNEL_UNBOXED_ONLY(ADD_NS(dot), ""dot"", Tensor (const Tensor &, const Tensor &), promote)
   KERNEL(ADD_NS(equal), ""equal"", bool (const Tensor &, const Tensor &), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""cat"", Tensor (TensorList, int64_t), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""cat.names"", Tensor (TensorList, Dimname), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), ""_cat"", Tensor (TensorList, int64_t), promote)
+  KERNEL(ADD_NS(cat), ""cat"", Tensor (TensorList, int64_t), promote)
+  KERNEL(ADD_NS(cat), ""cat.names"", Tensor (TensorList, Dimname), promote)
+  KERNEL(ADD_NS(_cat), ""_cat"", Tensor (TensorList, int64_t), promote)
   KERNEL_UNBOXED_ONLY(ADD_NS(stack), ""stack"", Tensor (TensorList, int64_t), promote)
 
   m.impl_UNBOXED(""binary_cross_entropy"", &at::autocast::binary_cross_entropy_banned);
```

@smessmer not sure if we should just rush ""get rid of unboxed only"" or start adding some checks to reject incorrect invocations of unboxed only.",1344
23364,torch.cat fails with torch.jit.script and torch.cuda.amp.autocast,"At a wild guess: this patch should fix it:

```
diff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp
index 7d85211e9c..7809b7d323 100644
--- a/aten/src/ATen/autocast_mode.cpp
+++ b/aten/src/ATen/autocast_mode.cpp
@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {
   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), ""tensordot"", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)
   KERNEL_UNBOXED_ONLY(ADD_NS(dot), ""dot"", Tensor (const Tensor &, const Tensor &), promote)
   KERNEL(ADD_NS(equal), ""equal"", bool (const Tensor &, const Tensor &), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""cat"", Tensor (TensorList, int64_t), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""cat.names"", Tensor (TensorList, Dimname), promote)
-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), ""_cat"", Tensor (TensorList, int64_t), promote)
+  KERNEL(ADD_NS(cat), ""cat"", Tensor (TensorList, int64_t), promote)
+  KERNEL(ADD_NS(cat), ""cat.names"", Tensor (TensorList, Dimname), promote)
+  KERNEL(ADD_NS(_cat), ""_cat"", Tensor (TensorList, int64_t), promote)
   KERNEL_UNBOXED_ONLY(ADD_NS(stack), ""stack"", Tensor (TensorList, int64_t), promote)
 
   m.impl_UNBOXED(""binary_cross_entropy"", &at::autocast::binary_cross_entropy_banned);
```

@smessmer not sure if we should just rush ""get rid of unboxed only"" or start adding some checks to reject incorrect invocations of unboxed only.",1344
23365,[JIT] lack of type support in tensor indexing.,"Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. ",2827
23366,[JIT] lack of type support in tensor indexing.,"Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. ",2827
23367,One PyTorch Upsample op balloons into over 20 ONNX operations,"This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.
The default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362",7642
23368,One PyTorch Upsample op balloons into over 20 ONNX operations,"This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.
The default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362",7642
23369,Unrecognized attribute: min for operator Clip,seems fixed and can't repro with `pytorch 1.6`,10659
23370,Unrecognized attribute: min for operator Clip,seems fixed and can't repro with `pytorch 1.6`,10659
23371,``ToTensor()`` Exception for ``num_workers`` in ``DataLoader`` when ``torch.set_default_tensor_type(torch.cuda.FloatTensor)``,"the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. ",10877
23372,``ToTensor()`` Exception for ``num_workers`` in ``DataLoader`` when ``torch.set_default_tensor_type(torch.cuda.FloatTensor)``,"the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. ",10877
23373,`verbose` unused in `torch.backends.cudnn`,I can fix it by removing this parameter.,3021
23374,`verbose` unused in `torch.backends.cudnn`,I can fix it by removing this parameter.,3021
23375,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?,"Hi,

This happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.
We fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.

You should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.
Something like that should work.
```
for step in range(10000):
    artist_paintings = artist_works()  # real painting from artist
    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas
    G_paintings = G(G_ideas)  # fake painting from G (random ideas)

    prob_artist1 = D(G_paintings)  # G tries to fool D

    G_loss = torch.mean(torch.log(1. - prob_artist1))
    opt_G.zero_grad()
    G_loss.backward()
    opt_G.step()

    prob_artist0 = D(artist_paintings)  # D try to increase this prob
    # detach here to make sure we don't backprop in G that was already changed.
    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob

    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))
    opt_D.zero_grad()
    D_loss.backward(retain_graph=True)  # reusing computational graph
    opt_D.step()
```

In the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/
We keep github issues for bug and features only. And more people look at the forum so you will get a faster answer.",2875
23376,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?,"Hi,

This happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.
We fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.

You should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.
Something like that should work.
```
for step in range(10000):
    artist_paintings = artist_works()  # real painting from artist
    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas
    G_paintings = G(G_ideas)  # fake painting from G (random ideas)

    prob_artist1 = D(G_paintings)  # G tries to fool D

    G_loss = torch.mean(torch.log(1. - prob_artist1))
    opt_G.zero_grad()
    G_loss.backward()
    opt_G.step()

    prob_artist0 = D(artist_paintings)  # D try to increase this prob
    # detach here to make sure we don't backprop in G that was already changed.
    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob

    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))
    opt_D.zero_grad()
    D_loss.backward(retain_graph=True)  # reusing computational graph
    opt_D.step()
```

In the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/
We keep github issues for bug and features only. And more people look at the forum so you will get a faster answer.",2875
23377,[JIT] Expected integer literal for index:,"@yangsenius changing the forward to look like this:
def forward(self, x: List[torch.Tensor]):
fixed that error for me. Now, I am stuck on this:
```
Expected a default value of type Tensor on parameter ""mask"".:
  File ""/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py"", line 152
    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        """"""
        ~~~
        Args:
        ~~~~~
            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                out_height, out_width]): offsets to be applied for each position in the
                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                convolution kernel.
                ~~~~~~~~~~~~~~~~~~~
            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                out_height, out_width]): masks to be applied for each position in the
                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                convolution kernel.
                ~~~~~~~~~~~~~~~~~~~
        """"""
        ~~~
        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             padding=self.padding, dilation=self.dilation, mask=mask)
                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

Note: I didn't need to change anything to trace but I got size issues.
This is all to fix scripting.",743
23378,[JIT] Expected integer literal for index:,"@yangsenius changing the forward to look like this:
def forward(self, x: List[torch.Tensor]):
fixed that error for me. Now, I am stuck on this:
```
Expected a default value of type Tensor on parameter ""mask"".:
  File ""/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py"", line 152
    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        """"""
        ~~~
        Args:
        ~~~~~
            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                out_height, out_width]): offsets to be applied for each position in the
                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                convolution kernel.
                ~~~~~~~~~~~~~~~~~~~
            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                out_height, out_width]): masks to be applied for each position in the
                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                convolution kernel.
                ~~~~~~~~~~~~~~~~~~~
        """"""
        ~~~
        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             padding=self.padding, dilation=self.dilation, mask=mask)
                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

Note: I didn't need to change anything to trace but I got size issues.
This is all to fix scripting.",743
23379,[FR] Add space as delimiter in TORCH_CHECK and other macros,"Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. ",991
23380,[FR] Add space as delimiter in TORCH_CHECK and other macros,"Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. ",991
23381,torch.distributed support on MacOS is missing,"For Windows support, please check this RFC (#42095)

Hey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:

0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies
1. then conda install libuv and pkg-config
2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`",2484
23382,torch.distributed support on MacOS is missing,"For Windows support, please check this RFC (#42095)

Hey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:

0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies
1. then conda install libuv and pkg-config
2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`",2484
23383,"Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives ""Getting ""module 'torch._C' has no attribute 'BoolStorageBase'"" with PyTorch 1.1""",This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state,7802
23384,"Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives ""Getting ""module 'torch._C' has no attribute 'BoolStorageBase'"" with PyTorch 1.1""",This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state,7802
23385,DLL error on Windows 10,"Let me conclude it for you:
1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk
2. Execute these commands:
```powershell
gflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps
cdb ${YOUR COMMAND} # e.g. python -c ""import torch""
# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.
gflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps
```",4094
23386,DLL error on Windows 10,"Let me conclude it for you:
1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk
2. Execute these commands:
```powershell
gflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps
cdb ${YOUR COMMAND} # e.g. python -c ""import torch""
# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.
gflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps
```",4094
23387,Non blocking tensor copy to GPU not working from torch 1.0,"You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.

non_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.

Anyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:

```python
import time
import torch

DELAY = 100000000 
x = torch.randn((1024, 1024), pin_memory=True)

torch.cuda.synchronize()
start = time.time()
torch.cuda._sleep(DELAY)
x.cuda(non_blocking=True)
end = time.time()

print('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU

torch.cuda.synchronize()
start = time.time()
torch.cuda._sleep(DELAY)
x.cuda(non_blocking=False)
end = time.time()


print('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU
```",8549
23388,Non blocking tensor copy to GPU not working from torch 1.0,"You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.

non_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.

Anyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:

```python
import time
import torch

DELAY = 100000000 
x = torch.randn((1024, 1024), pin_memory=True)

torch.cuda.synchronize()
start = time.time()
torch.cuda._sleep(DELAY)
x.cuda(non_blocking=True)
end = time.time()

print('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU

torch.cuda.synchronize()
start = time.time()
torch.cuda._sleep(DELAY)
x.cuda(non_blocking=False)
end = time.time()


print('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU
```",8549
23389,The result of  gloo all_gather error,"@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. 

Thanks for reporting, I will add a fix for it. ",679
23390,The result of  gloo all_gather error,"@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. 

Thanks for reporting, I will add a fix for it. ",679
23391,Error downloading MNIST dataset,"I solved my own problem too...
there was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...
I then built and installed torchvision 0.9.0 from source and it works correctly",3202
23392,Error downloading MNIST dataset,"I solved my own problem too...
there was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...
I then built and installed torchvision 0.9.0 from source and it works correctly",3202
23393,"""derivative for _thnn_fused_lstm_cell_backward is not implemented"" while using GPU","Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. 
The easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`.",1154
23394,"""derivative for _thnn_fused_lstm_cell_backward is not implemented"" while using GPU","Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. 
The easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`.",1154
23395,Implementing GELU activation,"This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.

```py
def gelu(x):
  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))
```",7649
23396,Implementing GELU activation,"This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.

```py
def gelu(x):
  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))
```",7649
23397,tensorboard not updating,"My current workaround:

while true; do
        timeout -sHUP 1m tensorboard --logdir=runs;
done",4288
23398,tensorboard not updating,"My current workaround:

while true; do
        timeout -sHUP 1m tensorboard --logdir=runs;
done",4288
23399,RuntimeError: Creating MTGP constants failed,This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue.,7670
23400,RuntimeError: Creating MTGP constants failed,This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue.,7670
23401,Install only a specific version via pip,"this issue can now be closed, because we use local version identifiers. 
For example, CPU-only version is:
```
pip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
```

So, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.

We dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)",10903
23402,Install only a specific version via pip,"this issue can now be closed, because we use local version identifiers. 
For example, CPU-only version is:
```
pip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
```

So, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.

We dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)",10903
23403,MobileNetV2 export to ONNX fails,"look into the mobilenet.py in torchvision, change the forward function:
    def forward(self, x):
        x = self.features(x)
        # x = x.mean([2, 3])   # this line will result in bug
        x = x.mean(3).mean(2)
        x = self.classifier(x)
        return x

",9826
23404,MobileNetV2 export to ONNX fails,"look into the mobilenet.py in torchvision, change the forward function:
    def forward(self, x):
        x = self.features(x)
        # x = x.mean([2, 3])   # this line will result in bug
        x = x.mean(3).mean(2)
        x = self.classifier(x)
        return x

",9826
23405,nn.CTCLoss RuntimeError on GPU,Perfect! I compile torchvision from source and it works well. Thanks you @t-vi,4755
23406,nn.CTCLoss RuntimeError on GPU,Perfect! I compile torchvision from source and it works well. Thanks you @t-vi,4755
23407,"StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value","Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.

```python
import torch
print(""pytorch version"",torch.__version__) 
import torch.nn as nn
model = nn.Linear(1, 1) # 'Net' is a simple MLP
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
schedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)

print('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))
for e in range(8):
  optimizer.step()
  schedular.step()
  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],
          optimizer.param_groups[0]['lr']))
```

Since #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists.",6851
23408,"StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value","Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.

```python
import torch
print(""pytorch version"",torch.__version__) 
import torch.nn as nn
model = nn.Linear(1, 1) # 'Net' is a simple MLP
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
schedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)

print('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))
for e in range(8):
  optimizer.step()
  schedular.step()
  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],
          optimizer.param_groups[0]['lr']))
```

Since #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists.",6851
23409,Couple hundred MB are taken just by initializing cuda,"> why it's need so many memory on the GPU?

It's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.

> can i release it?

No (other than quitting the process)

The standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. ",275
23410,Couple hundred MB are taken just by initializing cuda,"> why it's need so many memory on the GPU?

It's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.

> can i release it?

No (other than quitting the process)

The standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. ",275
23411,Saving state_dicts should capture shared state,"I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.

If you create your first version with:

```
a = A()
b = B(a=a)
c = C(a=a, b=b)
```

Then your second version of C needs to be created with the same structure:

```
a2 = A()
b2 = B(a=a2)
c2 = C(a=a2, b=b2)
```

If you instead do:

```
c2 = C()
```

You've create a different structure and shouldn't expect it to work the same as your first incarnation.
",3055
23412,Saving state_dicts should capture shared state,"I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.

If you create your first version with:

```
a = A()
b = B(a=a)
c = C(a=a, b=b)
```

Then your second version of C needs to be created with the same structure:

```
a2 = A()
b2 = B(a=a2)
c2 = C(a=a2, b=b2)
```

If you instead do:

```
c2 = C()
```

You've create a different structure and shouldn't expect it to work the same as your first incarnation.
",3055
23413,"Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented","Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation.",4161
23414,"Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented","Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation.",4161
23415,[Feature request] Get cell state from the last layer for each t when using LSTM,"considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward.",9116
23416,[Feature request] Get cell state from the last layer for each t when using LSTM,"considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward.",9116
23417,Compiling pytorch on MacOSX 10.13.5 with support for CUDA GeForceGT 750M,"https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html

You need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`",9533
23418,Compiling pytorch on MacOSX 10.13.5 with support for CUDA GeForceGT 750M,"https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html

You need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`",9533
23419,RuntimeError: DataLoader worker is killed by signal: Killed.,It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason.,3954
23420,RuntimeError: DataLoader worker is killed by signal: Killed.,It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason.,3954
23421,[Feature Request] tensordot,"I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.
Medium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)",3321
23422,[Feature Request] tensordot,"I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.
Medium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)",3321
23423,[proposal] out= doesn't resize storage,We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't,8250
23424,[proposal] out= doesn't resize storage,We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't,8250
23425,"[jit] gen_jit_dispatch generates duplicate ""descriptor""s","We no longer use descriptors, so this is fixed.",8280
23426,"[jit] gen_jit_dispatch generates duplicate ""descriptor""s","We no longer use descriptors, so this is fixed.",8280
23427,[jit] cannot trace tensor factory methods,"This is kind of expected. There are two main problems at play here:

1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the ""world token"" to inhibit optimizations.
2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property.",7655
23428,[jit] cannot trace tensor factory methods,"This is kind of expected. There are two main problems at play here:

1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the ""world token"" to inhibit optimizations.
2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property.",7655
23429,ImportError: DLL load failed: The specified module could not be found,"Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine",2203
23430,ImportError: DLL load failed: The specified module could not be found,"Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine",2203
23431,PackedSequence's sorted_indices is not put on cuda when to('cuda') is called.,"Issue is still present on `pytorch==1.3.1`

To fix, replace the following:
`X = X.to(device)`
With this:
`X = X.to(device=device)`
Provided that ""X"" is a packed sequence.",3899
23432,PackedSequence's sorted_indices is not put on cuda when to('cuda') is called.,"Issue is still present on `pytorch==1.3.1`

To fix, replace the following:
`X = X.to(device)`
With this:
`X = X.to(device=device)`
Provided that ""X"" is a packed sequence.",3899
23433,Schema not found for node torch::eye,"Are you able to extract a smaller repro?

`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well

```python
@torch.jit.script
def test(A):
    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)
```",1268
23434,Schema not found for node torch::eye,"Are you able to extract a smaller repro?

`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well

```python
@torch.jit.script
def test(A):
    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)
```",1268
23435,element-wise multiplication out of memory,"You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:

```python
import torch

g = torch.rand([20, 75, 1024, 1024])
w = torch.rand([1024, 1024])
g *= w
res = g
```

That will require ~6.3 GB of memory.

Also, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.

You're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data.",8649
23436,element-wise multiplication out of memory,"You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:

```python
import torch

g = torch.rand([20, 75, 1024, 1024])
w = torch.rand([1024, 1024])
g *= w
res = g
```

That will require ~6.3 GB of memory.

Also, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.

You're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data.",8649
23437,torch::tensor(std::vector) does not work properly in Microsoft Visual Studio Windows,`int64_t` works fine.,8729
23438,torch::tensor(std::vector) does not work properly in Microsoft Visual Studio Windows,`int64_t` works fine.,8729
23439,unable to load istream by using torch::jit::load(istream),"@lantiga
 I have solved it ，thanks

1. read pt model to char buffer by using ifstream

2.change buffer to istream  
strstreambuf  buf(pModelData,length);
std::istream in(&buf);

3、load istream using torch::jit::load
 ",626
23440,unable to load istream by using torch::jit::load(istream),"@lantiga
 I have solved it ，thanks

1. read pt model to char buffer by using ifstream

2.change buffer to istream  
strstreambuf  buf(pModelData,length);
std::istream in(&buf);

3、load istream using torch::jit::load
 ",626
23441,autodiff for user script functions aka torch.jit.script for autograd.Function,"I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. ",3244
23442,autodiff for user script functions aka torch.jit.script for autograd.Function,"I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. ",3244
23443,Build with MKLDNN broken,"@mdreammao
Pls try to apply the patch in #22910 , then run ""git submodule update --init --recursive"".

Thanks.",641
23444,Build with MKLDNN broken,"@mdreammao
Pls try to apply the patch in #22910 , then run ""git submodule update --init --recursive"".

Thanks.",641
23445,Auto-differentiating torch.cdist is broken on GPU,"Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the ""Backward is not reentrant"" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.

Notebook: 
[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)
",3854
23446,Auto-differentiating torch.cdist is broken on GPU,"Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the ""Backward is not reentrant"" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.

Notebook: 
[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)
",3854
23447,"conversion  to non-scalar type  torch::jit::load(""model.pt"")","It looks like you are using a nightly build. We recently changed the output type of load. This should work:

```
 torch::jit::script::Module module = torch::jit::load(""model.pt"");
```

Tutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2.",3965
23448,"conversion  to non-scalar type  torch::jit::load(""model.pt"")","It looks like you are using a nightly build. We recently changed the output type of load. This should work:

```
 torch::jit::script::Module module = torch::jit::load(""model.pt"");
```

Tutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2.",3965
23449,"Dependency issues with torch.utils.tensorboard: ""No module named past"" and ""No module named 'PIL'""",Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...),4489
23450,"Dependency issues with torch.utils.tensorboard: ""No module named past"" and ""No module named 'PIL'""",Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...),4489
23451,Cuda required when loading a TorchScript with map_location='cpu',"It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(""cuda:0""), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU).",3961
23452,Cuda required when loading a TorchScript with map_location='cpu',"It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(""cuda:0""), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU).",3961
23453,runtime error (7) : too many resources requested for launch at pytorch/aten/src/THC/THCTensorSort.cu:62,"The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.
The natural questions would be
- What is the tensor dtype and size and what are dim and k used ins topk?
- Does the non-sorting topk work for that?
- Does sort work for the non-sorted values?

(The last two also might offer a workaround until it is fixed in PyTorch).
",7168
23454,runtime error (7) : too many resources requested for launch at pytorch/aten/src/THC/THCTensorSort.cu:62,"The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.
The natural questions would be
- What is the tensor dtype and size and what are dim and k used ins topk?
- Does the non-sorting topk work for that?
- Does sort work for the non-sorted values?

(The last two also might offer a workaround until it is fixed in PyTorch).
",7168
23455,[Build Error]undefined reference to `__cudaPushCallConfiguration',"This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package

```
# Check the NVCC compile version(e.g.)
/usr/cuda-9.2/bin/nvcc --version
# Check the CUDAToolKit version(e.g.)
~/anaconda3/bin/conda list | grep cuda

# If you need to update your CUDAToolKit
~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2
```
Both of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.",7789
23456,[Build Error]undefined reference to `__cudaPushCallConfiguration',"This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package

```
# Check the NVCC compile version(e.g.)
/usr/cuda-9.2/bin/nvcc --version
# Check the CUDAToolKit version(e.g.)
~/anaconda3/bin/conda list | grep cuda

# If you need to update your CUDAToolKit
~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2
```
Both of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.",7789
23457,forward function can not insert pdb.trace,"Hi @DanlanChen,

We do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:

1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code
2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:

```
import torch

def debug_fn(a, b, c):
    print(a, b, c)
    import pdb; pdb.set_trace()

class FooMod(torch.jit.ScriptModule):
    @torch.jit.script_method
    def forward(self, x, y):
        z = x + y
        debug_fn(x, y, z)
        return z

fm = FooMod()
fm(torch.rand(3), torch.rand(3))
```

Hope this helps with your debugging!",2821
23458,forward function can not insert pdb.trace,"Hi @DanlanChen,

We do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:

1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code
2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:

```
import torch

def debug_fn(a, b, c):
    print(a, b, c)
    import pdb; pdb.set_trace()

class FooMod(torch.jit.ScriptModule):
    @torch.jit.script_method
    def forward(self, x, y):
        z = x + y
        debug_fn(x, y, z)
        return z

fm = FooMod()
fm(torch.rand(3), torch.rand(3))
```

Hope this helps with your debugging!",2821
23459,zip not allowed in forward function in torch.jit.ScriptModule,"As a workaround, you can do the following:

```
mod_list = []
for mod1, mod2 in zip(module1, module2):
    mod_list.append(nn.Sequential(mod1, mod2))
self.module = nn.ModuleList(mod_list)
```
",1292
23460,zip not allowed in forward function in torch.jit.ScriptModule,"As a workaround, you can do the following:

```
mod_list = []
for mod1, mod2 in zip(module1, module2):
    mod_list.append(nn.Sequential(mod1, mod2))
self.module = nn.ModuleList(mod_list)
```
",1292
23461,Cannot build libtorch: SLEEF does not allow in-source builds,"Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:

- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when
  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR
  enables sleef to be nested within (and built by) a larger CMake
  project (which also contains other projects).
- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source
  and binary directories (i.e. the first encountered), as a result if
  sleef is a nested project within a larger project, these would not
  correctly identify the source/binary directories for sleef (as they
  would refer to the root project in which sleef is nested).
- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses
  the correct source/binary directories, irrespective of whether sleef
  is nested or not.

Patch for `aten/src/ATen/CMakeLists.txt`:
------------------------------------------------------
```
diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
index 07b3c106b..6133c583a 100644
--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)
   set(BUILD_DFT OFF CACHE BOOL ""Don't build sleef DFT lib"" FORCE)
   set(BUILD_GNUABI_LIBS OFF CACHE BOOL ""Don't build sleef gnuabi libs"" FORCE)
   set(BUILD_TESTS OFF CACHE BOOL ""Don't build sleef tests"" FORCE)
+  set(sleef_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"")
+  set(sleef_BINARY_DIR ""${CMAKE_BINARY_DIR}/sleef"")
   add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
   set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)
+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)
+  link_directories(${sleef_BINARY_DIR}/lib)
   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)
 
   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})
```

Patch for cmake files in sleef:
---------------------------------------
[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)
",5936
23462,Cannot build libtorch: SLEEF does not allow in-source builds,"Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:

- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when
  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR
  enables sleef to be nested within (and built by) a larger CMake
  project (which also contains other projects).
- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source
  and binary directories (i.e. the first encountered), as a result if
  sleef is a nested project within a larger project, these would not
  correctly identify the source/binary directories for sleef (as they
  would refer to the root project in which sleef is nested).
- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses
  the correct source/binary directories, irrespective of whether sleef
  is nested or not.

Patch for `aten/src/ATen/CMakeLists.txt`:
------------------------------------------------------
```
diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
index 07b3c106b..6133c583a 100644
--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)
   set(BUILD_DFT OFF CACHE BOOL ""Don't build sleef DFT lib"" FORCE)
   set(BUILD_GNUABI_LIBS OFF CACHE BOOL ""Don't build sleef gnuabi libs"" FORCE)
   set(BUILD_TESTS OFF CACHE BOOL ""Don't build sleef tests"" FORCE)
+  set(sleef_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"")
+  set(sleef_BINARY_DIR ""${CMAKE_BINARY_DIR}/sleef"")
   add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
   set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)
+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)
+  link_directories(${sleef_BINARY_DIR}/lib)
   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)
 
   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})
```

Patch for cmake files in sleef:
---------------------------------------
[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)
",5936
23463,Confusing behavior with *= operator with torch.expand,"This is a duplicate of https://github.com/pytorch/pytorch/issues/957

I agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)",7597
23464,Confusing behavior with *= operator with torch.expand,"This is a duplicate of https://github.com/pytorch/pytorch/issues/957

I agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)",7597
23465,from torch._C import * ImportError: DLL load failed: The specified module could not be found.,"> the same issue today
> i've tried python 3.6.x and 3.7.1, didn't work.

first, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)
and then ,` pip install --upgrade numpy  and pip install intel-openmp` 
it works for me
",271
23466,from torch._C import * ImportError: DLL load failed: The specified module could not be found.,"> the same issue today
> i've tried python 3.6.x and 3.7.1, didn't work.

first, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)
and then ,` pip install --upgrade numpy  and pip install intel-openmp` 
it works for me
",271
23467,multivariate_normal.log_prob is slow,"@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with ""expanded"" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.

The current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.
```
sigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()
x_repeat = torch.randn(8000, 6, 1, 8)
```

I'll sketch out a solution which I have in mind for further discussion.",715
23468,multivariate_normal.log_prob is slow,"@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with ""expanded"" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.

The current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.
```
sigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()
x_repeat = torch.randn(8000, 6, 1, 8)
```

I'll sketch out a solution which I have in mind for further discussion.",715
23469,"Training on 360 sequences, validating on 0 sequences. python3: symbol lookup error: /home/ankitakulkarni/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack","Solved ...Update the version of python from 3.6.0 to 3.6.2 
Thanks!!",5956
23470,"Training on 360 sequences, validating on 0 sequences. python3: symbol lookup error: /home/ankitakulkarni/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack","Solved ...Update the version of python from 3.6.0 to 3.6.2 
Thanks!!",5956
23471,Windows pytorch CUDA 10 error,"FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment.",2369
23472,Windows pytorch CUDA 10 error,"FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment.",2369
23473,Broken indexing?,Resolved by upgrading to >= Python 3.6.1,5168
23474,Broken indexing?,Resolved by upgrading to >= Python 3.6.1,5168
23475,"""invalid parameter combination for AltiVec intrinsic"" error with ppc64le, g++ v7.4","Ahh!  I see, I over-corrected.  
So I put back vec_sldw on line 297, and the compile completed without error.  Whew!
Granted, this was a test of the one file that had failed rather than a full build, but it does look good.",995
23476,"""invalid parameter combination for AltiVec intrinsic"" error with ppc64le, g++ v7.4","Ahh!  I see, I over-corrected.  
So I put back vec_sldw on line 297, and the compile completed without error.  Whew!
Granted, this was a test of the one file that had failed rather than a full build, but it does look good.",995
23477,torch.linalg.cond return dtype inconsistent with doc and np.linalg.cond,"`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.

When normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct ""out="" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.
",8748
23478,torch.linalg.cond return dtype inconsistent with doc and np.linalg.cond,"`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.

When normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct ""out="" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.
",8748
23479,Is `torch.multiprocessing.spawn` compatible with `DataLoader`?,"hey @PetrochukM!

I think this is less a PyTorch issue and something the Lightning team should investigate. 

I'll hijack this issue for now to make sure my thoughts are documented!

Forgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:

https://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f

To fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!",9509
23480,Is `torch.multiprocessing.spawn` compatible with `DataLoader`?,"hey @PetrochukM!

I think this is less a PyTorch issue and something the Lightning team should investigate. 

I'll hijack this issue for now to make sure my thoughts are documented!

Forgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:

https://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f

To fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!",9509
23481,Using LAPACK on Pi4 (64Bit Raspberry Pi OS RAM 8GB),"I've got a way to work out: 
1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install
2. apt install libopenblas-dev
3. success!",3353
23482,Using LAPACK on Pi4 (64Bit Raspberry Pi OS RAM 8GB),"I've got a way to work out: 
1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install
2. apt install libopenblas-dev
3. success!",3353
23483,Quantized LeakyReLu Bug: Input tensor size determines the output values,"Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds.",2822
23484,Quantized LeakyReLu Bug: Input tensor size determines the output values,"Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds.",2822
23485,aten::normal_ not handled as a special op in RemoveTensorMutation pass.,The easiest fix is to probably just implement `normal`,7039
23486,aten::normal_ not handled as a special op in RemoveTensorMutation pass.,The easiest fix is to probably just implement `normal`,7039
23487,test_variant_consistency_jit tests fail on CPU for min & max when dtype is bfloat16 & dim argument is passed,"@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry ",606
23488,test_variant_consistency_jit tests fail on CPU for min & max when dtype is bfloat16 & dim argument is passed,"@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry ",606
23489,[collect_env] Unable to collect CUDA version anymore,"I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584

- For CUDA 11.0
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Thu_Jun_11_22:26:38_PDT_2020
Cuda compilation tools, release 11.0, V11.0.194
Build cuda_11.0_bu.TC445_37.28540450_0
```

- For CUDA 10.1
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
```

There is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled
by `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r""V(.*)$""` without `re.MULTILINE`.
Therefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0",3074
23490,[collect_env] Unable to collect CUDA version anymore,"I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584

- For CUDA 11.0
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Thu_Jun_11_22:26:38_PDT_2020
Cuda compilation tools, release 11.0, V11.0.194
Build cuda_11.0_bu.TC445_37.28540450_0
```

- For CUDA 10.1
```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
```

There is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled
by `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r""V(.*)$""` without `re.MULTILINE`.
Therefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0",3074
23491,M1 release and nightly binaries,"The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779

You can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package).",6995
23492,M1 release and nightly binaries,"The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779

You can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package).",6995
23493,[FR] Initialize module on specified device,"This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.

With this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:

```python
def __init__():
   self.meta_conv = Conv2d(..., device='meta')
   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))

def forward(x):
  params = self.parameter_predictor(x)
  with temporary_set_params(self.meta_conv, params):
      return self.meta_conv(x)
```",7736
23494,[FR] Initialize module on specified device,"This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.

With this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:

```python
def __init__():
   self.meta_conv = Conv2d(..., device='meta')
   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))

def forward(x):
  params = self.parameter_predictor(x)
  with temporary_set_params(self.meta_conv, params):
      return self.meta_conv(x)
```",7736
23495,Please fix all the related links format from http to https,closed via https://github.com/pytorch/pytorch.github.io/pull/128,9088
23496,Please fix all the related links format from http to https,closed via https://github.com/pytorch/pytorch.github.io/pull/128,9088
23497,ImportError: libcurand.so.9.0: cannot open shared object file: No such file or directory,"@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!",578
23498,ImportError: libcurand.so.9.0: cannot open shared object file: No such file or directory,"@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!",578
23499,Exception in Thread: ValueError: signal number 32 out of range,I have solved the problem by updating Python3.5 to Python3.7,3139
23500,Exception in Thread: ValueError: signal number 32 out of range,I have solved the problem by updating Python3.5 to Python3.7,3139
23501,[JIT] torch.tensor doesn't trace devices correctly,"I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.
The basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.

```
iff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index f9d6ffc62..d8c939232 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -244,7 +244,9 @@ Tensor internal_new_from_data(
       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,
       scalarType, tensor.type().elementSizeInBytes(), data);
   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;
-  return new_with_type_conversion(type_to_use, tensor, device_index);
+  auto device = device_opt.has_value() ? *device_opt : tensor.device();
+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);
+  //return new_with_type_conversion(type_to_use, tensor, device_index);
 }
 
 Tensor new_from_data_copy(
```
",3132
23502,[JIT] torch.tensor doesn't trace devices correctly,"I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.
The basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.

```
iff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index f9d6ffc62..d8c939232 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -244,7 +244,9 @@ Tensor internal_new_from_data(
       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,
       scalarType, tensor.type().elementSizeInBytes(), data);
   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;
-  return new_with_type_conversion(type_to_use, tensor, device_index);
+  auto device = device_opt.has_value() ? *device_opt : tensor.device();
+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);
+  //return new_with_type_conversion(type_to_use, tensor, device_index);
 }
 
 Tensor new_from_data_copy(
```
",3132
23503,gradient difference between single GPU and multi-GPU DataParallel,"the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected.",10872
23504,gradient difference between single GPU and multi-GPU DataParallel,"the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected.",10872
23505,UserWarning: ONNX export failed on ATen operator _argmax because torch.onnx.symbolic._argmax does not exist,"If I add the following function into torch/onnx/symbolic.py

@parse_args('v', 'i', 'i')
def _argmax(g, self, dim, keepdim=None):
    return g.op(""ArgMax"", self, axis_i=dim, keepdim_i=keepdim)


Then the export is able to proceed without error.
",3373
23506,UserWarning: ONNX export failed on ATen operator _argmax because torch.onnx.symbolic._argmax does not exist,"If I add the following function into torch/onnx/symbolic.py

@parse_args('v', 'i', 'i')
def _argmax(g, self, dim, keepdim=None):
    return g.op(""ArgMax"", self, axis_i=dim, keepdim_i=keepdim)


Then the export is able to proceed without error.
",3373
23507,cmake error about rocrand,"@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable.",500
23508,cmake error about rocrand,"@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable.",500
23509,[Aten] at::randint doesn't return a variable,The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s.,7359
23510,[Aten] at::randint doesn't return a variable,The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s.,7359
23511,libtorch latest-deps cuasing an error: PyTorch script module file is too old,Nevermind -- re-exported model with new nightly & it worked fine.,4305
23512,libtorch latest-deps cuasing an error: PyTorch script module file is too old,Nevermind -- re-exported model with new nightly & it worked fine.,4305
23513,Unable to pickle torch dtype objects in Python 3.5,"Actually torch dtype object is already serializable. Closing....
```
In [6]: b = copy.deepcopy(a)

In [7]: id(b)
Out[7]: 139818768678472

In [8]: id(a)
Out[8]: 139818768678472

In [9]: import pickle

In [10]: with open('/tmp/a', 'wb') as f:
    ...:     pickle.dump(torch.float32, f)
    ...:
```
",930
23514,Unable to pickle torch dtype objects in Python 3.5,"Actually torch dtype object is already serializable. Closing....
```
In [6]: b = copy.deepcopy(a)

In [7]: id(b)
Out[7]: 139818768678472

In [8]: id(a)
Out[8]: 139818768678472

In [9]: import pickle

In [10]: with open('/tmp/a', 'wb') as f:
    ...:     pickle.dump(torch.float32, f)
    ...:
```
",930
23515,do pytorch c++ jit trace run model need more gpu memory than python env of the same model?,"It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  ",3950
23516,do pytorch c++ jit trace run model need more gpu memory than python env of the same model?,"It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  ",3950
23517,the device of tensor can not be change,"> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines
> 
> ```
>         lp07.to(device)
>         lp14.to(device)
> ```
thanks for answering my question. i figure out it by modify my code:
`
            lp07 = lp07.to(device)
            lp14 = lp14.to(device)
`
but i don't know why should i do this. In pytorch 0.3.0, the tensor just use:
`tensor.cuda()`",209
23518,the device of tensor can not be change,"> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines
> 
> ```
>         lp07.to(device)
>         lp14.to(device)
> ```
thanks for answering my question. i figure out it by modify my code:
`
            lp07 = lp07.to(device)
            lp14 = lp14.to(device)
`
but i don't know why should i do this. In pytorch 0.3.0, the tensor just use:
`tensor.cuda()`",209
23519,torch.full and torch.randint are inconsistent in arg order,"I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too.",3245
23520,torch.full and torch.randint are inconsistent in arg order,"I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too.",3245
23521,JIT pickler should support both little endian and big endian systems,"Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation",2257
23522,JIT pickler should support both little endian and big endian systems,"Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation",2257
23523,Improve the way RPC unit tests are skipped for windows,"I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment.",3225
23524,Improve the way RPC unit tests are skipped for windows,"I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment.",3225
23525,Build pytorch from source in osx,I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/,3355
23526,Build pytorch from source in osx,I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/,3355
23527,Python crash during backward,Resolved in latest builds.,5169
23528,Python crash during backward,Resolved in latest builds.,5169
23529,Upsampling is not implemented for 1D (temporal) inputs,"i think i know this.

```
pip uninstall torch
pip uninstall torch
python setup.py clean
python setup.py build develop
```",9563
23530,Upsampling is not implemented for 1D (temporal) inputs,"i think i know this.

```
pip uninstall torch
pip uninstall torch
python setup.py clean
python setup.py build develop
```",9563
23531,"With backward method of CUDA Variables, grad is None if gradient is supplied","try this:
```
_x = Variable(torch.ones(10), requires_grad=True)
x = _x.cuda()
y = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()
y.backward(torch.ones(10).cuda())
print(x.grad)
print(_x.grad)
```
This is because x is an intermediate node.
Alternative is:
```
x = Variable(torch.ones(10).cuda(), requires_grad=True)
y = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()
y.backward(torch.ones(10).cuda())
print(x.grad)
```",11273
23532,"With backward method of CUDA Variables, grad is None if gradient is supplied","try this:
```
_x = Variable(torch.ones(10), requires_grad=True)
x = _x.cuda()
y = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()
y.backward(torch.ones(10).cuda())
print(x.grad)
print(_x.grad)
```
This is because x is an intermediate node.
Alternative is:
```
x = Variable(torch.ones(10).cuda(), requires_grad=True)
y = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()
y.backward(torch.ones(10).cuda())
print(x.grad)
```",11273
23533,from torch._C import * ImportError: numpy.core.multiarray failed to import,"$ pip install numpy -I 
has worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.
(py35) user@user-ASUS:~$ pip install numpy -I  
Collecting numpy
  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.13.1

(py35) user@user-ASUS:~$ python
Python 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>> x = torch.Tensor(2,3)
>>> x

 1.6534e+05  4.5673e-41  1.3032e-37
 0.0000e+00  4.4842e-44  0.0000e+00
[torch.FloatTensor of size 2x3]
",55
23534,from torch._C import * ImportError: numpy.core.multiarray failed to import,"$ pip install numpy -I 
has worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.
(py35) user@user-ASUS:~$ pip install numpy -I  
Collecting numpy
  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.13.1

(py35) user@user-ASUS:~$ python
Python 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>> x = torch.Tensor(2,3)
>>> x

 1.6534e+05  4.5673e-41  1.3032e-37
 0.0000e+00  4.4842e-44  0.0000e+00
[torch.FloatTensor of size 2x3]
",55
23535,"tensor.rand() and uniform_() returns numbers from [0,1] (right-hand inclusive)","Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:
```
#include <curand_globals.h>
#include <limits.h>
#include <iostream>
#include <iomanip>

int main(){
   uint x = UINT_MAX;
   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);
   std::cout << std::setprecision(14) << ""double max "" << d << ""\n"";
   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);
   std::cout << ""float max "" << f << ""\n"";
//TH backend
   double dc = UINT_MAX * (1.0/4294967296.0);   
   std::cout << std::setprecision(14) << ""double max cpu "" << dc << ""\n""; 
   float fc = (float)(UINT_MAX * (1.0/4294967296.0));
   std::cout << std::setprecision(14) << ""float max cpu "" << fc << ""\n""; 
  
}
```
Only cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]
gpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. 
gpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. 
Replacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). 
",2797
23536,"tensor.rand() and uniform_() returns numbers from [0,1] (right-hand inclusive)","Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:
```
#include <curand_globals.h>
#include <limits.h>
#include <iostream>
#include <iomanip>

int main(){
   uint x = UINT_MAX;
   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);
   std::cout << std::setprecision(14) << ""double max "" << d << ""\n"";
   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);
   std::cout << ""float max "" << f << ""\n"";
//TH backend
   double dc = UINT_MAX * (1.0/4294967296.0);   
   std::cout << std::setprecision(14) << ""double max cpu "" << dc << ""\n""; 
   float fc = (float)(UINT_MAX * (1.0/4294967296.0));
   std::cout << std::setprecision(14) << ""float max cpu "" << fc << ""\n""; 
  
}
```
Only cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]
gpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. 
gpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. 
Replacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). 
",2797
23537,"tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)",fixed in master. soon to be in nightlies that we're building.,9347
23538,"tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)",fixed in master. soon to be in nightlies that we're building.,9347
23539,How to extract middle layer features,"pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer.",10522
23540,How to extract middle layer features,"pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer.",10522
23541,topk cudaerror traceback...,"This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.
The NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.

This thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/

There's not a whole lot we can do from the pytorch end.

What GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)",7666
23542,topk cudaerror traceback...,"This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.
The NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.

This thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/

There's not a whole lot we can do from the pytorch end.

What GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)",7666
23543,Variable input size training is slow,do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit.,9193
23544,Variable input size training is slow,do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit.,9193
23545,Can't import saved pytorch model,"Update:

I finally figure it out the way to make it right.
solution for those who want to use SDN pox controller:
add the ""hparamDict"" definition before boot() function in the ~/any/pox/path/pox.py

I am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method ""torch.save(model.state_dict())"".
The attribute missing is actually a dict class I define for hyperparameters.

=========================================================
Hi, guys. any solutions?

I got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. 
Saying ""'module' object has no attribute 'hparamDict'"".

I am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.
The command I use is ""./pox.py pytorch_model""
",8065
23546,Can't import saved pytorch model,"Update:

I finally figure it out the way to make it right.
solution for those who want to use SDN pox controller:
add the ""hparamDict"" definition before boot() function in the ~/any/pox/path/pox.py

I am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method ""torch.save(model.state_dict())"".
The attribute missing is actually a dict class I define for hyperparameters.

=========================================================
Hi, guys. any solutions?

I got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. 
Saying ""'module' object has no attribute 'hparamDict'"".

I am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.
The command I use is ""./pox.py pytorch_model""
",8065
23547,LSTM architecture has met a explosive growth in the training process,I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it.,3129
23548,LSTM architecture has met a explosive growth in the training process,I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it.,3129
23549,TypeError: Type torch.LongTensor doesn't implement stateless method mean,"We dont implement `mean` for LongTensor.
You can do: `y=torch.mean(x.float())`",8266
23550,TypeError: Type torch.LongTensor doesn't implement stateless method mean,"We dont implement `mean` for LongTensor.
You can do: `y=torch.mean(x.float())`",8266
23551,None Grad with Custom Loss,"This line seems the one which interferes with the gradient:
```
            score_final = score_final * (score[..., i] <= 0).float()
```",7712
23552,None Grad with Custom Loss,"This line seems the one which interferes with the gradient:
```
            score_final = score_final * (score[..., i] <= 0).float()
```",7712
23553,[request] Encode/decode variables,"I can see two solutions to this issue:

1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.
2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.

[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147",3029
23554,[request] Encode/decode variables,"I can see two solutions to this issue:

1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.
2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.

[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147",3029
23555,no module named reduction,"Just in case anyone else stumbles into this issue like I just did:

You will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.

(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)",4029
23556,no module named reduction,"Just in case anyone else stumbles into this issue like I just did:

You will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.

(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)",4029
23557,Constructing a ParameterDict raises a warning,"Thanks, I am ignoring it now with:

```
import warnings
warnings.filterwarnings(""ignore"", message=""Setting attributes on ParameterDict is not supported."")
```

I will wait for the new release!!",6878
23558,Constructing a ParameterDict raises a warning,"Thanks, I am ignoring it now with:

```
import warnings
warnings.filterwarnings(""ignore"", message=""Setting attributes on ParameterDict is not supported."")
```

I will wait for the new release!!",6878
23559,flake8 errors are not shown by github actions,"Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression",2730
23560,flake8 errors are not shown by github actions,"Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression",2730
23561,"RuntimeError: ""mul_cuda"" not implemented for 'Bool'","Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68
",4224
23562,"RuntimeError: ""mul_cuda"" not implemented for 'Bool'","Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68
",4224
23563,[POLL][RFC] Can we retire Single-Process Multi-Device Mode from DistributedDataParallel?,The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now.,7244
23564,[POLL][RFC] Can we retire Single-Process Multi-Device Mode from DistributedDataParallel?,The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now.,7244
23565,Convolution operations are extremely slow on RTX 30 series GPU,"> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.
> Until then performance regressions on these devices are unfortunately expected.

Very thanks for your help.😃
So the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available？",248
23566,Convolution operations are extremely slow on RTX 30 series GPU,"> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.
> Until then performance regressions on these devices are unfortunately expected.

Very thanks for your help.😃
So the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available？",248
23567,torch.arange numerics are different after 1.7 update on CPU,"I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this

```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    return Vec256<double>(
        base + step * index_offset,
        base + step * (index_offset + 1),
        base + step * (index_offset + 2),
        base + step * (index_offset + 3));
  }
```

to this:
```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    Vec256<double> base_v(base, base, base, base);
    Vec256<double> step_v(step, step, step, step);
    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); 
    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));
  }
```

Evidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:

```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); 
    Vec256<double> step_v(step, step, step, step);
    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);
    return Vec256<double>(
      base + tmp.values[0],
      base + tmp.values[1],
      base + tmp.values[2],
      base + tmp.values[3]);
  }
```

This worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:

```
$ ATEN_CPU_CAPABILITY=avx2 python
>>> import torch
>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()
tensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)
>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()
 -5.0
 -3.6
 -2.2
 -0.8000000000000007
 0.5999999999999996
 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before
 3.3999999999999986
 4.799999999999999
[torch.DoubleStorage of size 8]
```

This result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.

I'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?",3293
23568,torch.arange numerics are different after 1.7 update on CPU,"I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this

```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    return Vec256<double>(
        base + step * index_offset,
        base + step * (index_offset + 1),
        base + step * (index_offset + 2),
        base + step * (index_offset + 3));
  }
```

to this:
```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    Vec256<double> base_v(base, base, base, base);
    Vec256<double> step_v(step, step, step, step);
    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); 
    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));
  }
```

Evidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:

```
  template<typename step_t>
  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {
    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); 
    Vec256<double> step_v(step, step, step, step);
    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);
    return Vec256<double>(
      base + tmp.values[0],
      base + tmp.values[1],
      base + tmp.values[2],
      base + tmp.values[3]);
  }
```

This worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:

```
$ ATEN_CPU_CAPABILITY=avx2 python
>>> import torch
>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()
tensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)
>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()
 -5.0
 -3.6
 -2.2
 -0.8000000000000007
 0.5999999999999996
 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before
 3.3999999999999986
 4.799999999999999
[torch.DoubleStorage of size 8]
```

This result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.

I'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?",3293
23569,`F.grid_sample` fails to dispatch correctly when args are of different subclasses,"Many thanks for these suggestions @hameerabbasi !

I'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility

I've tried this

```
class TensorBase:
    def __init__(self, data, metadata=None, **kwargs):
        self._fa_tensor = tensor(data)
        for k,v in kwargs.items(): setattr(self, k, v)
    def __torch_function__(self, func, types, args=(), kwargs=None):
        if kwargs is None: kwargs = {}
        args = [getattr(a,'_fa_tensor',a) for a in args]
        ret = func(*args, **kwargs)
        return TensorBase(ret, **self.__dict__)
    def __getattr__(self, k): return getattr(self._fa_tensor, k)
    def __getitem__(self, k): return self._fa_tensor[k]
```

But I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky",4210
23570,`F.grid_sample` fails to dispatch correctly when args are of different subclasses,"Many thanks for these suggestions @hameerabbasi !

I'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility

I've tried this

```
class TensorBase:
    def __init__(self, data, metadata=None, **kwargs):
        self._fa_tensor = tensor(data)
        for k,v in kwargs.items(): setattr(self, k, v)
    def __torch_function__(self, func, types, args=(), kwargs=None):
        if kwargs is None: kwargs = {}
        args = [getattr(a,'_fa_tensor',a) for a in args]
        ret = func(*args, **kwargs)
        return TensorBase(ret, **self.__dict__)
    def __getattr__(self, k): return getattr(self._fa_tensor, k)
    def __getitem__(self, k): return self._fa_tensor[k]
```

But I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky",4210
23571,torch.fft does not give the same result as torch.stft,"The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:

```
S_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)
```
Which results in:
![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)",7029
23572,torch.fft does not give the same result as torch.stft,"The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:

```
S_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)
```
Which results in:
![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)",7029
23573,Legacy tensor ctor returns uninitialized tensor when input and output device differ,"> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor

what is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check.",256
23574,Legacy tensor ctor returns uninitialized tensor when input and output device differ,"> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor

what is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check.",256
23575,Independent Distribution Wrapper Disguises Negative StdDev in Underlying Normal Distribution,"Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called
```py
torch.distributions.Distribution.set_default_validate_args(True)
```
In fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?",2830
23576,Independent Distribution Wrapper Disguises Negative StdDev in Underlying Normal Distribution,"Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called
```py
torch.distributions.Distribution.set_default_validate_args(True)
```
In fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?",2830
23577,torch.trace type promotion behavior is different on CPU vs CUDA,"1.6.0:
```
>>> import torch
>>> torch.__version__
'1.6.0'
>>> x = torch.ones(5, 5, dtype=torch.uint8)
>>> x = x.cuda()
>>> x.trace()
tensor(5, device='cuda:0')
>>> x.trace().dtype
torch.int64
```

1.5.1:
```
>>> import torch
>>> torch.__version__
'1.5.1'
>>> x = torch.ones(5, 5, dtype=torch.uint8)
>>> x = x.cuda()
>>> x.trace()
tensor(5, device='cuda:0', dtype=torch.uint8)
```",102
23578,torch.trace type promotion behavior is different on CPU vs CUDA,"1.6.0:
```
>>> import torch
>>> torch.__version__
'1.6.0'
>>> x = torch.ones(5, 5, dtype=torch.uint8)
>>> x = x.cuda()
>>> x.trace()
tensor(5, device='cuda:0')
>>> x.trace().dtype
torch.int64
```

1.5.1:
```
>>> import torch
>>> torch.__version__
'1.5.1'
>>> x = torch.ones(5, 5, dtype=torch.uint8)
>>> x = x.cuda()
>>> x.trace()
tensor(5, device='cuda:0', dtype=torch.uint8)
```",102
23579,"Jit Error with CUDA and FP16 -- identifier ""aten_add_flat__1"" is undefined",The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho.,7067
23580,"Jit Error with CUDA and FP16 -- identifier ""aten_add_flat__1"" is undefined",The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho.,7067
23581,Distributed weight updates,"@jianjiandandande 

After calling `loss.backward()`, can you try the following?

```python
for name, param in m.named_parameters():
  if not param.grad:
    print(f""detected unused parameter: {name}"")
```

This should tell you what parameters are not used. 

BTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? 
",613
23582,Distributed weight updates,"@jianjiandandande 

After calling `loss.backward()`, can you try the following?

```python
for name, param in m.named_parameters():
  if not param.grad:
    print(f""detected unused parameter: {name}"")
```

This should tell you what parameters are not used. 

BTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? 
",613
23583,How to add PyTorch to requirements.txt,"Looks like this works:

```
--find-links https://download.pytorch.org/whl/torch_stable.html
torch==1.3.1+cpu
```

Thanks for the help!",4154
23584,How to add PyTorch to requirements.txt,"Looks like this works:

```
--find-links https://download.pytorch.org/whl/torch_stable.html
torch==1.3.1+cpu
```

Thanks for the help!",4154
23585,Use non-system cuda path,"@mahmoodn I assume you are building from source right? 
Try setting `CUDA_HOME` env?",633
23586,Use non-system cuda path,"@mahmoodn I assume you are building from source right? 
Try setting `CUDA_HOME` env?",633
23587,list of registered buffers does not move to cuda,"@xonobo 
So if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. 
`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. 
Please let us know if this doesn't solve your question. Thanks!",741
23588,list of registered buffers does not move to cuda,"@xonobo 
So if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. 
`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. 
Please let us know if this doesn't solve your question. Thanks!",741
23589,Memory leak with Conv1d on CPU,"Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!

Thanks a lot for pointing out, @ezyang!",8436
23590,Memory leak with Conv1d on CPU,"Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!

Thanks a lot for pointing out, @ezyang!",8436
23591,nn.Transformer.generate_square_subsequent_mask does not behave as expected,It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions.,3999
23592,nn.Transformer.generate_square_subsequent_mask does not behave as expected,It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions.,3999
23593,Vanilla Resnet50 Not Computing on iOS via Libtorch (Pytorch Mobile),"Hi Hussain, 

As we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods.",2850
23594,Vanilla Resnet50 Not Computing on iOS via Libtorch (Pytorch Mobile),"Hi Hussain, 

As we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods.",2850
23595,Error when loading model with traced `to` call,"@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?",712
23596,Error when loading model with traced `to` call,"@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?",712
23597,died with <Signals.SIGSEGV: 11>,Try Python 3.6.,8012
23598,died with <Signals.SIGSEGV: 11>,Try Python 3.6.,8012
23599,Memory leak when evaluating model on CPU with dynamic size tensor input.,"os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!",10331
23600,Memory leak when evaluating model on CPU with dynamic size tensor input.,"os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!",10331
23601,torch.nn.parallel.DistributedDataParallel is slow on backpropagation,"A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation.",793
23602,torch.nn.parallel.DistributedDataParallel is slow on backpropagation,"A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation.",793
23603,Illegal instruction : 4,"YESSSSSS !!
Thanks a lot.

Here it is :

(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**
Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /Users/xxxxxx/miniconda3

  added / updated specs:
    - pytorch-nightly
    - torchvision


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch
    tqdm-4.38.0                |             py_0          51 KB
    ------------------------------------------------------------
                                           Total:        45.4 MB

The following NEW packages will be INSTALLED:

  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0

The following packages will be UPDATED:

  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0


Proceed ([y]/n)? y


Downloading and Extracting Packages
pytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% 
tqdm-4.38.0          | 51 KB     | #################################################################################### | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(base) iMac27:miniconda3 xxxxxx$ python3
Python 3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>> 
(base) iMac27:miniconda3 xxxxxx$ 
",8452
23604,Illegal instruction : 4,"YESSSSSS !!
Thanks a lot.

Here it is :

(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**
Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /Users/xxxxxx/miniconda3

  added / updated specs:
    - pytorch-nightly
    - torchvision


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch
    tqdm-4.38.0                |             py_0          51 KB
    ------------------------------------------------------------
                                           Total:        45.4 MB

The following NEW packages will be INSTALLED:

  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0

The following packages will be UPDATED:

  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0


Proceed ([y]/n)? y


Downloading and Extracting Packages
pytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% 
tqdm-4.38.0          | 51 KB     | #################################################################################### | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(base) iMac27:miniconda3 xxxxxx$ python3
Python 3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>> 
(base) iMac27:miniconda3 xxxxxx$ 
",8452
23605,ONNX export failed: Couldn't export operator aten::upsample_bilinear2d,"Fixed after adding 

torch.onnx.export(..,opset_version=11)
",2455
23606,ONNX export failed: Couldn't export operator aten::upsample_bilinear2d,"Fixed after adding 

torch.onnx.export(..,opset_version=11)
",2455
23607,Add SWA to PyTorch mainline,"In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).

In this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?",3732
23608,Add SWA to PyTorch mainline,"In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).

In this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?",3732
23609,The Bug of onnx model format exported by torch.onnx.export,"For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.
Is there a specific reason you're using a different export type?

To try out the repro code, I added the code below to read the image:

```
input_img = osp.join(
            osp.dirname(__file__), '../tests/data/color.jpg')
original_image = mmcv.imread(input_img)
original_image = torch.from_numpy(original_image)
```

With this code, I two errors in exporter regarding missing symbolics:
1- Unsupported: ONNX export of roi_align with aligned=True
2-  Exporting the operator new_empty to ONNX opset version 12 is not supported

Can you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?

Also, is there a reason you're exporting this model to external data format?
Running ONNX checker on models in external data format might be a bit different.
I tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),
and then tried export and checker with: onnx.checker.check_model(""mask_rcnn.onnx"")
Which seems to work and pass checker successfully.",2541
23610,The Bug of onnx model format exported by torch.onnx.export,"For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.
Is there a specific reason you're using a different export type?

To try out the repro code, I added the code below to read the image:

```
input_img = osp.join(
            osp.dirname(__file__), '../tests/data/color.jpg')
original_image = mmcv.imread(input_img)
original_image = torch.from_numpy(original_image)
```

With this code, I two errors in exporter regarding missing symbolics:
1- Unsupported: ONNX export of roi_align with aligned=True
2-  Exporting the operator new_empty to ONNX opset version 12 is not supported

Can you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?

Also, is there a reason you're exporting this model to external data format?
Running ONNX checker on models in external data format might be a bit different.
I tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),
and then tried export and checker with: onnx.checker.check_model(""mask_rcnn.onnx"")
Which seems to work and pass checker successfully.",2541
23611,How to use and debug mixed-precision in 1.6.0 ?,"You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. 

Btw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:

1. https://github.com/pytorch/pytorch/issues/42605
2. https://github.com/pytorch/pytorch/issues/36428

Also you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).

Finally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf.",8653
23612,How to use and debug mixed-precision in 1.6.0 ?,"You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. 

Btw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:

1. https://github.com/pytorch/pytorch/issues/42605
2. https://github.com/pytorch/pytorch/issues/36428

Also you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).

Finally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf.",8653
23613,"after updating to pytorch 1.6 mypy does not recognise the tensor attributes ndim, nonzero and T ","`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing.",8732
23614,"after updating to pytorch 1.6 mypy does not recognise the tensor attributes ndim, nonzero and T ","`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing.",8732
23615,How to use torch.utils.checkpoint and DistributedDataParallel together,"Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. 

```python
init_process_group(...)
model = MyModel(...)
model(inputs).sum().backward()
works = []
for p in model.parameters():
    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient
    works.append(dist.all_reduce(p.grad, async_op=True))
for work in works:
    work.wait()
...
```",2808
23616,How to use torch.utils.checkpoint and DistributedDataParallel together,"Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. 

```python
init_process_group(...)
model = MyModel(...)
model(inputs).sum().backward()
works = []
for p in model.parameters():
    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient
    works.append(dist.all_reduce(p.grad, async_op=True))
for work in works:
    work.wait()
...
```",2808
23617,complex32 seems to be doing very very weird things on CPU,"Currently there is almost no support for `torch.complex32`, so yeah we should disable it.",2049
23618,complex32 seems to be doing very very weird things on CPU,"Currently there is almost no support for `torch.complex32`, so yeah we should disable it.",2049
23619,[jit] TorchScript does not work with Python coverage package,"Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? ",2832
23620,[jit] TorchScript does not work with Python coverage package,"Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? ",2832
23621,Trying to get pytorch working for the first time,"Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)",4156
23622,Trying to get pytorch working for the first time,"Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)",4156
23623,Feature Request: Add Pixel Unshuffle,"The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:
```python
class SpaceToDepth(nn.Module):
    def __init__(self, block_size=4):
        super().__init__()
        assert block_size in {2, 4}, ""Space2Depth only supports blocks size = 4 or 2""
        self.block_size = block_size

    def forward(self, x):
        N, C, H, W = x.size()
        S = self.block_size
        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)
        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)
        return x

    def extra_repr(self):
        return f""block_size={self.block_size}""
```",7114
23624,Feature Request: Add Pixel Unshuffle,"The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:
```python
class SpaceToDepth(nn.Module):
    def __init__(self, block_size=4):
        super().__init__()
        assert block_size in {2, 4}, ""Space2Depth only supports blocks size = 4 or 2""
        self.block_size = block_size

    def forward(self, x):
        N, C, H, W = x.size()
        S = self.block_size
        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)
        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)
        return x

    def extra_repr(self):
        return f""block_size={self.block_size}""
```",7114
23625,Timeout option for parallel DataLoader,"We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.

I adapted as following to make it run:
```
import torch.utils.data

class Dataset(object):
  def __len__(self):
    return 100

  def __getitem__(self, i):
    return list(range(100000))


class Sampler(torch.utils.data.Sampler):
  def __iter__(self):
    return (range(100000) for batch_ind in range(100))

  def __len__(self):
    return 100

d = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)
for i, x in enumerate(d):
    print(i)
    
```",8278
23626,Timeout option for parallel DataLoader,"We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.

I adapted as following to make it run:
```
import torch.utils.data

class Dataset(object):
  def __len__(self):
    return 100

  def __getitem__(self, i):
    return list(range(100000))


class Sampler(torch.utils.data.Sampler):
  def __iter__(self):
    return (range(100000) for batch_ind in range(100))

  def __len__(self):
    return 100

d = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)
for i, x in enumerate(d):
    print(i)
    
```",8278
23627,numpy like tensor.all and tensor.any,"As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation.",1285
23628,numpy like tensor.all and tensor.any,"As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation.",1285
23629,Bad error message when concatting different type Tensor(Variable),"FYI, the same misleading message happens when trying to concat a Variable with a Tensor.

> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:
>  * (sequence[torch.cuda.FloatTensor] seq)
>       didn't match because some of the arguments have invalid types: (list)
>  * (sequence[torch.cuda.FloatTensor] seq, int dim)
> ",2398
23630,Bad error message when concatting different type Tensor(Variable),"FYI, the same misleading message happens when trying to concat a Variable with a Tensor.

> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:
>  * (sequence[torch.cuda.FloatTensor] seq)
>       didn't match because some of the arguments have invalid types: (list)
>  * (sequence[torch.cuda.FloatTensor] seq, int dim)
> ",2398
23631,Feature request: nn.View,"You could define a module like this:
```
class View(nn.Module):
       def __init__(self):
            super(View, self).__init__()
   
        def forward(self, x):
            return x.view(-1) 
```

This flattens the input but similarly you could provide a size object.

Now you can use this as part of the model.",8610
23632,Feature request: nn.View,"You could define a module like this:
```
class View(nn.Module):
       def __init__(self):
            super(View, self).__init__()
   
        def forward(self, x):
            return x.view(-1) 
```

This flattens the input but similarly you could provide a size object.

Now you can use this as part of the model.",8610
23633,function expand_as() works incorrectly on latest Pytorch 0.2.0_1,"Replace sum(1) with sum(1, keepdim=True)

This is caused by the change of sum in 0.2.0.",5157
23634,function expand_as() works incorrectly on latest Pytorch 0.2.0_1,"Replace sum(1) with sum(1, keepdim=True)

This is caused by the change of sum in 0.2.0.",5157
23635,PyTorch 0.2.0_1 Freezes at nn.Conv2d(),"Just to Update:

It works fine if we use 'spawn' start method.

Updated Snippet:

```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.multiprocessing as mp


class Net(nn.Module):
    def __init__(self, input_size):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)

    def forward(self, input):
        print('Before Conv1 call')
        x = F.elu(self.conv1(input))
        print('After Conv1 call')
        return x


def train():
    net = Net(1)
    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))
    print('Passed!')


if __name__ == '__main__':
    mp.set_start_method('spawn')

    # directly calling the method works
    train()

    # Works fine as well with 'spawn'
    p = mp.Process(target=train, args=())
    p.start()
    p.join()
```
Output:

```
Before Conv1 call
After Conv1 call
Passed!
Before Conv1 call
After Conv1 call
Passed!
```
",4034
23636,PyTorch 0.2.0_1 Freezes at nn.Conv2d(),"Just to Update:

It works fine if we use 'spawn' start method.

Updated Snippet:

```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.multiprocessing as mp


class Net(nn.Module):
    def __init__(self, input_size):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)

    def forward(self, input):
        print('Before Conv1 call')
        x = F.elu(self.conv1(input))
        print('After Conv1 call')
        return x


def train():
    net = Net(1)
    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))
    print('Passed!')


if __name__ == '__main__':
    mp.set_start_method('spawn')

    # directly calling the method works
    train()

    # Works fine as well with 'spawn'
    p = mp.Process(target=train, args=())
    p.start()
    p.join()
```
Output:

```
Before Conv1 call
After Conv1 call
Passed!
Before Conv1 call
After Conv1 call
Passed!
```
",4034
23637,Advanced Indexing doesn't work with uniform,"This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:

> Advanced indexing always returns a copy of the data.",7684
23638,Advanced Indexing doesn't work with uniform,"This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:

> Advanced indexing always returns a copy of the data.",7684
23639,Segfault (free() on invalid pointer),"Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:

Find the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)

Now, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a 
slight ""cost"" that all the notebooks you start has torch already loaded into memory.

Just remember to remove this hack after this issue is fixed :P ;)
",4607
23640,Segfault (free() on invalid pointer),"Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:

Find the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)

Now, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a 
slight ""cost"" that all the notebooks you start has torch already loaded into memory.

Just remember to remove this hack after this issue is fixed :P ;)
",4607
23641,undefined symbol in master,you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards.,11449
23642,undefined symbol in master,you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards.,11449
23643,weight_norm assertion error when using bias=False and using cuda,"This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).

I realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)

I've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).
`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`
(Just realized the error has a typo in ""greatly"" but oh well ;))

You could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.

@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast.",7705
23644,weight_norm assertion error when using bias=False and using cuda,"This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).

I realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)

I've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).
`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`
(Just realized the error has a typo in ""greatly"" but oh well ;))

You could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.

@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast.",7705
23645,CUDA error (3): initialization error (multiprocessing),"OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.

```python
import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.nn.functional as F
from torch.autograd import Variable


def task(pid, model):
    x = Variable(torch.rand(64, 10))
    y = model(x)
    t = y.clone() * 0.99
    loss = F.smooth_l1_loss(y, t)

    # here it breaks
    loss.backward()

    print(""Process %d finished"" % pid)


if __name__ == ""__main__"":

    # comment manual_seed and the CUDA initialization error is gone.
    torch.manual_seed(23)

    net = nn.Linear(10, 4)
    net.share_memory()

    processes = []
    for pid in range(8):
        p = mp.Process(target=task, args=(pid, net))
        p.start()

    for p in processes:
        p.join()

    print(""Done."")
```

edit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal.",4552
23646,CUDA error (3): initialization error (multiprocessing),"OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.

```python
import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.nn.functional as F
from torch.autograd import Variable


def task(pid, model):
    x = Variable(torch.rand(64, 10))
    y = model(x)
    t = y.clone() * 0.99
    loss = F.smooth_l1_loss(y, t)

    # here it breaks
    loss.backward()

    print(""Process %d finished"" % pid)


if __name__ == ""__main__"":

    # comment manual_seed and the CUDA initialization error is gone.
    torch.manual_seed(23)

    net = nn.Linear(10, 4)
    net.share_memory()

    processes = []
    for pid in range(8):
        p = mp.Process(target=task, args=(pid, net))
        p.start()

    for p in processes:
        p.join()

    print(""Done."")
```

edit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal.",4552
23647,Runtime error when trying to swap first two axes of a four dimensional Tensor with torch.transpose.,This is a printing issue... We fixed it on master. Sorry about it.,7607
23648,Runtime error when trying to swap first two axes of a four dimensional Tensor with torch.transpose.,This is a printing issue... We fixed it on master. Sorry about it.,7607
23649,Transposed Convolution output shape does not take into account dilation,"I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.

Considering the code

```python.
        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)
        input_shape = (1, 1, 10, 10)
        module(torch.rand(*input_shape)).shape
```

According  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).

Modifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. ",3153
23650,Transposed Convolution output shape does not take into account dilation,"I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.

Considering the code

```python.
        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)
        input_shape = (1, 1, 10, 10)
        module(torch.rand(*input_shape)).shape
```

According  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).

Modifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. ",3153
23651,Caffe2: ONNX building with lite proto failure,"Hi @bddppq
yes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.
Thank you very much for helping!",2826
23652,Caffe2: ONNX building with lite proto failure,"Hi @bddppq
yes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.
Thank you very much for helping!",2826
23653,[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64,"Due to this bug, I was getting error in a different form. 
I was not using CUDA, but was trying to export the model using -
```
from caffe2.python.predictor import mobile_exporter

mobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) 
```
This was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).

Thanks for @laggui for the fix given above. I was circumventing this in different way.",2219
23654,[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64,"Due to this bug, I was getting error in a different form. 
I was not using CUDA, but was trying to export the model using -
```
from caffe2.python.predictor import mobile_exporter

mobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) 
```
This was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).

Thanks for @laggui for the fix given above. I was circumventing this in different way.",2219
23655,"FP16 results in ""Floating point exception""",Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions.,8050
23656,"FP16 results in ""Floating point exception""",Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions.,8050
23657,torch.argmin behaves differently on CPU and GPU,@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return,551
23658,torch.argmin behaves differently on CPU and GPU,@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return,551
23659,Library not loaded: libmklml.dylib use c++ front,@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910,745
23660,Library not loaded: libmklml.dylib use c++ front,@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910,745
23661,Batched SVD using cuSolver,Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop.,1611
23662,Batched SVD using cuSolver,Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop.,1611
23663,cross_entropy - class weights is a bit unclear,"I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss
Closing, please feel free to reopen if you have other questions. Thanks!",3002
23664,cross_entropy - class weights is a bit unclear,"I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss
Closing, please feel free to reopen if you have other questions. Thanks!",3002
23665,nn.parallel.DistributedDataParallel raise CUDA error,"The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu.",7228
23666,nn.parallel.DistributedDataParallel raise CUDA error,"The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu.",7228
23667,"""log_softmax_lastdim_kernel_impl"" not implemented for 'torch.LongTensor'",Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`),5953
23668,"""log_softmax_lastdim_kernel_impl"" not implemented for 'torch.LongTensor'",Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`),5953
23669,RNN have CuDNN error: CUDNN_STATUS_SUCCESS with Tesla T4,Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me.,8069
23670,RNN have CuDNN error: CUDNN_STATUS_SUCCESS with Tesla T4,Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me.,8069
23671,Crash when autograd function returns list instead of tuple,"Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better",4535
23672,Crash when autograd function returns list instead of tuple,"Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better",4535
23673,[feature request] Removing hooks from module,"```python
model = ...
handle = model.register_forward_hook(...)
handle.remove()
# hook will no longer trigger
```",8722
23674,[feature request] Removing hooks from module,"```python
model = ...
handle = model.register_forward_hook(...)
handle.remove()
# hook will no longer trigger
```",8722
23675,Unable to build from source,"Searching issues for ""PRId64"" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571

The problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `""%"" PRId64 ""` with `""%lld""` in the two files currently causing this error on master. A better way is to find the proper place for addding

```
#define __STDC_FORMAT_MACROS
#include <inttypes.h>
```
as described in the issue linked above.",5756
23676,Unable to build from source,"Searching issues for ""PRId64"" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571

The problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `""%"" PRId64 ""` with `""%lld""` in the two files currently causing this error on master. A better way is to find the proper place for addding

```
#define __STDC_FORMAT_MACROS
#include <inttypes.h>
```
as described in the issue linked above.",5756
23677,"Give a better error when we run out of shared memory, instead of ""RuntimeError: DataLoader worker (pid 13) is killed by signal: Bus error.""","Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...

If anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`.",2169
23678,"Give a better error when we run out of shared memory, instead of ""RuntimeError: DataLoader worker (pid 13) is killed by signal: Bus error.""","Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...

If anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`.",2169
23679,RuntimeError: cuda runtime error (38),"Problem solved.
I made a very stupid mistake.
There is a line in the head which is
os.environ[""CUDA_VISIBLE_DEVICES""] = '3'.
I did not notice it first time I ran the program and got another error. I revised it to os.environ[""CUDA_VISIBLE_DEVICES""] = '0' without restarting the kernel. Then l got this error.

Restart the program with os.environ[""CUDA_VISIBLE_DEVICES""] = '0'can solve the problem.",4904
23680,RuntimeError: cuda runtime error (38),"Problem solved.
I made a very stupid mistake.
There is a line in the head which is
os.environ[""CUDA_VISIBLE_DEVICES""] = '3'.
I did not notice it first time I ran the program and got another error. I revised it to os.environ[""CUDA_VISIBLE_DEVICES""] = '0' without restarting the kernel. Then l got this error.

Restart the program with os.environ[""CUDA_VISIBLE_DEVICES""] = '0'can solve the problem.",4904
23681,np.random generates the same random numbers for each data batch,"This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn.",7629
23682,np.random generates the same random numbers for each data batch,"This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn.",7629
23683,Potential bug when sampling from categorical distribution,"On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: 

```
import torch.distributions as dis
import torch
import numpy as np

G = 3
D = 2
p_dG = torch.Tensor(G, D)
p_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])
p_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])

p_dg = p_dG[:, 0]
z = p_dg.multinomial(250, replacement=True)
true_z_np = z.numpy()
v, c = np.unique(true_z_np, return_counts=True)
print(v)
print(c)
```
and also
```
z = torch.multinomial(p_dg, 250, replacement=True)
true_z_np = z.numpy()
v, c = np.unique(true_z_np, return_counts=True)
print(v)
print(c)
```
",4624
23684,Potential bug when sampling from categorical distribution,"On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: 

```
import torch.distributions as dis
import torch
import numpy as np

G = 3
D = 2
p_dG = torch.Tensor(G, D)
p_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])
p_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])

p_dg = p_dG[:, 0]
z = p_dg.multinomial(250, replacement=True)
true_z_np = z.numpy()
v, c = np.unique(true_z_np, return_counts=True)
print(v)
print(c)
```
and also
```
z = torch.multinomial(p_dg, 250, replacement=True)
true_z_np = z.numpy()
v, c = np.unique(true_z_np, return_counts=True)
print(v)
print(c)
```
",4624
23685,Recent git pull breaks working pytorch build,"after a git pull, do:

```
git submodule update --init --recursive
```

or whatever it takes to update your submodules to their marked commits.

if that doesn't work, just do a fresh git clone.",8841
23686,Recent git pull breaks working pytorch build,"after a git pull, do:

```
git submodule update --init --recursive
```

or whatever it takes to update your submodules to their marked commits.

if that doesn't work, just do a fresh git clone.",8841
23687,MKL Error when import torch after installing 0.3.0 on CentOS,"Hi @malbergo,
My solution was quite hacky. 
I add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.

I still not quite like it because it should be automatically triggered when I call `source activate myenv`",2835
23688,MKL Error when import torch after installing 0.3.0 on CentOS,"Hi @malbergo,
My solution was quite hacky. 
I add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.

I still not quite like it because it should be automatically triggered when I call `source activate myenv`",2835
23689,Segmentation fault (core dumped) on LSTMCell on pytorch,"Hi @saitarslanboun 
Some sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.

In your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.

Now, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:
```
# create lstmcell module
lstm = nn.LSTM(embed_size * 2, hidden_size)

# transpose from batch first to batch second for recurrent layers
x = x.transpose(0, 1).contiguous()
output, (h_t, c_t) = lstm(x)
```

Now let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:
```
# create lstmcell module
lstm = nn.LSTMCell(embed_size * 2, hidden_size)

# initialize h,c outside for loop
h_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())
c_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())

# loop over time steps
for time_step in range(x.size(1)):
    x_t = x[:, time_step, :]
    (h_t, c_t) = lstm(x_t, (h_t, c_t))
```

From what I have understood from your snippet, there are a few potential mistakes:
- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`
- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong

Let me know if this helps you and/or if there are any details in your question that I have misunderstood.",2842
23690,Segmentation fault (core dumped) on LSTMCell on pytorch,"Hi @saitarslanboun 
Some sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.

In your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.

Now, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:
```
# create lstmcell module
lstm = nn.LSTM(embed_size * 2, hidden_size)

# transpose from batch first to batch second for recurrent layers
x = x.transpose(0, 1).contiguous()
output, (h_t, c_t) = lstm(x)
```

Now let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:
```
# create lstmcell module
lstm = nn.LSTMCell(embed_size * 2, hidden_size)

# initialize h,c outside for loop
h_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())
c_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())

# loop over time steps
for time_step in range(x.size(1)):
    x_t = x[:, time_step, :]
    (h_t, c_t) = lstm(x_t, (h_t, c_t))
```

From what I have understood from your snippet, there are a few potential mistakes:
- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`
- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong

Let me know if this helps you and/or if there are any details in your question that I have misunderstood.",2842
23691,ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory,"Ok, great. For now i fixed it with: 
> conda install -c anaconda cudatoolkit==9.0",4602
23692,ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory,"Ok, great. For now i fixed it with: 
> conda install -c anaconda cudatoolkit==9.0",4602
23693,`Normal` distribution: Gaussian policy with zero gradient of mean head,"Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)",8475
23694,`Normal` distribution: Gaussian policy with zero gradient of mean head,"Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)",8475
23695,Wrong torch.svd Calculation Result,"Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem.",8666
23696,Wrong torch.svd Calculation Result,"Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem.",8666
23697,Issue with pytorch update (version 0.4.1) with cuda9.1.85,"@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it.",563
23698,Issue with pytorch update (version 0.4.1) with cuda9.1.85,"@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it.",563
23699,Incorrect PROTOBUF version,"Solved by doing
```
conda uninstall libprotobuf
```",5957
23700,Incorrect PROTOBUF version,"Solved by doing
```
conda uninstall libprotobuf
```",5957
23701,implement dirichlet / beta GPU grad,Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in.,4480
23702,implement dirichlet / beta GPU grad,Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in.,4480
23703,seg fault on import caffe2.python.onnx.backend,"Yes this works. Thanks for the fix. 
```
>>> import caffe2.python.onnx.backend as backend
>>> import numpy as np
```
",8490
23704,seg fault on import caffe2.python.onnx.backend,"Yes this works. Thanks for the fix. 
```
>>> import caffe2.python.onnx.backend as backend
>>> import numpy as np
```
",8490
23705,cufft errors after lots of plan generation,"As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.

Please let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. ",1312
23706,cufft errors after lots of plan generation,"As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.

Please let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. ",1312
23707,Backward through sparse_coo_tensor,"the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients.",10854
23708,Backward through sparse_coo_tensor,"the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients.",10854
23709,Inexplicable `test_variant_consistency_{eager/jit}_index_select` failure  for `torch.bfloat16` and `torch.float16`,"Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):
```
In [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           

In [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 

In [28]: torch.index_select(x,0,i)                                                                                                                                                                             
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-28-573672115355> in <module>
----> 1 torch.index_select(x,0,i)

RuntimeError: ""index_select"" not implemented for 'Half'

In [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               
Out[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)

In [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               

In [31]: i=torch.tensor([0,2])                                                                                                                                                                                 

In [32]: torch.index_select(x,0,i)                                                                                                                                                                             
Out[32]: 
tensor([[ 0.6143,  0.4473,  0.3953, -0.4465],
        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)

In [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             
Out[33]: 
tensor([[ 0.6143,  0.4473,  0.3953, -0.4465],
        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',
       dtype=torch.float16)

```
",1155
23710,Inexplicable `test_variant_consistency_{eager/jit}_index_select` failure  for `torch.bfloat16` and `torch.float16`,"Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):
```
In [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           

In [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 

In [28]: torch.index_select(x,0,i)                                                                                                                                                                             
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-28-573672115355> in <module>
----> 1 torch.index_select(x,0,i)

RuntimeError: ""index_select"" not implemented for 'Half'

In [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               
Out[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)

In [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               

In [31]: i=torch.tensor([0,2])                                                                                                                                                                                 

In [32]: torch.index_select(x,0,i)                                                                                                                                                                             
Out[32]: 
tensor([[ 0.6143,  0.4473,  0.3953, -0.4465],
        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)

In [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             
Out[33]: 
tensor([[ 0.6143,  0.4473,  0.3953, -0.4465],
        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',
       dtype=torch.float16)

```
",1155
23711,amp does not work with LayerNorm gradient checkpointing,"Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:
```
>>> import sys
>>> import torch
>>> sys.modules[""torch""]
```
and edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[""torch""]` points to.",1523
23712,amp does not work with LayerNorm gradient checkpointing,"Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:
```
>>> import sys
>>> import torch
>>> sys.modules[""torch""]
```
and edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[""torch""]` points to.",1523
23713,Printing should not have (bad) autograd side effects,"It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.

In fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.

In the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:
* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`
* `apply()` in `torch/csrc/autograd/custom_function.h`
* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`

Of course, the inevitable check for grad mode enabled still happens at some point.

With this in mind, I propose that we:
1. Remove the grad enabled check / empty list return logic from `collect_next_edges`
2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled

While I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.

If this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.

Thoughts on this? @albanD @gchanan ",4013
23714,Printing should not have (bad) autograd side effects,"It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.

In fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.

In the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:
* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`
* `apply()` in `torch/csrc/autograd/custom_function.h`
* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`

Of course, the inevitable check for grad mode enabled still happens at some point.

With this in mind, I propose that we:
1. Remove the grad enabled check / empty list return logic from `collect_next_edges`
2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled

While I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.

If this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.

Thoughts on this? @albanD @gchanan ",4013
23715,Different Dice accuracy  using DataParallel,It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm.,3914
23716,Different Dice accuracy  using DataParallel,It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm.,3914
23717,rfftn / irfftn is not functioning properly,"This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this.",7703
23718,rfftn / irfftn is not functioning properly,"This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this.",7703
23719,torch.autograd.jacobian returns tensors with all zeros,This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you.,7663
23720,torch.autograd.jacobian returns tensors with all zeros,This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you.,7663
23721,[Bug] Sometimes gradient doesn't back-propagate after view,"I don't think this is a bug. He is checking the gradient of a non-leaf variable.
Change the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good.",3065
23722,[Bug] Sometimes gradient doesn't back-propagate after view,"I don't think this is a bug. He is checking the gradient of a non-leaf variable.
Change the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good.",3065
23723,pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.,"I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.

Can you run your python script with
```python
CUDA_VISIBLE_DEVICES=1 python my_script.py
```
while you don't update pytorch?",3005
23724,pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.,"I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.

Can you run your python script with
```python
CUDA_VISIBLE_DEVICES=1 python my_script.py
```
while you don't update pytorch?",3005
23725,roi_crop (from Detectron.pytorch) building consistently fails,"@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=""-std=c99` before `sh make.sh`. ",669
23726,roi_crop (from Detectron.pytorch) building consistently fails,"@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=""-std=c99` before `sh make.sh`. ",669
23727,Crash with SIGFPE due to unhandled cases in distributions.MultivariateNormal,"There needs to be two changes in the code:
1. `bvec.size(-1)` to `bmat.size(-1)` in
https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32

2. `*shape` to `shape` in
https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173

The sample function works fine. The distributions test suite passes as well.

```python
>>> import torch
>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))
>>> m.sample()
tensor([-0.2011])
```",7420
23728,Crash with SIGFPE due to unhandled cases in distributions.MultivariateNormal,"There needs to be two changes in the code:
1. `bvec.size(-1)` to `bmat.size(-1)` in
https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32

2. `*shape` to `shape` in
https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173

The sample function works fine. The distributions test suite passes as well.

```python
>>> import torch
>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))
>>> m.sample()
tensor([-0.2011])
```",7420
23729,Improvement to unsupported backward OpInfo test,,10891
23730,error: unknown type name 'mkldnn_shuffle_desc_t',"```
cd third_party/ideep # this is on branch 
 grokmachine@Dendis-MacBook-Pro  | ~/dev/facebook/pytorch/third_party/ideep     remotes/origin/mkldnn_0.17  git submodule update --init --recursive
 grokmachine@Dendis-MacBook-Pro  | ~/dev/facebook/pytorch/third_party/ideep     remotes/origin/mkldnn_0.17  git checkout master
M        mkl-dnn
Previous HEAD position was d06f361 Fix klocwork issues
Switched to branch 'master'
Your branch is up to date with 'origin/master'.
```
then I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install`",4306
23731,torch.script crashes with Union type,,7484
23732,Improve torch.linalg documentation,Some Unicode symbols (for example â€œâ‚™â€,7936
23733,,"A *different* option is to replace the current:      def forward(self, *input: Any, **kwargs: Any) -> T_co: ...  # type: ignore

with the more permissive:

    forward: Callable[..., T_co]

Because mypy requires that the subclass method's input types be ""wider"" than their superclass method's input types, the existing signature will always fail to type check (i.e. they can vary contravariantly).
And nothing is more expansive than Any.


This change would ensure mypy doesn't complain to you about forward() typing, which resolves the problem nicely. I'll put a PR when I get a sec. ",9951
23734,pack_padded_sequence output is not as expected,,7646
23735,OpenCV get Stuck in Transform when used in DataLoader,,2708
23736,torch.manual_seed raises an error in forked processes,,4330
23737,segfault in python multithreaded setting,,4287
23738,share_memory_ needs a check for empty tensor,,3246
23739,"get error when load checkpoint and finetune with optimizer, RuntimeError: expected backend CPU and dtype Float but got backend CUDA and dtype Float",,11479
23740,"Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice. You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation. Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626",,9560
23741,CUDA for Pytorch,,4972
23742,Turn on exact_dtype by default on test_sparse.py,,64
23743,Any reason to keep AT_WARN?,,8454
23744,"I could male libtorch dll ,lib from soruce in Windows , but build error happened in build for mnist.cpp",,1396
23745,tensor._copy should not copy when src and dst point to the same memory,,8085
23746,Different gradients with batch norm on GPU and CPU,,7160
23747,"RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::thnn_conv2d_forward' is only available for these backends: [CPUTensorId, VariableTensorId].",,3077
23748,Could you please add a torch setting like torch.set_checkpoint_enabled(True),,6796
23749,DISABLED test_keypoint_rcnn (__main__.TestONNXRuntime_opset11),,573
23750,ImportError: cannot import name 'GoogLeNetOutputs' from 'torchvision.models.googlenet',,201
23751,torch.distributions.Categorical.sample uses unnecessary huge amount of memory,,614
23752,[FR] More consistent matrix norm for torch.norm,,3911
23753,Pylint Error `torch.tensor is not callable`,,7500
23754,Transformer Encoder Layer with src_key_padding makes NaN,,7664
23755,ONNX Exporter Supporting ROIAlign,,7838
23756,Can pytorch 1.2 support cuda 9.0?,,2999
23757,in-place operations over ranges with overlap can lead to unexpected results.,,7680
23758,Let new and existing backends easily register for testing,,65
23759,Run time error with backward() after upgrading to pytorch 1.2.0,,3247
23760,[Windows] Including Windows.h before LibTorch headers causes compilation errors,,3291
23761,Issue with maxInt in utils.py,,9741
23762,SobolEngine should use random seed by default when scrambling,,482
23763,libtorch_python.so: undefined symbol: _PyThreadState_UncheckedGet,,143
23764,Implement RAdam optimizer ?,,3145
23765,Lazy conjugated tensor is not numpy interoperable,,226
23766,add `--init_method` to `torch.distributed.launch`,,2814
23767,`OMP_NUM_THREADS` doesn't work as intended in some cases,,3036
23768,Compilation issue with CUDA 92,,4851
23769,DISABLED test_backward_rref_nested (__main__.ProcessGroupDistAutogradTestWithSpawn),,234
23770,Annoying warning with nn.MaxPool2d,,2764
23771,Warning with torch::nn::init::orthogonal_ with LibTorch 1.9.0,,9755
23772,torch.load issue on different folder,,10270
23773,"torch.package: Module did not match against any action pattern. Extern, mock, or intern it.",,2871
23774,[C++] grad_input may have different memory_format from grad_output in structured kernels,,7227
23775,Docs of torch.Tensor.permute refer to docs of torch.permute for which do not exist,,487
23776,"On CPU, vectorized float tensor addition might be slower than unvectorized float tensor addition",,3983
23777,[torch.profiler] non-context manager use,,11361
23778,Multiple failures in distributed/elastic tests,,7849
23779,make torch.all and torch.any support all dtypes,,7598
23780,fx: assertions based on tensor shapes are not symbolically traceable,,3694
23781,`emit_nvtx` example in docs seems to be broken.,,3242
23782,libtorch load trace module failed in Qt5,,596
23783,fx: unable to symbolically trace model with torch.zeroes(Proxy),,4593
23784,a new type of nn.Module param for large transient data (that doesn't get saved),,2200
23785,Inconsistent result from FBGEMM and QNNPACK quantization backends,,3071
23786,The support for 3080 or 3090,,3033
23787,GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.,,2991
23788,"Whether create_graph=true or false, torch.autograd.grad returns different gradients.",,3138
23789,Error in test suite: an illegal memory access was encountered,,8238
23790,[JIT] Support for CUDA streams in TorchScript,,7837
23791,TestTensorExprFuser.test_unused is failing in release/1.6 branch.,,9036
23792,torch.distributed NCCL backend does not support bitwise reduction ops,,5997
23793,"ConvTranspose1d extremely slow on GPU (T4), even slower than CPU",,2201
23794,Searchsorted differentiable?,,1136
23795,LowRankMultivariateNormal creates Illegal Memory Access in magma_spotrf_batched,,4162
23796,Is it planning to support nn.Embeddings quantization?,,8256
23797,Memory corruption and crash in quantized convolution with small inputs,,9544
23798,[RFC] torch.distributed.app (role-based + higher-level RPC APIs),,6797
23799,Replace blacklist/whitelist throughout the PyTorch codebase,,2897
23800,received 0 items of ancdata when appending dic elements to a list,,10888
23801,test_nn failures on power,,535
23802,Adam implementation different from paper,,261
23803,Update SobolEngine direction numbers,,481
23804,Unflatten module for nn.Sequential,,5710
23805,Dataloader in DistributedDataParallel hangs,,10740
23806,RuntimeError: nvrtc: error: invalid value for --gpu-architecture (-arch),,592
23807,Incorrect info about overriding torch tensors in version 1.7.0,,8530
23808,"Build from source: CMake not using compilers installed by Anaconda, fails with stl_pair.h errors",,4456
23809,Error in test_quantize - missing FBGEMM implementation,,6823
23810,Core dump when checking that basic CNN works (Python 3.9),,7243
23811,Incorrect result of `fmod` and `remainder` operator.,,4196
23812,Support `torch.linalg.norm` for complex tensors on both CPU and CUDA,,656
23813,"I found such a strange thing. Add code anywhere in the definition model that is not associated with the current model, such as torch.nn.Linear () will affect the model",,2233
23814,Linker error when building from source with CUDA 11.0,,7161
23815,INTERNAL ASSERT FAILED for `S.unique().shape`,,3330
23816,Tensor iteration changes behavior in 1.7 with no hint,,3169
23817,Using model.eval() with batchnorm gives high error,,3971
23818,"packaging question, setup.py egg_info relies on submodule",,2955
23819,[feature proposal] An effortless way of loading pre-trained embeddings,,3131
23820,Can't Resize_() IntTensor,,4865
23821,[docs] Missing docs online for F.softplus,,4063
23822,release 0.3.1 checklist,,4219
23823,ModuleNotFoundError: No module named 'torch',,733
23824,Inconsistency between advanced indexing of Tensors and Variables,,4606
23825,remove cuda capability 3.0 and 5.0 from binaries,,1921
23826,Using CUDNN gives bad results.,,3902
23827,Make UndefinedTensor's data_ptr() return nullptr,,4225
23828,CUDNN_LIBRARY selection,,3212
23829,"""CUDA error: out of memory"" for the last Batch",,3133
23830,DataParallel module leaks memory,,621
23831,"Support torch.abs for ByteTensor, CharTensor",,600
23832,Some operations after `.copy_()` results in error in backward,,98
23833,Incorrect NaN gradient from distribution.Normal.log_prob when using subset,,3258
23834,The font size of equations in tutorial is too small,,550
23835,Pytorch 1.0.0 with Serverless and AWS Lambda timeout; Error in cpuinfo,,1955
23836,"Problem ""torch.jit.trace()""ing pretrained Inception_v3 model",,511
23837,Can't open x64.obj while installing pytorch with win10,,4852
23838,torch.tril and torch.triu produce incorrect results with device='cuda',,755
23839,backward pass slow when using Conv2d,,6975
23840,Pytorch1.3 can't be compiled successfully with Python3.8.0,,7610
23841,PyTorch1.3 cannot find kernel to dispatch when runing a Quantization Model,,3992
23842,"torch.var fails to pass ""dim"" as tuple ints",,11467
23843,Store and retry futures that are created when nodes cleanup DistAutogradContext,,1338
23844,Implement destructor for ProcessGroupAgent that cleans up resources and doesn't wait,,686
23845,ROCm CI can fail test_sync_params_with_buffers,,10886
23846,RuntimeError: CUDA out of memory.,,3282
23847,"`default_qconfig` doesn't exist as an attribute, so it is better to label it as ``default_qconfig``",,272
23848,Master process finishes and leave workers hanging when master and worker has different gpu numbers,,3365
23849,[ONNX] Export Pad(11) opset11,,564
23850,Can not load model,,3294
23851,Module 'torch.nn' has no attribute 'backends',,618
23852,Adding a .fit() method to nn.Module,,4850
23853,Torch.log2 + Torch.round() different and strange behaviour,,3056
23854,DDP should set grad to None for (globally) unused params,,676
23855,RRef.to_here() and local_value() should return future instead of blocking,,49
23856,`CUDNN_STATUS_BAD_PARAM` reported during traced model execution,,266
23857,"What values are specified to replaceNaN, positive infinity, and negative infinity values in input?","bynan,posinf, andneginf",5158
23858,"By default,NaN is replaced with what value?",zero,5158
23859,What values are used to replace negative infinity values in input?,"bynan,posinf, andneginf",5158
23860,What is the default value for negative infinity?,the least finite value,5158
23861,What is the value to replace positive infinity values with?,posinf,9966
23862,What is the value to replace negative infinity values with?,neginf,9966
23863,What is the greatest finite value represented by?,posinf,9666
23864,What does Alias for torch.linalg.slogdet() do?,Performs a batch matrix-matrix product of matrices stored in input and mat2. Returns the matrix product of the NNN2-D tensors,4775
23865,Returns the what solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization,LU,4775
23866,Performs what of matrices stored in input and mat2?,a batch matrix-matrix product,4775
23867,Computes the dot product for 1D tensors. Computes the eigenvalues and eigenvector,Alias of torch.outer(),4775
23868,Computes the dot product for what?,1D tensors,4775
23869,What is the name of the function that computes the dot product for 1D tensors?,Alias of torch.outer(),4775
23870,What is a low-level function for calling LAPACK’s geqrf directly?,eigenvalues and eigenvectors of a real square matrix,4775
23871,What does TORCH.DIV do?,Divides each element of the input input by the corresponding element of other.,2167
23872,Always promotes integer types to what type?,default scalar type,6139
23873,Performs what type of rounding if both input and other are integer types?,no rounding,6116
23874,What is equivalent to NumPy's np.true_divide?,TORCH.DIV,6174
23875,What is the Python equivalent to floor division in Pytorch ?,TORCH.DIV,8035
23876,What type of integer division is trunc equivalent to?,C-style,8035
23877,What is equivalent to the /operator in Python and NumPy'snp.true_divide?,true division,8035
23878,What rounds the results of the division towards zero?,trunc,8035
23879,What type of rounding rounds the results of the division down?,floor,8035
23880,What does the // operator and NumPy'snp.floor_divide equivalent to?,floor division,8035
23881,What type of rounding does None perform?,no rounding,8035
23882,What type of division is equivalent to trunc?,C-style integer division,8035
23883,What is the name of the rounding that rounds the results of a division down?,floor,8035
23884,What is equivalent to floor division in Python?,NumPy’snp.floor_divide,8035
23885,What is the case when the inputs are promoted to the default scalar type?,if both input and other are integer types,9650
23886,What is equivalent to true division in Python?,C-style integer division,10622
23887,What is the Pytorch equivalent to TORCH.DIV to in Python?,true division,10622
23888,What is the default behavior of rounding in TORCH.DIV?,None,10622
23889,"Does rounding_mode(str,optional) perform any rounding?",no,10622
23890,"When does rounding_mode(str,optional) perform no rounding?",if both input and other are integer types,10622
23891,What is equivalent to the /operator in Python?,true division,10622
23892,What happens if both input and otherare integer types?,Performs no rounding,10333
23893,What is the name of Alias for torch.trunc?,torch.fix,8333
23894,What does Alias for torch.le stand for?,TORCH.LESS_EQUAL,36
23895,What creates a new tensor?,TORCH.COLUMN_STACK,6173
23896,What is each zero or one dimensional tensortin tensors first reshaped into?,"a(t.numel(),1)column",2000
23897,"What does tensors(sequence of Tensors) concatenate out(Tensor,optional",output tensor,2000
23898,What TORCH.IS_TENSOR does?,Returns True if obj is a PyTorch tensor.,5218
23899,What TORCH.IS_TENSOR return?,True,5218
23900,What is the function that returns true if obj is a PyTorch tensor?,TORCH.IS_TENSOR,5218
23901,What is the return value for if obj is a PyTorch tensor?,True,5218
23902,What is the function that returns True if obj is a PyTorch tensor?,"doingisinstance(obj,Tensor)",5218
23903,What is better for typechecking with mypy?,isinstance,5217
23904,What is that isinstance check better for?,typechecking with mypy,5217
23905,What does set_flush_denormal() do?,Disables denormal floating numbers on CPU,2154
23906,On what architectures is set_flush_denormal() only supported?,x86 architectures,2154
23907,What controls whether to enable flush denormal mode or not?,mode(bool),2154
23908,What does set_flush_denormal() do on CPU?,Disables denormal floating numbers,2154
23909,Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on what convention?,Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.,6087
23910,What does TORCH.SPECIAL do?,"Computes the entropy on input (as defined below), elementwise.",10348
23911,What does TORCH.BARTLETT_WINDOW do?,The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(),7133
23912,What is the Bartlett window function?,"The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact 	ext{window_length} + 1window_length+1. Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).",7134
23913,"What is the same as totorch.bartlett_window(L+1,periodic=False)?",havetorch.bartlett_window,7143
23914,What does TORCH.BARTLETT_WINDOW returns?,"A 1-D tensor of size (	ext{window_length},)(window_length,) containing the window",3653
23915,What is the desired data type of returned tensor?,"dtype(torch.dtype, optional)",4476
23916,What is non-integerstep subject to when comparing againstend?,floating point rounding errors,4476
23917,What is the starting value for the set of points?,start(Number),4476
23918,What is the ending value for the set of points?,end(Number),4476
23919,When the data type is inferred from the other input arguments?,If dtype is not given,4476
23920,"What is the default dtype inferred to if any ofstart,end, orstopare floating-point?",betorch.int64,4476
23921,What is the default for the layout of the returned Tensor?,Default:torch.strided,4476
23922,What is the length of the window that contains a single value?,window_length=1=1=1,3550
23923,What is the size of returned window periodic?,window_length,11408
23924,What is the desired layout of returned window tensor?,layout,11408
23925,What type of layout is supported?,Only torch.strided,11408
23926,What is the desired device of returned tensor by torch.bartlett_window?,"device(torch.device, optional)",11408
23927,What should record operations on the returned tensor?,autograd,11408
23928,What is the default value for autograd to record operations on the returned tensor?,Default:False,11408
23929,What is the library part of?,thePyTorchproject,7709
23930,What is the release status of PyTorch?,Stable,7709
23931,What is the library part of?,thePyTorchproject,7709
23932,What is PyTorch?,open source machine learning framework,7711
23933,What does PyTorch expect to maintain?,backwards compatibility,7711
23934,What project is this library part of?,thePyTorchproject,7711
23935,What do we expect to maintain?,backwards compatibility,7711
23936,What are we not committing to?,backwards compatibility,7711
23937,We are not committing to what?,backwards compatibility,7711
23938,"Thetorchaudiopackage consists of I/O, common audio transformations, and what?",popular datasets,7711
23939,"What consists of I/O, popular datasets and common audio transformations?",Thetorchaudiopackage,7711
23940,What type of machine learning framework is PyTorch?,open source,7708
23941,What is the release status of the features described in this documentation?,Stable,2410
23942,What is the release status of features described in this documentation?,Stable,2412
23943,What may the API change based on?,user feedback,2412
23944,What classification are Beta features committed to seeing through?,"Features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete.",2412
23945,What libraries are included in Thetorchaudiopackage?,Package Reference PyTorch Libraries,2412
23946,What type of features will be maintained long-term?,Stable,6053
23947,How long will these features be maintained?,long-term,6053
23948,Why are features tagged as Beta?,the performance needs to improve,1441
23949,What classification do we commit to seeing a feature through to?,Stable,1441
23950,What are features sometimes hidden behind?,run-time flags,4913
23951,What are features at an early stage for?,feedback and testing,4913
23952,What does Thetorchaudiopackage consist of?,"I/O, common audio transformations, popular datasets",4913
23953,What are PyPI and Conda features sometimes hidden behind?,early stage for feedback and testing.,4913
23954,What are Prototype features at an early stage for?,feedback and testing,4913
23955,What is the LU solve of the linear systemAx=bAx = bAx=busing?,LU factorization of A fromtorch.lu(),5511
23956,"This function supports float,double, and what other type of input?",cfloat,5511
23957,What types of types does this function support?,"float,double,cfloatandcdoubledtypes forinput",5513
23958,What is the pivoted LU factorization of A fromtorch.lu()?,LU_data(Tensor),5513
23959,"What is the RHS tensor of size(,m,k)(*, m, k)(,m",b(Tensor),8939
23960,"What is the pivoted LU factorization of A fromtorch.lu()of size(,m,m)(*,",LU_data,8939
23961,"What are the pivots of the LU factorization fromtorch.lu()of size(,m)(*, m",LU_pivots,8939
23962,What must the batch dimensions ofLU_pivots be equal to?,the batch dimensions ofLU_data,8939
23963,Returns what of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A,LU solve,1724
23964,"Out(Tensor,optional) – what?",output tensor,10351
23965,Returns the indices that sort a tensor along a given dimension in what order?,ascending order by value,5567
23966,What does the documentation of bytorch.sort() provide?,exact semantics,5567
23967,What do you need to know about the second value returned bytorch.sort()?,semantics,5567
23968,What is argsort?,argsort Returns the indices that sort a tensor along a given dimension in ascending order by value,5567
23969,argsort Returns the indices that sort a tensor along a given dimension in what order?,ascending order by value,5567
23970,What is the second value returned?,bytorch.sort(),5567
23971,What is the dimension to sort along descending?,"dim(int,optional)",5567
23972,What is returned bytorch.sort()?,second value,5567
23973,"What is the dimension to sort along descending(bool,optional)?","dim(int,optional)",5567
23974,What returns the cumulative maximum of elements of input in the dimensiondim?,a namedtuple,5316
23975,What is the index location of each maximum value found in the dimensiondim?,Andindices,5319
23976,What type of window types are supported?,floating point types,11407
23977,What returns a window to be used as periodic function?,torch.bartlett_window,10404
23978,What types are supported only by torch.bartlett_window?,floating point types,9219
23979,What is supported for dense layout by torch.bartlett_window?,Only torch.strided,9775
23980,"A 1-D tensor of size(window_length,)(textwindow_length,)(window_",window Tensor,9169
23981,What is the default setting for autograd to record operations on the returned tensor?,False,9169
23982,"What is a window Tensor of size(window_length,)(textwindow_length,)(wind",1-D tensor,9169
23983,What does if None use for the default tensor type?,current device,9266
23984,What is step(Number)?,the gap between each pair of adjacent points,9266
23985,What happens when a data type is inferred from the other input arguments?,If dtype is not given,9266
23986,What is the default layout of returned tensor?,Default:torch.strided,9266
23987,Returns what of a given tensor?,matrix norm or vector norm,11
23988,How does Glorot describe training deep feedforward neural networks?,using a uniform distribution,8997
23989,"What function does a 2-dimensionaltorch.Tensor Examples Fills the 3, 4, 5-dimensional inputTens",Dirac delta function,8997
23990,"What is it called when a tensor has values sampled fromU(a,a)mathcalU(",Glorot initialization,8997
23991,What is the term for a tensor?,tensor,8933
23992,"In what year did Glorot, X. & Bengio, Y. begin training deep feedforward neural networks?",2010,10830
23993,What is a Tensor gain?,an optional scaling factor,10830
23994,"Fills the 3, 4, 5-dimensional input Tensor with what function?",Dirac delta function,2420
23995,What is another name for Glorot initialization?,"torch.nn.init.xavier_uniform_(tensor, gain=1.0)",10821
23996,What is a tensor gain?,an optional scaling factor,10821
23997,"Glorot, X. & Bengio, Y. (2010), using what distribution?",normal distribution,10821
23998,What does tensor fill the 2-dimensional input Tensor with?,identity matrix,10821
23999,"What function does tensor fill the 3, 4, 5-dimensional input Tensor with?",Dirac delta function,10821
24000,"What is the resulting tensor's values sampled fromN(0,std2)mathcalN(",Glorot initialization,10821
24001,What is an optional tensor gain?,scaling factor,10821
24002,What is batch1(Tensor)?,first batch of matrices to be multiplied batch2(Tensor),4765
24003,What is an example of a batch matrix-matrix product?,Example,4765
24004,What does this class wrap?,arbitrary optim.Optimizer,7475
24005,What happens after parameters are updated locally?,each rank will broadcast its parameters to all other peers,7475
24006,What kind of algorithm does ZeroRedundancyOptimizer use?,greedy,7475
24007,What is the partition of each parameter at each rank?,arbitrary,7475
24008,Restore the global parameter groups. to(int) – the rank that receives the global states. (default: 0) ,Restore the global parameter groups,7475
24009,Each parameter belongs to what?,a single rank,8691
24010,Params(Iterable) – anIterableof torch.Tensors optimizer_class(tor,local optimizer,8691
24011,"When disabled, each what will be communicated separately?",individual parameter,8691
24012,What uses a greedy algorithm to pack a number of parameters at each rank?,ZeroRedundancyOptimizer,8691
24013,What is the partition of the algorithm?,arbitrary,8691
24014,What is the default behavior of ZeroRedundancyOptimizer?,all trailing arguments will be forwarded to the given optimizer,8691
24015,state_dict(dict) – what state should be an object returned from a call tostate_dict() Gets this rank’sstate,optimizer state,8691
24016,What should state_dict(dict) be?,an object returned from a call tostate_dict() Gets this rank’sstate_dict,8691
24017,What is a list ofparam_groups?,a list ofparam_groups,8691
24018,What corresponds to the param_groups for a rank?,Element 0,8691
24019,"What group(ProcessGroup, optional) –torch.distributedProcessGroup?","group(ProcessGroup, optional) –torch.distributedProcessGroup",10324
24020,"When enabled, parameters will be packed into what?",larger buckets,10324
24021,What is the default in torch.distributed.optim.ZeroRedundancyOptimizer?,all trailing arguments will be forwarded to the given optimizer,10324
24022,How many update the consolidated state_dict list?,one per rank,10324
24023,When can adding a param group to theOptimizersparam_groups be useful?,when fine tuning a pre-trained network,10324
24024,How many consolidated state_dict lists are updated per rank?,one per rank,10324
24025,To(int) – the rank that receives the global states. (default: what?,0,10324
24026,"When disabled, what happens to each individual parameter?",each individual parameter will be communicated separately,10324
24027,State_dict(dict) – optimizer state. Should be what?,an object returned from a call tostate_dict() Gets this rank’sstate_dict,10324
24028,Returns what for a given rank?,local_state_dict,10324
24029,What is the class of the local optimizer?,optimizer_class,10324
24030,What should be optimized along with group specific optimization options?,Tensors,10324
24031,"What corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for a",Element 0,10324
24032,globalstate_dict is what?,last known global optimizer state,10324
24033,What performs performs?,Performs,10324
24034,When can a param group be useful?,when fine tuning a pre-trained network,10391
24035,What is params(Iterable) – anIterableof torch.Tensors?,optimizer,10391
24036,"Update the consolidated state_dict list, how many per rank?",one per rank,10391
24037,Restore the global parameter groups as well as the what?,shard,10391
24038,state_dict(dict) – what?,optimizer state,10391
24039,"Which element corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for ",Element 0,10391
24040,What package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors?,torch,7332
24041,What counterpart does the torch package have?,CUDA,7332
24042,What does the torch package return?,total number of elements in theinputtensor,7332
24043,What does the torch package construct?,a tensor withdata,7332
24044,What does the COO(rdinate) format contain?,specified values at the givenindices,7332
24045,What does the torch package do?,Create,7332
24046,Returns what if theinputis a single element tensor?,True if theinputis a single element tensor,5245
24047,Creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.rand,Random sampling,5245
24048,"Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize. Returns ",Returns a tensor,5245
24049,Get the what?,current default floating pointtorch.dtype,5243
24050,Random sampling creation ops are listed under Random samplingand include:torch.rand()torch.rand()torch.,Random sampling creation ops,2152
24051,Constructs what?,a tensor withdata,1854
24052,"What type of tensor of sizesteps whose values are evenly spaced from start to end, inclusive?",one-dimensional,1854
24053,"What returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize?",a tensor filled with the scalar value0,5247
24054,What does a tensor in COO(rdinate) format contain?,specified values at the givenindices,5093
24055,What is created of sizesteps whose values are evenly spaced from start to end inclusive?,one-dimensional tensor,5093
24056,Returns a tensor filled with the shape defined by the variable argumentsize. Returns a tensor filled with what?,scalar value1,6
24057,Returns a what tensor with ones on the diagonal and zeros elsewhere?,2-D,6
24058,Returns a what?,random permutation of integers from0ton-1,7
24059,Random numbers from a normal distribution with mean0and variance1 is also called what?,standard normal distribution,7
24060,Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean what?,0 and variance 1,7
24061,Where is each element sampled from?,Poisson distribution,7
24062,What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive),a tensor,7
24063,What does Sets the default floating point dtype tod get?,current default floating pointtorch.dtype,5841
24064,Returns a what type of tensor of size end start stepleftlceil fractextend,1-D tensor,5841
24065,What is the current default floating pointtorch.dtype?,current default floating pointtorch.dtype,2698
24066,Returns a what type of tensor?,1-D tensor,2698
24067,Returns a tensor filled with what value1?,scalar,5847
24068,Creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.randin,Random sampling,5847
24069,Returns a what type of tensor of size end start step+1leftlfloor fractext,1-D tensor,5680
24070,Returns what tensor of size end start step+1leftlfloor fractextend -,1-D tensor,5824
24071,What type of tensor is returned?,tensor filled with the scalar value1,5232
24072,What type of tensor of sizesteps whose values are evenly spaced from start to end inclusive?,one-dimensional tensor,1950
24073,Returns what tensor with ones on the diagonal and zeros elsewhere?,2-D tensor,1950
24074,Returns an uninitialized tensor with what size?,same size asinput,1950
24075,Concatenates what in the given dimension?,given sequence ofseqtensors,1819
24076,"Splitsinput, a tensor with three or more dimensions, into what?",multiple tensors depthwise,1819
24077,Gathers values along an axis specified what?,by dim,1819
24078,Expectsinputto be = what?,2-D tensor,1819
24079,"Splitsinput, a tensor with one or more dimensions, into what?",multiple tensors,1999
24080,What is the name of the index that returns a new tensor which indexes theinputtensor along dimensiondimusing,a LongTensor,1999
24081,Creates a new tensor by doing what?,horizontally stacking the tensors in tensors,1999
24082,What happens along an axis specified by dim?,Gathers values,1999
24083,What does Alias for torch.movedim() return a new tensor that is?,a narrowed version of inputtensor,1999
24084,"What function returns a tensor with the same data and number of elements asinput, but with the specified shape?",Alias of torch.vstack(),1999
24085,What are the elements of input at the given?,indices,1999
24086,Stack tensors in sequence what way (column wise)?,horizontally,14
24087,What is the boolean maskmask that returns a new 1-D tensor which indexes theinputtensor,a BoolTensor,14
24088,What is the new tensor that returns a new tensor that is?,a narrowed version of inputtensor,14
24089,What does Alias for torch.transpose() stand for?,"tack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",14
24090,What happens when a sequence of tensors is stacked along a new dimension?,Concatenates a sequence of tensors along a new dimension,14
24091,Selects values frominput at what?,1-dimensional indices,14
24092,Where do Stack tensors stack in sequence?,depthwise,14
24093,Gathers values along what axis?,axis specified by dim,14
24094,Splitsinput into what horizontally according toindices_or_sections?,multiple tensors,14
24095,What is the name of the index which indexes theinputtensor along dimensiondimusing the entries inindex?,a LongTensor,14
24096,Returns what with the elements of input at the given indices?,a new tensor,14
24097,Sets the seed for generating random numbers to what?,non-deterministic random number,5860
24098,Sets the seed for generating random numbers to a non-deterministic random number.,Sets the seed for generating random numbers,5860
24099,Random numbers drawn from separate normal distributions where mean and standard deviation are given?,whose mean and standard deviation are given,5860
24100,What is filled with random numbers from a normal distribution with mean 0 and variance 1?,a tensor with the same size asinput,5860
24101,What is the in-place version of torch.Tensor.bernoulli_()?,torch.Tensor.bernoulli_(),5860
24102,Returns the initial seed for generating random numbers as what?,Python long,5574
24103,Returns a tensor filled with random integers generated uniformly what?,betweenlow(inclusive) andhigh(exclusive),5574
24104,Returns a tensor of random numbers drawn from separate normal distributions where mean and standard deviation are given?,whose mean and standard deviation are given,5574
24105,What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?,a tensor,5574
24106,Draws binary random numbers (0 or 1) from a what distribution?,Bernoulli,2211
24107,What is returned with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive,a tensor,2211
24108,What is the name of the number drawn from the Cauchy distribution torch?,torch.Tensor.cauchy_(),2211
24109,Each element sampled from a what distribution with rate parameter given by the corresponding element in input?,Poisson,2211
24110,What returns a tensor filled with random numbers from a normal distribution with mean 0 and variance1?,random permutation of integers from0ton-1,2211
24111,What is the name of the in-place version of the Torch.bernoulli() torch?,torch.Tensor.bernoulli_(),2211
24112,What is the tensor filled with in Bernoulli distribution?,random integers,2211
24113,Returns what where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row,a tensor,5856
24114,What returns a random permutation of integers from0ton-1?,random permutation of integers from0ton-1,5856
24115,Sets what state?,random number generator state,5856
24116,Returns what tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive),a tensor,5856
24117,From what distribution is each element sampled?,Poisson distribution,5465
24118,Returns a tensor of random numbers drawn from separate normal distributions what?,whose mean and standard deviation are given,5465
24119,Where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ten,a tensor,5465
24120,Returns a tensor with what shape as Tensorinput?,same shape,5465
24121,What is the name of the in-place version of Torch.bernoulli() torch?,torch.Tensor.bernoulli_(),5465
24122,What is the name of the function that returns the initial seed for generating random numbers as a Python long?,Sets the seed for generating random numbers,5863
24123,Returns the initial seed for generating random numbers as a what?,Python long,5863
24124,Sets the seed for generating random numbers. Returns the random number generator state as atorch.ByteTensor.,random number generator state,5863
24125,What are the numbers drawn from the Cauchy distribution?,torch.Tensor.cauchy_(),5863
24126,What is a tensor filled with?,uninitialized data,2014
24127,Computes the what value of each element in input?,absolute value,1693
24128,Adds what to each element of the inputinputand returns a new resulting tensor?,scalar other,1693
24129,What clamps all elements in inputinto the range?,Clamps all elements in inputinto the range,1693
24130, torch.abs() Computes the what cosine of each element in input?,inverse,33
24131,What does Alias for torch.acosh() add?,scalar otherto each element of the inputinput,33
24132,What does Alias for torch.abs() do?,Computes the inverse cosine of each element in input,33
24133,What is the cosine of the elements of input?,inverse hyperbolic,1772
24134,Computes the what cosine of each element in input?,inverse,1772
24135,Computes the inverse cosine of each element in input. Computes the smallest integer greater than or equal to each element.,torch.clamp(),1772
24136,Performs the element-wise division what?,of tensor1 by tensor2,4816
24137,Computes the giveninputtensor's what?,element-wise angle,4816
24138,Which function returns a new tensor with the arctangent of the elements of input?,torch.asinh(),4816
24139,Performs the element-wise multiplication what?,of tensor1 by tensor2,4816
24140, torch.asin(). Returns a new tensor with what of the elements of input?,inverse hyperbolic sine,4816
24141, torch.asinh(). Returns a new tensor with the what of the elements ofin,arctangent,4816
24142,"Clamps which elements in inputinto the range[min,max].?","all elements in inputinto the range[min,max].",4816
24143, torch.clamp(). Computes the what of the giveninputtensor?,element-wise conjugate,4816
24144,"Create a new what tensor with the magnitude of inputand the sign ofother, elementwise?",floating-point tensor,4816
24145,Performs what?,element-wise multiplication,34
24146,Inputi/otheritextinput_i / textotheriinputi /other,quadrant,34
24147,Computes the giveninputtensor. Returns a new tensor with the arcsine of the elements ofin,element-wise angle,34
24148,What function adds the scalar otherto each element of the inputinputand returns a new resulting tensor?, torch.acosh(),34
24149,Returns a new tensor with what of the element?,cosine,34
24150,What does Alias for torch.asinh() return a new tensor with?,arctangent,1737
24151,What is the smallest integer greater than or equal to each element?,the ceil of the elements of input,1737
24152,What is computed?,the element-wise angle (in radians) of the giveninputtensor,1737
24153,Who returns a new tensor with the inverse hyperbolic tangent of the elements of input?, torch.atanh(),5397
24154,What function computes the element-wise conjugate of the giveninputtensor?, torch.clamp(),5397
24155,Who returns a new tensor with the arctangent of the elements of input?, torch.asinh(),5397
24156,What is the name of the function that returns a new tensor with the exponential of the elements of input?, torch.special.erfinv(),5397
24157,Returns a new tensor with what of the elements of the elements of the input?,inverse hyperbolic sine of the elements of input,5397
24158,Computes the what of inputandother?,bitwise XOR,2
24159,Clamps all elements in inputinto what range?,"range[min,max]",2
24160,"Create a new what with the magnitude of inputand the sign ofother, elementwise?",floating-point tensor,2
24161,Returns a new tensor with what element of the elements of input?,cosine,2
24162,What are the elements of inputconverted from?,angles in degrees to radians,2
24163,What happens to each element of the inputinput by the corresponding element ofother?,Divides each element of the inputinputby the corresponding element ofother,2
24164,Computes the logarithmic derivative of the gamma function oninput?, torch.div(),2
24165,Computes the what of the gamma function oninput?,logarithmic derivative,2
24166,What does Alias for torch call?,special.erf(),2
24167,What is another name for Alias for torch.special.erf()?, torch.special.erfc(),2
24168,What element of the elements of input does Alias for torch.asinh() return a new tensor with,arctangent,2
24169,Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Alias fort,atan(),2
24170,The arctangent of inputi/otheri is considered with consideration of what?,quadrant,2
24171,What function returns a new tensor with the exponential of the elements of the input tensorinput?, torch.special.erfinv(),2
24172,Returns a new tensor with what of the elements of the input tensorinput?,exponential,2
24173,What is the name of the function that returns a new tensor with the exponential of the elements of the input tensorinput, torch.special.exp2(),2
24174,Who returns a new tensor with the inverse hyperbolic sine of the elements of input?,torch.asin(),5373
24175,Computes the logarithmic derivative of the gamma function oninput. Computes the logarithmic derivative of the, torch.special.erf(),5373
24176,Clamps all elements in inputinto what?,"range[min,max]",4828
24177,Which function returns a new tensor with the inverse hyperbolic tangent of the elements of input?, torch.atanh(),4828
24178,Inputi/otheritextinputi / textotheriinputi /otheri with consideration of,quadrant,4828
24179,Multiply the result by the scalarvalue and add it toinput. Computes the element-wise angle (in radi,of tensor1 by tensor2,4828
24180,Computes what of the giveninputtensor?,the element-wise angle,4828
24181,Computes the element-wise what of the giveninputtensor?,conjugate,4828
24182,What is returned with each of the elements of inputconverted from angles in degrees to radians?,a new tensor,4828
24183,What does divide each element of the inputinput by the corresponding element ofother?,Divides each element of the inputinputby the corresponding element ofother,4828
24184,What type of tensor is created with the magnitude of input?,floating-point tensor,5391
24185,Returns the indices of what value of all elements in theinputtensor?,the maximum value,12
24186,Tests if all elements in inputevaluate what?,toTrue,12
24187,Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. Returns the mean,p-norm of (input-other) Returns the log of summed exponentials,12
24188,"Returns the median of the values in input, doing what?",ignoringNaNvalues,12
24189,Computes what quantiles of each row of theinputtensor along the dimensiondim?,q-th,12
24190,What is the result of Computes the q-th quantiles of each row of theinputtensor along the dimensiondim,variant of torch.quantile()that “ignores”NaNvalues,12
24191,"If unbiased is True, what will be used?",Bessel’s correction,12
24192,Bessel's correction will be used to calculate what?,standard deviation,12
24193,"Returns the input tensor. Eliminates all but the first element from every consecutive group of equivalent elements. If unbiased is True,",unique elements,12
24194,Which element is eliminated from every consecutive group of equivalent elements?,Eliminates all but the first element,12
24195,When Bessel’s correction will be used.,If unbiased is True,12
24196,Returns the maximum value of all elements in theinputtensor. Returns the minimum value of all elements in theinputtensor,input tensor,12
24197,Returns a what dimension view of each input tensor with zero dimensions?,3-dimensional,5264
24198,What is one way to create a block diagonal matrix from provided tensors?,Count the frequency of each value in an array of non-negative ints,5264
24199,"Returns what of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries",indices,5264
24200,What Sums the product of the elements of the inputoperandsalong dimensions specified using a notation?,Sums the product of the elements of the inputoperandsalong dimensions specified using a notation,5264
24201,What is created from provided tensors?,Create a block diagonal matrix,5278
24202,What product of the given sequence of tensors. Computes batched the p-norm distance between each pair of the,Do cartesian product,5278
24203,Compute combinations of lengthrrrof the given tensor. Returns a copy of input. Returns a copy of,Compute combinations of lengthrrrof the given tensor,5278
24204,Returns a view of each input tensor with zero dimensions. Count the frequency of each value in an array of non-negative in,3-dimensional,5278
24205,What is another way to create a block diagonal matrix from provided tensors?,Count the frequency of each value in an array of non-negative ints,5278
24206,What is the result of Flattensinput by reshaping it into a one-dimensional tensor?,Reverse,5278
24207,Broadcasts the given tensors according to what?,Broadcasting semantics,1932
24208,How do you count the frequency of each value in an array of non-negative ints?,Count the frequency of each value in an array of non-negative ints,1932
24209,Create what from provided tensors?,block diagonal matrix,1932
24210,Compute combinations of lengthrrrof the given tensor. Returns the cross product of vectors in dimensiondimof input,Compute combinations of lengthrrrof the given tensor,1932
24211,Reverse the order of what along given axis?,n-D tensor,1932
24212,Returns what wherevaluesis the cumulative minimum of elements of inputin the dimensiondim?,"a namedtuple(values,indices)",1661
24213,What does Rotate by 90 degrees in the plane specified by dims axis?,Rotate a n-D tensor,1661
24214,Computes the histogram of what?,element-wise greatest common divisor,1661
24215,Computes what along the given dimension?,n-th forward difference,5298
24216,What is done by reshaping it into a one-dimensional tensor?,Flattensinput,5298
24217,Sums the product of the elements of inputoperandsalong dimensions specified using a notation based on what?,Einstein,5298
24218,Compute combinations of lengthrrof the given tensor. Returns the cross product of vectors in dimensiondimof inputand,Compute combinations of lengthrrrof the given tensor,5298
24219,What returns a new tensor?,Flip tensor in the up/down direction,5298
24220,"In the left/right direction, returning a new tensor. Flip tensor in the up/down direction, returning a",Flip tensor,5298
24221,"Computes what product, denoted by otimes, of inputandother?",Kronecker,5298
24222,What does Rotate a n-D tensor by 90 degrees in the plane specified by dims axis?,Rotate,5298
24223,Computes the element-wise greatest common divisor of inputandother. Computes the histogram of a tensor.,GCD,5298
24224,Computes what of a tensor?,histogram,5298
24225,What can be either scalar or scalar?,TakeNNNtensors,5298
24226,For what do Broadcast_tensors() work?,shapes,5913
24227,Flattensinput by reshaping it into what?,one-dimensional tensor,5913
24228,"In the left/right direction, returning a new tensor. In the up/down direction, returning a new tensor",Flip tensor,5913
24229,Reverse the order of what along given axis in dims?,n-D tensor,5540
24230,Performs what of matrices stored inbatch1andbatch2 with a reduced add step (all matrix multiplications get ,a batch matrix-matrix product,4778
24231,Performs a batch matrix-matrix product of what?,matrices inbatch1andbatch2,4778
24232,Returns what matrix product of the NNN2-D tensors?,matrix product of -D tensors,4778
24233,What does Computes the LU factorization of a matrix?,Computes the LU factorization of a matrix,4778
24234,What product of the matrixmatand the vectorvec is performed?,matrix-vector product,4794
24235,Performs what of the matrices mat1 and mat2?,matrix multiplication,4794
24236,Performs the outer-product of vectors vec1 and vec2and adds it to what?,matrix input,4803
24237,Performs what of the matrixmatand the vectorvec?,matrix-vector product,4803
24238,Computes the inverse of a symmetric positive-definite matrixAAAusing its what?,Cholesky factoruuu,5600
24239,Positive semidefinite matrix to be inverted given its what?,Cholesky factor matrixuuu,5600
24240,What does the matrix product of two tensors return?,Matrix product of two tensors,5600
24241,What does matrix_power() return?,numerical rank,5600
24242,Computes the solution to what problems for a full rank matrix?,least squares and least norm problems,5600
24243,Computes the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices,Cholesky,5600
24244,Returns what of the NNN2-D tensors?,matrix product,5600
24245,Computes what of a real square matrix?,eigenvalues and eigenvectors,5600
24246,What is this function for calling LAPACK's geqrf directly?,low-level function,5600
24247,What does a linear system of equations with a positive semidefinite matrix have to be inverted given?,Cholesky factor matrixuuu,5600
24248,Computes the dot product of two 1D tensors. Computes the eigenvalues and eigenve,Computes the dot product of two 1D tensors,5965
24249,Computes the what factorization of a matrix or batches of matricesA?,LU,5965
24250,Returns what solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of,LU,5965
24251, torch.linalg.matrix_power() Returns what of a 2-D tensor?,numerical rank,5965
24252,What product of the matrix inputand the vectorvec is performed?,matrix-vector,5965
24253,What is the householder_product of Alias for torch.linalg?,householder_product,5965
24254,What does Alias of torch.outer() do?,Computes the dot product for 1D tensors,5965
24255,Computes the eigenvalues and eigenvectors of a real square matrix. This is a what?,low-level function,1720
24256,Computes the dot product for 1D tensors. Computes the dot product for 1D tensors,Alias of torch.outer(),1720
24257,Calculates what of a square matrix or batches of square matrices?,log determinant,1720
24258,Computes the what of a matrix or batches of matricesA?,LU factorization,1720
24259,What is the LU factorization of a tensor?,LU factorization of a tensor into tensorsLandU,1720
24260,Computes the matrix exponential of a square matrix or of each square matrix in a batch ,Computes the matrix exponential of a square matrix or of each square matrix in a batch,1720
24261,Performs what of the matricesinput and mat2?,matrix multiplication,1720
24262, torch.linalg.matrix_power() Returns the what of a 2-D tensor,numerical rank,1720
24263,Performs a what product of the matrix inputand the vectorvec?,matrix-vector,1720
24264,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix?,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix,1720
24265,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Outer product,householder_product(),1720
24266,What product of inputandvec2?,Outer product,1720
24267,Who computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix?, torch,1720
24268,What is the dot product for?,1D tensors,1720
24269,Computes the eigenvalues and eigenvectors of what?,real square matrix,1720
24270,What is the batch matrix-matrix product of?,matrices inbatch1andbatch2,4758
24271,Performs the what of vectors vec1 and vec2and adds it to the matrix input?,outer-product,4838
24272,Unpacks the data and pivots from a LU factorization of a tensor into what?,tensorsLand,4838
24273,What is a matrix product of two tensors?,Matrix product of two tensors,1719
24274,What does Alias of torch.outer() compute the dot product for?,1D tensors,1719
24275,What does Alias for torch.linalg.det() calculate of a square matrix or batches of square matrices,log determinant,1719
24276,What is a low-level function for calling LAPACK's geqrf directly?,Alias of torch.outer(),1719
24277,What is computed by Computes the dot product of two tensors?,Matrix product of two tensors,1709
24278,Computes the matrix-matrix multiplication of a product of what with a general matrix?,Householder matrices,1709
24279,What does Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix do?,Out,1709
24280,What is the current state of this module?,BETA,7762
24281,"See what for details. Computes the entropy oninput(as defined below), elementwise.",documentation of each function,7762
24282,Computes the error function of input. The error function is defined as follows: what is the input tensor?,input(Tensor) – the input tensor,7762
24283,Computes the error function of input. The error function is defined as follows: input(Tensor) – the input ten,output tensor,7762
24284,Computes the complementary error function of input. The complementary error function is defined as follows: what is the input tensor?,input(Tensor) – the input tensor,7762
24285,What is an example of a Computes the exponential of the elements minus 1 of input?,Bessel function,7762
24286,What is the name of the function that Computes the error function of input?,Computes the complementary error function of input,1758
24287,What is the name of the function that Computes the inverse error function of input?,Computes the inverse error function of input,1758
24288,Computes the elements minus 1 of input. Note This function provides greater precision than exp(x) - 1 for small values of ,exponential,1758
24289,Computes the absolute value of the gamma function oninput. input(Tensor) – the input tens,natural logarithm,1758
24290,Computes the natural logarithm of the absolute value of the gamma function oninput?,input(Tensor),7337
24291,"Input(Tensor) – the input tensor. Out(Tensor,optional) – the output","out(Tensor,optional)",10349
24292,What is the error function defined as?,input(Tensor) – the input tensor,10349
24293,What is an example of a function that computes the error function of input?,Computes the complementary error function of input,10349
24294,"What is an example of an error function that is defined in the range(1,1)(-1, 1)(1,1)?",Computes the inverse error function of input,10349
24295,What is the first kind?,exponentially scaled zeroth order modified Bessel function,1756
24296,What is also known as the logistic sigmoid function?,Computes the complementary error function of input.,9672
24297,What is an example of a computation?,Computes the natural logarithm of the absolute value of the gamma function oninput,9672
24298,Computes the first kind for each element of input. input(Tensor) – the input tensor. out(,exponentially scaled zeroth order modified Bessel function,9672
24299,What is an example of a function that computes the inverse error function of input?,Computes the inverse error function of input,9672
24300,Computes the first kind of what function for each element of input?,exponentially scaled zeroth order modified Bessel function,9668
24301,What is deprecated and may be removed in a future PyTorch release?,torch.norm,11176
24302,What is used when computing vector norms?,torch.linalg.vector_norm(),8101
24303,What function is used when computing vector norms?,torch.linalg.vector_norm(),8216
24304,What does torch.linalg.matrix_norm() do?,matrix norm or vector norm of a given tensor.,11175
24305,What function can be used  instead of torch.linalg.norm()?,torch.linalg.vector_norm(),11175
24306,What type of data type must the input tensor have for torch.norm??,floating point or complex type,9644
24307,How is the norm calculated for complex inputs?,the absolute value of each element,9644
24308,Nuclear norm can only be calculated across what?,exactly two dimensions,9644
24309,What is the default order of norms?,Default:'fro',10873
24310,What is the name of the order in which a norm can be calculated in torch.norm?,"fro, nuc, Number",10873
24311,The corresponding dimensions of input are what in torch.norm?,flattened,9881
24312,What is the vector norm 'fro'?,Frobenius norm,9881
24313,What is the name of the norm that can be calculated in torch.norm?,Frobenius norm,10365
24314,What is the ord matrix norm vector norm?,"fro, nuc, Number",10327
24315,What is the vector norm fro?,ord matrix norm,10327
24316,When does Frobenius norm throw an error?,when dimis a list of three or more dims,2610
24317,What can only be calculated across exactly two dimensions?,Nuclear norm,2610
24318,What produces the same result asp=2 in all cases except when dimis a list of three or more dims?,Frobenius norm,5592
24319,Nuclear norm can only be calculated across exactly how many dimensions?,two,5592
24320,What can be calculated across any number of dimensions?,vector norm,7360
24321,What happens to the corresponding dimensions of input in torch.norm?,flattened,10874
24322,The vector norm can be calculated across what?,any number of dimensions,10782
24323,What is the vector norm calculated across any number of dimensions?,sum(abs(x)**ord)**(1./ord),10782
24324,What is the Frobenius norm?,"the corresponding dimensions of input are flattened into one dimension, and the norm is calculated on the flattened dimension. Frobenius norm produces the same result as p=2 in all cases except when dim is a list of three or more dims, in which case Frobenius norm throws an error.",11490
24325,What does nuc stand for?,nuclear norm,11487
24326,What case does Frobenius norm throw an error?,whendimis a list of three or more dims,11487
24327,What is nuclear norm?,nuclear norm – Number – sum(abs(x)**ord)**(1./ord),10273
24328,What is Number – sum(abs(x)**ord)**(1./ord))?,nuclear norm,10273
24329,What is the value of the vector norm?,Number,11482
24330,What is the default value for atorch.Tensor?,Default:1e-15,11085
24331,What does hermitian indicate if complex or symmetric if real?,Hermitian,11085
24332,What is the default value of Hermitian?,False,11085
24333,What is the tolerance value to determine when is a singular value zero?,rcond,11085
24334,"What is the tensor of shape(*, m, n)where*is zero or more batch dimensions?",A(Tensor),11085
24335,What does torch.linalg.inv() compute of a square matrix?,inverse,11083
24336,What is the default value of the atorch.Tensor?,1e-15,11083
24337,What does torch.linalg.inv() compute?,the inverse of a square matrix,11083
24338,What is the tensor of shape where*is zero or more batch dimensions?,A(Tensor),11083
24339,What returns the cumulative minimum of elements of input in the dimension dim?,a namedtuple,5317
24340,"What is the dimension to do the operation over out(tuple,optional)?",dim(int),5317
24341,What returns the mode value of each row of the input tensor in the given dimension dim?,"a namedtuple(values,indices)",5652
24342,Indicesis what of each mode value found?,index location,5322
24343,"By default,dimis is what?",the last dimension of the input tensor,5322
24344,How often does a value appear in a given row of the input tensor?,most often,5322
24345,"By default,dimis the what dimension of the input tensor?",last dimension,5323
24346,Where is this function not defined?,for torch.cuda.Tensoryet,5323
24347,What is the name of the function that determines whether the output tensor hasdimretained or not?,keepdim,5323
24348,When are output tensors of the same size as input?,If keepdim is True,5323
24349,What is the result tuple of two output tensors?,"out(tuple,optional)",5323
24350,This function is not defined what?,for torch.cuda.Tensoryet,5323
24351,What is the function for  dimension to reduce?,dim(int),5323
24352,"By default,dimis the last dimension of what?",the input tensor,1494
24353,When are the output tensors of the same size asinput except in the dimension dimwhere they are of size 1?,If keepdim is True,1494
24354,"By default,dimis what dimension of the input tensor?",last dimension,1494
24355,In what case are output tensors of the same size as input except in the dimension dimwhere they are of size 1?,If keepdim is True,1494
24356,What is another name for squeezed output tensors?,seetorch.squeeze(),1494
24357,What determines whether the output tensor hasdimretained or not?,keepdim(bool),1494
24358,What is the result of the output tensors being squeezed?,1 fewer dimension than input,3444
24359,Note This function is not defined what?,for torch.cuda.Tensoryet,3444
24360,What makes the output tensors of the same size as input except in the dimension dimwhere they are of size 1?,If keepdim is True,3444
24361,What is the name of the function that results in the output tensors having 1 fewer dimension than input?,seetorch.squeeze(),3444
24362,What is this function not defined?,for torch.cuda.Tensoryet,4421
24363,"Out(tuple,optional) is the result tuple of two what?",output tensors,4421
24364,What website does Alias for torch.ne belong to?, torch.ne,37
24365,What is the name of the website?, torch.ne,37
24366,What does optimizer_class(torch.nn.Optimizer) contain?,the class of the local optimizer,10320
24367,What will be forwarded to the given optimizer?,all trailing arguments,10320
24368,What can be useful when fine tuning a pre-trained network?,Add a param group to the Optimizers param_groups,10320
24369,What can be made trainable and added to theOptimizeras training progresses?,frozen layers,10320
24370,What is to(int)?,the rank that receives the global states,10320
24371,What is the default value for the rank that receives the global states?,0,10320
24372,What is the rank that receives the global states?,to(int),10320
24373,What is the state of the optimizer?,state of the optimizer as adict,10320
24374,How many entries does the state of the optimizer as adict contain?,two,10320
24375,What is a dict containing all parameter groups Partitions parameters across distributed data parallel ranks?,param_groups,10320
24376,How many entries does the state of the optimizer contain?,two,10320
24377,What is the name of the optimizer state?,state_dict(dict),10320
24378,What is a list of dicts?,a list of param_groups,10320
24379,Element 0 corresponds to what rank?,"rank 0,",10320
24380,What function returns the local_state_dict for a given rank?,insidestep(),10320
24381,What is the last known global optimizer state?,globalstate_dict,10320
24382,What is the name of the local optimizer?,group,10318
24383,What is a bool for when parameters are packed into larger buckets?,parameters_as_bucket_views,10318
24384,What will remain intact when disabled?,but params.data,10379
24385,What is an example of how to add a param group to the Optimizers param_groups?,Add a param group to the Optimizers param_groups,10379
24386,Param.datafields will point to what at different offsets?,bucket views,10380
24387,What does state_dict(dict) represent?,optimizer state,10380
24388,What does param_group(dict) specify?,Tensors,10375
24389,How many consolidated state_dicts are there per rank?,one,10375
24390,What is the default value for the rank that receives global states?,0,10375
24391,What does state_dict(dict) contain?,optimizer state,10375
24392,What is the name of the call that returns the state of the optimizer?,tostate_dict(),10375
24393,What is a list of dict?,a list of param_groups,10375
24394,"Which element corresponds to rank 0, etc.?",Element 0,10375
24395,We need all the ranks for the broadcast what?,insidestep(),10375
24396,What does insidestep() return for a given rank?,local_state_dict,10375
24397,What does param_groups contain across distributed data parallel ranks?,Partitions parameters,10375
24398,What does state_dict(dict) return?,globalstate_dict,10375
24399,What can be made trainable by adding a param group to the Optimizers param_groups?,frozen layers,74
24400,What Specifies what Tensors should be optimized along with group specific optimization options?,param_group(dict),10390
24401,How many consolidated state_dicts are updated per rank?,one per rank,7471
24402,What is the default value for global parameter groups?,0,7471
24403,How many states does the consolidated state_dict list update?,one per rank,2298
24404,To(int) – the rank that receives the global states. (default: what) Restore the global parameter groups as well as the,0,10929
24405,Should be an object returned from a call from what?,tostate_dict(),10929
24406,How many consolidated state_dicts does the Optimizers param_groups update?,one per rank,941
24407,What does param_group(dict) do?,Specifies what Tensors should be optimized along with group specific optimization options,10319
24408,What is a dict containing all parameter groups?,param_groups,8064
24409,What is state_dict(dict)?,optimizer state,10928
24410,Restore the global parameter groups as well as what else?,shard,5173
24411,What dict contains all parameter groups Partitions parameters across distributed data parallel ranks?,param_groups,2705
24412,What is the state of the optimizer as adict?,Gets this rank’sstate_dict,2705
24413,What does insidestep return for a given rank?,local_state_dict,2705
24414,Restore what as well as the shard?,global parameter groups,5174
24415,What is the globalstate_dict?,last known global optimizer state,5174
24416,What is a closure that reevaluates the model and returns the loss?,closure,5174
24417,What type of loss depends on the underlying optimizer?,Optional,5174
24418,What does optional loss depend on?,underlying optimizer,5174
24419,Should be an object returned from a call to what?,state_dict(),10755
24420,What is the name of the broadcast we need all the ranks for?,insidestep(),10377
24421,What is the name of the function that returns the local_state_dict for a given rank?,insidestep(),10377
24422,What does getlocal_state_dict for state_dict(dict) return?,globalstate_dict,10377
24423,Param_groups contains all parameter groups across distributed data parallel ranks.,Partitions parameters,10377
24424,"Which element of the list corresponds to rank 0, etc.?",Element 0,10377
24425,What does get local_state_dict for state_dict(dict) return?,globalstate_dict,10377
24426,We need all the ranks for the broadcast for what?,insidestep(),24
24427,What does rank(int) return?,get local_state_dict for,7309
24428,What is the name of the rank to get local_state_dict for?,rank,7309
24429,What differs between?,optimizer classes,9179
24430,What class differs between param_groups and param_groups?,optimizer classes,9179
24431,A list of param_groups is a list of what?,dict,4751
24432,What is a part of distributed data parallel ranks?,Partitions parameters,4751
24433,Globalstate_dict consist of a list of what?,shards,4751
24434,Partitions parameters across distributed data what?,parallel ranks,4751
24435,"What corresponds to rank 0, etc.?",Element 0,7472
24436,What is a single optimization step called?,parameter update,5584
24437,What is returned for a given rank?,local_state_dict,5584
24438,What is the name of the rank to getlocal_state_dict for state_dict(dict)?,rank,5584
24439,What is expected to be the inverse of stft()?,Inverse short time Fourier Transform,3860
24440,What should the Inverse short time Fourier Transform return?,least squares estimation,3860
24441,What condition will the algorithm check using?,NOLA condition,3860
24442,Inverse short time Fourier Transform is expected to be the inverse of what?,of stft(),3859
24443,What is used to compile TorchScript code?,TorchScript compiler,5121
24444,When it is first called during tracing?,Compilesfn,5121
24445,What is created by the asynchronous task executingfuncand a reference to the value of the result of this execution?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,5121
24446,Any TorchScript program can be saved from a what process?,Python,5121
24447,Freezing aScriptModulewill clone it and attempt to inline what as constants in the TorchScript IR ,"attempt to inline the cloned module’s submodules, parameters, and attributes",5121
24448,What version of the module can be saved for use in a separate process?,offline,5121
24449,What does aScriptModule do?,Load aScriptModuleorScriptFunction,5121
24450,What is used to optimize a script?,just-in-time compilation,1474
24451,"When it is first called during tracing, what does Compilesfn do?",Compilesfn,1474
24452,What is the wrapper around C++torch::jit::Module?,Functionally equivalent to aScriptModule,1474
24453,What is an executableScriptModule that will be optimized using?,just-in-time compilation,2607
24454,What action does TorchScript perform?,Forces completion of atorch.jit.Future[T]asynchronous task,2607
24455,Freezing aScriptModulewill what?,clone it,2607
24456,What is a wrapper around C++torch::jit::Module?,wrapper around C++torch::jit::Module,2147
24457,What is a Wrapper around C++torch::jit::Module?,represents a single function and does not have any attributes or Parameters,2147
24458,What does JIT do for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Append,Disable JIT,2147
24459,What is a gentle introduction to?,TorchScript,4045
24460,Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create,Known Issues,4045
24461,What is saved an offline version of aScriptModule?,Save an offline version,4045
24462,What is the benefit of using TorchScript?,Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency,4940
24463,What is an example of a TorchScript program that can be run independently from Python?,a standalone C++ program,4942
24464,What is functionally equivalent to aScriptModule?,represents a single function and does not have any attributes or Parameters,5006
24465,Where can a TorchScript program be run independently from Python?,a standalone C++ program,5006
24466,Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix,Python Language Reference Comparison Debugging,5006
24467,Python Functions and Modules Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently As,Python Language Reference Comparison,5004
24468,"A wrapper around what. Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or",C++torch::jit::Module,1164
24469,What can you do to save an offline version of this module?,Save an offline version of this module,1164
24470,What will be used to optimize a script?,just-in-time compilation,4246
24471,What version of the module can be saved for use in the TorchScript IR Graph?,offline,4246
24472,What does Scripting a function ornn.Module compile as using the TorchScript compiler?,TorchScript code,2499
24473,What is the name of the method that provides for conatiner type refinement in TorchScript?,a pass-through function that returnsvalue,2499
24474,What function is called when it is first called during tracing?,Compilesfn,13
24475,What does asynchronous task executingfuncand a reference to the value of the result of this execution do?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,13
24476,What is the TorchScript method that indicates to the compiler that the left-hand expression is a class instance attribute with type oftype?,a pass-through function that returnsvalue,13
24477,What is a pass-through function that indicates to the TorchScript compiler that the left-hand side expression is a class instance attribute with type,returnsthe_value,2591
24478,What is the difference between aScriptModule and C++torch::jit::Module?,represents a single function and does not have any attributes or Parameters,2591
24479,What can a traced function do?,can call an encoder module generated using tracing,2591
24480,What is used when it is first called during tracing?,Compilesfn,7971
24481,What is the difference between C++torch::jit::Module and aScriptModule?,represents a single function and does not have any attributes or Parameters,7971
24482,What does Trace a module and return an executableScriptModule that will be optimized using just-in-time compilation?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,7975
24483,What type of version of a module can you save for use in a separate process?,offline,7975
24484,The beam search of a sequence to sequence model will typically be written in script but can call what?,an encoder module generated using tracing,7975
24485,What does tracing create?,Creates an asynchronous task executingfuncand a reference to the value of the result of this execution,1627
24486,What type of version of a script module can you save for use in a separate process?,offline,1627
24487,When are traced functions particularly useful?,when you need to use control-flow around a simple feed-forward model,2036
24488,What is an example of a scripted function that can call an encoder module generated using tracing?,a traced function in script,2036
24489,What does C++torch::jit::Module have in common with aScriptModule?,represents a single function and does not have any attributes or Parameters,2036
24490,Scripted functions can call what?,traced functions,2036
24491,What is used to generate an encoder module?,tracing,2036
24492,Tracing and scripting can be composed to what?,suit the particular requirements of a part of a model,1
24493,What is a pass-through function that indicates to the TorchScript compiler the type of the_value?,returnsthe_value,1
24494,What can a traced function call?,can call an encoder module generated using tracing,1
24495,What can an encoder module be generated using?,tracing,3739
24496,Debugging withpdbworks except for what?,when we invoke the@torch.jit.scriptfunction,2077
24497,How can we disable JIT?,globally disable JIT,2077
24498,What does TorchScript provide a code pretty-printer for allScriptModuleinstances?,Python syntax,2077
24499,What will you need to access.codeon if theScriptModulehas more than one method?,the module,2077
24500,What is the output produced by the example above?,TorchScript’s compilation of the code for theforwardmethod,8866
24501,We will be able to step into the@torch.jit.scriptfunction as what?,normal Python function,8866
24502,What does the code pretty-printer give an interpretation of the script method’s code as valid?,Python syntax,8866
24503,What document describes the rules for forwardmethod lookup?,theInspecting Codesection,8866
24504,What is the schema for?,built-in functions likeaten,8866
24505,"TorchScript has a representation at a lower level than the code pretty- printer, in the form of what?",IR graphs,7951
24506,What is TorchScript's static single assignment?,SSA,7951
24507,TorchScript follows the same rules described in theInspecting Codesection with regard to what?,forwardmethod lookup,7951
24508,Tracing of in-place operations of tensor views (e.g. what?,indexing,7951
24509,What is a static single assignment?,SSA,7593
24510,What can you use TorchScript’s compilation of the code for theforwardmethod?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,7593
24511,What is dependent on the underlying code?,Tracing of control flow,7593
24512,What is the name of the C++ backend of PyTorch?,ATen,7949
24513,What section describes the rules for forwardmethod lookup?,theInspecting Codesection,7949
24514,The graph follows the same rules described in what section?,Inspecting Codesection,7959
24515,What language does TorchScript provide a code pretty-printer for allScriptModuleinstances?,Python syntax,7959
24516,What produces this output?,The example above,7959
24517,"If theScriptModulehas more than one method, you will need to what?",access.codeon the method itself and not the module,907
24518,What is the output of the example above?,TorchScript’s compilation of the code for theforwardmethod,907
24519,What can you use this output for?,to ensure TorchScript (tracing or scripting) has captured your model code correctly,907
24520,Where are these associatedblocks found?,In the graph print-out,907
24521,Why are operators formatted in the graph print-out?,to reflect their equivalent source code forms to facilitate easy debugging,6183
24522,Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations,edge cases,6183
24523,What does this message indicate to us that the computation differed between when we first traced it and when we traced it with traced?,diagnostic information,6183
24524,What does TorchScript use to assign the output to a (unique) value namedrv.1?,%rv.1:Tensormeans,7965
24525,What are some cases where the trace of a given Python function/module will not be representative of the underlying code?,edge cases,7965
24526,"Tracing of in-place operations of tensor views (e.g., what on the left-hand side of an assignment)",indexing,59
24527,Where is the location in the original source file that generated this instruction?,"on line 9, and at character 10",59
24528,What does this message indicate to us that the computation differed between when we first traced it and when we traced it with thecheck_in,diagnostic information,59
24529,What does the tracer produce?,The tracer produces warnings for several problematic patterns in traced computation,7405
24530,Which version of TorchScript does this section detail the changes to TorchScript in?,PyTorch 1.2,7797
24531,If you are new to TorchScript you can do what?,skip this section,7797
24532,What does Torch.jit.script now do?,"attempt to recursively compile functions, methods, and classes",7797
24533,"What is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule?",2.torch.jit.script(nn_module_instance),7797
24534,Methods called fromforwardare what in the order they are used inforward?,lazily compiled,7797
24535,"To stop the compiler from what, add@torch.jit.ignoreor@torch.jit.unused. @ignor",compiling a method,7797
24536,@ignoredcannot be exported;@unusedcan?,@ignoredcannot be exported;@unusedcan,7797
24537,"To compile a method other thanforwardthat is not called fromforward, what is done?",add@torch.jit.export,137
24538,What does @ignoreleaves the method as a call to python?,@ignoreleaves the method as a call to python,137
24539,What is now the preferred way to createScriptModules?,2.torch.jit.script(nn_module_instance),137
24540,Methods called fromforwardare what?,lazily compiled,137
24541,Add@torch.jit.ignoreor@torch.jit.unused. @ignoreleaves the method as a,compiling a method,137
24542,"For empty container types, what should empty container types do?",annotate their types usingPEP 526-styleclass annotations,137
24543,Annotate their types usingPEP 526-styleclass annotations.,empty container types,4265
24544,What is used as an entry point into aScriptModuleand should be compiled?,annn.Module,4265
24545,What is an example of a function that does not need a decorator?,@torch.jit.exporton a method,4265
24546,Before PyTorch 1.2 what decorator was used to make a function or method callable from code that is exported?,@ignore,8189
24547,What is compiled in the order they are used inforward?,@torch.jit.exportmethods,8189
24548,What is used as an entry point into aScriptModule and should be compiled?,annn.Module,8189
24549,Warning TorchScript class support is experimental. Currently it is best suited for what?,simple record-like types,8189
24550,Warning The@torch.jit.ignoreannotation’s behavior changes in what?,PyTorch 1.2,8189
24551,See@torch.jit.ignoreand@torch.jit.unused for what?,details,8189
24552,Warning TorchScript class support is what?,experimental,8189
24553,If a type cannot be inferred and is what?,not explicitly annotated,8189
24554,What did PyTorch 1.2 use to get the @ignore decorator back?,use@torch.jit.unused(),7381
24555,TorchScript class support is what?,experimental,7381
24556,Everything in a user definedTorchScript Classis exported what way?,by default,7381
24557,When does the @torch.jit.ignoreannotation’s behavior change?,PyTorch 1.2,7381
24558,"Functions don’t change much, they can be decorated with what?",@torch.jit.ignoreortorch.jit.unusedif needed,7381
24559,What decorator was used to make a function or method callable from code that is exported?,@ignore,7381
24560,What does @torch.jit.ignore and@torch.jit.unused provide?,details,7381
24561,What cannot have their types inferred and must have their types annotated withPEP 526-styleclass?,Empty lists and dicts,7381
24562,"To stop the compiler from compiling a method, add what?",@torch.jit.ignoreor@torch.jit.unused,7920
24563,What eleaves the method as a call to python?,@ignor,7920
24564,What can empty container types do?,annotate their types,7920
24565,What compiler compiles the module?,TorchScript,7920
24566,See@torch.jit.ignoreand@torch.jit.unusedfor what?,details,7375
24567,"To get this functionality back, use what?",@torch.jit.unused(),7375
24568,What is TorchScript class support best suited for?,simple record-like types,7375
24569,What does the TorchScript compiler need to know the types ofmodule attributes?,Most types,7375
24570,"To get this functionality back, what did PyTorch 1.2 use to get it back?",use@torch.jit.unused(),4999
24571,What is an example of a Python 3 type hints?,@torch.jit.exporton a method,4999
24572,What cannot have their types inferred from the value of the member?,Empty lists and dicts,7444
24573,Computes the bitwise OR ofinputandother. Computes the bitwise XOR ofinputandother.,AND,4817
24574,Computes the bitwise what ofinputandother?,OR,4
24575,What does Computes the bitwise NOT of the given input tensor?,Computes the bitwise NOT,5398
24576,What does Computes the bitwise OR ofinputandother?,Computes the bitwise OR ofinputandother,4829
24577,Computes what NOT of the given input tensor?,bitwise,1738
24578,"What is the name of the function that clamps all elements in inputinto the range[min,max]?",torch.clamp(),1738
24579,What does Alias fortorch.clamp() clamp?,"all elements in inputinto the range[min,max]",10987
24580,Computes the bitwise NOT of the given input tensor. Computes the bitwise OR ofinputandother?,Computes the bitwise OR ofinputandother,3
24581,What does Alias fortorch.clamp() compute?,the element-wise conjugate of the giveninputtensor,3
24582,What does Alias fortorch.clamp() create a new floating-point tensor with?,the magnitude ofinputand the sign ofother,3
24583,What tangent does Alias fortorch.atan() return a new tensor with?,inverse hyperbolic tangent,3
24584,What computes the element-wise conjugate of the given inputtensor?,torch.clamp(),5375
24585,What computes the element-wise conjugate of the giveninputtensor?,torch.clamp(),1739
24586,What does Alias fortorch.clamp() do?,"Clamps all elements in inputinto the range[min,max].",1739
24587,"Clamps which elements in inputinto the range[min,max]?","all elements in inputinto the range[min,max].",5399
24588,What returns the indices of the maximum value of all elements in theinputtensor?,Returns the indices of the maximum value of all elements in theinputtensor,5551
24589,What returns the maximum value of each slice of theinputtensor in the given dimension(s)dim?,the indices of the minimum value(s) of the flattened tensor or along a dimension,5551
24590,Returns what of the minimum value(s) of the flattened tensor or along a dimension?,the indices,5561
24591,Returns what value in the given dimension(s)dim?,the maximum value of each slice of theinputtensor,5614
24592,What does Returns the matrix norm or vector norm of a given tensor return?,the sum of all elements,5614
24593,What does Returns ignoreNaNvalues?,the median of the values in input,6763
24594,What quantiles of each row of theinputtensor along the dimensiondim?,q-th,6763
24595,Tests if all elements in inputevaluate to what?,True,6763
24596,Returns what value?,the minimum value of all elements in theinputtensor,6763
24597,"Returns what value, ignoringNaNvalues?",the median of the values in input,5589
24598,What does the variant of torch.quantile() do?,ignores,5627
24599,"If unbiased is True, Bessel's correction will be used to calculate what?",standard deviation,5627
24600,What does Bessel's correction return?,the sum of all elements in theinputtensor,5627
24601,Returns what of the input tensor?,unique elements,5627
24602,Returns what elements of the input tensor?,unique elements,5627
24603,What does Bessel's correction remove from every consecutive group of equivalent elements?,Eliminates all but the first element,5627
24604,What is Bessel's correction used for?,If unbiased is True,5627
24605,What is returned when Not a Numbers (NaNs) are treated as zero?,the sum of all elements,5629
24606,Bessel's correction will be used to calculate the standard deviation.,If unbiased is True,5629
24607,What happens to every consecutive group of equivalent elements?,Eliminates all but the first element,5629
24608,What does Returns the sum of all elements in theinputtensor?,the product of all elements in theinputtensor,5655
24609,What does Returns treat Not a Numbers (NaNs) as zero?,the sum of all elements,5655
24610,The variant of torch.quantile()ignores what ifNaNvalues in inputdid not exist?,quantilesqas,5636
24611,What is Bessel's correction?,If unbiased is True,5636
24612,"If Bessel's correction is used, what will be used to calculate the standard deviation?",If unbiased is True,5618
24613,Returns the sum of all elements in the inputtensor.,unique elements,5618
24614,What are Broadcast_tensors() used for?,shapes,5914
24615,What is returned when the indices of the buckets are set byboundaries?,Do cartesian product of the given sequence of tensors,5914
24616,What returns the cumulative product of elements ofinputin the dimensiondim?,Returns the cumulative product of elements ofinputin the dimensiondim,5914
24617,Returns what of the buckets to which each value in the input belongs?,indices,5542
24618,Computes batched the what?,p-norm distance between each pair of the two collections of row vectors,1662
24619,What does return a copy ofinput?,Compute combinations of lengthrrrof the given tensor,5541
24620,Returns the indices of what?,buckets,5541
24621,What returns the indices of the buckets to which each value in the input belongs?,Do cartesian product of the given sequence of tensors,5541
24622,"What is returned when a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the",the cumulative product of elements ofinputin the dimensiondim,5541
24623,"Ifinputis a vector (1-D tensor), then returns what?",a 2-D square tensor,10
24624,What product of the given sequence of tensors?,Do cartesian product,2174
24625,"What does the namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim",the cumulative product of elements ofinputin the dimensiondim,2174
24626,What is used to get the @ignore functionality back?,@torch.jit.unused(),7379
24627,What is an example of a method that does not need the @ignore decorator?,@torch.jit.exporton a method,8187
24628,Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on what?,Einstein summation convention,6090
24629,What specifies the subscripts for each dimension of the inputoperandsin the same order as the dimensions?,Theequationstring,7386
24630,What is the envelop created by?,the summation of all the windows,3681
24631,What is the envelop created by the summation of all the windows at a certain point in time?,0,3681
24632,Why does stft return a shorter signal than the original signal?,Since stft()discards elements at the end of the signal if they do not fit in a frame,3681
24633,What is the envelop created by the summation of all windows never zero at certain point in time?,0,3681
24634,"If the signal isn’t padded, what can result in a shorter signal than the original signal?",ifcenteris False,3681
24635,What is the envelop created by the summation of all windows at certain point in time?,never zero,3682
24636,What can be trimmed off exactly because they can be calculated?,Left padding,3682
24637,"If the signal isn't padded, what can result in a shorter signal than the original signal?",ifcenteris False,3682
24638,What discards elements at the end of the signal if they do not fit in a frame?,Since stft(),5926
24639,"If the signal isn't padded, what happens?",If center is True,5926
24640,"If center is True, there will be padding e.g. what?",constant,5926
24641,Why can left padding be trimmed off exactly?,because they can be calculated,3402
24642,"If there is padding, what is it called?",If center is True,3402
24643,"If centerisTrue, then there will be padding e.g.'reflect','reflect', etc.",constant,3402
24644,What is the default setting for padding?,If center is True,3402
24645,"Who wrote ""Signal estimation from modified short-time Fourier transform""?",D. W. Griffin and J. S. Lim,8693
24646,What is the input tensor expected to be?,output of stft(),8693
24647,When was real input deprecated?,1.8.0,8693
24648,What is the channel dimension?,optional,7129
24649,Since what version is real input deprecated?,1.8.0,2115
24650,What is hop_length?,The distance between neighboring sliding window frames,2115
24651,Whether the STFT was onesided or onesided?,onesided,2115
24652,What is the size of Fourier transform hop_length?,The distance between neighboring sliding window frame,9958
24653,What is the default value of hop_length(Optional[int])?,n_fft//4),9640
24654,What is expected to be output of stft()?,input tensor,7130
24655,Since what version is the input tensor deprecated?,1.8.0,7130
24656,What is the size of window frame and STFT filter?,win_length,11395
24657,What is window(Optional[torch.Tensor])?,optional window function,11395
24658,What is center(bool)?,Whether input was padded on both sides,11395
24659,What is the amount to trim the signal by?,the original signal length,11395
24660,Is return_complex compatible or incompatible with onesided=True?,incompatible,11395
24661,"What is the value of size (..., signal_length)?",Least squares estimation of the original signal,11395
24662,What is Torch.ones(Optional[torch.Tensor]) called?,win_length,2114
24663,What is the default value for center(bool)?,True,9523
24664,What is the distance between neighboring window frames?,hop_length,9523
24665,What is win_length?,The size of window frame and STFT filter,9523
24666,What is the default value of center(bool)?,True,9523
24667,What is the distance between neighboring sliding window frames?,hop_length,9523
24668,What is the optional window function?,"Whether input was padded on both sides so that thettt-th frame is
centered",11403
24669,What does true if n_fft! mean in the input size?,fft_size,9641
24670,What is onesided(Optional[bool])?,Whether the STFT was onesided,10303
24671,What is the default value for the output of a STFT?,return_complex,10303
24672,What is Optional[bool]) – Whether the output should be complex or if the input should be assumed to derive from ,return_complex,9784
24673,What is incompatible with return_complex?,withonesided=True,9784
24674,What is the tensor of size?,1-D,5256
24675,Non-integerstepis subject to what when comparing againstend?,floating point rounding errors,5256
24676,What happens when the data type is inferred from the other input arguments?,If dtype is not given,5256
24677,"If any ofstart,end, orstopare floating-point, what is inferred to be the default dtype?",thedtypeis,5256
24678,What is the default value for the starting value for the set of points?,Default:0,5255
24679,What is returned by size end start stepleftlceil fractextend?,1-D tensor,5255
24680,"Out(Tensor,optional) – what is the output tensor?",output tensor,10751
24681,What is the default value for the ending value for the set of points step(Number)?,Default:1,9265
24682,What is the gap between each pair of adjacent points?,step(Number),10763
24683,What is the default tensor type used by if None?,Default,10356
24684,What will infer the data type from the other input arguments?,If dtype is not given,9218
24685,"If any ofstart,end, orstopare what, thedtypeis inferred to be the default dtype?",floating-point,9218
24686,What is the default dtype inferred to?,betorch.int64,9218
24687,"If any ofstart,end, orstopare floating-point, thedtypeis inferred to what?",betorch.int64,9218
24688,What uses a global default (seetorch.set_default_tensor_type())?,Default: if None,9218
24689,What is the default layout of returned Tensor?,Default:torch.strided,9218
24690,What contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors?,The torch package,7327
24691,What does the torch package provide?,"it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities",7327
24692,What does torch have that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0?,CUDA counterpart,7327
24693,What does the CUDA counterpart return?,True if obj is a PyTorch storage object,7327
24694,What utility does the torch package provide for Tensors and arbitrary types?,efficient serializing,7327
24695,What does True if obj is return?,PyTorch storage object,7327
24696,What package contains data structures for multi-dimensional tensors?,torch,7330
24697,What has the counterpart of the torch package?,CUDA,7330
24698,What enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0?,CUDA counterpart,3919
24699,What does True return if obj is a PyTorch tensor?,PyTorch storage object,3919
24700,What are two examples of a complex data type?,"one of torch.complex64, andtorch.complex128",3919
24701,What is the name of the GPU that allows you to run tensor computations on an NVIDIA GPU with compute capability >= 3.0,CUDA,3919
24702,What type of data type is the data type of input?,floating point data type,5225
24703,"What is returned when a namedtuple(values,indices) returns the cumulative minimum of elements of input in the dimension dim",cumulative product of elements of input in the dimension dim,5295
24704,What does Returns the cumulative product of elements of input in the dimension dim?,the cumulative sum of elements of input in the dimension dim,1637
24705,What is the sum of elements of input in the dimension dim?,cumulative product,8
24706,"What is returned when a namedtuple(values,indices) returns the cumulative product of elements of input in the dimension dim",cumulative sum of elements of input in the dimension dim,9
24707,What is reduced in the batch matrix-matrix product of matrices stored in batch1 and batch2?,add step,4769
24708,What does it perform of matrices in batch1 and batch2?,batch matrix-matrix product,4769
24709,What happens to all matrix multiplications along the first dimension?,all matrix multiplications get accumulated along the first dimension,4769
24710,What does the matrix multiplication of matrices perform?,matrices mat1 and mat2,4769
24711,What is performed of the matrices mat1 and mat2?,matrix multiplication,4770
24712,What does it perform and adds it to the matrix input?,outer-product of vectors vec1 and vec2,4770
24713,What does the inverse of a symmetric positive-definite matrix do?,returns matrix inv,4770
24714,What does the matrixmatand the vectorvec perform?,matrix-vector product,4793
24715,Performs what of the matrices mat1 andmat2?,matrix multiplication,4793
24716,What is the matrix product of?,NNN2-D tensors,4793
24717,What product of the matrixmatand the vectorvec?,matrix-vector product,4802
24718,What happens to the outer-product of vectors vec1 and vec2?,adds it to the matrix input,4802
24719,What returns the matrix product of the NNN2-D tensors?,Returns the matrix product of the NNN2-D tensors,4802
24720,What product of vectorsvec1 andvec2 is added to the matrix input?,outer-product,4801
24721,What is performed of the matrixmatand the vectorvec?,matrix-vector product,4801
24722,What does the matrix-vector product of matrices perform?,batch matrix-matrix product of matrices in batch1 and batch2,4801
24723,What performs a batch matrix-matrix product of matrices stored in input and mat2?,batch matrix-matrix product of matrices in batch1 and batch2.,4792
24724,Returns the matrix product of what?,NNN2-D tensors,4774
24725,What is used to perform a batch matrix-matrix product?,matrices stored in input and mat2,4774
24726,What does a real square matrix compute?,eigenvalues and eigenvectors,4774
24727,What is the low-level function for calling LAPACK’s geqrf directly?,Alias of torch.outer(),4774
24728,What is this function for calling LAPACK’s geqrf directly?,low-level function,4774
24729,Is this a low-level function for calling LAPACK's geqrf directly?,low-level function,4774
24730,What calculates the log determinant of a square matrix or batches of square matrices?, torch.linalg.slogdet,4774
24731,Who computes the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices,Cholesky,4836
24732,What performs the outer-product of vectorsvec1 andvec2?,batch matrix-matrix product of matrices in batch1 and batch2,4836
24733,Performs the outer-product of what and adds it to the matrix input?,vectors vec1 and vec2,4837
24734,What decomposition of a symmetric positive-definite matrixAAor is computed for batches of symmetric positive-definite matrices,Cholesky,4837
24735,What is the dot product of two 1D tensors?,dot product of two 1D tensors,4837
24736,What does this low-level function call directly?,LAPACK’s geqrf,4837
24737,What returns the inverse of a symmetric positive-definite matrix?,returns matrix inv,16
24738,Solves a linear system of equations with a positive semidefinite matrix to be inverted given what?,Cholesky factor matrixuuu,16
24739,What is the low-level function for calling LAPACK's geqrf directly?,Alias of torch.outer(),16
24740,What is the matrixuuu of a positive semidefinite matrix?,Cholesky factor,16
24741,Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?,a linear system of equations,1672
24742,What does the Cholesky factoruuu compute?,dot product of two 1D tensors,1672
24743,What is the Cholesky decomposition of for batches of symmetric positive-definite matrices?,symmetric positive-definite matrixAAAor,1672
24744,What does the Cholesky factoruuu return?,returns matrix inv,5599
24745,What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu return?,matrix inv,5599
24746,What is returned by Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?,matrix inv,1673
24747,What does this function compute?,the base two exponential function of input,1764
24748,What does Alias of torch.outer compute the dot product for?,1D tensors,1708
24749,Computes the eigenvalues and eigenvectors of a real square matrix?,dot product,1708
24750,What calculates the dot product for 1D tensors?,Alias of torch.outer(),1708
24751,A linear system of equations with a positive semidefinite matrix to be inverted given what matrix?,Cholesky factor matrix,5963
24752,What does Alias for torch.linalg.det() call?, torch.linalg.inv(),5963
24753,What does Alias for torch.linalg.det() calculate?,log determinant,5964
24754,Computes the dot product for 1D tensors?,Alias of torch.outer(),5964
24755,What computes the dot product for 1D tensors?,Alias of torch.outer(),5
24756,What problems does Alias for torch.linalg.slogdet() solve?,least squares and least norm problems,5
24757,What is the sum of the vector norm?,sum(abs(x)**ord)**(1./ord),11484
24758,When is the output tensor ignored in torch.norm?,ifdim=None,10354
24759,What is the input tensor casted to when performing the operation?,:attr:’dtype’,10354
24760,What is the default value of the input tensor in torch.norm?,None,2303
24761,"If specified, the input tensor is casted to what?",:attr:’dtype’,9214
24762,What applies only to tensors with exactly two dimensions?,Frobenius norm,9214
24763,What is an example of a tensor that can only be applied across exactly two dimensions?,Frobenius norm,9214
24764,What does torch.ARCTANH do?, torch.atanh(),35
24765,What does torch.ANY do?,Tests if any element in input evaluates to True,6769
24766,torch.ANY function matches the behavior of what function?,"This function matches the behaviour of NumPy in returning
output of dtypeboolfor all supported dtypes exceptuint8.
Foruint8the dtype of output isuint8itself.",6769
24767,What is the dtype of output for uint8?,For uint8the dtype of output isuint8 itself,9658
24768,What does input(Tensor) do?,returns True if any element in the row evaluate to True and False otherwise.,9658
24769,What is the dtype of output in torch.any?,uint8,7549
24770,What does the function return for each row of input in the given dimension dim?,Trueif any element in the row evaluate toTrueandFalseotherwise,7549
24771,What function returns the output tensor of the same size asinput?,If keepdim is True,7549
24772,"If keepdimisTrue, the output tensor has what?","the output tensor is of the same size
asinput",7549
24773,What function ensures that the output tensor is of the same size as input except in the dimension dim where it is of size 1?,keepdim is True,7549
24774,"If keepdim is True, the output tensor is of the same size asinput except in the dimension dimwhere it is",1 fewer dimension,7549
24775,What is the dtype of foruint8?,output isuint8itself,4423
24776,What function returns output of dtypebool for all supported dtypes exceptuint8?,NumPy,4423
24777,What does the function return for each row of inputin the given dimension dim?,Trueif any element in the row evaluate toTrueandFalseotherwise,4423
24778,"For each row of inputin the given dimension dim, returns what?",Trueif any element in the row evaluate toTrueandFalseotherwise,4423
24779,"For each row of input in the given dimension dim, returns what?",True if any element in the row evaluate toTrueandFalseotherwise,2511
24780,Where is the output tensor of the same size asinput except in the dimension dim?,"keepdimisTrue, the output tensor is of the same size
asinput",2511
24781,What happens if the output tensor is squeezed?,1 fewer dimension than input,2511
24782,What types of input does this function support?,"float,double,cfloatandcdoubledtypes",7566
24783,What does LU_data(Tensor) do?,LU factorization,7566
24784,What type of input does this function support?,for input,7566
24785,What must the batch dimensions of LU_pivots be equal to?,the batch dimensions ofLU_data,8937
24786,What must be the batch dimensions of LU_pivots be to the batch dimensions of LU_data?,equal,8937
24787,What returns the cumulative maximum of elements of input in the dimension dim?,a namedtuple,5314
24788,What is the output specified by for a 3-D tensor?,inputandindexmust have the same number of dimensions,2672
24789,Do inputandindexdo broadcast against each other?,not broadcast against each other,2672
24790,What does index(LongTensor) contain?,"indices of elements to gather sparse_grad(bool,optional)",2672
24791,What must have the same number of dimensions?,input and index,9637
24792,What type of elements represent if each element of input is real-valued or not?,boolean elements,5361
24793,What is a boolean tensor?,A boolean tensor that is True where input is real and False elsewhere,5361
24794,What is the default value of a boolean tensor?,True,5361
24795,Returns a new tensor with boolean elements representing what?,if each element of input is real-valued or not,5361
24796,What type of values are considered real?,Complex values are considered real when their imaginary part is 0,5361
24797,When are complex values considered real?,when their imaginary part is 0. input(Tensor) – the input tensor,5361
24798,What is a boolean tensor that is whereinput is real and False elsewhere?,True,5361
24799,What is nonlinearity gain?,"Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\frac{5}{3}35​ ReLU 2\sqrt{2}2​ Leaky Relu 21+negative_slope2\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}1+negative_slope22​​ SELU 34\frac{3}{4}43​",10258
24800,What does this give the initial weights?,variance of1/N,8168
24801,What does the default gain forSELUsacrifice?,normalisation effect,8168
24802,What does the default gain forSELU sacrifice for more stable gradient flow in rectangular layers?,normalisation effect,140
24803,What is the value to fill the tensor with Examples?,an n-dimensionaltorch.Tensor val,8996
24804,Fills the input Tensor with what value1?,scalar,2425
24805,What value does a tensor fill the input Tensor with?,scalar value0,10827
24806,What value does the input Tensor fill with?,scalar,10761
24807,What does a tensor do when as many inputs are preserved as possible?,Preserves the identity of the inputs inLinearlayers,10832
24808,What value does Tensor val fill the input Tensor with?,scalar value,10832
24809,What does a tensor do in Linearlayers?,Preserves the identity of the inputs,10832
24810,Fills the 2-dimensional input tensorwith with scalar value1?,identity matrix,2421
24811,What is the function that preserves the identity of the inputs in Linearlayers?,Preserves the identity of the inputs in Linearlayers,2417
24812,What does Linearlayers do?,Preserves the identity of the inputs,11328
24813,What is the input Tensor filled with?,scalar value 0,10823
24814,What does Examples Fill the input Tensor with?,scalar value0,2332
24815,What does filling the 2-dimensional input tensor with the identity matrix do?,Preserves the identity of the inputs with the scalar value 1,2416
24816,What does tensor fill the 2-dimensional input tensor with?,the identity matrix,10822
24817,What does tensor do in Linearlayers?,Preserves the identity of the inputs,10822
24818,What does Examples fill the input Tensor with?,scalar value 0,2331
24819,What does Filling the 2-dimensional input tensor with the identity matrix do?,Preserves the identity of the inputs,2331
24820,What reduces the amount of matrix multiplications in a batch matrix-matrix product?,addbmm,4759
24821,Batch1 and batch2 must be what?,3-D tensors,4759
24822,What must batch1 and batch2 be?,3-D tensors,4760
24823,What does addbmm operator support?,This operator supportsTensorFloat32,4760
24824,What happens in a reduced add step?,all matrix multiplications get accumulated along the first dimension,4761
24825,"For inputs of typeFloatTensororDoubleTensor, arguments beta and alpha must be what",real numbers,4761
24826,"If input is ignored, andnanandinfin it will not be propagated.","Ifbetais 0,",4761
24827,What must batch1 and batch2 be?,3-D tensors,8947
24828,Which inputs will not be propagated ifbetais 0?,andnanandinfin,3601
24829,inputs of addbmm must be?,argumentsbetaandalphamust be real numbers otherwise they should be integers. This operator supportsTensorFloat32.,3601
24830,"If input is ignored, what is it?","Ifbetais 0,",3603
24831,What must argumentsbeta andalpha be for inputs of typeFloatTensororDoubleTensor?,argumentsbetaandalphamust be real numbers,3603
24832,What is the first batch of matrices to be multiplied?,batch1,2547
24833,What is the name of the Moore-Penrose inverse?,pseudoinverse,1798
24834,What is the most computationally convenient way to understand the pseudoinverse?,SVD,1798
24835,What does the Moon-Penrose inverse stand for?,the pseudoinverse,1798
24836,What does the SVD support?,batches of matrices,7239
24837,"What supports input of float, double, cfloat, and cdouble dtypes?",SVD,7239
24838,What is a matrix on the left multiplied by?,the pseudoinverse,7239
24839,The pseudoinverse is more computationally convenient to understand through what?,SVD,1800
24840,What part of the matrix is used in computations?,lower triangular part of the matrix,1800
24841,Ifhermitian= what is assumed to be Hermitian if complex or symmetric if real?,True,7237
24842,What is assumed to be Hermitian if complex or symmetric if real?,If hermitian= True,3615
24843,The singular values below the specifiedrcondthreshold are treated as what?,zero,3615
24844,What is assumed to be if complex or symmetric if real?,Hermitian,3615
24845,What values that are below the specifiedrcondthreshold are treated as zero and discarded in the computation?,The singular values,3615
24846,What does this function synchronize with the CPU for?,CUDA inputs,3615
24847,Supports input of what types of input?,"float, double, cfloat and cdouble dtypes",6128
24848,What type of matrices does Ais support?,batches of matrices,6128
24849,"Ifhermitian= what, is Ais assumed to be Hermitian if complex or symmetric if real?",True,6128
24850,What part of the matrix is used instead of Hermitian?,lower triangular part of the matrix,3613
24851,What should be done to the singular values that are below the specifiedrcondthreshold?,treated as zero and discarded in the computation.,3613
24852,"Supports input of float, double, cfloat and what other dtype?",cdouble,6130
24853,"Supports input of float, double, cfloat and cdouble dtypes. Also supports what?",batches of matrices,6130
24854,What does this function synchronize with the CPU?,CUDA inputs,6130
24855,What are the singular values that are below the specifiedrcondthreshold treated as?,zero,7298
24856,What function synchronizes a CUDA input with the CPU?,usestorch.linalg.svd()ifhermitian= False,7298
24857,Why is it always preferable to uselstsq()?,faster and more numerically stable,7569
24858,What is the default value of atorch.Tensor?,1e-15,5769
24859,What computes the inverse of a square matrix?,torch.linalg.inv(),5769
24860,What is PyTorch,PyTorch  torch is one of the most popular machine learning and deep learning frameworks,28
24861,Which package manager is recommended for PyTorch?,Anaconda package manager is recommended to install PyTorch,28
24862,Why you need CUDA toolkit?,CUDA toolkit is a development environment for creating high-performance applications,29
24863,Why you need CUDA toolkit?,CUDA toolkit is a development environment for creating high-performance applications,29
24864,How pytorch replaces numpy?,unlike numpy PyTorch uses power of GPUs and other accelerators,8945
24865,How pytorch differs from numpy ?,unlike numpy PyTorch uses power of GPUs and other accelerators,8945
24866,Differences between pytorch and numpy ?,unlike numpy PyTorch uses power of GPUs and other accelerators,8945
24867,The differences between pytorch and numpy  ?,unlike numpy PyTorch uses power of GPUs and other accelerators,8945
24868,The differences between pytorch and numpy  ?,unlike numpy PyTorch uses power of GPUs and other accelerators,8945
24869,what happens in model training,training a model is an iterative process  in each iteration  the model makes a guess about the output  calculates the error in its guess  collects the derivatives of the error with respect to its parameters and optimizes these parameters using gradient descent,9280
24870,what happens in model training,training a model is an iterative process  in each iteration  the model makes a guess about the output  calculates the error in its guess  collects the derivatives of the error with respect to its parameters and optimizes these parameters using gradient descent,9280
24871,What happens while a model is being trained? ,training a model is an iterative process  in each iteration  the model makes a guess about the output  calculates the error in its guess  collects the derivatives of the error with respect to its parameters and optimizes these parameters using gradient descent,9280
24872,What happens during the training of a model? ,training a model is an iterative process  in each iteration  the model makes a guess about the output  calculates the error in its guess  collects the derivatives of the error with respect to its parameters and optimizes these parameters using gradient descent,9280
24873,Which pytorch module helps in using own datasets ? ,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24874,Which Python package facilitates the use of one's own datasets? ,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24875,Which Python module makes working with one's own data simple? ,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24876,Which Python module makes it easy to work with one's own data?,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24877,what is the use of torch.utils.data.DataLoader?,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24878,What is torch.utils.data.DataLoader used for?,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24879,What is the purpose of torch.utils.data.DataLoader?,torch.utils.data.DataLoader and torch.utils.data.Dataset allow you to use pre-loaded datasets as well as your own data,1614
24880,What is the purpose of target_transform?,target_transform modifies the labels that accept callables containing the transformation logic,10706
24881,What is target transform's purpose?,target_transform modifies the labels that accept callables containing the transformation logic,10706
24882,What’s the difference between a Sequential and a torch.nn.ModuleList? ,"A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.",10707
24883,What exactly is the distinction between a Sequential and a torch.nn.ModuleList?  ,"A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.",10707
24884,What exactly is the distinction between a Sequential and a torch.nn.ModuleList?  ,"A ModuleList is exactly what it sounds like: a list where Modules can be stored! A Sequential, on the other hand, has layers that are connected in a cascading fashion.",10707
24885,What exactly is the distinction between a Sequential and a torch.nn.ModuleList?  ,"A ModuleList is exactly what it sounds like: a list of Modules! A Sequential, on the other hand, is composed of layers that are linked in a cascading way.",10707
24886,What exactly is the distinction between a Sequential and a torch.nn.ModuleList?  ,"A ModuleList is just that: a collection of Modules! A Sequential, on the other hand, is made up of layers that are connected in a cascading fashion.",10707
24887,What exactly is the distinction between a Sequential and a torch.nn.ModuleList?  ,"A ModuleList is nothing more than a collection of Modules! A Sequential, on the other hand, is composed of layers that are linked in a cascading way.",10707
24888, how does nn.leakyrelu differ from relu?  ,It allows a small gradient when the unit is not active,8847
24889, What distinguishes nn.leakyrelu from relu?  ,It allows a small gradient when the unit is not active,8847
24890,how to get random numbers in pytorch?  ,torch dot random returns a tensor filled with random numbers from a normal distribution with mean zero and variance one,10882
24891,how to get random numbers in pytorch?  ,torch dot random returns a tensor filled with random numbers from a normal distribution with mean zero and variance one,10882
24892,how to generate random numbers in pytorch?  ,torch dot random returns a tensor filled with random numbers from a normal distribution with mean zero and variance one,10882
24893,how to make random numbers with pytorch?  ,torch dot random returns a tensor filled with random numbers from a normal distribution with mean zero and variance one,10882
24894,how to make random numbers with pytorch?  ,torch dot random returns a tensor filled with random numbers from a normal distribution with mean zero and variance one,10882
24895,how dataloader works?  ,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24896,How does the dataloader work?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24897,What is the purpose of the dataloader?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24898,What is the purpose of the dataloader?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24899,What is the dataloader's purpose?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24900,What is the function of the dataloader?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24901,What role does the dataloader play?,"dataloader wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading",10879
24902,Is a cifar dataset available in torchvision?  ,"Yes, the torchvisiondatasets module contains dataset objects for many realworld vision data like cifar coco",10879
24903,Is a coco dataset available in torchvision?  ,"Yes, the torchvisiondatasets module contains dataset objects for many realworld vision data like cifar coco",10879
24904,what is optimization?  ,optimization is the process of adjusting model parameters to reduce model error in each training step,10316
24905,what is optimization?  ,optimization is the process of adjusting model parameters to reduce model error in each training step,10316
24906,which optimizers are supported in pytorch?  ,there are many different optimizers available in PyTorch such as ADAM and RMSProp,10316
24907,"In Pytorch, which optimizers are supported?",there are many different optimizers available in PyTorch such as ADAM and RMSProp,10316
24908,"In Pytorch, which optimizers are supported?","Many alternative optimizers, such as ADAM and RMSProp, are accessible in PyTorch. ",10316
24909,"In Pytorch, which optimizers are supported?","PyTorch has a number of different optimizers, such as ADAM and RMSProp. ",10316
24910,what is CTCloss?,Calculates loss between a continuous  time series and a target sequence,9136
24911,What's the difference between tf.nn.ctc_loss with pytorch.nn.CTCLoss,Calculates loss between a continuous  time series and a target sequence,9136
24912,What are sparse arrays,Sparse arrays have a property of having a vast portion of elements being equal to zero which means that a lot of memory as well as processor resources can be spared if only the non-zero elements are stored or/and processed,10288
24913,What are sparse arrays,"Sparse arrays have the attribute of having a large part of its items equal to zero, which means that if just the non-zero elements are kept or processed, a lot of memory and CPU resources can be saved. ",10288
24914,What are sparse arrays,"Sparse arrays have the attribute of having a large part of its items equal to zero, which means that if just the non-zero elements are kept or processed, a lot of memory and CPU resources can be saved. ",10288
24915,What is the use of stride argument in nn.conv2d function,stride controls the stride for the crosscorrelation a single number or a tuple,8871
24916,What is the use of padding argument in nn.conv2d function,padding controls the amount of padding applied to the input it can be either a string valid same or a tuple of ints giving the amount of implicit padding applied on both sides,8871
24917,What is the use of dilation argument in nn.conv2d function,dilation controls the spacing between the kernel points,8871
24918,can i use tuple for padding argument in nn.conv2d function,padding controls the amount of padding applied to the input it can be either a string valid same or a tuple of ints giving the amount of implicit padding applied on both sides,8871
24919,can i use dictionary for padding argument in nn.conv2d function,"no, padding controls the amount of padding applied to the input it can be either a string valid same or a tuple of ints giving the amount of implicit padding applied on both sides",8871
24920,can i use dictionary for stride argument in nn.conv2d function,stride controls the stride for the crosscorrelation a single number or a tuple,8871
24921,which argument  controls the spacing between the kernel points,dilation controls the spacing between the kernel points,8871
24922,how to control the spacing between the kernel points in conv2d,dilation controls the spacing between the kernel points,8871
24923,how to control the spacing between the kernel points in conv2d,dilation controls the spacing between the kernel points,4202
24924,can tenor datatype be changed?,you can convert a tensor to a different type  by specifying it at initialization or later using one of the typecasting methods.,2235
24925,Is it possible to alter the datatype of a tensor?,you can convert a tensor to a different type  by specifying it at initialization or later using one of the typecasting methods.,2235
24926,Is it feasible to change a tensor's datatype?,you can convert a tensor to a different type  by specifying it at initialization or later using one of the typecasting methods.,2235
24927,how tensors are initialized?,"There are two ways to specify the initialization type: either by directly calling the constructor of a specific tensor type, such as FloatTensor or LongTensor, or using a special method, torch.tensor(), and providing the dtype.",2235
24928,how to initialize tensor?,"There are two ways to specify the initialization type: either by directly calling the constructor of a specific tensor type, such as FloatTensor or LongTensor, or using a special method, torch.tensor(), and providing the dtype.",2235
24929,what are the ways to initialize tensor?,"There are two ways to specify the initialization type: either by directly calling the constructor of a specific tensor type, such as FloatTensor or LongTensor, or using a special method, torch.tensor(), and providing the dtype.",2235
24930,how to check GPU is available?,we can check  whether a GPU is available by using torch.cuda.is_available(),4964
24931,how to check GPU is available?,we can check  whether a GPU is available by using torch.cuda.is_available(),4964
24932,is there a way to check GPU is available?,we can check  whether a GPU is available by using torch.cuda.is_available(),4964
24933,Is there a method to check whether or not a GPU is accessible?,we can check  whether a GPU is available by using torch.cuda.is_available(),4964
24934,is there inbuilt sigmoid function?,Pytorch has inbuild sigmoid function. torch implements the sigmoid as torch.sigmoid(),4954
24935,how to use sigmoid function?,Pytorch has inbuild sigmoid function. torch implements the sigmoid as torch.sigmoid(),4954
24936,how  tanh is different from sigmoid function?,"tanh is simply a linear transform of the sigmoid function tanh, likethe sigmoid, is also a “squashing” function, except that it maps the set of real values from (–∞, +∞) tothe range [­1, +1].",7319
24937,what is relu?,it is activation function. it clips the negative values to zero,5100
24938,what is softmax?,the softmax function squashes the output of each unit to be between 0 and 1,1140
24939,how is softmax different from simoid?,"the softmax operation also divides each output by the sum of all the outputs, which gives us a discrete probability distribution",1140
24940,how is softmax different from simoid?,"In addition, the softmax operation divides each output by the total of all outputs, yielding a discrete probability distribution.",1140
24941,how is softmax different from simoid?,"Furthermore, the softmax operation divides each output by the sum of all outputs, resulting in a discrete probability distribution.",1140
24942,what is the use of categorical crossentropy loss?,the categorical crossentropy loss is typically used in a multiclass classification setting in which the outputs are interpreted as predictions of class membership probabilitie,10842
24943,where is categorical crossentropy loss used?,the categorical crossentropy loss is typically used in a multiclass classification setting in which the outputs are interpreted as predictions of class membership probabilitie,10842
24944,what is the use of categorical crossentropy loss?,the categorical crossentropy loss is typically used in a multiclass classification setting in which the outputs are interpreted as predictions of class membership probabilitie,10842
24945,which loss is used for multiclass classification?,the categorical crossentropy loss is typically used in a multiclass classification setting in which the outputs are interpreted as predictions of class membership probability,10842
24946,what is supervised learning?,Supervised learning is the problem of learning how to map observations to specified targets given labeled examples,6091
24947,is training data labelled in  supervised learning?,Supervised learning is the problem of learning how to map observations to specified targets given labeled examples,6091
24948,is labelled data needed in  supervised learning?,Supervised learning is the problem of learning how to map observations to specified targets given labeled examples,6091
24949,is labelled data needed in  supervised learning?,Supervised learning is the problem of learning how to map observations to specified targets given labeled examples,6091
24950,what could be the issue with SGD optimizer?,"SGD has convergence issues, often leading to poorer models",6940
24951,what is default learning rate for adam optimizer?,"With Adam, the default learning rate is 0.001. ",6940
24952,what is use of zero_grad()?,"any bookkeeping information, such as gradients, currently stored inside the model (perceptron) object is cleared with a function named zero_grad() ",4102
24953,what is use of backward()?,iteratively propagates the loss backward through the computational graph and notifies each parameter of its gradient,4102
24954,what is use of optimizer?,the optimizer(opt) instructs the parameters how to update their values knowing the gradient with a function named step(),4102
24955,what is use of backward() functntion of loss object?,iteratively propagates the loss backward through the computational graph and notifies each parameter of its gradient,4102
24956,which function is used to propagate loss backwards?,backward() iteratively propagates the loss backward through the computational graph and notifies each parameter of its gradient,4102
24957,how to propagate loss backwards in pytorch?,backward() iteratively propagates the loss backward through the computational graph and notifies each parameter of its gradient,4102
24958,which function is used to update value of weights, the optimizer (opt) instructs the parameters how to update their values knowing the gradient with a function named step().,4102
24959,what is the use of step() function? , the optimizer (opt) instructs the parameters how to update their values knowing the gradient with a function named step().,4102
24960,What are arguments to embedding layer,The Embedding layer that is used here is parameterized primarily by two numbers: the number of embeddings (size of the vocabulary) and the size of the embeddings (embedding dimension),22
24961,does  embedding layer need size of the vocabulary? ,The Embedding layer that is used here is parameterized primarily by two numbers: the number of embeddings (size of the vocabulary) and the size of the embeddings (embedding dimension),22
24962,does  embedding layer need embedding dimension? ,The Embedding layer that is used here is parameterized primarily by two numbers: the number of embeddings (size of the vocabulary) and the size of the embeddings (embedding dimension),22
24963,Does the embedding layer require an embedding dimension?,The Embedding layer that is used here is parameterized primarily by two numbers: the number of embeddings (size of the vocabulary) and the size of the embeddings (embedding dimension),22
24964,Is embedding dimension required for the embedding layer?,The Embedding layer that is used here is parameterized primarily by two numbers: the number of embeddings (size of the vocabulary) and the size of the embeddings (embedding dimension),22
24965,What are the functions utilised for reinforcement??,The distributions package contains parameterizable probability distributions and sampling functions,7032
24966,What is included in the distribution package? ,The distributions package contains parameterizable probability distributions and sampling functions,7032
24967,Where is reinforce function used? ,REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning,7032
24968,Where is pathwise derivative estimator  function used? ,the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders.,7032
24969,Which functions are used to implement reinforce? , we only need sample() and log_prob() to implement REINFORCE,11366
24970,Which functions are employed in the implementation of reinforce? , we only need sample() and log_prob() to implement REINFORCE,11366
24971,Which functions are used in the reinforcement implementation? , we only need sample() and log_prob() to implement REINFORCE,11366
24972,Which functions are utilised in the execution of reinforcement? , we only need sample() and log_prob() to implement REINFORCE,11366
24973,Which functions are used to carry out reinforcement? , we only need sample() and log_prob() to implement REINFORCE,11366
24974,What functions are utilised for reinforcement? , we only need sample() and log_prob() to implement REINFORCE,11366
24975,What are the functions that are used for reinforcement? , we only need sample() and log_prob() to implement REINFORCE,11366
24976,What are the functions that are used for reinforcement? , we only need sample() and log_prob() to implement REINFORCE,11366
24977,What is the use of packageimporter and packageexporter? , packageimporter and packageexporter provide a filestructure method which will return a printable and queryable folder,11307
24978,What do packageimporter and packageexporter do? , packageimporter and packageexporter provide a filestructure method which will return a printable and queryable folder,11307
24979,What are the functions of packageimporter and packageexporter? , packageimporter and packageexporter provide a filestructure method which will return a printable and queryable folder,11307
24980,What are the function of packageimporter ? ,a packageimporter will add the attribute torchpackage to every module that it initializes your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not,8764
24981,How torch.package finds your code’s dependencies ,PackageExporter will pickle the object normally. The dependency resolver will gather up all GLOBAL ops and mark them as dependencies of your pickled object.,8394
24982,How torch.package finds module dependencies ,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24983,How does torch.package discover module dependencies? ,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24984,Torch.package discovers module dependencies in what way? ,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24985,In what method does Torch.package find module dependencies?,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24986,Torch.package determines module dependencies in what way?,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24987,How does Torch.package detect module dependencies?,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24988,How does Torch.package detect module dependencies?,"torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies",1134
24989,How does I see what inside  package?,"The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should work for exploring the contents.",7999
24990,How can I view what's inside the package?,"The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should work for exploring the contents.",7999
24991,How can I find out what's inside the package?,"The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should work for exploring the contents.",7999
24992,what is Distributed Data-Parallel Training?,"Distributed Data-Parallel Training (DDP) is a widely adopted single-program multiple-data training paradigm. With DDP, the model is replicated on every process, and every model replica will be fed with a different set of input data samples. DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training.",1313
24993,what is Distributed Data-Parallel Training?,"Distributed Data-Parallel Training (DDP) is a widely adopted single-program multiple-data training paradigm. With DDP, the model is replicated on every process, and every model replica will be fed with a different set of input data samples. DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training.",1313
24994, is Distributed Data-Parallel Training used for single-program multiple-data training?,"Distributed Data-Parallel Training (DDP) is a widely adopted single-program multiple-data training paradigm. With DDP, the model is replicated on every process, and every model replica will be fed with a different set of input data samples. DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training.",1313
24995, how Distributed Data-Parallel Training  keep model replicas synchronized?,DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training,1313
24996, does  Distributed Data-Parallel Training  keep model replicas synchronized?,DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training,1313
24997,whic is used for p2p communication DDP or RPC?,RPC is used for p2p communication,1313
24998,whic is used for p2p communication DDP or RPC?,RPC is used for p2p communication,1313
24999,"Which  is utilised for peer-to-peer communication, DDP or RPC?",RPC is used for p2p communication,1313
25000,"DDP or RPC, is used for peer-to-peer communication?",RPC is used for p2p communication,1313
25001,what are named tensor?,Named Tensors allow users to give explicit names to tensor dimensions,4297
25002,can I give names to tensor dimension?,Named Tensors allow users to give explicit names to tensor dimensions,4297
25003,can I give name tensor dimension?,Named Tensors allow users to give explicit names to tensor dimensions,4297
25004,Can I assign a name to the tensor dimension?,Named Tensors allow users to give explicit names to tensor dimensions,4297
25005,what is the use of named tensor?,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25006,What is the function of a named tensor??,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25007,WWhat is a named tensor and what does it do?,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25008,What is the purpose of a named tensor?,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25009,What is a named tensor's purpose?,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25010,What is the purpose of a named tensor?,Named tensors use names to automatically check that APIs are being called correctly at runtime.,4298
25011,What happens during check names?, an operator may perform automatic checks at runtime that check that certain dimension names must match.,4298
25012,What happens in check names step?, an operator may perform automatic checks at runtime that check that certain dimension names must match.,4298
25013,in which step dimensions names are checked?, check names,4298
25014,are dimension names checked during name inferencing?," yes, during check names step",4298
25015,what does propagate names step do?, name inference propagates names to output tensors,4298
25016,what happens in propagate names step ?, name inference propagates names to output tensors,4298
25017,are names propagated to output tensor ?, name inference propagates names to output tensors,4298
25018,name propagation happens in runtime ?, name inference consists of the following two steps:Check names: an operator may perform automatic checks at runtime that check that certain dimension names must match Propagate names: name inference propagates names to output tensors.,4298
25019,name propagation happens in runtime ?, name inference consists of the following two steps:Check names: an operator may perform automatic checks at runtime that check that certain dimension names must match Propagate names: name inference propagates names to output tensors.,4298
25020,are names propagated to output tensor ?, name inference propagates names to output tensors,4299
25021,bleu_score function is in which module of pytorch ?, torchtext.data.metrics,11244
25022,in which module i can find bleu_score ?, torchtext.data.metrics,11244
25023,which torchtext module has blue_score ?, torchtext.data.metrics,11244
25024,where can i find bleu_score?, torchtext.data.metrics,11244
25025,where can i find bleu_score function?, torchtext.data.metrics,11244
25026,where can i find bleu_score function?, torchtext.data.metrics,11244
25027, weights argument datatype in blue_score?,  a list of weights used for each n-gram category (uniform by default),11244
25028," how to specify the max ,n-grams in blue_score?",  you could specify by using max_n argument,11244
25029, what is candidate_corpus arg in blue_score for?,  candidate_corpus is an iterable of candidate translations. Each translation is an iterable of tokens,11244
25030, what is purpose of candidate_corpus arg in blue_score for?,  candidate_corpus is an iterable of candidate translations. Each translation is an iterable of tokens,11244
25031, what is purpose of candidate_corpus arg in blue_score for?,  candidate_corpus is an iterable of candidate translations. Each translation is an iterable of tokens,11244
25032, what is purpose of references_corpus arg in blue_score for?, references_corpus is an iterable of iterables of reference translations. Each translation is an iterable of tokens,11244
25033, what is pytorch geometric for?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25034, which module is for geometric deep learning?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25035,does pytorch has geometric deep learning support?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25036,which pytorch extension support geometric deep learning ?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25037,can we do geometric deep learning in pytorch?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25038,does pytorh suport geometric deep learning?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25039,does pytorh suport  deep learning on graphs?," PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers",4943
25040,does pytorh suport  3D meshes?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support 3D meshes,4943
25041,does pytorh suport  3D meshes?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support 3D meshes,4943
25042,does pytorh suport point clouds?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25043,which library support point clouds?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25044,does pytorch has a library support for point clouds?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25045,are point clouds supported?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25046,is there point clouds support in pytorch?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25047,how pytorch support point clouds?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25048,how pytorch support 3D meshes?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support point clouds,4943
25049,does PyTorch Geometric support multi gpu?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25050,are mutiple gpus supported by ptorch geometric?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25051,is ther  mutiple gpus support by ptorch geometric?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25052,is ther  mutiple gpus support by ptorch geometric?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25053, mutiple gpus supported by ptorch geometric?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25054, mutiple gpus are supported by ptorch geometric?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25055,  ptorch geometric has multiple gpu support?, PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25056,Is ptorch geometric compatible with many GPUs? , PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25057,are multiple gpus compatible with pytorch geometric? , PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25058,multiple gpus compatible with pytorch geometric? , PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
25059,multiple gpus are compatible with pytorch geometric? , PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It support multiple gpus,4943
