,group_id,context,groups
0,0,"  A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of
a model requires some control-flow even though most of the model is just a feed-forward
network. Control-flow inside of a script function called by a traced function is
preserved correctly. Example (calling a script function in a traced function): ","[590, 595, 596]"
1,1,"  Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.    ","[256, 260, 271, 287, 288, 289, 290, 291, 293, 294, 297, 298, 299, 300, 301, 302]"
2,2,"  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ","[679, 681, 683, 685]"
3,3,"  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.    ",[673]
4,4,  Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   ,"[876, 880]"
5,5,"  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element in input. ","[174, 187]"
6,6,"  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ","[175, 222, 230, 243, 244]"
7,7,"  Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    ",[815]
8,8,"  Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    Ifinput is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinput is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",[816]
9,9,"  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",[720]
10,10,"  Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",[107]
11,11,"  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ","[303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 689]"
12,12,"  Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. ","[575, 577, 578]"
13,13,"  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is a LongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is a BoolTensor.   Moves the dimension(s) of input at the position(s) insourceto the position(s) indestination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of inputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values frominput at the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ","[195, 197, 198, 199, 205, 207, 208, 209, 210, 211, 212]"
14,14, Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   ,"[837, 851, 854, 856]"
15,15," The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. ",[516]
16,16," torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   ","[247, 259, 261]"
17,17," torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   ","[251, 255, 265, 270, 272]"
18,18, torch.atanh(). ,[892]
19,19, torch.le(). ,[31]
20,20, torch.ne(). ,"[467, 468]"
21,21,"%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when
we first traced it and when we traced it with thecheck_inputs. Indeed,
the loop within the body ofloop_in_traced_fndepends on the shape
of the inputx, and thus when we try anotherxwith a different
shape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: ","[626, 628, 629]"
22,22,"**default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) ",[482]
23,23,"2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.
These changes combine to provide a simpler, easier-to-use API for converting
yournn.Modules intoScriptModules, ready to be optimized and executed in a
non-Python environment. The new usage looks like this: The module’sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place of torch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. ","[637, 639, 642, 643, 644, 645]"
24,24,"21+negative_slope2\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}1+negative_slope22​​ SELU 34\frac{3}{4}43​ Warning In order to implementSelf-Normalizing Neural Networks,
you should usenonlinearity='linear'instead ofnonlinearity='selu'.
This gives the initial weights a variance of1/N,
which is necessary to induce a stable fixed point in the forward pass.
In contrast, the default gain forSELUsacrifices the normalisation
effect for more stable gradient flow in rectangular layers. ",[941]
25,25,"AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.
If theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the
code of a method namedfooon aScriptModuleby accessing.foo.code.
The example above produces this output: This is TorchScript’s compilation of the code for theforwardmethod.
You can use this to ensure TorchScript (tracing or scripting) has captured
your model code correctly. TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. ","[609, 615, 616, 617]"
26,26,"Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. ",[488]
27,27,"Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save ","[561, 568]"
28,28,"Beta:Features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete. For Beta features, we are
committing to seeing the feature through to the Stable classification.
We are not, however, committing to backwards compatibility. ","[66, 67]"
29,29,"Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   ","[546, 552, 554]"
30,30,"By default,dimis the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size asinputexcept in the dimension dimwhere they are of size 1.
Otherwise,dimis squeezed (seetorch.squeeze()), resulting
in the output tensors having 1 fewer dimension than input. Note This function is not defined for torch.cuda.Tensoryet. input(Tensor) – the input tensor. dim(int) – the dimension to reduce. keepdim(bool) – whether the output tensor hasdimretained or not. ","[451, 452, 455, 456, 457, 911]"
31,31,"Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): ","[583, 584]"
32,32,"Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    ",[814]
33,33,"Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   ","[321, 341, 342]"
34,34,"Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) ofinputandother.   ",[717]
35,35,Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ,"[841, 843, 846]"
36,36,Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   ,[849]
37,37,"Computes the absolute value of each element in input.   Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   ","[246, 249, 258]"
38,38,"Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   ","[862, 870, 872]"
39,39,"Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   ","[381, 385, 386]"
40,40,"Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ","[370, 868, 873, 878]"
41,41,"Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ","[355, 356, 357, 361, 369, 377, 378, 382, 383, 384, 387, 388, 389, 855, 861]"
42,42,"Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",[83]
43,43,"Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   ","[253, 257, 375]"
44,44,"Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   ","[676, 678]"
45,45,"Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   ","[682, 686]"
46,46,"Computes the entropy oninput(as defined below), elementwise. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also knfor torchthe logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. ",[403]
47,47,"Computes the entropy oninput(as defined below), elementwise. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. ","[394, 396, 401, 402]"
48,48,"Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. ",[860]
49,49,"Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   ","[248, 262, 264]"
50,50,"Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. ","[972, 973, 975]"
51,51,"Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: ","[976, 979]"
52,52,"Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is a LongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is a BoolTensor.   Moves the dimension(s) of input at the position(s) insourceto the position(s) indestination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of inputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values frominput at the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   ","[191, 192, 193, 200]"
53,53,"Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ","[169, 186]"
54,54,"Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, of inputandother.   ","[317, 328, 329, 330, 332]"
55,55,"Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ","[188, 189, 190]"
56,56,"Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is a LongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is a BoolTensor.   Moves the dimension(s) of input at the position(s) insourceto the position(s) indestination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of inputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values frominput at the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   ","[194, 196, 201, 202, 203, 204, 206]"
57,57,"Creates a new tensor by horizontally stacking the tensors in tensors. Equivalent totorch.hstack(tensors), except each zero or one dimensional tensortin tensorsis first reshaped into a(t.numel(),1)column before being stacked horizontally. tensors(sequence of Tensors) – sequence of tensors to concatenate out(Tensor,optional) – the output tensor. Example: ","[33, 34]"
58,58,"Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",[238]
59,59,"Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): ","[585, 586, 589, 591, 592]"
60,60,"Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable
JIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script
is calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the
TorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This
pretty-printer gives an interpretation of the script method’s code as valid
Python syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.
If theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the
code of a method namedfooon aScriptModuleby accessing.foo.code.
The example above produces this output: This is TorchScript’s compilation of the code for theforwardmethod.
You can use this to ensure TorchScript (tracing or scripting) has captured
your model code correctly. TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. ","[598, 599, 600, 601]"
61,61,"Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) ",[761]
62,62,"Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) normalized(bool) – Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) length(Optional[int]) – The amount to trim the signal by (i.e. the
original signal length). (Default: whole signal) ","[753, 758, 769]"
63,63,"Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   ","[549, 550, 562]"
64,64,"Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",[168]
65,65,"Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it
successfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) – Controls whether to enable flush denormal mode or not Example: ","[42, 43, 44, 45]"
66,66,"Divides each element of the input input by the corresponding element of other. Supports broadcasting to a common shape,type promotion, and integer, float, and complex inputs.
Always promotes integer types to the default scalar type. input(Tensor) – the dividend other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ",[8]
67,67,"Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   ","[724, 725]"
68,68,"Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ","[216, 221, 234, 237, 239, 245, 888]"
69,69,"Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. ",[486]
70,70,"Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier ",[884]
71,71,"Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ","[957, 958]"
72,72,"Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input tensorwith the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input tensorwith values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. ",[953]
73,73,"Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). ",[61]
74,74,"Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). Beta:Features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete. For Beta features, we are
committing to seeing the feature through to the Stable classification.
We are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of
binary distributions like PyPI or Conda, except sometimes behind run-time
flags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries ","[63, 69, 70, 78]"
75,75,"Fills the input Tensor with the scalar value 1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ",[954]
76,76,"Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ",[950]
77,77,"Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input Tensor with the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input Tensor with the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inDelving deep into rectifiers: Surpassing human-level
performance on ImageNet classification- He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled fromU(−bound,bound)\mathcal{U}(-\text{bound}, \text{bound})U(−bound,bound)where Also known as He initialization. tensor– an n-dimensionaltorch.Tensor ",[114]
78,78,"Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input tensorwith the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input tensorwith values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input tensorwith values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input tensorwith values according to the method
described inDelving deep into rectifiers: Surpassing human-level
performance on ImageNet classification- He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled fromU(−bound,bound)\mathcal{U}(-\text{bound}, \text{bound})U(−bound,bound)where Also known as He initialization. tensor– an n-dimensionaltorch.Tensor ",[948]
79,79,"Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples ",[943]
80,80,"For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. ","[574, 576]"
81,81,"For each row of input in the given dimension dim,
returnsTrueif any element in the row evaluate toTrueandFalseotherwise. If keepdim is True, the output tensor is of the same size
asinputexcept in the dimension dimwhere it is of size 1.
Otherwise,dimis squeezed (seetorch.squeeze()), resulting in
the output tensor having 1 fewer dimension than input. input(Tensor) – the input tensor. dim(int) – the dimension to reduce. keepdim(bool) – whether the output tensor hasdimretained or not. ","[905, 908, 909]"
82,82,"For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) – the first batch of matrices to be multiplied batch2(Tensor) – the second batch of matrices to be multiplied beta(Number,optional) – multiplier for input(β\betaβ) input(Tensor) – matrix to be added alpha(Number,optional) – multiplier forbatch1 @ batch2(α\alphaα) out(Tensor,optional) – the output tensor. Example: ",[970]
83,83,"Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of
a model requires some control-flow even though most of the model is just a feed-forward
network. Control-flow inside of a script function called by a traced function is
preserved correctly. ","[579, 593, 594]"
84,84,"Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   ","[548, 563, 564]"
85,85,"Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ","[424, 438]"
86,86,"Gathers values along an axis specified by dim. For a 3-D tensor the output is specified by: inputandindexmust have the same number of dimensions.
It is also required that index.size(d)<=input.size(d)for all
dimensionsd!=dim.outwill have the same shape asindex.
Note thatinputandindexdo not broadcast against each other. input(Tensor) – the source tensor dim(int) – the axis along which to index index(LongTensor) – the indices of elements to gather sparse_grad(bool,optional) – IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) – the destination tensor Example: ","[927, 928, 929]"
87,87,"Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ","[178, 180]"
88,88,"Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. ","[502, 507, 511]"
89,89,"If center is True, then there will be padding e.g.'constant','reflect', etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
calculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0] ","[745, 746, 747, 748]"
90,90,"If keepdim is True, the output tensors are of the same size asinputexcept in the dimension dimwhere they are of size 1.
Otherwise,dimis squeezed (seetorch.squeeze()), resulting
in the output tensors having 1 fewer dimension than input. Note This function is not defined for torch.cuda.Tensoryet. input(Tensor) – the input tensor. dim(int) – the dimension to reduce. keepdim(bool) – whether the output tensor hasdimretained or not. out(tuple,optional) – the result tuple of two output tensors (values, indices) ","[453, 458, 462, 463]"
91,91,"If window_length=1=1=1, the returned window contains a single value 1. window_length(int) – the size of returned window periodic(bool,optional) – If True, returns a window to be used as periodic
function. If False, return a symmetric window. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. ",[53]
92,92,"Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. ","[964, 965]"
93,93,"Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) – the first batch of matrices to be multiplied batch2(Tensor) – the second batch of matrices to be multiplied beta(Number,optional) – multiplier for input(β\betaβ) input(Tensor) – matrix to be added ","[968, 969]"
94,94,"Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note ","[984, 988]"
95,95,"Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more
numerically stable than computing the pseudoinverse explicitly. Warning ","[980, 987, 989, 990, 992]"
96,96,"Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) – the size of returned window periodic(bool,optional) – If True, returns a window to be used as periodic
function. If False, return a symmetric window. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. device(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(seetorch.set_default_tensor_type()).devicewill be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) – If autograd should record operations on the
returned tensor. Default:False. A 1-D tensor of size (	ext{window_length},)(window_length,) containing the window",[51]
97,97,"Important consideration in the parameterswindowandcenterso that the envelop
created by the summation of all the windows is never zero at certain point in time. Specifically,∑t=−∞∞∣w∣2[n−t×hop_length]=0\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0∑t=−∞∞​∣w∣2[n−t×hop_length]=​0. Since stft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False
since the signal isn’t padded). ","[734, 735, 736, 738, 739]"
98,98,"Important consideration in the parameterswindowandcenterso that the envelop
created by the summation of all the windows is never zero at certain point in time. Specifically,∑t=−∞∞∣w∣2[n−t×hop_length]=0\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0∑t=−∞∞​∣w∣2[n−t×hop_length]=​0. Since stft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False
since the signal isn’t padded). If center is True, then there will be padding e.g.'constant','reflect', etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
calculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0] ","[737, 743, 744]"
99,99,"In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of
a model requires some control-flow even though most of the model is just a feed-forward
network. Control-flow inside of a script function called by a traced function is
preserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate
a submodule using tracing that can be called from the methods of a script module. Example (using a traced module): ",[597]
100,100,"Inverse short time Fourier Transform. This is expected to be the inverse of stft().
It has the same parameters (+ additional optional parameter oflength) and it should return the
least squares estimation of the original signal. The algorithm will check using the NOLA condition (
nonzero overlap). ",[543]
101,101,"Inverse short time Fourier Transform. This is expected to be the inverse of stft().
It has the same parameters (+ additional optional parameter oflength) and it should return the
least squares estimation of the original signal. The algorithm will check using the NOLA condition (
nonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop
created by the summation of all the windows is never zero at certain point in time. Specifically,∑t=−∞∞∣w∣2[n−t×hop_length]=0\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0∑t=−∞∞​∣w∣2[n−t×hop_length]=​0. Since stft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False
since the signal isn’t padded). If center is True, then there will be padding e.g.'constant','reflect', etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
calculated without additional information. ","[542, 544, 733]"
102,102,"It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ","[808, 809, 810, 812]"
103,103,"Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   ","[551, 565, 566]"
104,104,"Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   ","[569, 570]"
105,105,"Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place of torch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. ","[641, 654, 655]"
106,106,"Note This function is not defined for torch.cuda.Tensoryet. input(Tensor) – the input tensor. dim(int) – the dimension to reduce. keepdim(bool) – whether the output tensor hasdimretained or not. out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ","[465, 466]"
107,107,"Note This function matches the behaviour of NumPy in returning
output of dtypeboolfor all supported dtypes exceptuint8.
Foruint8the dtype of output isuint8itself. Example: For each row of input in the given dimension dim,
returnsTrueif any element in the row evaluate toTrueandFalseotherwise. ","[898, 900, 901, 902]"
108,108,"Note that non-integerstepis subject to floating point rounding errors when
comparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) – the starting value for the set of points. Default:0. end(Number) – the ending value for the set of points step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. layout(torch.layout, optional) – the desired layout of returned Tensor.
Default:torch.strided. ","[52, 779, 780, 786, 789, 790, 889]"
109,109,"Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. ","[522, 530, 531, 532]"
110,110,"Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   ",[360]
111,111,"Performs a batch matrix-matrix product of matrices stored
in batch1 and batch2,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).input is added to the final result. batch1andbatch2must be 3-D tensors each containing the
same number of matrices. Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. ","[959, 962]"
112,112,"Performs a batch matrix-matrix product of matrices stored
in batch1 and batch2,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).input is added to the final result. batch1andbatch2must be 3-D tensors each containing the
same number of matrices. Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) – the first batch of matrices to be multiplied batch2(Tensor) – the second batch of matrices to be multiplied beta(Number,optional) – multiplier for input(β\betaβ) input(Tensor) – matrix to be added alpha(Number,optional) – multiplier forbatch1 @ batch2(α\alphaα) ","[960, 966]"
113,113,"Performs a batch matrix-matrix product of matrices stored
in batch1 and batch2,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).input is added to the final result. batch1andbatch2must be 3-D tensors each containing the
same number of matrices. Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) – the first batch of matrices to be multiplied batch2(Tensor) – the second batch of matrices to be multiplied beta(Number,optional) – multiplier for input(β\betaβ) input(Tensor) – matrix to be added alpha(Number,optional) – multiplier forbatch1 @ batch2(α\alphaα) out(Tensor,optional) – the output tensor. Example: ","[961, 967, 971]"
114,114,"Performs a batch matrix-matrix product of matrices stored
inbatch1andbatch2,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the
same number of matrices. Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) – the first batch of matrices to be multiplied batch2(Tensor) – the second batch of matrices to be multiplied beta(Number,optional) – multiplier forinput(β\betaβ) input(Tensor) – matrix to be added alpha(Number,optional) – multiplier forbatch1 @ batch2(α\alphaα) out(Tensor,optional) – the output tensor. Example: ","[122, 123]"
115,115,"Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   ","[817, 821, 822, 823]"
116,116,"Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ","[818, 820, 847]"
117,117,Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   ,"[832, 838, 848, 865, 867, 871, 874]"
118,118,"Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   ","[7, 365, 367, 368, 858, 863, 866]"
119,119,"Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ","[347, 350, 351, 358]"
120,120,Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   ,[828]
121,121,Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   ,"[819, 826, 829]"
122,122,"Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   ","[348, 359]"
123,123,Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ,"[827, 830, 831]"
124,124,Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   ,"[824, 825, 839]"
125,125,"Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   ","[349, 362]"
126,126,"Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   ","[250, 252, 263, 273, 274, 275, 278, 279, 280]"
127,127,"Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",[672]
128,128,"Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   ","[267, 276, 277, 281, 282, 284, 285, 286]"
129,129,"Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",[675]
130,130,Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ,"[833, 835]"
131,131,Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   ,"[834, 836, 852, 853]"
132,132,"Performs the outer-product of vectors vec1 and vec2and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   ","[364, 366]"
133,133,"Prototype:These features are typically not available as part of
binary distributions like PyPI or Conda, except sometimes behind run-time
flags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries ","[72, 73, 75, 76, 913]"
134,134,"PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. ",[555]
135,135,"PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   ",[556]
136,136,"Python 3 type hints can be used in place of torch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. ","[656, 658]"
137,137,"Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   ",[558]
138,138,"Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   ","[557, 559, 560]"
139,139,"Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ","[172, 185]"
140,140,"References TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any TorchScript program can be saved from a Python
process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program
to a TorchScript program that can be run independently from Python, such as in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools in Python and then export
the model via TorchScript to a production environment where Python programs may be disadvantageous
for performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   ","[545, 547, 553, 567, 571, 572, 573]"
141,141,"ReplacesNaN, positive infinity, and negative infinity values in inputwith the values specified bynan,posinf, andneginf, respectively.
By default,NaN`sarereplacedwithzero,positiveinfinityisreplacedwiththegreatestfinitevaluerepresentableby:attr:`input’s dtype, and negative infinity
is replaced with the least finite value representable byinput’s dtype. input(Tensor) – the input tensor. nan(Number,optional) – the value to replaceNaNs with. Default is zero. ","[0, 1, 2, 3]"
142,142,"Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. ",[501]
143,143,"Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer ","[503, 534, 539, 540, 541]"
144,144,"Returns True if obj is a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).
Using that isinstance check is better for typechecking with mypy,
and more explicit - so it’s recommended to use that instead ofis_tensor. obj(Object) – Object to test Example: ","[38, 41]"
145,145,"Returns True if obj is a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).
Using thatisinstancecheck is better for typechecking with mypy,
and more explicit - so it’s recommended to use that instead ofis_tensor. obj(Object) – Object to test Example: ","[35, 36, 37, 39, 40]"
146,146,"Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinput is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",[811]
147,147,"Returns True if the data type of inputis a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of inputis a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",[184]
148,148,"Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   ",[167]
149,149,"Returns True ifobjis a PyTorch storage object.   Returns True if the data type of inputis a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of inputis a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ","[162, 171, 173]"
150,150,"Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type of inputis a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of inputis a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",[170]
151,151,"Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when
comparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) – the starting value for the set of points. Default:0. ","[781, 782]"
152,152,"Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when
comparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) – the starting value for the set of points. Default:0. end(Number) – the ending value for the set of points step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. ","[778, 783, 792, 797]"
153,153,"Returns a 1-dimensional view of each input tensor with zero dimensions.   Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   ","[314, 315, 318, 323]"
154,154,"Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   ","[316, 319, 320, 324, 325, 327]"
155,155,"Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    Ifinput is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinput is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",[813]
156,156,"Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimensiondim.   ","[322, 326, 331, 334, 338, 339, 340, 343, 344, 345, 346]"
157,157,"Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of
elements of input in the dimension dim. Andindicesis the index
location of each maximum value found in the dimension dim. input(Tensor) – the input tensor. dim(int) – the dimension to do the operation over out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ",[925]
158,158,"Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of
elements ofinputin the dimensiondim. Andindicesis the index
location of each maximum value found in the dimensiondim. input(Tensor) – the input tensor. dim(int) – the dimension to do the operation over out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ",[92]
159,159,"Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of
elements of input in the dimension dim. Andindicesis the index
location of each maximum value found in the dimension dim. input(Tensor) – the input tensor. dim(int) – the dimension to do the operation over out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ","[445, 926]"
160,160,"Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of
elements ofinputin the dimensiondim. Andindicesis the index
location of each maximum value found in the dimensiondim. input(Tensor) – the input tensor. dim(int) – the dimension to do the operation over out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ",[93]
161,161,"Returns a namedtuple(values,indices)wherevaluesis the mode
value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often
in that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of the input tensor. ","[447, 448, 449]"
162,162,"Returns a namedtuple(values,indices)wherevaluesis the mode
value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often
in that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size asinputexcept in the dimension dimwhere they are of size 1.
Otherwise,dimis squeezed (seetorch.squeeze()), resulting
in the output tensors having 1 fewer dimension than input. Note This function is not defined for torch.cuda.Tensoryet. input(Tensor) – the input tensor. dim(int) – the dimension to reduce. keepdim(bool) – whether the output tensor hasdimretained or not. out(tuple,optional) – the result tuple of two output tensors (values, indices) Example: ","[450, 454, 459, 460, 461, 464, 910]"
163,163,"Returns a new tensor with boolean elements representing if each element of input is real-valued or not.
All real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) – the input tensor. A boolean tensor that is True whereinput is real and False elsewhere Example: ","[931, 932, 933, 934, 935, 936, 937]"
164,164,"Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   ","[266, 292]"
165,165,"Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   ",[680]
166,166,"Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   ",[269]
167,167,"Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   ","[254, 268, 283, 295, 296]"
168,168,"Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",[674]
169,169,"Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   ",[684]
170,170,"Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine ","[219, 229, 240, 241, 242]"
171,171,"Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted
LU factorization of A fromtorch.lu(). This function supportsfloat,double,cfloatandcdoubledtypes for input. b(Tensor) – the RHS tensor of size(∗,m,k)(*, m, k)(∗,m,k), where∗*∗is zero or more batch dimensions. LU_data(Tensor) – the pivoted LU factorization of A fromtorch.lu()of size(∗,m,m)(*, m, m)(∗,m,m),
where∗*∗is zero or more batch dimensions. ","[79, 84]"
172,172,"Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted
LU factorization of A fromtorch.lu(). This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) – the RHS tensor of size(∗,m,k)(*, m, k)(∗,m,k), where∗*∗is zero or more batch dimensions. LU_data(Tensor) – the pivoted LU factorization of A fromtorch.lu()of size(∗,m,m)(*, m, m)(∗,m,m),
where∗*∗is zero or more batch dimensions. ","[80, 82]"
173,173,"Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   ",[336]
174,174,"Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    ","[718, 721, 722, 723]"
175,175,"Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   ",[715]
176,176,"Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   ","[687, 688]"
177,177,"Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",[690]
178,178,"Returns the indices that sort a tensor along a given dimension in ascending
order by value. This is the second value returned bytorch.sort().  See its documentation
for the exact semantics of this method. input(Tensor) – the input tensor. dim(int,optional) – the dimension to sort along descending(bool,optional) – controls the sorting order (ascending or descending) Example: ","[89, 90, 91, 919, 920, 921, 922, 923, 924]"
179,179,"Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ","[215, 220, 232, 233]"
180,180,"Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer ","[535, 536, 537]"
181,181,"Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",[696]
182,182,"Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when
computing matrix norms. Note, however, the signature for these functions
is slightly different than the signature for torch.norm. input(Tensor) – The input tensor. Its data type must be either a floating
point or complex type. For complex inputs, the norm is calculated using the
absolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result’s data type will
be the corresponding floating point type (e.g. float ifinput is
complexfloat). p(int,float,inf,-inf,'fro','nuc',optional) – the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) – Specifies which dimension or dimensions of input to
calculate the norm across. IfdimisNone, the norm will
be calculated across all dimensions of input. If the norm
type indicated bypdoes not support the specified number of
dimensions, an error will occur. keepdim(bool,optional) – whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) – the output tensor. Ignored ifdim=Noneandout=None. ","[426, 429]"
183,183,Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ,"[842, 845]"
184,184,"Returns the matrix product of the NNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   ","[352, 353, 371, 372, 373, 840, 844, 850, 857, 859]"
185,185,"Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ","[691, 693]"
186,186,"Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ","[707, 708]"
187,187,"Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel’s correction will be used.   ","[697, 705, 706, 709, 711, 712, 713]"
188,188,"Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ","[698, 704, 710]"
189,189,"Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   ","[700, 701]"
190,190,"Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   ",[446]
191,191,"Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant of torch.quantile()that “ignores”NaNvalues, computing the quantilesqas ifNaNvalues in inputdid not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   ","[699, 703]"
192,192,"Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",[181]
193,193,"See also torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a
numerically stable algorithm. A(Tensor) – tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) – the tolerance value to determine when is a singular value zero
If it is atorch.Tensor, its shape must be
broadcastable to that of the singular values ofAas returned bytorch.svd().
Default:1e-15. ","[1000, 1001]"
194,194,"Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data. ",[183]
195,195,"Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ","[176, 177]"
196,196,"Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil⌈stepend−start​⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1⌊stepend−start​⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced frombasestart{{\text{{base}}}}^{{\text{{start}}}}basestarttobaseend{{\text{{base}}}}^{{\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ","[179, 182]"
197,197,"Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ","[217, 224, 235, 236]"
198,198,"Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ","[213, 214, 218, 223, 225]"
199,199,"Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in inputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution ","[226, 227, 228, 231]"
200,200,"Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimof inputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimensiondim.   Returns the cumulative product of elements of inputin the dimensiondim.   Returns the cumulative sum of elements of inputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by⊗\otimes⊗, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   ","[333, 335, 337]"
201,201,"Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   ","[714, 716, 719]"
202,202,"Since stft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False
since the signal isn’t padded). If center is True, then there will be padding e.g.'constant','reflect', etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
calculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0] ","[740, 741, 742]"
203,203,Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   ,"[864, 869]"
204,204,Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ,"[875, 877]"
205,205,"Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK’s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m×n)(m \times n)(m×n)and a matrixBBBof size(m×k)(m \times k)(m×k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinput and mat2.   Performs a matrix-vector product of the matrix inputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   ","[354, 363, 374, 376, 379, 380, 879]"
206,206,"Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). ","[64, 65]"
207,207,"Sums the product of the elements of the inputoperandsalong dimensions specified using a notation
based on the Einstein summation convention. ",[46]
208,208,"Sums the product of the elements of the inputoperandsalong dimensions specified using a notation
based on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them
in a short-hand format based on the Einstein summation convention, given byequation. The details of
this format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing
the product of the elements of theoperandsalong the dimensions whose subscripts are not part of the
output. For example, matrix multiplication can be computed using einsum astorch.einsum(“ij,jk->ik”, A, B).
Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[‘a’, ‘z’]) for each dimension of
the inputoperandsin the same order as the dimensions, separating subcripts for each operand by a
comma (‘,’), e.g.‘ij,jk’specify subscripts for two 2D operands. The dimensions labeled with the same subscript
must be broadcastable, that is, their size must either match or be1. The exception is if a subscript is
repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand
must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that
appear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.
The output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based
on the subscripts, and then summing out the dimensions whose subscripts are not part of the output. ",[731]
209,209,"Supports broadcasting to a common shape,type promotion, and integer, float, and complex inputs.
Always promotes integer types to the default scalar type. input(Tensor) – the dividend other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and otherare integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ",[10]
210,210,"Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. ","[981, 982, 983]"
211,211,"Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more
numerically stable than computing the pseudoinverse explicitly. Warning ","[985, 986, 995]"
212,212,"Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.
Always promotes integer types to the default scalar type. input(Tensor) – the dividend other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ",[9]
213,213,"TORCH.COLUMN_STACK Creates a new tensor by horizontally stacking the tensors in tensors. Equivalent totorch.hstack(tensors), except each zero or one dimensional tensortin tensorsis first reshaped into a(t.numel(),1)column before being stacked horizontally. tensors(sequence of Tensors) – sequence of tensors to concatenate out(Tensor,optional) – the output tensor. Example: ",[32]
214,214,"TORCH.DIV Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.
Always promotes integer types to the default scalar type. input(Tensor) – the dividend other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ",[11]
215,215,"Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when
we first traced it and when we traced it with thecheck_inputs. Indeed,
the loop within the body ofloop_in_traced_fndepends on the shape
of the inputx, and thus when we try anotherxwith a different
shape, the trace differs. ","[619, 625, 627]"
216,216,"Tests if all elements in inputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ","[692, 694, 695, 702]"
217,217,"Tests if any element in inputevaluates toTrue. Note This function matches the behaviour of NumPy in returning
output of dtypeboolfor all supported dtypes exceptuint8.
Foruint8the dtype of output isuint8itself. Example: For each row of input in the given dimension dim,
returnsTrueif any element in the row evaluate toTrueandFalseotherwise. ","[893, 894]"
218,218,"The input tensor. Expected to be output of stft(),
can either be complex (channel,fft_size,n_frame), or real
(channel,fft_size,n_frame, 2) where the channel dimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) ",[752]
219,219,"The input tensor. Expected to be output of stft(),
can either be complex (channel,fft_size,n_frame), or real
(channel,fft_size,n_frame, 2) where the channel dimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) normalized(bool) – Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) ","[756, 757]"
220,220,The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft() ,[48]
221,221,"The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact 	ext{window_length} + 1window_length+1. Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).",[49]
222,222,"The inputwindow_lengthis a positive integer controlling the
returned window size.periodicflag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin
above formula is in factwindow_length+1\text{window\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). ",[50]
223,223,"The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. ",[978]
224,224,"The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and ifAis a batch of matrices then
the output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: ","[974, 977, 993]"
225,225,"The singular values (or the norm of the eigenvalues whenhermitian= True)
that are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: ","[991, 994]"
226,226,"The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for ","[518, 520]"
227,227,"The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   ","[800, 801, 802, 803, 805, 807]"
228,228,"The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinput is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   ","[804, 806]"
229,229,"The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type of inputis a complex data type i.e., one of torch.complex64, andtorch.complex128.   Returns True if the data type of inputis a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand
include:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader
range of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ","[160, 161, 163, 164, 165, 166]"
230,230,"The torch.special module, modeled after SciPy’sspecialmodule. This module is in BETA. New functions are still being added, and some
functions may change in future PyTorch releases. See the documentation of each
function for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. ",[398]
231,231,"The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ",[427]
232,232,"The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types
can be inferred from the value of the member. Empty lists and dicts cannot have their
types inferred and must have their types annotated withPEP 526-styleclass annotations.
If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute
to the resultingScriptModule Old API: New API: ","[651, 663, 665, 666]"
233,233,"The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. ",[726]
234,234,"The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types
can be inferred from the value of the member. Empty lists and dicts cannot have their
types inferred and must have their types annotated withPEP 526-styleclass annotations.
If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute
to the resultingScriptModule Old API: New API: ","[647, 659, 661, 662, 664, 667, 668, 670]"
235,235,"Theequationstring specifies the subscripts (lower case letters[‘a’, ‘z’]) for each dimension of
the inputoperandsin the same order as the dimensions, separating subcripts for each operand by a
comma (‘,’), e.g.‘ij,jk’specify subscripts for two 2D operands. The dimensions labeled with the same subscript
must be broadcastable, that is, their size must either match or be1. The exception is if a subscript is
repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand
must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that
appear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.
The output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based
on the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (‘->’) at the end of the equation
followed by the subscripts for the output. For instance, the following equation computes the transpose of a
matrix multiplication: ‘ij,jk->ki’. The output subscripts must appear at least once for some input operand and
at most once for the output. ",[732]
236,236,"There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when
we first traced it and when we traced it with thecheck_inputs. Indeed,
the loop within the body ofloop_in_traced_fndepends on the shape
of the inputx, and thus when we try anotherxwith a different
shape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced
computation. As an example, take a trace of a function that contains an
in-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but
rather build up the result tensor out-of-place withtorch.cat: ",[631]
237,237,"Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types
can be inferred from the value of the member. Empty lists and dicts cannot have their
types inferred and must have their types annotated withPEP 526-styleclass annotations.
If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute
to the resultingScriptModule Old API: New API: ",[669]
238,238,"This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. ","[484, 490]"
239,239,"This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). ",[523]
240,240,"This class wraps an arbitrary optim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for
updating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,
each rank will broadcast its parameters to all other peers to keep all
model replicas in the same state.ZeroRedundancyOptimizercan be used
in conjunction withtorch.nn.parallel.DistributedDataparallelto
reduce per-rank peak memory consumption. ZeroRedundancyOptimizer use a greedy algorithm to pack a number of
parameters at each rank. Each parameter belongs to a single rank and is not
divided among ranks. The partition is arbitrary and might not match the
the parameter registration or usage order. params(Iterable) – anIterableof torch.Tensors optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, butparams.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  ","[124, 125, 126, 128, 136]"
241,241,"This function matches the behaviour of NumPy in returning
output of dtypeboolfor all supported dtypes exceptuint8.
Foruint8the dtype of output isuint8itself. Example: For each row of input in the given dimension dim,
returnsTrueif any element in the row evaluate toTrueandFalseotherwise. If keepdim is True, the output tensor is of the same size
asinputexcept in the dimension dimwhere it is of size 1.
Otherwise,dimis squeezed (seetorch.squeeze()), resulting in
the output tensor having 1 fewer dimension than input. ","[897, 899, 903, 904, 906, 907]"
242,242,"This function supportsfloat,double,cfloatandcdoubledtypes for input. b(Tensor) – the RHS tensor of size(∗,m,k)(*, m, k)(∗,m,k), where∗*∗is zero or more batch dimensions. LU_data(Tensor) – the pivoted LU factorization of A fromtorch.lu()of size(∗,m,m)(*, m, m)(∗,m,m),
where∗*∗is zero or more batch dimensions. ","[914, 915, 916]"
243,243,"This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.
For CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by
the the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more
numerically stable than computing the pseudoinverse explicitly. Warning ",[996]
244,244,"This is TorchScript’s compilation of the code for theforwardmethod.
You can use this to ensure TorchScript (tracing or scripting) has captured
your model code correctly. TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) ","[604, 618, 620]"
245,245,"This library is part of thePyTorchproject. PyTorch is an open source
machine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). ",[60]
246,246,"This library is part of thePyTorchproject. PyTorch is an open source
machine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). Beta:Features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete. For Beta features, we are
committing to seeing the feature through to the Stable classification.
We are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of
binary distributions like PyPI or Conda, except sometimes behind run-time
flags, and are at an early stage for feedback and testing. ","[55, 57, 912]"
247,247,"This library is part of thePyTorchproject. PyTorch is an open source
machine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). Beta:Features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete. For Beta features, we are
committing to seeing the feature through to the Stable classification.
We are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of
binary distributions like PyPI or Conda, except sometimes behind run-time
flags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries ","[56, 58, 59, 62, 68, 71, 74, 77]"
248,248,"This module is in BETA. New functions are still being added, and some
functions may change in future PyTorch releases. See the documentation of each
function for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. ","[390, 391, 392, 393, 395, 397]"
249,249,"This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can
skip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,
methods, and classes that it encounters. Once you calltorch.jit.script,
compilation is “opt-out”, rather than “opt-in”. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.
These changes combine to provide a simpler, easier-to-use API for converting
yournn.Modules intoScriptModules, ready to be optimized and executed in a
non-Python environment. The new usage looks like this: The module’sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place of torch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. ","[632, 633, 634, 635, 636, 638, 640]"
250,250,"To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place of torch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). ","[648, 649, 650, 652]"
251,251,"TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. ","[605, 630]"
252,252,"TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. ","[603, 610, 621, 622]"
253,253,"TorchScript provides a code pretty-printer for allScriptModuleinstances. This
pretty-printer gives an interpretation of the script method’s code as valid
Python syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.
If theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the
code of a method namedfooon aScriptModuleby accessing.foo.code.
The example above produces this output: This is TorchScript’s compilation of the code for theforwardmethod.
You can use this to ensure TorchScript (tracing or scripting) has captured
your model code correctly. TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. ","[606, 613, 614]"
254,254,"TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described
by aScriptModuleis correct, in both automated and manual fashion, as
described below. There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. ","[623, 624]"
255,255,"Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. ","[580, 582]"
256,256,"Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.
Tracing and scripting can be composed to suit the particular requirements
of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing. Example (calling a traced function in script): ","[581, 587, 588]"
257,257,"Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ""trunc""- rounds the results of the division towards zero.
Equivalent to C-style integer division. ""floor""- rounds the results of the division down.
Equivalent to floor division in Python (the //operator) and NumPy’snp.floor_divide. ","[12, 13, 15, 16, 24, 25, 26, 27, 28, 29]"
258,258,"Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups ",[496]
259,259,"Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when
computing matrix norms. Note, however, the signature for these functions
is slightly different than the signature for torch.norm. input(Tensor) – The input tensor. Its data type must be either a floating
point or complex type. For complex inputs, the norm is calculated using the
absolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result’s data type will
be the corresponding floating point type (e.g. float ifinput is
complexfloat). p(int,float,inf,-inf,'fro','nuc',optional) – the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. ",[412]
260,260,"Warning In order to implementSelf-Normalizing Neural Networks,
you should usenonlinearity='linear'instead ofnonlinearity='selu'.
This gives the initial weights a variance of1/N,
which is necessary to induce a stable fixed point in the forward pass.
In contrast, the default gain forSELUsacrifices the normalisation
effect for more stable gradient flow in rectangular layers. nonlinearity– the non-linear function (nn.functionalname) param– optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform
distributionU(a,b)\mathcal{U}(a, b)U(a,b). tensor– an n-dimensionaltorch.Tensor a– the lower bound of the uniform distribution b– the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input tensorwith the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples ","[939, 940]"
261,261,"Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): ",[730]
262,262,"Warning The@torch.jit.ignoreannotation’s behavior changes in
PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function
or method callable from code that is exported. To get this functionality back,
use@torch.jit.unused().@torch.jit.ignoreis now equivalent
to@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module’s data is
copied to aScriptModuleand the TorchScript compiler compiles the module.
The module’sforwardis compiled by default. Methods called fromforwardare
lazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.
Functions and methods called fromforwardare compiled as they are seen
by the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don’t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited
for simple record-like types (think aNamedTuplewith methods
attached). Everything in a user definedTorchScript Classis
exported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types
can be inferred from the value of the member. Empty lists and dicts cannot have their
types inferred and must have their types annotated withPEP 526-styleclass annotations.
If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute
to the resultingScriptModule Old API: New API: ","[646, 653, 657, 660, 671, 727, 728, 729]"
263,263,"Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when
computing matrix norms. Note, however, the signature for these functions
is slightly different than the signature for torch.norm. ",[413]
264,264,What is the Alias for torch.trunc?,[30]
265,265,"ZeroRedundancyOptimizer use a greedy algorithm to pack a number of
parameters at each rank. Each parameter belongs to a single rank and is not
divided among ranks. The partition is arbitrary and might not match the
the parameter registration or usage order. params(Iterable) – anIterableof torch.Tensors optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, butparams.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to getlocal_state_dictfor state_dict(dict) – globalstate_dict ","[127, 129, 132, 137, 138, 139, 143, 144, 145, 146]"
266,266,"[1] D. W. Griffin and J. S. Lim, “Signal estimation from modified short-time Fourier transform,”
IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984. input(Tensor) – The input tensor. Expected to be output of stft(),
can either be complex (channel,fft_size,n_frame), or real
(channel,fft_size,n_frame, 2) where the channel dimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. ","[749, 750, 751]"
267,267,"and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the
TorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This
pretty-printer gives an interpretation of the script method’s code as valid
Python syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.
If theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the
code of a method namedfooon aScriptModuleby accessing.foo.code.
The example above produces this output: This is TorchScript’s compilation of the code for theforwardmethod.
You can use this to ensure TorchScript (tracing or scripting) has captured
your model code correctly. TorchScript also has a representation at a lower level than the code pretty-
printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection
with regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for
example. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. ","[602, 607, 608, 611, 612]"
268,268,"a– the lower bound of the uniform distribution b– the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input Tensor with the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input Tensor with the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. ",[109]
269,269,"b(Tensor) – the RHS tensor of size(∗,m,k)(*, m, k)(∗,m,k), where∗*∗is zero or more batch dimensions. LU_data(Tensor) – the pivoted LU factorization of A fromtorch.lu()of size(∗,m,m)(*, m, m)(∗,m,m),
where∗*∗is zero or more batch dimensions. LU_pivots(IntTensor) – the pivots of the LU factorization fromtorch.lu()of size(∗,m)(*, m)(∗,m),
where∗*∗is zero or more batch dimensions.
The batch dimensions of LU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) – the output tensor. ","[917, 918]"
270,270,"b(Tensor) – the RHS tensor of size(∗,m,k)(*, m, k)(∗,m,k), where∗*∗is zero or more batch dimensions. LU_data(Tensor) – the pivoted LU factorization of A fromtorch.lu()of size(∗,m,m)(*, m, m)(∗,m,m),
where∗*∗is zero or more batch dimensions. LU_pivots(IntTensor) – the pivots of the LU factorization fromtorch.lu()of size(∗,m)(*, m)(∗,m),
where∗*∗is zero or more batch dimensions.
The batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) – the output tensor. ","[81, 85, 86, 87]"
271,271,"batch1andbatch2must be 3-D tensors each containing the
same number of matrices. Ifbatch1is a(b×n×m)(b \times n \times m)(b×n×m)tensor,batch2is a(b×m×p)(b \times m \times p)(b×m×p)tensor,inputmust bebroadcastablewith a(n×p)(n \times p)(n×p)tensor
andoutwill be a(n×p)(n \times p)(n×p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin
it will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. ",[963]
272,272,"b– the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples ",[942]
273,273,"b– the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input Tensor with the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input Tensor with the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor ","[108, 110, 111]"
274,274,"device(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(seetorch.set_default_tensor_type()).devicewill be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) – If autograd should record operations on the
returned tensor. Default:False. A 1-D tensor of size(window_length,)(\text{window\_length},)(window_length,)containing the window Tensor ","[103, 105, 106]"
275,275,"differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict ","[521, 525]"
276,276,"dtype(torch.dtype, optional) – the desired data type of
returned tensor. If specified, the input tensor is casted to
:attr:’dtype’ while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true
mathematical definition of Frobenius norm only applies to tensors with
exactly two dimensions.torch.linalg.norm()withord='fro'aligns
with the mathematical definition, since it can only be applied across
exactly two dimensions. Example: ","[885, 886, 887]"
277,277,"dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. layout(torch.layout, optional) – the desired layout of returned Tensor.
Default:torch.strided. ","[794, 795, 796, 798, 799, 891]"
278,278,"dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. ",[98]
279,279,"end(Number) – the ending value for the set of points step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. ",[787]
280,280,"end(Number) – the ending value for the set of points step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. layout(torch.layout, optional) – the desired layout of returned Tensor.
Default:torch.strided. device(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(seetorch.set_default_tensor_type()).devicewill be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types. ","[104, 784, 791, 890]"
281,281,"hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) ","[763, 764, 765, 766, 767]"
282,282,"input and indexmust have the same number of dimensions.
It is also required that index.size(d)<=input.size(d)for all
dimensionsd!=dim.outwill have the same shape asindex.
Note thatinputandindexdo not broadcast against each other. input(Tensor) – the source tensor dim(int) – the axis along which to index index(LongTensor) – the indices of elements to gather sparse_grad(bool,optional) – IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) – the destination tensor Example: ",[930]
283,283,"input(Tensor) – The input tensor. Expected to be output of stft(),
can either be complex (channel,fft_size,n_frame), or real
(channel,fft_size,n_frame, 2) where the channel dimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) ",[755]
284,284,"input(Tensor) – The input tensor. Expected to be output of stft(),
can either be complex (channel,fft_size,n_frame), or real
(channel,fft_size,n_frame, 2) where the channel dimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) normalized(bool) – Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) ",[770]
285,285,"input(Tensor) – The input tensor. Its data type must be either a floating
point or complex type. For complex inputs, the norm is calculated using the
absolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result’s data type will
be the corresponding floating point type (e.g. float ifinput is
complexfloat). p(int,float,inf,-inf,'fro','nuc',optional) – the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ","[416, 417, 431]"
286,286,"input(Tensor) – the dividend other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ""trunc""- rounds the results of the division towards zero.
Equivalent to C-style integer division. ",[14]
287,287,"input(Tensor) – the input tensor. Tests if any element in inputevaluates toTrue. Note This function matches the behaviour of NumPy in returning
output of dtypeboolfor all supported dtypes exceptuint8.
Foruint8the dtype of output isuint8itself. Example: For each row of input in the given dimension dim,
returnsTrueif any element in the row evaluate toTrueandFalseotherwise. ","[895, 896]"
288,288,"input(Tensor) – the input tensor. nan(Number,optional) – the value to replaceNaNs with. Default is zero. posinf(Number,optional) – if a Number, the value to replace positive infinity values with.
If None, positive infinity values are replaced with the greatest finite value representable byinput’s dtype.
Default is None. ",[5]
289,289,"input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier ",[408]
290,290,"input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier other(NumberorTensor) – Argument Note At least one of inputorothermust be a tensor. out(Tensor,optional) – the output tensor. Example: ","[406, 407, 409, 410]"
291,291,"layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. device(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(seetorch.set_default_tensor_type()).devicewill be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) – If autograd should record operations on the
returned tensor. Default:False. ",[99]
292,292,"length(Optional[int]) – The amount to trim the signal by (i.e. the
original signal length). (Default: whole signal) return_complex(Optional[bool]) – Whether the output should be complex, or if the input should be
assumed to derive from a real signal and window.
Note that this is incompatible withonesided=True.
(Default:False) Least squares estimation of the original signal of size (…, signal_length) Tensor ","[776, 777]"
293,293,"matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. ","[419, 422]"
294,294,"n_fft(int) – Size of Fourier transform hop_length(Optional[int]) – The distance between neighboring sliding window frames.
(Default:n_fft//4) win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) ",[754]
295,295,"nan(Number,optional) – the value to replaceNaNs with. Default is zero. posinf(Number,optional) – if a Number, the value to replace positive infinity values with.
If None, positive infinity values are replaced with the greatest finite value representable byinput’s dtype.
Default is None. neginf(Number,optional) – if a Number, the value to replace negative infinity values with.
If None, negative infinity values are replaced with the lowest finite value representable byinput’s dtype.
Default is None. ","[4, 6]"
296,296,"nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\frac{5}{3}35​ ReLU 2\sqrt{2}2​ Leaky Relu 21+negative_slope2\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}1+negative_slope22​​ SELU 34\frac{3}{4}43​ Warning ",[938]
297,297,"nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ","[435, 436]"
298,298,"onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) length(Optional[int]) – The amount to trim the signal by (i.e. the
original signal length). (Default: whole signal) return_complex(Optional[bool]) – Whether the output should be complex, or if the input should be
assumed to derive from a real signal and window.
Note that this is incompatible withonesided=True.
(Default:False) Least squares estimation of the original signal of size (…, signal_length) ","[772, 773]"
299,299,"optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. ","[470, 471]"
300,300,"optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) ",[489]
301,301,"optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer ","[469, 474, 476, 477, 480, 481, 485, 494, 495, 497, 500, 504, 512, 513, 533, 538]"
302,302,"optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, butparams.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to getlocal_state_dictfor state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer ","[130, 131, 133, 135, 140, 141, 142, 148, 152, 154, 155, 156, 157, 158, 159]"
303,303,"ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. ","[423, 425]"
304,304,"other(TensororNumber) – the divisor rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ""trunc""- rounds the results of the division towards zero.
Equivalent to C-style integer division. ",[18]
305,305,"out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier other(NumberorTensor) – Argument Note ",[47]
306,306,"out(Tensor,optional) – the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function of input.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function of input.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
of input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of input. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements of input.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier other(NumberorTensor) – Argument Note ","[399, 400, 404, 405]"
307,307,"out(Tensor,optional) – the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the complementary error function ofinput.
The complementary error function is defined as follows: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the inverse error function ofinput.
The inverse error function is defined in the range(−1,1)(-1, 1)(−1,1)as: input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponential of the elements minus 1
ofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element ofinput. input(Tensor) – the input tensor. out(Tensor,optional) – the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.
When eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) – the input tensor. eps(float,optional) – the epsilon for input clamp bound. Default:None out(Tensor,optional) – the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy’sscipy.special.xlog1py. input(NumberorTensor) – Multiplier other(NumberorTensor) – Argument Note ",[88]
308,308,"out(Tensor,optional) – the output tensor. Ignored ifdim=Noneandout=None. dtype(torch.dtype, optional) – the desired data type of
returned tensor. If specified, the input tensor is casted to
:attr:’dtype’ while performing the operation. Default: None. Note ","[882, 883]"
309,309,"out(Tensor,optional) – the output tensor. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. ",[793]
310,310,"p(int,float,inf,-inf,'fro','nuc',optional) – the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. ",[420]
311,311,"param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict ","[478, 479, 491, 492, 493, 508, 509, 510, 515, 517, 524]"
312,312,"param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict ","[514, 519, 526, 527, 528, 529]"
313,313,"parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. ","[472, 475]"
314,314,"parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  ","[473, 505]"
315,315,"params(Iterable) – anIterableof torch.Tensors optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, but params.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to the Optimizers param_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. ",[483]
316,316,"params(Iterable) – anIterableof torch.Tensors optimizer_class(torch.nn.Optimizer) – the class of the local
optimizer. group(ProcessGroup, optional) –torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) – when enabled, parameters will
be packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different
offsets. When disabled, each individual parameter will be
communicated separately, butparams.datawill stay intact. **default– all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to theOptimizeras
training progresses. param_group(dict) – Specifies what Tensors should be optimized
along with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to getlocal_state_dictfor state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). closure(callable) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers. ","[134, 147, 149, 150, 151, 153]"
317,317,"periodic(bool,optional) – If True, returns a window to be used as periodic
function. If False, return a symmetric window. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. ",[97]
318,318,"rounding_mode(str,optional) – Type of rounding applied to the result: None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type.
Equivalent to true division in Python (the /operator) and NumPy’snp.true_divide. ""trunc""- rounds the results of the division towards zero.
Equivalent to C-style integer division. ","[17, 19, 20, 21, 22, 23]"
319,319,"start(Number) – the starting value for the set of points. Default:0. end(Number) – the ending value for the set of points step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. ",[785]
320,320,"state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. ",[506]
321,321,std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples ,[945]
322,322,"step(Number) – the gap between each pair of adjacent points. Default:1. out(Tensor,optional) – the output tensor. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
arguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to
betorch.int64. ",[788]
323,323,"sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ","[430, 439]"
324,324,"tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input Tensor with the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input Tensor with the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inDelving deep into rectifiers: Surpassing human-level
performance on ImageNet classification- He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled fromU(−bound,bound)\mathcal{U}(-\text{bound}, \text{bound})U(−bound,bound)where Also known as He initialization. tensor– an n-dimensionaltorch.Tensor ","[115, 116, 117, 118, 119, 120, 121]"
325,325,"tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ","[955, 956]"
326,326,"tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input tensorwith the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input tensorwith values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input tensorwith values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input tensorwith values according to the method
described inDelving deep into rectifiers: Surpassing human-level
performance on ImageNet classification- He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled fromU(−bound,bound)\mathcal{U}(-\text{bound}, \text{bound})U(−bound,bound)where Also known as He initialization. tensor– an n-dimensionaltorch.Tensor ",[952]
327,327,"tensor– an n-dimensionaltorch.Tensor a– the lower bound of the uniform distribution b– the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal
distributionN(mean,std2)\mathcal{N}(\text{mean}, \text{std}^2)N(mean,std2). tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ",[944]
328,328,"tensor– an n-dimensionaltorch.Tensor mean– the mean of the normal distribution std– the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\text{val}val. tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input Tensor with the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional input Tensor with the Dirac
delta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case
of groups>1, each group of channels preserves identity tensor– a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) – number of groups in the conv layer (default: 1) Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled fromU(−a,a)\mathcal{U}(-a, a)U(−a,a)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples Fills the input Tensor with values according to the method
described inUnderstanding the difficulty of training deep feedforward
neural networks- Glorot, X. & Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled fromN(0,std2)\mathcal{N}(0, \text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor– an n-dimensionaltorch.Tensor gain– an optional scaling factor Examples ","[112, 113]"
329,329,"tensor– an n-dimensionaltorch.Tensor val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ","[946, 947, 949]"
330,330,"the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. ","[418, 421]"
331,331,"the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) – Specifies which dimension or dimensions of input to
calculate the norm across. IfdimisNone, the norm will
be calculated across all dimensions of input. If the norm
type indicated bypdoes not support the specified number of
dimensions, an error will occur. ",[428]
332,332,"to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. ",[498]
333,333,"to(int) – the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) – optimizer state. Should be an object returned
from a call tostate_dict() Gets this rank’sstate_dict.  The state of the optimizer as adict.
It contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list of param_groups(which is a list of dict) where each
element of the list contains the param_groups for a rank. Element 0
corresponds to rank 0, etc. We need all the ranks for the broadcast
insidestep(). Returns the local_state_dict for a given rank. rank(int) – rank to get local_state_dict for state_dict(dict) – globalstate_dict the last known global optimizer state, which consist of a list of
the shards. Performs a single optimization step (parameter update). ","[487, 499]"
334,334,"torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\text{input}_{i} / \text{other}_{i}inputi​/otheri​with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",[677]
335,335,"torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a
numerically stable algorithm. A(Tensor) – tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) – the tolerance value to determine when is a singular value zero
If it is atorch.Tensor, its shape must be
broadcastable to that of the singular values ofAas returned bytorch.svd().
Default:1e-15. ","[441, 442, 997, 998]"
336,336,"torch.linalg.lstsq()computesA.pinv() @Bwith a
numerically stable algorithm. A(Tensor) – tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) – the tolerance value to determine when is a singular value zero
If it is atorch.Tensor, its shape must be
broadcastable to that of the singular values ofAas returned bytorch.svd().
Default:1e-15. hermitian(bool,optional) – indicates whetherAis Hermitian if complex
or symmetric if real. Default:False. ","[440, 443, 444, 999, 1002]"
337,337,"torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when
computing matrix norms. Note, however, the signature for these functions
is slightly different than the signature for torch.norm. ","[414, 415]"
338,338,"torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when
computing matrix norms. Note, however, the signature for these functions
is slightly different than the signature for torch.norm. input(Tensor) – The input tensor. Its data type must be either a floating
point or complex type. For complex inputs, the norm is calculated using the
absolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result’s data type will
be the corresponding floating point type (e.g. float ifinput is
complexfloat). p(int,float,inf,-inf,'fro','nuc',optional) – the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) ",[411]
339,339,"val– the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor– an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor– an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional input tensorwith the identity
matrix. Preserves the identity of the inputs inLinearlayers, where as
many inputs are preserved as possible. tensor– a 2-dimensionaltorch.Tensor Examples ",[951]
340,340,"win_length(Optional[int]) – The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) normalized(bool) – Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) length(Optional[int]) – The amount to trim the signal by (i.e. the
original signal length). (Default: whole signal) return_complex(Optional[bool]) – Whether the output should be complex, or if the input should be
assumed to derive from a real signal and window.
Note that this is incompatible withonesided=True.
(Default:False) Least squares estimation of the original signal of size (…, signal_length) Tensor ","[759, 760, 762, 771, 774, 775]"
341,341,"window(Optional[torch.Tensor]) – The optional window function.
(Default:torch.ones(win_length)) center(bool) – Whether input was padded on both sides so that thettt-th frame is
centered at timet×hop_lengtht \times \text{hop\_length}t×hop_length.
(Default:True) normalized(bool) – Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) – Whether the STFT was onesided.
(Default:true if n_fft!=fft_sizein the input size) ",[768]
342,342,"window_length(int) – the size of returned window periodic(bool,optional) – If True, returns a window to be used as periodic
function. If False, return a symmetric window. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. ",[94]
343,343,"window_length(int) – the size of returned window periodic(bool,optional) – If True, returns a window to be used as periodic
function. If False, return a symmetric window. dtype(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) – the desired layout of returned window tensor. Only torch.strided(dense layout) is supported. device(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(seetorch.set_default_tensor_type()).devicewill be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) – If autograd should record operations on the
returned tensor. Default:False. A 1-D tensor of size(window_length,)(\text{window\_length},)(window_length,)containing the window Tensor ","[54, 95, 96, 100, 101, 102]"
344,344,"– Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ",[437]
345,345,"– sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ",[881]
346,346,"‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ","[433, 434]"
347,347,"’fro’ Frobenius norm – ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.
The corresponding dimensions of inputare flattened into
one dimension, and the norm is calculated on the flattened
dimension. Frobenius norm produces the same result asp=2in all cases
except whendimis a list of three or more dims, in which
case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. ",[432]
