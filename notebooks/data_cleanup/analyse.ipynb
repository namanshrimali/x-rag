{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Reading and data and display sample\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset, load_metric\n",
    "import time\n",
    "import json\n",
    "from pathlib import PurePath\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f'device is {device}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "file_path = '/Users/nik/Documents/Codes/random_stuff/Final_end/data/nikhil_onlytextpytorch.json'\n",
    "\n",
    "# path = os.path.join(os.getcwd(),file_path)\n",
    "\n",
    "print(path)\n",
    "\n",
    "train = pd.read_json(file_path, orient='index')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/nik/Documents/Codes/random_stuff/Final_end/data_cleanup/data/nikhil_onlytextpytorch.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>What values are specified to replaceNaN, positive infinity, and negative infinity values in input?</th>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>By default,NaN is replaced with what value?</th>\n",
       "      <td>zero</td>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What values are used to replace negative infinity values in input?</th>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the default value for negative infinity?</th>\n",
       "      <td>the least finite value</td>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the value to replace positive infinity values with?</th>\n",
       "      <td>posinf</td>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>nan(Number,optional) – the value to replaceNaN...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the tensor of shape where*is zero or more batch dimensions?</th>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>torch.linalg.inv()computes the inverse of a sq...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the tolerance value to determine when is a singular value zero?</th>\n",
       "      <td>rcond</td>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the default value of atorch.Tensor?</th>\n",
       "      <td>1e-15</td>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What computes the inverse of a square matrix?</th>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What is the tensor of shape(*, m, n)where*is zero or more batch dimensions?</th>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     answer  \\\n",
       "What values are specified to replaceNaN, positi...  bynan,posinf, andneginf   \n",
       "By default,NaN is replaced with what value?                            zero   \n",
       "What values are used to replace negative infini...  bynan,posinf, andneginf   \n",
       "What is the default value for negative infinity?     the least finite value   \n",
       "What is the value to replace positive infinity ...                   posinf   \n",
       "...                                                                     ...   \n",
       "What is the tensor of shape where*is zero or mo...                A(Tensor)   \n",
       "What is the tolerance value to determine when i...                    rcond   \n",
       "What is the default value of atorch.Tensor?                           1e-15   \n",
       "What computes the inverse of a square matrix?            torch.linalg.inv()   \n",
       "What is the tensor of shape(*, m, n)where*is ze...                A(Tensor)   \n",
       "\n",
       "                                                                                             question  \\\n",
       "What values are specified to replaceNaN, positi...  What values are specified to replaceNaN, posit...   \n",
       "By default,NaN is replaced with what value?               By default,NaN is replaced with what value?   \n",
       "What values are used to replace negative infini...  What values are used to replace negative infin...   \n",
       "What is the default value for negative infinity?     What is the default value for negative infinity?   \n",
       "What is the value to replace positive infinity ...  What is the value to replace positive infinity...   \n",
       "...                                                                                               ...   \n",
       "What is the tensor of shape where*is zero or mo...  What is the tensor of shape where*is zero or m...   \n",
       "What is the tolerance value to determine when i...  What is the tolerance value to determine when ...   \n",
       "What is the default value of atorch.Tensor?               What is the default value of atorch.Tensor?   \n",
       "What computes the inverse of a square matrix?           What computes the inverse of a square matrix?   \n",
       "What is the tensor of shape(*, m, n)where*is ze...  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                                                                                              context  \\\n",
       "What values are specified to replaceNaN, positi...  ReplacesNaN, positive infinity, and negative i...   \n",
       "By default,NaN is replaced with what value?         ReplacesNaN, positive infinity, and negative i...   \n",
       "What values are used to replace negative infini...  ReplacesNaN, positive infinity, and negative i...   \n",
       "What is the default value for negative infinity?    ReplacesNaN, positive infinity, and negative i...   \n",
       "What is the value to replace positive infinity ...  nan(Number,optional) – the value to replaceNaN...   \n",
       "...                                                                                               ...   \n",
       "What is the tensor of shape where*is zero or mo...  torch.linalg.inv()computes the inverse of a sq...   \n",
       "What is the tolerance value to determine when i...  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...   \n",
       "What is the default value of atorch.Tensor?         See also torch.linalg.inv()computes the invers...   \n",
       "What computes the inverse of a square matrix?       See also torch.linalg.inv()computes the invers...   \n",
       "What is the tensor of shape(*, m, n)where*is ze...  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...   \n",
       "\n",
       "                                                                                               source  \n",
       "What values are specified to replaceNaN, positi...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "By default,NaN is replaced with what value?         https://pytorch.org/docs/stable/generated/torc...  \n",
       "What values are used to replace negative infini...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "What is the default value for negative infinity?    https://pytorch.org/docs/stable/generated/torc...  \n",
       "What is the value to replace positive infinity ...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "...                                                                                               ...  \n",
       "What is the tensor of shape where*is zero or mo...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "What is the tolerance value to determine when i...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "What is the default value of atorch.Tensor?         https://pytorch.org/docs/stable/generated/torc...  \n",
       "What computes the inverse of a square matrix?       https://pytorch.org/docs/stable/generated/torc...  \n",
       "What is the tensor of shape(*, m, n)where*is ze...  https://pytorch.org/docs/stable/generated/torc...  \n",
       "\n",
       "[1003 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "train_data = train.reset_index()[['question', 'answer', 'context']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_data = train_data.reset_index().rename(columns={'index': 'id'})\n",
    "display(train_data)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>zero</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>the least finite value</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>posinf</td>\n",
       "      <td>nan(Number,optional) – the value to replaceNaN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>torch.linalg.inv()computes the inverse of a sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>rcond</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>1e-15</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           question  \\\n",
       "0        0  What values are specified to replaceNaN, posit...   \n",
       "1        1        By default,NaN is replaced with what value?   \n",
       "2        2  What values are used to replace negative infin...   \n",
       "3        3   What is the default value for negative infinity?   \n",
       "4        4  What is the value to replace positive infinity...   \n",
       "...    ...                                                ...   \n",
       "998    998  What is the tensor of shape where*is zero or m...   \n",
       "999    999  What is the tolerance value to determine when ...   \n",
       "1000  1000        What is the default value of atorch.Tensor?   \n",
       "1001  1001      What computes the inverse of a square matrix?   \n",
       "1002  1002  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                       answer  \\\n",
       "0     bynan,posinf, andneginf   \n",
       "1                        zero   \n",
       "2     bynan,posinf, andneginf   \n",
       "3      the least finite value   \n",
       "4                      posinf   \n",
       "...                       ...   \n",
       "998                 A(Tensor)   \n",
       "999                     rcond   \n",
       "1000                    1e-15   \n",
       "1001       torch.linalg.inv()   \n",
       "1002                A(Tensor)   \n",
       "\n",
       "                                                context  \n",
       "0     ReplacesNaN, positive infinity, and negative i...  \n",
       "1     ReplacesNaN, positive infinity, and negative i...  \n",
       "2     ReplacesNaN, positive infinity, and negative i...  \n",
       "3     ReplacesNaN, positive infinity, and negative i...  \n",
       "4     nan(Number,optional) – the value to replaceNaN...  \n",
       "...                                                 ...  \n",
       "998   torch.linalg.inv()computes the inverse of a sq...  \n",
       "999   torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "1000  See also torch.linalg.inv()computes the invers...  \n",
       "1001  See also torch.linalg.inv()computes the invers...  \n",
       "1002  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "\n",
       "[1003 rows x 4 columns]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_groups = train_data.groupby('context')['id'].apply(list).reset_index(name = 'groups')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df_groups = df_groups.reset_index().rename(columns={'index': 'group_id'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_groups"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>context</th>\n",
       "      <th>groups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A wrapper around C++torch::jit::Module.   Fu...</td>\n",
       "      <td>[590, 595, 596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alias for torch.asinh().   Returns a new ten...</td>\n",
       "      <td>[256, 260, 271, 287, 288, 289, 290, 291, 293, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Alias fortorch.asinh().   Returns a new tens...</td>\n",
       "      <td>[679, 681, 683, 685]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Alias fortorch.asinh().   Returns a new tens...</td>\n",
       "      <td>[673]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Alias of torch.outer().   Computes the dot p...</td>\n",
       "      <td>[876, 880]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>343</td>\n",
       "      <td>window_length(int) – the size of returned wind...</td>\n",
       "      <td>[54, 95, 96, 100, 101, 102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>344</td>\n",
       "      <td>– Number – sum(abs(x)**ord)**(1./ord) The vect...</td>\n",
       "      <td>[437]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>– sum(abs(x)**ord)**(1./ord) The vector norm c...</td>\n",
       "      <td>[881]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>346</td>\n",
       "      <td>‘nuc’ nuclear norm – Number – sum(abs(x)**ord)...</td>\n",
       "      <td>[433, 434]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>’fro’ Frobenius norm – ‘nuc’ nuclear norm – Nu...</td>\n",
       "      <td>[432]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     group_id                                            context  \\\n",
       "0           0    A wrapper around C++torch::jit::Module.   Fu...   \n",
       "1           1    Alias for torch.asinh().   Returns a new ten...   \n",
       "2           2    Alias fortorch.asinh().   Returns a new tens...   \n",
       "3           3    Alias fortorch.asinh().   Returns a new tens...   \n",
       "4           4    Alias of torch.outer().   Computes the dot p...   \n",
       "..        ...                                                ...   \n",
       "343       343  window_length(int) – the size of returned wind...   \n",
       "344       344  – Number – sum(abs(x)**ord)**(1./ord) The vect...   \n",
       "345       345  – sum(abs(x)**ord)**(1./ord) The vector norm c...   \n",
       "346       346  ‘nuc’ nuclear norm – Number – sum(abs(x)**ord)...   \n",
       "347       347  ’fro’ Frobenius norm – ‘nuc’ nuclear norm – Nu...   \n",
       "\n",
       "                                                groups  \n",
       "0                                      [590, 595, 596]  \n",
       "1    [256, 260, 271, 287, 288, 289, 290, 291, 293, ...  \n",
       "2                                 [679, 681, 683, 685]  \n",
       "3                                                [673]  \n",
       "4                                           [876, 880]  \n",
       "..                                                 ...  \n",
       "343                        [54, 95, 96, 100, 101, 102]  \n",
       "344                                              [437]  \n",
       "345                                              [881]  \n",
       "346                                         [433, 434]  \n",
       "347                                              [432]  \n",
       "\n",
       "[348 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df_merged = pd.merge(train_data, df_groups).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df_merged = df_merged[['question', 'answer', 'group_id']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "df_merged"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>zero</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>the least finite value</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>posinf</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>What are the singular values that are below th...</td>\n",
       "      <td>zero</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>What function synchronizes a CUDA input with t...</td>\n",
       "      <td>usestorch.linalg.svd()ifhermitian= False</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Why is it always preferable to uselstsq()?</td>\n",
       "      <td>faster and more numerically stable</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>1e-15</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     What values are specified to replaceNaN, posit...   \n",
       "1           By default,NaN is replaced with what value?   \n",
       "2     What values are used to replace negative infin...   \n",
       "3      What is the default value for negative infinity?   \n",
       "4     What is the value to replace positive infinity...   \n",
       "...                                                 ...   \n",
       "998   What are the singular values that are below th...   \n",
       "999   What function synchronizes a CUDA input with t...   \n",
       "1000         Why is it always preferable to uselstsq()?   \n",
       "1001        What is the default value of atorch.Tensor?   \n",
       "1002      What computes the inverse of a square matrix?   \n",
       "\n",
       "                                        answer  group_id  \n",
       "0                      bynan,posinf, andneginf       141  \n",
       "1                                         zero       141  \n",
       "2                      bynan,posinf, andneginf       141  \n",
       "3                       the least finite value       141  \n",
       "4                                       posinf       295  \n",
       "...                                        ...       ...  \n",
       "998                                       zero       225  \n",
       "999   usestorch.linalg.svd()ifhermitian= False       225  \n",
       "1000        faster and more numerically stable       243  \n",
       "1001                                     1e-15       193  \n",
       "1002                        torch.linalg.inv()       193  \n",
       "\n",
       "[1003 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df_merged.to_csv('qna_gid.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "display(df_groups.iloc[141])\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "group_id                                                  141\n",
       "context     ReplacesNaN, positive infinity, and negative i...\n",
       "groups                                           [0, 1, 2, 3]\n",
       "Name: 141, dtype: object"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df_groups.to_csv('groups_master.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "train_data = train_data.reset_index().rename(columns={'index': 'id'})\n",
    "# display(train_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "display(train_data)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>zero</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>the least finite value</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>posinf</td>\n",
       "      <td>nan(Number,optional) – the value to replaceNaN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>torch.linalg.inv()computes the inverse of a sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>rcond</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>1e-15</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           question  \\\n",
       "0        0  What values are specified to replaceNaN, posit...   \n",
       "1        1        By default,NaN is replaced with what value?   \n",
       "2        2  What values are used to replace negative infin...   \n",
       "3        3   What is the default value for negative infinity?   \n",
       "4        4  What is the value to replace positive infinity...   \n",
       "...    ...                                                ...   \n",
       "998    998  What is the tensor of shape where*is zero or m...   \n",
       "999    999  What is the tolerance value to determine when ...   \n",
       "1000  1000        What is the default value of atorch.Tensor?   \n",
       "1001  1001      What computes the inverse of a square matrix?   \n",
       "1002  1002  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                       answer  \\\n",
       "0     bynan,posinf, andneginf   \n",
       "1                        zero   \n",
       "2     bynan,posinf, andneginf   \n",
       "3      the least finite value   \n",
       "4                      posinf   \n",
       "...                       ...   \n",
       "998                 A(Tensor)   \n",
       "999                     rcond   \n",
       "1000                    1e-15   \n",
       "1001       torch.linalg.inv()   \n",
       "1002                A(Tensor)   \n",
       "\n",
       "                                                context  \n",
       "0     ReplacesNaN, positive infinity, and negative i...  \n",
       "1     ReplacesNaN, positive infinity, and negative i...  \n",
       "2     ReplacesNaN, positive infinity, and negative i...  \n",
       "3     ReplacesNaN, positive infinity, and negative i...  \n",
       "4     nan(Number,optional) – the value to replaceNaN...  \n",
       "...                                                 ...  \n",
       "998   torch.linalg.inv()computes the inverse of a sq...  \n",
       "999   torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "1000  See also torch.linalg.inv()computes the invers...  \n",
       "1001  See also torch.linalg.inv()computes the invers...  \n",
       "1002  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "\n",
       "[1003 rows x 4 columns]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_bert_tokens(text, max_length=512):\n",
    "    tk = tokenizer(\n",
    "            text[3],\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    model_op = model(**tk)\n",
    "    return model_op"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "records = set([i['context'].strip() for i in train_data.to_dict('records')])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "100%100"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import gc\n",
    "new_tensor_dict = {}\n",
    "\n",
    "for index, i in tqdm(enumerate(records)):\n",
    "    new_tensor_dict[index] = {'text':i, 'model_op': get_bert_tokens(i)}\n",
    "    if index%100==0:\n",
    "        torch.save(new_tensor_dict, f'file_{index}.pt')\n",
    "        new_tensor_dict = {}\n",
    "        gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "348it [08:09,  1.41s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "records = set([i['context'].strip() for i in train_data.to_dict('records')])\n",
    "\n",
    "print(len(records))\n",
    "\n",
    "\n",
    "print(list(records)[8])\n",
    "# new_tensor_dict = {}\n",
    "\n",
    "# for i in tqdm(records):\n",
    "#     new_tensor_dict[] = {'model_op': inference(i), 'top_similar' : [], 'least_similar':[]}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "348\n",
      "‘nuc’ nuclear norm – Number – sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\n",
      "The corresponding dimensions of inputare flattened into\n",
      "one dimension, and the norm is calculated on the flattened\n",
      "dimension. Frobenius norm produces the same result asp=2in all cases\n",
      "except whendimis a list of three or more dims, in which\n",
      "case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "lengths = [len(i) for i in records]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "index = 400\n",
    "torch.save(new_tensor_dict, f'file_{index}.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "sum(lengths)/len(lengths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1097.83908045977"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "list(zip(lengths, records))[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2033,\n",
       " 'torch.acosh().   Adds the scalar otherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\\\text{input}_{i} / \\\\text{other}_{i}inputi\\u200b/otheri\\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in inputinto the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude of inputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().')"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(zip(lengths, records))[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dict2 = torch.load('tensor_dict.pt')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/tbgv3_8n2kg77xkdv85jvcb00000gn/T/ipykernel_54859/3485217336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensor_dict.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "# count of sentences above and below 1k\n",
    "\n",
    "sum([1 for i in records if len(i) < 512])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dict2.update(tensor_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "torch.save(dict2, 'tensor_dict1.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_encodings(id, text):\n",
    "    dataset = TextTokenizer(train, tokenizer, column_name='context')\n",
    "    for text in dataset:\n",
    "        model(**text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "torch.save(tensor_dict, 'tensor_dict.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get cosine similarity between vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_cosine_similarity():\n",
    "    input1 = torch.randn(100, 128)\n",
    "    input2 = torch.randn(100, 128)\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    output = cos(input1, input2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import faiss\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "dataSetI = [.1, .2, .3]\n",
    "dataSetII = [.4, .5, .6]\n",
    "#dataSetII = [.1, .2, .3]\n",
    "\n",
    "x = np.array([dataSetI]).astype(np.float32)\n",
    "q = np.array([dataSetII]).astype(np.float32)\n",
    "index = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "index.ntotal\n",
    "faiss.normalize_L2(x)\n",
    "index.add(x)\n",
    "faiss.normalize_L2(q)\n",
    "distance, index = index.search(q, 5)\n",
    "\n",
    "# norm_distance = 25 * (4 - distance)\n",
    "print(f'Distance by FAISS:{distance}, Index:{index}')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Distance by FAISS:[[ 9.7463179e-01 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      "  -3.4028235e+38]], Index:[[ 0 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "dict2 = torch.load('tensor_dict.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for i in dict2.values():\n",
    "    print(i['model_op']['pooler_output'].shape)\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "a = torch.randn([1, 768])\n",
    "b = torch.randn([1, 768])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "torch.cat((a, b), 0).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get list of tensors from dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "final_op.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "a = [i['model_op']['pooler_output'] for i in dict1.values()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "len(a)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "dict1 = torch.load('file_100.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "dict2 = torch.load('file_200.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "dict1.update(dict2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "dict3 = torch.load('file_300.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "dict4 = torch.load('file_400.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "dict1.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "dict1.update(dict3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "dict1.update(dict4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "final_op = [i['model_op'] for i in dict1.values()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "final_op = torch.cat(a, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "final_op.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([347, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "shape = final_op.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d = "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(shape)\n",
    "print(index.is_trained)\n",
    "index.add(final_op.cpu().detach().numpy())\n",
    "print(index.ntotal)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "token_text = r\"\"\"Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is a LongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is a BoolTensor.   Moves the dimension(s) of input at the position(s) insourceto the position(s) indestination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of inputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values frominput at the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.\"\"\"\n",
    "\n",
    "max_length=512\n",
    "\n",
    "tokens = tokenizer(\n",
    "            token_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "token_index = model(**tokens)['pooler_output']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "D, I = index.search(token_index.cpu().detach().numpy(), 100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "print(D, I)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[126.97375 180.0263  180.0263  180.0263  180.0263  180.0263  180.0263\n",
      "  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263\n",
      "  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263\n",
      "  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263  180.0263\n",
      "  180.0263  180.0263  180.0263  235.4931  235.4931  253.60294 253.60294\n",
      "  253.60294 254.90831 254.90831 254.90831 254.90831 254.90831 254.90831\n",
      "  254.90831 254.90831 257.8427  257.8427  257.8427  257.8427  257.8427\n",
      "  257.8427  257.8427  257.8427  257.8427  257.8427  257.8427  257.8427\n",
      "  257.8427  257.8427  257.8427  257.8427  261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375 261.40375 261.40375 261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375 261.40375 261.40375 261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375 261.40375 261.40375 261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375 261.40375 261.40375 261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375 261.40375 261.40375 261.40375 261.40375 261.40375\n",
      "  261.40375 261.40375]] [[  1 180 246 262 105 150 238  37 147 201 138  93 307 208 146 164 281 320\n",
      "  192 301 344 152  46 161 324 326 182 144 280 133 313 234  80 212 276 142\n",
      "  197 215 119  20 327  77   2  89 194 195 210 240 122 220   3 217 284 244\n",
      "   28 337 331  66 296  33 189 193  90 104 198  42  39 207 108 107 112 114\n",
      "   50  17  15 130 127  55 137 141  59  57  51   9  68 155 154  29 159  82\n",
      "  170  85  34 177 175 179  88  11  10   0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "list(records)[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size⌈end−startstep⌉\\\\left\\\\lceil \\\\frac{\\\\text{end} - \\\\text{start}}{\\\\text{step}} \\\\right\\\\rceil⌈stepend−start\\u200b⌉with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size⌊end−startstep⌋+1\\\\left\\\\lfloor \\\\frac{\\\\text{end} - \\\\text{start}}{\\\\text{step}} \\\\right\\\\rfloor + 1⌊stepend−start\\u200b⌋+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizesteps whose values are evenly spaced from start to end, inclusive.'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "nlist = 5\n",
    "quantizer = faiss.IndexFlatL2(shape)\n",
    "index1 = faiss.IndexIVFFlat(quantizer, shape, nlist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "index1.is_trained"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "index1.train(final_op.cpu().detach().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "index1.is_trained"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "index1.add(final_op.cpu().detach().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "D, I = index1.search(token_index.cpu().detach().numpy(), 100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "print(D, I)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.2697375e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02 1.8002631e+02\n",
      "  1.8002631e+02 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      "  3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38]] [[  1 326 133 344 320 324 152 161 280 201 182 281 301 146 238 246 192 105\n",
      "  144 313 307 208 262 150 147  93 164 180 138  46  37  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "index1.ntotal"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "b = list(records)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "a = list(records)[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "a.append(b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "token_text = b\n",
    "max_length=512\n",
    "\n",
    "tokens = tokenizer(\n",
    "            a,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(shape)\n",
    "print(index.is_trained)\n",
    "index.add(token_index.cpu().detach().numpy())\n",
    "print(index.ntotal)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "token_index = model(**tokens)['pooler_output']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "D, I = index.search(b.cpu().detach().numpy(), 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "print(D, I)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[6.4574947e-12 6.4574947e-12 1.6628124e+01 2.1520056e+01 2.3134634e+01]] [[10  0  1  9  4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "tokens = tokenizer(\n",
    "            b,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "b = model(**tokens)['pooler_output']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "b.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# We will use metric inner product as faiss for similarity matching\n",
    "\n",
    "index = faiss.index_factory(768, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "index.ntotal\n",
    "faiss.normalize_L2(token_index.cpu().detach().numpy())\n",
    "index.add(token_index.cpu().detach().numpy())\n",
    "faiss.normalize_L2(b.cpu().detach().numpy())\n",
    "distance, index = index.search(b.cpu().detach().numpy(), 10)\n",
    "\n",
    "# norm_distance = 25 * (4 - distance)\n",
    "print(f'Distance by FAISS:{distance}, Index:{index}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Distance by FAISS:[[1.0000002  1.0000002  0.9655337  0.9551681  0.95176923 0.9280489\n",
      "  0.9217062  0.88812137 0.8688997  0.7612742 ]], Index:[[10  0  1  9  4  7  6  8  2  3]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def clean_unnecessary_spaces(out_string):\n",
    "    if not isinstance(out_string, str):\n",
    "        warnings.warn(f\">>> {out_string} <<< is not a string.\")\n",
    "        out_string = str(out_string)\n",
    "    out_string = (\n",
    "        out_string.replace(\" .\", \".\")\n",
    "        .replace(\" ?\", \"?\")\n",
    "        .replace(\" !\", \"!\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace(\" ' \", \"'\")\n",
    "        .replace(\" n't\", \"n't\")\n",
    "        .replace(\" 'm\", \"'m\")\n",
    "        .replace(\" 's\", \"'s\")\n",
    "        .replace(\" 've\", \"'ve\")\n",
    "        .replace(\" 're\", \"'re\")\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import glob\n",
    "file_list = (glob.glob(\"data/*.json\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(file_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Commmon data model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path1 = file_list[0]\n",
    "file_path2 = file_list[1]\n",
    "file_path3 = file_list[2]\n",
    "file_path4 = file_list[3]\n",
    "file_path5 = file_list[4]\n",
    "file_path6 = file_list[5]\n",
    "file_path7 = file_list[6]\n",
    "file_path8 = file_list[7]\n",
    "# file_path9 = file_list[8]\n",
    "\n",
    "\n",
    "path1 = os.path.join(os.getcwd(),file_path1)\n",
    "path2 = os.path.join(os.getcwd(),file_path2)\n",
    "path3 = os.path.join(os.getcwd(),file_path3)\n",
    "path4 = os.path.join(os.getcwd(),file_path4)\n",
    "path5 = os.path.join(os.getcwd(),file_path5)\n",
    "path6 = os.path.join(os.getcwd(),file_path6)\n",
    "path7 = os.path.join(os.getcwd(),file_path7)\n",
    "path8 = os.path.join(os.getcwd(),file_path8)\n",
    "# path9 = os.path.join(os.getcwd(),file_path9)\n",
    "\n",
    "print(path5)\n",
    "train1 = pd.read_json(path1)\n",
    "train2 = pd.read_json(path2, orient='index')\n",
    "train3 = pd.read_json(path3)\n",
    "train4 = pd.read_json(path4)\n",
    "train5 = pd.read_json(path5)\n",
    "train6 = pd.read_json(path6)\n",
    "train7 = pd.read_json(path7)\n",
    "train8 = pd.read_json(path8)\n",
    "# train9 = pd.read_json(path9, orient='index')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/nik/Documents/Codes/random_stuff/TSAI_END2_Phase1/data/capstone-1-100.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "train6"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'x': 'What are arguments to embedding layer',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'x': 'does  embedding layer need size of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'x': 'does  embedding layer need embedding di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'x': 'Does the embedding layer require an emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'x': 'Is embedding dimension required for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{'x': '  ptorch geometric has multiple gpu sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>{'x': 'Is ptorch geometric compatible with man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{'x': 'are multiple gpus compatible with pytor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'x': 'multiple gpus compatible with pytorch g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{'x': 'multiple gpus are compatible with pytor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data\n",
       "0   {'x': 'What are arguments to embedding layer',...\n",
       "1   {'x': 'does  embedding layer need size of the ...\n",
       "2   {'x': 'does  embedding layer need embedding di...\n",
       "3   {'x': 'Does the embedding layer require an emb...\n",
       "4   {'x': 'Is embedding dimension required for the...\n",
       "..                                                ...\n",
       "95  {'x': '  ptorch geometric has multiple gpu sup...\n",
       "96  {'x': 'Is ptorch geometric compatible with man...\n",
       "97  {'x': 'are multiple gpus compatible with pytor...\n",
       "98  {'x': 'multiple gpus compatible with pytorch g...\n",
       "99  {'x': 'multiple gpus are compatible with pytor...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "col2_expanded = train6.data.apply(lambda x:pd.Series(x))\n",
    "col2_expanded.columns = ['{}.{}'.format('data',i) for i in col2_expanded]\n",
    "col2_expanded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data.x</th>\n",
       "      <th>data.y</th>\n",
       "      <th>data.z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are arguments to embedding layer</td>\n",
       "      <td>The Embedding layer that is used here is param...</td>\n",
       "      <td>The Embedding layer that is used here is para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>does  embedding layer need size of the vocabul...</td>\n",
       "      <td>The Embedding layer that is used here is param...</td>\n",
       "      <td>The Embedding layer that is used here is para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does  embedding layer need embedding dimension?</td>\n",
       "      <td>The Embedding layer that is used here is param...</td>\n",
       "      <td>The Embedding layer that is used here is para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the embedding layer require an embedding ...</td>\n",
       "      <td>The Embedding layer that is used here is param...</td>\n",
       "      <td>The Embedding layer that is used here is para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is embedding dimension required for the embedd...</td>\n",
       "      <td>The Embedding layer that is used here is param...</td>\n",
       "      <td>The Embedding layer that is used here is para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ptorch geometric has multiple gpu support?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Is ptorch geometric compatible with many GPUs?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>are multiple gpus compatible with pytorch geom...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>multiple gpus compatible with pytorch geometric?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>multiple gpus are compatible with pytorch geom...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data.x  \\\n",
       "0               What are arguments to embedding layer   \n",
       "1   does  embedding layer need size of the vocabul...   \n",
       "2    does  embedding layer need embedding dimension?    \n",
       "3   Does the embedding layer require an embedding ...   \n",
       "4   Is embedding dimension required for the embedd...   \n",
       "..                                                ...   \n",
       "95         ptorch geometric has multiple gpu support?   \n",
       "96    Is ptorch geometric compatible with many GPUs?    \n",
       "97  are multiple gpus compatible with pytorch geom...   \n",
       "98  multiple gpus compatible with pytorch geometric?    \n",
       "99  multiple gpus are compatible with pytorch geom...   \n",
       "\n",
       "                                               data.y  \\\n",
       "0   The Embedding layer that is used here is param...   \n",
       "1   The Embedding layer that is used here is param...   \n",
       "2   The Embedding layer that is used here is param...   \n",
       "3   The Embedding layer that is used here is param...   \n",
       "4   The Embedding layer that is used here is param...   \n",
       "..                                                ...   \n",
       "95   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "96   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "97   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "98   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "99   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "\n",
       "                                               data.z  \n",
       "0    The Embedding layer that is used here is para...  \n",
       "1    The Embedding layer that is used here is para...  \n",
       "2    The Embedding layer that is used here is para...  \n",
       "3    The Embedding layer that is used here is para...  \n",
       "4    The Embedding layer that is used here is para...  \n",
       "..                                                ...  \n",
       "95  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "96  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "97  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "98  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "99  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "train5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'x': 'What is PyTorch', 'y': 'PyTorch  torch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'x': 'Which package manager is recommended fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'x': 'Why you need CUDA toolkit?', 'y': 'CUDA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'x': 'Why you need CUDA toolkit?', 'y': 'CUDA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'x': 'How pytorch replaces numpy?', 'z': 'bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{'x': 'what is use of backward() functntion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>{'x': 'which function is used to propagate los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{'x': 'how to propagate loss backwards in pyto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'x': 'which function is used to update value ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{'x': 'what is the use of step() function? ', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data\n",
       "0   {'x': 'What is PyTorch', 'y': 'PyTorch  torch ...\n",
       "1   {'x': 'Which package manager is recommended fo...\n",
       "2   {'x': 'Why you need CUDA toolkit?', 'y': 'CUDA...\n",
       "3   {'x': 'Why you need CUDA toolkit?', 'y': 'CUDA...\n",
       "4   {'x': 'How pytorch replaces numpy?', 'z': 'bas...\n",
       "..                                                ...\n",
       "95  {'x': 'what is use of backward() functntion of...\n",
       "96  {'x': 'which function is used to propagate los...\n",
       "97  {'x': 'how to propagate loss backwards in pyto...\n",
       "98  {'x': 'which function is used to update value ...\n",
       "99  {'x': 'what is the use of step() function? ', ...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "\n",
    "\n",
    "col3_expanded = train5.data.apply(lambda x:pd.Series(x))\n",
    "col3_expanded.columns = ['{}.{}'.format('data',i) for i in col3_expanded]\n",
    "col3_expanded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data.x</th>\n",
       "      <th>data.y</th>\n",
       "      <th>data.z</th>\n",
       "      <th>data.video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is PyTorch</td>\n",
       "      <td>PyTorch  torch is one of the most popular mach...</td>\n",
       "      <td>hi everybody welcome to a new tutorial series...</td>\n",
       "      <td>EMXfZB8FVUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which package manager is recommended for PyTorch?</td>\n",
       "      <td>Anaconda package manager is recommended to ins...</td>\n",
       "      <td>hi everybody welcome to a new tutorial series...</td>\n",
       "      <td>EMXfZB8FVUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why you need CUDA toolkit?</td>\n",
       "      <td>CUDA toolkit is a development environment for ...</td>\n",
       "      <td>if you haven't installed anaconda yet and don...</td>\n",
       "      <td>EMXfZB8FVUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why you need CUDA toolkit?</td>\n",
       "      <td>CUDA toolkit is a development environment for ...</td>\n",
       "      <td>if you haven't installed anaconda yet and don...</td>\n",
       "      <td>EMXfZB8FVUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How pytorch replaces numpy?</td>\n",
       "      <td>unlike numpy PyTorch uses power of GPUs and ot...</td>\n",
       "      <td>basically enabling researchers to be as expres...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>what is use of backward() functntion of loss o...</td>\n",
       "      <td>iteratively propagates the loss backward throu...</td>\n",
       "      <td>Let’s take a look at how this gradient­steppin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>which function is used to propagate loss backw...</td>\n",
       "      <td>backward() iteratively propagates the loss bac...</td>\n",
       "      <td>Let’s take a look at how this gradient­steppin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>how to propagate loss backwards in pytorch?</td>\n",
       "      <td>backward() iteratively propagates the loss bac...</td>\n",
       "      <td>Let’s take a look at how this gradient­steppin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>which function is used to update value of weights</td>\n",
       "      <td>the optimizer (opt) instructs the parameters ...</td>\n",
       "      <td>Let’s take a look at how this gradient­steppin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>what is the use of step() function?</td>\n",
       "      <td>the optimizer (opt) instructs the parameters ...</td>\n",
       "      <td>Let’s take a look at how this gradient­steppin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data.x  \\\n",
       "0                                     What is PyTorch   \n",
       "1   Which package manager is recommended for PyTorch?   \n",
       "2                          Why you need CUDA toolkit?   \n",
       "3                          Why you need CUDA toolkit?   \n",
       "4                         How pytorch replaces numpy?   \n",
       "..                                                ...   \n",
       "95  what is use of backward() functntion of loss o...   \n",
       "96  which function is used to propagate loss backw...   \n",
       "97        how to propagate loss backwards in pytorch?   \n",
       "98  which function is used to update value of weights   \n",
       "99               what is the use of step() function?    \n",
       "\n",
       "                                               data.y  \\\n",
       "0   PyTorch  torch is one of the most popular mach...   \n",
       "1   Anaconda package manager is recommended to ins...   \n",
       "2   CUDA toolkit is a development environment for ...   \n",
       "3   CUDA toolkit is a development environment for ...   \n",
       "4   unlike numpy PyTorch uses power of GPUs and ot...   \n",
       "..                                                ...   \n",
       "95  iteratively propagates the loss backward throu...   \n",
       "96  backward() iteratively propagates the loss bac...   \n",
       "97  backward() iteratively propagates the loss bac...   \n",
       "98   the optimizer (opt) instructs the parameters ...   \n",
       "99   the optimizer (opt) instructs the parameters ...   \n",
       "\n",
       "                                               data.z data.video_id  \n",
       "0    hi everybody welcome to a new tutorial series...   EMXfZB8FVUA  \n",
       "1    hi everybody welcome to a new tutorial series...   EMXfZB8FVUA  \n",
       "2    if you haven't installed anaconda yet and don...   EMXfZB8FVUA  \n",
       "3    if you haven't installed anaconda yet and don...   EMXfZB8FVUA  \n",
       "4   basically enabling researchers to be as expres...           NaN  \n",
       "..                                                ...           ...  \n",
       "95  Let’s take a look at how this gradient­steppin...           NaN  \n",
       "96  Let’s take a look at how this gradient­steppin...           NaN  \n",
       "97  Let’s take a look at how this gradient­steppin...           NaN  \n",
       "98  Let’s take a look at how this gradient­steppin...           NaN  \n",
       "99  Let’s take a look at how this gradient­steppin...           NaN  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "\n",
    "df_56 = pd.concat([col3_expanded, col2_expanded]).drop(columns='data.video_id').rename(columns={\n",
    "    'data.x': 'question',\n",
    "    'data.y': 'answer',\n",
    "    'data.z': 'context'\n",
    "})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "df_56"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is PyTorch</td>\n",
       "      <td>PyTorch  torch is one of the most popular mach...</td>\n",
       "      <td>hi everybody welcome to a new tutorial series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which package manager is recommended for PyTorch?</td>\n",
       "      <td>Anaconda package manager is recommended to ins...</td>\n",
       "      <td>hi everybody welcome to a new tutorial series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why you need CUDA toolkit?</td>\n",
       "      <td>CUDA toolkit is a development environment for ...</td>\n",
       "      <td>if you haven't installed anaconda yet and don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why you need CUDA toolkit?</td>\n",
       "      <td>CUDA toolkit is a development environment for ...</td>\n",
       "      <td>if you haven't installed anaconda yet and don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How pytorch replaces numpy?</td>\n",
       "      <td>unlike numpy PyTorch uses power of GPUs and ot...</td>\n",
       "      <td>basically enabling researchers to be as expres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ptorch geometric has multiple gpu support?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Is ptorch geometric compatible with many GPUs?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>are multiple gpus compatible with pytorch geom...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>multiple gpus compatible with pytorch geometric?</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>multiple gpus are compatible with pytorch geom...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep l...</td>\n",
       "      <td>PyTorch Geometric (PyG) is a geometric deep le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                                     What is PyTorch   \n",
       "1   Which package manager is recommended for PyTorch?   \n",
       "2                          Why you need CUDA toolkit?   \n",
       "3                          Why you need CUDA toolkit?   \n",
       "4                         How pytorch replaces numpy?   \n",
       "..                                                ...   \n",
       "95         ptorch geometric has multiple gpu support?   \n",
       "96    Is ptorch geometric compatible with many GPUs?    \n",
       "97  are multiple gpus compatible with pytorch geom...   \n",
       "98  multiple gpus compatible with pytorch geometric?    \n",
       "99  multiple gpus are compatible with pytorch geom...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   PyTorch  torch is one of the most popular mach...   \n",
       "1   Anaconda package manager is recommended to ins...   \n",
       "2   CUDA toolkit is a development environment for ...   \n",
       "3   CUDA toolkit is a development environment for ...   \n",
       "4   unlike numpy PyTorch uses power of GPUs and ot...   \n",
       "..                                                ...   \n",
       "95   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "96   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "97   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "98   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "99   PyTorch Geometric (PyG) is a geometric deep l...   \n",
       "\n",
       "                                              context  \n",
       "0    hi everybody welcome to a new tutorial series...  \n",
       "1    hi everybody welcome to a new tutorial series...  \n",
       "2    if you haven't installed anaconda yet and don...  \n",
       "3    if you haven't installed anaconda yet and don...  \n",
       "4   basically enabling researchers to be as expres...  \n",
       "..                                                ...  \n",
       "95  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "96  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "97  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "98  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "99  PyTorch Geometric (PyG) is a geometric deep le...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "ri = str(random.randint(1,100)) \n",
    "col4_expanded = train1.data.apply(lambda x:pd.Series(x))\n",
    "col4_expanded.columns = ['{}.{}'.format('data',i) for i in col4_expanded]\n",
    "col4_expanded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data.X</th>\n",
       "      <th>data.Z</th>\n",
       "      <th>data.Y</th>\n",
       "      <th>data.Y</th>\n",
       "      <th>data.Z</th>\n",
       "      <th>data.X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calculating loss for entire batch using nlllos...</td>\n",
       "      <td>Defake:loss = criterion_test(dec_outs.view(-1,...</td>\n",
       "      <td>I believe you should use criterion test(dec ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>masked fill operates weirdly</td>\n",
       "      <td>From the error mesage, it is a size issue on t...</td>\n",
       "      <td>The error message indicates that there is a si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>creating custom nn module for softmargin softmax</td>\n",
       "      <td>Prefer not to use for loops, try to vectorize ...</td>\n",
       "      <td>Avoid using for loops whenever feasible, and a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gru autoencoder is not working</td>\n",
       "      <td>I solve this problem.Target tensor was wrong.</td>\n",
       "      <td>Target tensor was wrong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issue with multiple gpu loss convergence</td>\n",
       "      <td>I solved my issue. Since batch wasnt my first ...</td>\n",
       "      <td>mention dim=1 in the data parallel, that is t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>current status of automatic quantization support</td>\n",
       "      <td>Yes, graph mode quantization will be ready for...</td>\n",
       "      <td>Yes, graph mode quantization will be ready for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>check if grad is enabled</td>\n",
       "      <td>You can check it by accessing torch.Tensor's r...</td>\n",
       "      <td>You may check it out by going to torch. The re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>non gradient trackable convolutions</td>\n",
       "      <td>Yes, that's possible.You could either use the ...</td>\n",
       "      <td>You could either use the functional API with a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>best way to encode latent variable with cnn</td>\n",
       "      <td>I think you might need a flatten in 1, too,We ...</td>\n",
       "      <td>I think you might need a flatten in 1, too.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>custom color mapping in data loader for unet i...</td>\n",
       "      <td>Figured it out! The fill tool I was using on G...</td>\n",
       "      <td>The GIMP fill tool I was using had anti-aliasi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data.X   \\\n",
       "0    calculating loss for entire batch using nlllos...   \n",
       "1                         masked fill operates weirdly   \n",
       "2     creating custom nn module for softmargin softmax   \n",
       "3                       gru autoencoder is not working   \n",
       "4             issue with multiple gpu loss convergence   \n",
       "..                                                 ...   \n",
       "548   current status of automatic quantization support   \n",
       "549                           check if grad is enabled   \n",
       "550                non gradient trackable convolutions   \n",
       "551        best way to encode latent variable with cnn   \n",
       "552  custom color mapping in data loader for unet i...   \n",
       "\n",
       "                                               data.Z   \\\n",
       "0    Defake:loss = criterion_test(dec_outs.view(-1,...   \n",
       "1    From the error mesage, it is a size issue on t...   \n",
       "2    Prefer not to use for loops, try to vectorize ...   \n",
       "3        I solve this problem.Target tensor was wrong.   \n",
       "4    I solved my issue. Since batch wasnt my first ...   \n",
       "..                                                 ...   \n",
       "548  Yes, graph mode quantization will be ready for...   \n",
       "549  You can check it by accessing torch.Tensor's r...   \n",
       "550  Yes, that's possible.You could either use the ...   \n",
       "551  I think you might need a flatten in 1, too,We ...   \n",
       "552  Figured it out! The fill tool I was using on G...   \n",
       "\n",
       "                                               data.Y  data.Y data.Z data.X  \n",
       "0    I believe you should use criterion test(dec ou...    NaN    NaN    NaN  \n",
       "1    The error message indicates that there is a si...    NaN    NaN    NaN  \n",
       "2    Avoid using for loops whenever feasible, and a...    NaN    NaN    NaN  \n",
       "3                              Target tensor was wrong    NaN    NaN    NaN  \n",
       "4     mention dim=1 in the data parallel, that is t...    NaN    NaN    NaN  \n",
       "..                                                 ...    ...    ...    ...  \n",
       "548  Yes, graph mode quantization will be ready for...    NaN    NaN    NaN  \n",
       "549  You may check it out by going to torch. The re...    NaN    NaN    NaN  \n",
       "550  You could either use the functional API with a...    NaN    NaN    NaN  \n",
       "551        I think you might need a flatten in 1, too.    NaN    NaN    NaN  \n",
       "552  The GIMP fill tool I was using had anti-aliasi...    NaN    NaN    NaN  \n",
       "\n",
       "[553 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "df_1 = col4_expanded.drop(columns=['data.Z','data.X','data.Y']).rename(columns={\n",
    "    'data.X ': 'question',\n",
    "    'data.Y ': 'answer',\n",
    "    'data.Z ': 'context'\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "df_1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calculating loss for entire batch using nlllos...</td>\n",
       "      <td>Defake:loss = criterion_test(dec_outs.view(-1,...</td>\n",
       "      <td>I believe you should use criterion test(dec ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>masked fill operates weirdly</td>\n",
       "      <td>From the error mesage, it is a size issue on t...</td>\n",
       "      <td>The error message indicates that there is a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>creating custom nn module for softmargin softmax</td>\n",
       "      <td>Prefer not to use for loops, try to vectorize ...</td>\n",
       "      <td>Avoid using for loops whenever feasible, and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gru autoencoder is not working</td>\n",
       "      <td>I solve this problem.Target tensor was wrong.</td>\n",
       "      <td>Target tensor was wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issue with multiple gpu loss convergence</td>\n",
       "      <td>I solved my issue. Since batch wasnt my first ...</td>\n",
       "      <td>mention dim=1 in the data parallel, that is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>current status of automatic quantization support</td>\n",
       "      <td>Yes, graph mode quantization will be ready for...</td>\n",
       "      <td>Yes, graph mode quantization will be ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>check if grad is enabled</td>\n",
       "      <td>You can check it by accessing torch.Tensor's r...</td>\n",
       "      <td>You may check it out by going to torch. The re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>non gradient trackable convolutions</td>\n",
       "      <td>Yes, that's possible.You could either use the ...</td>\n",
       "      <td>You could either use the functional API with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>best way to encode latent variable with cnn</td>\n",
       "      <td>I think you might need a flatten in 1, too,We ...</td>\n",
       "      <td>I think you might need a flatten in 1, too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>custom color mapping in data loader for unet i...</td>\n",
       "      <td>Figured it out! The fill tool I was using on G...</td>\n",
       "      <td>The GIMP fill tool I was using had anti-aliasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    calculating loss for entire batch using nlllos...   \n",
       "1                         masked fill operates weirdly   \n",
       "2     creating custom nn module for softmargin softmax   \n",
       "3                       gru autoencoder is not working   \n",
       "4             issue with multiple gpu loss convergence   \n",
       "..                                                 ...   \n",
       "548   current status of automatic quantization support   \n",
       "549                           check if grad is enabled   \n",
       "550                non gradient trackable convolutions   \n",
       "551        best way to encode latent variable with cnn   \n",
       "552  custom color mapping in data loader for unet i...   \n",
       "\n",
       "                                               context  \\\n",
       "0    Defake:loss = criterion_test(dec_outs.view(-1,...   \n",
       "1    From the error mesage, it is a size issue on t...   \n",
       "2    Prefer not to use for loops, try to vectorize ...   \n",
       "3        I solve this problem.Target tensor was wrong.   \n",
       "4    I solved my issue. Since batch wasnt my first ...   \n",
       "..                                                 ...   \n",
       "548  Yes, graph mode quantization will be ready for...   \n",
       "549  You can check it by accessing torch.Tensor's r...   \n",
       "550  Yes, that's possible.You could either use the ...   \n",
       "551  I think you might need a flatten in 1, too,We ...   \n",
       "552  Figured it out! The fill tool I was using on G...   \n",
       "\n",
       "                                                answer  \n",
       "0    I believe you should use criterion test(dec ou...  \n",
       "1    The error message indicates that there is a si...  \n",
       "2    Avoid using for loops whenever feasible, and a...  \n",
       "3                              Target tensor was wrong  \n",
       "4     mention dim=1 in the data parallel, that is t...  \n",
       "..                                                 ...  \n",
       "548  Yes, graph mode quantization will be ready for...  \n",
       "549  You may check it out by going to torch. The re...  \n",
       "550  You could either use the functional API with a...  \n",
       "551        I think you might need a flatten in 1, too.  \n",
       "552  The GIMP fill tool I was using had anti-aliasi...  \n",
       "\n",
       "[553 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "import random\n",
    "random.randint(1,)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "\n",
    "df_2 = train2.reset_index().drop(columns=['source', 'index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "df_2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero</td>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the least finite value</td>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>posinf</td>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>nan(Number,optional) – the value to replaceNaN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>torch.linalg.inv()computes the inverse of a sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>rcond</td>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1e-15</td>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       answer  \\\n",
       "0     bynan,posinf, andneginf   \n",
       "1                        zero   \n",
       "2     bynan,posinf, andneginf   \n",
       "3      the least finite value   \n",
       "4                      posinf   \n",
       "...                       ...   \n",
       "998                 A(Tensor)   \n",
       "999                     rcond   \n",
       "1000                    1e-15   \n",
       "1001       torch.linalg.inv()   \n",
       "1002                A(Tensor)   \n",
       "\n",
       "                                               question  \\\n",
       "0     What values are specified to replaceNaN, posit...   \n",
       "1           By default,NaN is replaced with what value?   \n",
       "2     What values are used to replace negative infin...   \n",
       "3      What is the default value for negative infinity?   \n",
       "4     What is the value to replace positive infinity...   \n",
       "...                                                 ...   \n",
       "998   What is the tensor of shape where*is zero or m...   \n",
       "999   What is the tolerance value to determine when ...   \n",
       "1000        What is the default value of atorch.Tensor?   \n",
       "1001      What computes the inverse of a square matrix?   \n",
       "1002  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                                                context  \n",
       "0     ReplacesNaN, positive infinity, and negative i...  \n",
       "1     ReplacesNaN, positive infinity, and negative i...  \n",
       "2     ReplacesNaN, positive infinity, and negative i...  \n",
       "3     ReplacesNaN, positive infinity, and negative i...  \n",
       "4     nan(Number,optional) – the value to replaceNaN...  \n",
       "...                                                 ...  \n",
       "998   torch.linalg.inv()computes the inverse of a sq...  \n",
       "999   torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "1000  See also torch.linalg.inv()computes the invers...  \n",
       "1001  See also torch.linalg.inv()computes the invers...  \n",
       "1002  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...  \n",
       "\n",
       "[1003 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "df_47 = pd.concat([train7, train4]).drop(columns='null').rename(columns={\n",
    "    'x': 'question',\n",
    "    'y': 'answer',\n",
    "    'z': 'context'\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "df_47\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support multiple simultaneous LR schedulers</td>\n",
       "      <td>A solution was implemented in #26423. Closing ...</td>\n",
       "      <td>This issue has been fixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Want RTX 2080ti Support!!! RuntimeError: cubla...</td>\n",
       "      <td>We'll supply PyTorch binaries w/ CUDA 10 in th...</td>\n",
       "      <td>You need to build from source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crash when reading pandas parquet file after i...</td>\n",
       "      <td>uninstall the pyarrow installed by pip and the...</td>\n",
       "      <td>You need to uninstall the pyarrow installed by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RuntimeError: Only tuples, lists and Variables...</td>\n",
       "      <td>Solved.Just change the output of your model fr...</td>\n",
       "      <td>You need to change the output of your model fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>torch.autograd.jacobian returns tensors with a...</td>\n",
       "      <td>This is not a bug. It happens because in your ...</td>\n",
       "      <td>This is not a bug. It happens because in your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>[Bug] Sometimes gradient doesn't back-propagat...</td>\n",
       "      <td>I don't think this is a bug. He is checking th...</td>\n",
       "      <td>I don't think this is a bug. He is checking th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>pytorch 0.4.0 always allocates memory on GPU:0...</td>\n",
       "      <td>I believe this has been fixed in https://githu...</td>\n",
       "      <td>I believe this has been fixed in https://githu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>roi_crop (from Detectron.pytorch) building con...</td>\n",
       "      <td>@phalexo You could try out to install `pytorch...</td>\n",
       "      <td>@phalexo You could try out to install `pytorch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Crash with SIGFPE due to unhandled cases in di...</td>\n",
       "      <td>There needs to be two changes in the code:\\n1....</td>\n",
       "      <td>There needs to be two changes in the code:\\n1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4526 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                                                   NaN   \n",
       "1           Support multiple simultaneous LR schedulers   \n",
       "2     Want RTX 2080ti Support!!! RuntimeError: cubla...   \n",
       "3     Crash when reading pandas parquet file after i...   \n",
       "4     RuntimeError: Only tuples, lists and Variables...   \n",
       "...                                                 ...   \n",
       "1991  torch.autograd.jacobian returns tensors with a...   \n",
       "1992  [Bug] Sometimes gradient doesn't back-propagat...   \n",
       "1993  pytorch 0.4.0 always allocates memory on GPU:0...   \n",
       "1994  roi_crop (from Detectron.pytorch) building con...   \n",
       "1995  Crash with SIGFPE due to unhandled cases in di...   \n",
       "\n",
       "                                                context  \\\n",
       "0                                                   NaN   \n",
       "1     A solution was implemented in #26423. Closing ...   \n",
       "2     We'll supply PyTorch binaries w/ CUDA 10 in th...   \n",
       "3     uninstall the pyarrow installed by pip and the...   \n",
       "4     Solved.Just change the output of your model fr...   \n",
       "...                                                 ...   \n",
       "1991  This is not a bug. It happens because in your ...   \n",
       "1992  I don't think this is a bug. He is checking th...   \n",
       "1993  I believe this has been fixed in https://githu...   \n",
       "1994  @phalexo You could try out to install `pytorch...   \n",
       "1995  There needs to be two changes in the code:\\n1....   \n",
       "\n",
       "                                                 answer  \n",
       "0                                                   NaN  \n",
       "1                             This issue has been fixed  \n",
       "2                         You need to build from source  \n",
       "3     You need to uninstall the pyarrow installed by...  \n",
       "4     You need to change the output of your model fr...  \n",
       "...                                                 ...  \n",
       "1991  This is not a bug. It happens because in your ...  \n",
       "1992  I don't think this is a bug. He is checking th...  \n",
       "1993  I believe this has been fixed in https://githu...  \n",
       "1994  @phalexo You could try out to install `pytorch...  \n",
       "1995  There needs to be two changes in the code:\\n1....  \n",
       "\n",
       "[4526 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "train8\n",
    "\n",
    "col4_expanded = train8.data.apply(lambda x:pd.Series(x))\n",
    "col4_expanded.columns = ['{}.{}'.format('data',i) for i in col4_expanded]\n",
    "col4_expanded\n",
    "\n",
    "df_8 = col4_expanded.rename(columns={\n",
    "    'data.x': 'question',\n",
    "    'data.y': 'answer',\n",
    "    'data.z': 'context'\n",
    "})\n",
    "\n",
    "df_8"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does torchvision.transforms.CenterCrop do?</td>\n",
       "      <td>Crops the given image at the center</td>\n",
       "      <td>torchvision.transforms.CenterCrop(size)[SOURC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of torchvision.transforms....</td>\n",
       "      <td>Crops the given image at the center</td>\n",
       "      <td>torchvision.transforms.CenterCrop(size)[SOURC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is torchvision.transforms.CenterCrop used...</td>\n",
       "      <td>Crops the given image at the center</td>\n",
       "      <td>torchvision.transforms.CenterCrop(size)[SOURC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is center cropping available?</td>\n",
       "      <td>torchvision.transforms.CenterCrop Crops the g...</td>\n",
       "      <td>torchvision.transforms.CenterCrop(size)[SOURC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there center cropping available in torchvis...</td>\n",
       "      <td>torchvision.transforms.CenterCrop Crops the g...</td>\n",
       "      <td>torchvision.transforms.CenterCrop(size)[SOURC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Why isn't the GPU released when Python is ter...</td>\n",
       "      <td>If your GPU memory isn’t freed even after Pyt...</td>\n",
       "      <td>PyTorch uses a caching memory allocator to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>When Python is ended, why isn't the GPU relea...</td>\n",
       "      <td>If your GPU memory isn’t freed even after Pyt...</td>\n",
       "      <td>PyTorch uses a caching memory allocator to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>When Python is ended, why isn't the GPU relea...</td>\n",
       "      <td>If your GPU memory isn’t freed even after Pyt...</td>\n",
       "      <td>PyTorch uses a caching memory allocator to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Why isn't the GPU released when Python is fin...</td>\n",
       "      <td>If your GPU memory isn’t freed even after Pyt...</td>\n",
       "      <td>PyTorch uses a caching memory allocator to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Why isn't the GPU made available after Python...</td>\n",
       "      <td>If your GPU memory isn’t freed even after Pyt...</td>\n",
       "      <td>PyTorch uses a caching memory allocator to sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0      What does torchvision.transforms.CenterCrop do?   \n",
       "1    What is the purpose of torchvision.transforms....   \n",
       "2    What is torchvision.transforms.CenterCrop used...   \n",
       "3                        is center cropping available?   \n",
       "4    is there center cropping available in torchvis...   \n",
       "..                                                 ...   \n",
       "96    Why isn't the GPU released when Python is ter...   \n",
       "97    When Python is ended, why isn't the GPU relea...   \n",
       "98    When Python is ended, why isn't the GPU relea...   \n",
       "99    Why isn't the GPU released when Python is fin...   \n",
       "100   Why isn't the GPU made available after Python...   \n",
       "\n",
       "                                                answer  \\\n",
       "0                  Crops the given image at the center   \n",
       "1                  Crops the given image at the center   \n",
       "2                  Crops the given image at the center   \n",
       "3     torchvision.transforms.CenterCrop Crops the g...   \n",
       "4     torchvision.transforms.CenterCrop Crops the g...   \n",
       "..                                                 ...   \n",
       "96    If your GPU memory isn’t freed even after Pyt...   \n",
       "97    If your GPU memory isn’t freed even after Pyt...   \n",
       "98    If your GPU memory isn’t freed even after Pyt...   \n",
       "99    If your GPU memory isn’t freed even after Pyt...   \n",
       "100   If your GPU memory isn’t freed even after Pyt...   \n",
       "\n",
       "                                               context  \n",
       "0     torchvision.transforms.CenterCrop(size)[SOURC...  \n",
       "1     torchvision.transforms.CenterCrop(size)[SOURC...  \n",
       "2     torchvision.transforms.CenterCrop(size)[SOURC...  \n",
       "3     torchvision.transforms.CenterCrop(size)[SOURC...  \n",
       "4     torchvision.transforms.CenterCrop(size)[SOURC...  \n",
       "..                                                 ...  \n",
       "96    PyTorch uses a caching memory allocator to sp...  \n",
       "97    PyTorch uses a caching memory allocator to sp...  \n",
       "98    PyTorch uses a caching memory allocator to sp...  \n",
       "99    PyTorch uses a caching memory allocator to sp...  \n",
       "100   PyTorch uses a caching memory allocator to sp...  \n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "df_3 = train3.rename(columns={\n",
    "    'X': 'question',\n",
    "    'Y': 'answer',\n",
    "    'Z': 'context'\n",
    "})\n",
    "\n",
    "df_3"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In what platform do the key nn modules run?</td>\n",
       "      <td>FP32</td>\n",
       "      <td>torch.nn.intrinsic.qat This module implements ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What module implements versions of the key nn ...</td>\n",
       "      <td>torch.nn.quantized</td>\n",
       "      <td>This module implements the functions you call ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is needed for quantization aware training?</td>\n",
       "      <td>fused operations</td>\n",
       "      <td>This module implements the versions of those f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conv2d and Linear() use rounding to simulate t...</td>\n",
       "      <td>INT8 quantization</td>\n",
       "      <td>This module implements the versions of those f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What module implements versions of the key nn ...</td>\n",
       "      <td>torch.nn.quantized</td>\n",
       "      <td>This module implements the functions you call ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18677</th>\n",
       "      <td>digamma Computes what derivative of the gamma ...</td>\n",
       "      <td>logarithmic derivative</td>\n",
       "      <td>digamma Computes the logarithmic derivative of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18678</th>\n",
       "      <td>What computes the logarithmic derivative of th...</td>\n",
       "      <td>digamma</td>\n",
       "      <td>digamma Computes the logarithmic derivative of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18679</th>\n",
       "      <td>exp Returns a new tensor with what of the elem...</td>\n",
       "      <td>exponential</td>\n",
       "      <td>exp Returns a new tensor with the exponential ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18680</th>\n",
       "      <td>What is the Alias for torch.special.expm1?</td>\n",
       "      <td>expm1</td>\n",
       "      <td>expm1 Alias for torch.special.expm1().</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18681</th>\n",
       "      <td>What is the Alias for torch.special.expm1()?</td>\n",
       "      <td>expm1</td>\n",
       "      <td>expm1 Alias for torch.special.expm1().</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18682 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0            In what platform do the key nn modules run?   \n",
       "1      What module implements versions of the key nn ...   \n",
       "2        What is needed for quantization aware training?   \n",
       "3      Conv2d and Linear() use rounding to simulate t...   \n",
       "4      What module implements versions of the key nn ...   \n",
       "...                                                  ...   \n",
       "18677  digamma Computes what derivative of the gamma ...   \n",
       "18678  What computes the logarithmic derivative of th...   \n",
       "18679  exp Returns a new tensor with what of the elem...   \n",
       "18680         What is the Alias for torch.special.expm1?   \n",
       "18681       What is the Alias for torch.special.expm1()?   \n",
       "\n",
       "                       answer  \\\n",
       "0                        FP32   \n",
       "1          torch.nn.quantized   \n",
       "2            fused operations   \n",
       "3           INT8 quantization   \n",
       "4          torch.nn.quantized   \n",
       "...                       ...   \n",
       "18677  logarithmic derivative   \n",
       "18678                 digamma   \n",
       "18679             exponential   \n",
       "18680                   expm1   \n",
       "18681                   expm1   \n",
       "\n",
       "                                                 context  \n",
       "0      torch.nn.intrinsic.qat This module implements ...  \n",
       "1      This module implements the functions you call ...  \n",
       "2      This module implements the versions of those f...  \n",
       "3      This module implements the versions of those f...  \n",
       "4      This module implements the functions you call ...  \n",
       "...                                                  ...  \n",
       "18677  digamma Computes the logarithmic derivative of...  \n",
       "18678  digamma Computes the logarithmic derivative of...  \n",
       "18679  exp Returns a new tensor with the exponential ...  \n",
       "18680             expm1 Alias for torch.special.expm1().  \n",
       "18681             expm1 Alias for torch.special.expm1().  \n",
       "\n",
       "[18682 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "final_pd = pd.concat([df_3, df_8, df_1, df_47, df_2, df_56])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "final_pd.to_csv('data/final_pd.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d964e81d6599016aee694e721ea343ca3a42c4f371ca966a53ff9f7507121df1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('caps': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}