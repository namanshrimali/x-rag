{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6r9mPtyPp_V",
    "outputId": "73cc4185-6438-4241-dd10-bd1ec82e0852"
   },
   "outputs": [],
   "source": [
    "# # verify GPU availability\n",
    "# import tensorflow as tf\n",
    "\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CrutTC5PuZh",
    "outputId": "c9eb9afe-846c-48a9-b949-1d97c1f1d2e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSxbglFS4MDw",
    "outputId": "4ae03845-efd9-40bf-8181-273a00fcf7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.4.6-py3-none-any.whl (922 kB)\n",
      "\u001b[K     |████████████████████████████████| 922 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyDeprecate==0.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.3.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2021.8.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.9.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (4.59.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.5.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (20.9)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (5.4.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.20.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.39.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (20.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.6.3)\n",
      "Installing collected packages: pytorch-lightning\n",
      "Successfully installed pytorch-lightning-1.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujQTsR1D4Xib",
    "outputId": "e2099471-70e1-4bac-a895-6b258f14f30a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"transformers_version\": \"4.10.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a1M_Vmb03TxG",
    "outputId": "f5774441-864e-4e58-92ea-327dd3bbdb86"
   },
   "outputs": [],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# specify GPU device\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "97C_hk2nBiar"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are specified to replaceNaN, posit...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>zero</td>\n",
       "      <td>By default,NaN is replaced with what value?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>bynan,posinf, andneginf</td>\n",
       "      <td>What values are used to replace negative infin...</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>the least finite value</td>\n",
       "      <td>What is the default value for negative infinity?</td>\n",
       "      <td>ReplacesNaN, positive infinity, and negative i...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>posinf</td>\n",
       "      <td>What is the value to replace positive infinity...</td>\n",
       "      <td>nan(Number,optional) – the value to replaceNaN...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape where*is zero or m...</td>\n",
       "      <td>torch.linalg.inv()computes the inverse of a sq...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>rcond</td>\n",
       "      <td>What is the tolerance value to determine when ...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>1e-15</td>\n",
       "      <td>What is the default value of atorch.Tensor?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>torch.linalg.inv()</td>\n",
       "      <td>What computes the inverse of a square matrix?</td>\n",
       "      <td>See also torch.linalg.inv()computes the invers...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>A(Tensor)</td>\n",
       "      <td>What is the tensor of shape(*, m, n)where*is z...</td>\n",
       "      <td>torch.linalg.lstsq()computesA.pinv() @Bwith a\\...</td>\n",
       "      <td>https://pytorch.org/docs/stable/generated/torc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  index  \\\n",
       "0     What values are specified to replaceNaN, posit...   \n",
       "1           By default,NaN is replaced with what value?   \n",
       "2     What values are used to replace negative infin...   \n",
       "3      What is the default value for negative infinity?   \n",
       "4     What is the value to replace positive infinity...   \n",
       "...                                                 ...   \n",
       "998   What is the tensor of shape where*is zero or m...   \n",
       "999   What is the tolerance value to determine when ...   \n",
       "1000        What is the default value of atorch.Tensor?   \n",
       "1001      What computes the inverse of a square matrix?   \n",
       "1002  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                       answer  \\\n",
       "0     bynan,posinf, andneginf   \n",
       "1                        zero   \n",
       "2     bynan,posinf, andneginf   \n",
       "3      the least finite value   \n",
       "4                      posinf   \n",
       "...                       ...   \n",
       "998                 A(Tensor)   \n",
       "999                     rcond   \n",
       "1000                    1e-15   \n",
       "1001       torch.linalg.inv()   \n",
       "1002                A(Tensor)   \n",
       "\n",
       "                                               question  \\\n",
       "0     What values are specified to replaceNaN, posit...   \n",
       "1           By default,NaN is replaced with what value?   \n",
       "2     What values are used to replace negative infin...   \n",
       "3      What is the default value for negative infinity?   \n",
       "4     What is the value to replace positive infinity...   \n",
       "...                                                 ...   \n",
       "998   What is the tensor of shape where*is zero or m...   \n",
       "999   What is the tolerance value to determine when ...   \n",
       "1000        What is the default value of atorch.Tensor?   \n",
       "1001      What computes the inverse of a square matrix?   \n",
       "1002  What is the tensor of shape(*, m, n)where*is z...   \n",
       "\n",
       "                                                context  \\\n",
       "0     ReplacesNaN, positive infinity, and negative i...   \n",
       "1     ReplacesNaN, positive infinity, and negative i...   \n",
       "2     ReplacesNaN, positive infinity, and negative i...   \n",
       "3     ReplacesNaN, positive infinity, and negative i...   \n",
       "4     nan(Number,optional) – the value to replaceNaN...   \n",
       "...                                                 ...   \n",
       "998   torch.linalg.inv()computes the inverse of a sq...   \n",
       "999   torch.linalg.lstsq()computesA.pinv() @Bwith a\\...   \n",
       "1000  See also torch.linalg.inv()computes the invers...   \n",
       "1001  See also torch.linalg.inv()computes the invers...   \n",
       "1002  torch.linalg.lstsq()computesA.pinv() @Bwith a\\...   \n",
       "\n",
       "                                                 source  \n",
       "0     https://pytorch.org/docs/stable/generated/torc...  \n",
       "1     https://pytorch.org/docs/stable/generated/torc...  \n",
       "2     https://pytorch.org/docs/stable/generated/torc...  \n",
       "3     https://pytorch.org/docs/stable/generated/torc...  \n",
       "4     https://pytorch.org/docs/stable/generated/torc...  \n",
       "...                                                 ...  \n",
       "998   https://pytorch.org/docs/stable/generated/torc...  \n",
       "999   https://pytorch.org/docs/stable/generated/torc...  \n",
       "1000  https://pytorch.org/docs/stable/generated/torc...  \n",
       "1001  https://pytorch.org/docs/stable/generated/torc...  \n",
       "1002  https://pytorch.org/docs/stable/generated/torc...  \n",
       "\n",
       "[1003 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = 'data/nikhil_onlytextpytorch.json'\n",
    "\n",
    "path = os.path.join(os.getcwd(),file_path)\n",
    "\n",
    "train = pd.read_json(path, orient='index').reset_index()\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "class GetEncodings:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def encode(text, tokenizer, model, max_length=512):\n",
    "        '''Tokenize data and get encoded embeddings from model'''\n",
    "        tk = tokenizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        model_op = model(**tk)\n",
    "        return model_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class SearchSimilar:\n",
    "    def __init__(self, embeddings, shape):\n",
    "\n",
    "        self.embeddings = embeddings.cpu().detach().numpy()\n",
    "        self.index = faiss.index_factory(shape, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "        self.total = self.index.ntotal\n",
    "        \n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def get_n_similar_vectors(self, text, n=2):\n",
    "        '''Find the top n similar vector from text using cosine similarity FAISS'''\n",
    "        text = text.cpu().detach().numpy()\n",
    "        faiss.normalize_L2(text)\n",
    "        distance, index = self.index.search(text, n)\n",
    "        return distance, index\n",
    "    \n",
    "    def get_n_dissimilar_vectors(self, text, n=2):\n",
    "        '''Find the top n dissimilar vector from text using cosine similarity FAISS'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = torch.load('data/context_encodings/file_100.pt')\n",
    "dict2 = torch.load('data/context_encodings/file_200.pt')\n",
    "dict3 = torch.load('data/context_encodings/file_300.pt')\n",
    "dict4 = torch.load('data/context_encodings/file_400.pt')\n",
    "\n",
    "dict1.update(dict2)\n",
    "dict1.update(dict3)\n",
    "dict1.update(dict4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "class TextTokenizer(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data:pd.DataFrame, \n",
    "        tokenizer:BertTokenizer,\n",
    "        column_name:str,\n",
    "        source_max_length: int=24,\n",
    "        context_max_length: int=320,\n",
    "        answer_max_length: int=30\n",
    "        \n",
    "        ):\n",
    "        \n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.question_max_length = question_max_length\n",
    "        self.answer_max_length = answer_max_length\n",
    "        self.context_max_length = context_max_length\n",
    "\n",
    "        self.column_name = column_name\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index:int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        question_encoding = tokenizer(\n",
    "            data_row['question'],\n",
    "            max_length=self.source_max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        context_encoding = tokenizer(\n",
    "            data_row['answer'],\n",
    "            max_length=self.source_max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        similar_context = get_closest_context(data_row['answer'], n=2)\n",
    "        \n",
    "        \n",
    "        answer_encoding = tokenizer(\n",
    "            data_row['answer'],\n",
    "            max_length=self.source_max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        return dict(\n",
    "            input_ids_questions_merged = question_encoding['input_ids'].flatten(),\n",
    "            attention_mask_questions_merged = question_encoding['attention_mask'].flatten(),\n",
    "            input_ids_answers = answer_encoding['input_ids'].flatten(),\n",
    "            attention_mask_answers = answer_encoding['attention_mask'].flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_ver = dict1[1]['model_op']['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = [i['model_op']['pooler_output'] for i in dict1.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = torch.cat(final_op, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([347, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SearchSimilar(final_op, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, i = ss.get_n_similar_vectors(value_ver, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install huggingface BART MOdel\n",
    "from transformers import BartModel, BartConfig\n",
    "\n",
    "# Initializing a BART facebook/bart-large style configuration\n",
    "configuration = BartConfig()\n",
    "\n",
    "# Initializing a model from the facebook/bart-large style configuration\n",
    "model = BartModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_sample = torch.load('file_100.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BART_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrainBART, self).__init__()\n",
    "        configuration = BartConfig()\n",
    "        self.bart_model = BartModel(configuration)\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "\n",
    "        bart_outputs = self.bart_model(source, target)        \n",
    "        \n",
    "        return question_outputs\n",
    "    \n",
    "    \n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_bart import shift_tokens_right\n",
    "\n",
    "dataset = ... # some Datasets object with train/validation split and columns 'text' and 'summary'\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['text'], pad_to_max_length=True, max_length=1024, truncation=True))\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['summary'], pad_to_max_length=True, max_length=1024, truncation=True))\n",
    "    \n",
    "    labels = target_encodings['input_ids']\n",
    "    decoder_input_ids = shift_tokens_right(labels, model.config.pad_token_id)\n",
    "    labels[labels[:, :] == model.config.pad_token_id] = -100\n",
    "    \n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'decoder_input_ids': decoder_input_ids,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "dataset = dataset.map(convert_to_features, batched=True)\n",
    "columns = ['input_ids', 'labels', 'decoder_input_ids','attention_mask',] \n",
    "dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bart-summarizer',          \n",
    "    num_train_epochs=1,           \n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,   \n",
    "    warmup_steps=500,               \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',          \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                       \n",
    "    args=training_args,                  \n",
    "    train_dataset=dataset['train'],        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleTokens().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_encoding, answer_encoding = model(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "criterion = torch.nn.CosineSimilarity()\n",
    "\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon=1e-8\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_train_epochs = 10\n",
    "fraction_per_epoch = 0.2\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\" % len(dataset))\n",
    "print(\"  Num Epochs = %d\" % num_train_epochs)\n",
    "print(\"  Batch size = %d\" % BATCH_SIZE)\n",
    "print(\"  Total optimization steps = %d\" % (len(train_loader) // (num_train_epochs * fraction_per_epoch)))\n",
    "\n",
    "model.zero_grad()\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "train_loss_set = []\n",
    "loss = 0\n",
    "for _ in train_iterator:\n",
    "    print(loss)\n",
    "    epoch_iterator = tqdm(iter(train_loader), desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "    #   if step < global_step + 1:\n",
    "    #     continue\n",
    "\n",
    "      model.train()\n",
    "    #   batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      question_encoding, answer_encoding = model(batch)\n",
    "      loss = 1. - criterion(question_encoding[0], answer_encoding[0])\n",
    "      train_loss_set.append(loss)\n",
    "      print(f'####$$$$$$$%%%%%% Loss  {loss.mean()}')\n",
    "    #   print(loss, loss.shape)\n",
    "      loss = loss.mean().backward()\n",
    "      \n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    #   tr_loss += loss.item()\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "    #   global_step += 1\n",
    "    \n",
    "    #   if global_step % 1000 == 0:\n",
    "    #     print(\"Train loss: {}\".format(tr_loss/global_step))\n",
    "    #     output_dir = 'checkpoint-{}'.format(global_step)\n",
    "    #     if not os.path.exists(output_dir):\n",
    "    #         os.makedirs(output_dir)\n",
    "    #     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    #     model_to_save.save_pretrained(output_dir)\n",
    "    #     torch.save(torch.tensor(train_loss_set), os.path.join(output_dir, 'training_loss.pt'))\n",
    "    #     print(\"Saving model checkpoint to %s\" % output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BARTmodel.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d964e81d6599016aee694e721ea343ca3a42c4f371ca966a53ff9f7507121df1"
  },
  "kernelspec": {
   "display_name": "caps",
   "language": "python",
   "name": "caps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
