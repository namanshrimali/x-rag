{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# # verify GPU availability\n",
    "# import tensorflow as tf\n",
    "\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6r9mPtyPp_V",
    "outputId": "73cc4185-6438-4241-dd10-bd1ec82e0852"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# specify GPU device\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a1M_Vmb03TxG",
    "outputId": "f5774441-864e-4e58-92ea-327dd3bbdb86"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "pl.seed_everything(42)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# file_path = 'data/nikhil_onlytextpytorch.json'\n",
    "\n",
    "# path = os.path.join(os.getcwd(),file_path)\n",
    "\n",
    "# train = pd.read_json(path, orient='index').reset_index()\n",
    "# display(train)"
   ],
   "outputs": [],
   "metadata": {
    "id": "97C_hk2nBiar"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "# class GetEncodings:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def encode(text, tokenizer, model, max_length=512):\n",
    "#         '''Tokenize data and get encoded embeddings from model'''\n",
    "#         tk = tokenizer(\n",
    "#                 text,\n",
    "#                 max_length=max_length,\n",
    "#                 padding='max_length',\n",
    "#                 return_attention_mask=True,\n",
    "#                 add_special_tokens=True,\n",
    "#                 truncation=True,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#         model_op = model(**tk)\n",
    "#         return model_op"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import faiss\n",
    "\n",
    "class SearchSimilar:\n",
    "    def __init__(self, embeddings, shape):\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.index = faiss.index_factory(shape, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "        self.total = self.index.ntotal\n",
    "        \n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def get_n_similar_vectors(self, text, n=2):\n",
    "        '''Find the top n similar vector from text using cosine similarity FAISS'''\n",
    "        text = text.cpu().detach().numpy()\n",
    "        faiss.normalize_L2(text)\n",
    "        distance, index = self.index.search(text, n)\n",
    "        return distance, index\n",
    "    \n",
    "    def get_n_dissimilar_vectors(self, text, n=2):\n",
    "        '''Find the top n dissimilar vector from text using cosine similarity FAISS'''\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# df_context = pd.read_csv('context_groups.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# df_merged = pd.read_csv('merged_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "# class GetEncodings:\n",
    "#     def __init__(self):\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "#     def encode(self, text, max_length=100):\n",
    "#         '''Tokenize data and get encoded embeddings from model'''\n",
    "#         tk = self.tokenizer(\n",
    "#                 text,\n",
    "#                 max_length=max_length,\n",
    "#                 padding='max_length',\n",
    "#                 return_attention_mask=True,\n",
    "#                 add_special_tokens=True,\n",
    "#                 truncation=True,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#         model_op = self.model(**tk)\n",
    "#         return model_op"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# import torch\n",
    "# torch.save(model_op['pooler_output'], 'encoded_context.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# \n",
    "# ge = GetEncodings()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# model_op = ge.encode(list(df_context['context'].values))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# model_op['pooler_output']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# import torch\n",
    "# torch.save(model_op['pooler_output'], 'encoded_context.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# model_op = torch.load('encoded_context.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "# torch.save(model_op['pooler_output'].cpu().detach().numpy(), 'encoded_context.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\n",
    "model_op = torch.load('encoded_context.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model_op.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(348, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "ss = SearchSimilar(model_op, 768)\n",
    "\n",
    "d, i = ss.get_n_similar_vectors(value_ver, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dict1 = torch.load('data/context_encodings/file_400.pt')\n",
    "# dict2 = torch.load('data/context_encodings/file_200.pt')\n",
    "# dict3 = torch.load('data/context_encodings/file_300.pt')\n",
    "# dict4 = torch.load('data/context_encodings/file_400.pt')\n",
    "\n",
    "# dict1.update(dict2)\n",
    "# dict1.update(dict3)\n",
    "# dict1.update(dict4)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# import torch\n",
    "\n",
    "# class TextTokenizer(torch.utils.data.Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data:pd.DataFrame, \n",
    "#         tokenizer:BertTokenizer,\n",
    "#         column_name:str,\n",
    "#         source_max_length: int=24,\n",
    "#         context_max_length: int=320,\n",
    "#         answer_max_length: int=30\n",
    "        \n",
    "#         ):\n",
    "        \n",
    "        \n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.data = data\n",
    "#         self.question_max_length = question_max_length\n",
    "#         self.answer_max_length = answer_max_length\n",
    "#         self.context_max_length = context_max_length\n",
    "\n",
    "#         self.column_name = column_name\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index:int):\n",
    "#         data_row = self.data.iloc[index]\n",
    "#         question_encoding = tokenizer(\n",
    "#             data_row['question'],\n",
    "#             max_length=self.source_max_length,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             add_special_tokens=True,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\")\n",
    "        \n",
    "#         context_encoding = tokenizer(\n",
    "#             data_row['answer'],\n",
    "#             max_length=self.source_max_length,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             add_special_tokens=True,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\")\n",
    "        \n",
    "#         similar_context = get_closest_context(data_row['answer'], n=2)\n",
    "        \n",
    "        \n",
    "#         answer_encoding = tokenizer(\n",
    "#             data_row['answer'],\n",
    "#             max_length=self.source_max_length,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             add_special_tokens=True,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#         return dict(\n",
    "#             input_ids_questions_merged = question_encoding['input_ids'].flatten(),\n",
    "#             attention_mask_questions_merged = question_encoding['attention_mask'].flatten(),\n",
    "#             input_ids_answers = answer_encoding['input_ids'].flatten(),\n",
    "#             attention_mask_answers = answer_encoding['attention_mask'].flatten()\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "value_ver = dict1[1]['model_op']['pooler_output']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_op = [i['model_op']['pooler_output'] for i in dict1.values()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_op = torch.cat(final_op, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "final_op = final_op.cpu().detach().numpy()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "pd.read_csv('context_groups.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shape = 768\n",
    "\n",
    "index = faiss.index_factory(shape, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "total = index.ntotal\n",
    "\n",
    "faiss.normalize_L2(final_op)\n",
    "self.index.add(final_op)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gc.collect()\n",
    "print('asfasdfsd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ss = SearchSimilar(final_op, 768)\n",
    "\n",
    "\n",
    "d, i = ss.get_n_similar_vectors(value_ver, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# install huggingface BART MOdel\n",
    "from transformers import BartModel, BartConfig\n",
    "\n",
    "# Initializing a BART facebook/bart-large style configuration\n",
    "configuration = BartConfig()\n",
    "\n",
    "# Initializing a model from the facebook/bart-large style configuration\n",
    "model = BartModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i[0].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "data_sample = torch.load('file_100.pt')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class BART_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrainBART, self).__init__()\n",
    "        configuration = BartConfig()\n",
    "        self.bart_model = BartModel(configuration)\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "\n",
    "        bart_outputs = self.bart_model(source, target)        \n",
    "        \n",
    "        return question_outputs\n",
    "    \n",
    "    \n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_bart import shift_tokens_right\n",
    "\n",
    "dataset = ... # some Datasets object with train/validation split and columns 'text' and 'summary'\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['text'], pad_to_max_length=True, max_length=1024, truncation=True))\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['summary'], pad_to_max_length=True, max_length=1024, truncation=True))\n",
    "    \n",
    "    labels = target_encodings['input_ids']\n",
    "    decoder_input_ids = shift_tokens_right(labels, model.config.pad_token_id)\n",
    "    labels[labels[:, :] == model.config.pad_token_id] = -100\n",
    "    \n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'decoder_input_ids': decoder_input_ids,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "dataset = dataset.map(convert_to_features, batched=True)\n",
    "columns = ['input_ids', 'labels', 'decoder_input_ids','attention_mask',] \n",
    "dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bart-summarizer',          \n",
    "    num_train_epochs=1,           \n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,   \n",
    "    warmup_steps=500,               \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',          \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                       \n",
    "    args=training_args,                  \n",
    "    train_dataset=dataset['train'],        \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = EnsembleTokens().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "question_encoding, answer_encoding = model(next(iter(train_loader)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(model.children())[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "criterion = torch.nn.CosineSimilarity()\n",
    "\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon=1e-8\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "num_train_epochs = 10\n",
    "fraction_per_epoch = 0.2\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\" % len(dataset))\n",
    "print(\"  Num Epochs = %d\" % num_train_epochs)\n",
    "print(\"  Batch size = %d\" % BATCH_SIZE)\n",
    "print(\"  Total optimization steps = %d\" % (len(train_loader) // (num_train_epochs * fraction_per_epoch)))\n",
    "\n",
    "model.zero_grad()\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "train_loss_set = []\n",
    "loss = 0\n",
    "for _ in train_iterator:\n",
    "    print(loss)\n",
    "    epoch_iterator = tqdm(iter(train_loader), desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "    #   if step < global_step + 1:\n",
    "    #     continue\n",
    "\n",
    "      model.train()\n",
    "    #   batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      question_encoding, answer_encoding = model(batch)\n",
    "      loss = 1. - criterion(question_encoding[0], answer_encoding[0])\n",
    "      train_loss_set.append(loss)\n",
    "      print(f'####$$$$$$$%%%%%% Loss  {loss.mean()}')\n",
    "    #   print(loss, loss.shape)\n",
    "      loss = loss.mean().backward()\n",
    "      \n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    #   tr_loss += loss.item()\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "    #   global_step += 1\n",
    "    \n",
    "    #   if global_step % 1000 == 0:\n",
    "    #     print(\"Train loss: {}\".format(tr_loss/global_step))\n",
    "    #     output_dir = 'checkpoint-{}'.format(global_step)\n",
    "    #     if not os.path.exists(output_dir):\n",
    "    #         os.makedirs(output_dir)\n",
    "    #     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    #     model_to_save.save_pretrained(output_dir)\n",
    "    #     torch.save(torch.tensor(train_loss_set), os.path.join(output_dir, 'training_loss.pt'))\n",
    "    #     print(\"Saving model checkpoint to %s\" % output_dir)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BARTmodel.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d964e81d6599016aee694e721ea343ca3a42c4f371ca966a53ff9f7507121df1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('caps': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}